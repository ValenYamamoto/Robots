epoch: 0, step: 0
	action: tensor([[-0.2437,  0.0097, -0.6718, -0.0397,  0.5068, -0.5371,  0.0905]],
       dtype=torch.float64)
	q_value: tensor([[-25.4870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03394486878144265, distance: 1.1247542660810592 entropy 0.9217694997787476
epoch: 0, step: 1
	action: tensor([[-0.8278,  0.3994,  0.4714, -0.2171,  0.4870,  0.2565, -0.4830]],
       dtype=torch.float64)
	q_value: tensor([[-18.0178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4099822948851599, distance: 1.3588249823931735 entropy 0.9217694997787476
epoch: 0, step: 2
	action: tensor([[ 0.9252,  0.3784,  0.0726, -0.4440,  0.0733, -0.7493, -0.3329]],
       dtype=torch.float64)
	q_value: tensor([[-17.2397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6901316705128893, distance: 0.6370085898548131 entropy 0.9217694997787476
epoch: 0, step: 3
	action: tensor([[ 0.8865,  0.3207,  0.7328,  0.2386,  0.0255,  0.1401, -0.3469]],
       dtype=torch.float64)
	q_value: tensor([[-23.6189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9542532182637878, distance: 0.2447580400326826 entropy 0.9217694997787476
epoch: 0, step: 4
	action: tensor([[ 0.2456, -0.4502,  1.0619, -0.8945, -0.3760, -0.4232, -0.4262]],
       dtype=torch.float64)
	q_value: tensor([[-23.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4533137592610985, distance: 1.379546634020416 entropy 0.9217694997787476
epoch: 0, step: 5
	action: tensor([[ 0.9504,  0.5578,  0.2390,  0.6065,  0.3042, -0.3170, -0.1627]],
       dtype=torch.float64)
	q_value: tensor([[-26.4307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9499739962416246, distance: 0.25594968474855767 entropy 0.9217694997787476
epoch: 0, step: 6
	action: tensor([[-0.3805,  0.0265,  0.8083, -0.2780,  0.1645, -0.2570, -0.3072]],
       dtype=torch.float64)
	q_value: tensor([[-23.0079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.389479490738321, distance: 1.348909345864197 entropy 0.9217694997787476
epoch: 0, step: 7
	action: tensor([[ 0.2671, -0.0958, -0.5527,  0.4342, -0.5510, -0.9386,  0.5841]],
       dtype=torch.float64)
	q_value: tensor([[-19.1551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6239049602090143, distance: 0.7017872861564026 entropy 0.9217694997787476
epoch: 0, step: 8
	action: tensor([[ 0.5576,  0.7795,  0.5342, -0.6464, -0.2986, -0.4019,  0.6841]],
       dtype=torch.float64)
	q_value: tensor([[-22.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8073368674881185, distance: 0.5022917000581754 entropy 0.9217694997787476
epoch: 0, step: 9
	action: tensor([[-0.0887,  0.2769, -0.2957,  1.1875,  0.0105,  0.9318, -0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-22.1749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5045363892734478, distance: 0.8054944917504477 entropy 0.9217694997787476
epoch: 0, step: 10
	action: tensor([[ 0.7495,  0.9482, -0.7308, -0.1277, -1.0252,  0.7510, -0.2943]],
       dtype=torch.float64)
	q_value: tensor([[-19.8353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9268956205586065, distance: 0.3094057573658952 entropy 0.9217694997787476
epoch: 0, step: 11
	action: tensor([[ 0.2197,  0.5133, -0.5435,  0.0837,  0.1448, -0.9578,  0.7366]],
       dtype=torch.float64)
	q_value: tensor([[-24.6039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7919210954447535, distance: 0.5220002604373426 entropy 0.9217694997787476
epoch: 0, step: 12
	action: tensor([[-0.5048, -1.2774,  0.2607,  0.8618, -0.9692,  0.6757,  0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-21.0379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.389113181335893, distance: 1.348731527209738 entropy 0.9217694997787476
epoch: 0, step: 13
	action: tensor([[ 0.1125,  0.6300, -0.4486,  0.2344,  0.1174,  1.1024,  0.5635]],
       dtype=torch.float64)
	q_value: tensor([[-26.9066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6508063447678698, distance: 0.6762228866581672 entropy 0.9217694997787476
epoch: 0, step: 14
	action: tensor([[-0.0793,  1.7250,  0.6531,  0.0073,  0.8392, -0.9629,  0.9946]],
       dtype=torch.float64)
	q_value: tensor([[-19.0336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 15
	action: tensor([[ 0.5673,  0.1051, -0.2430,  0.2959,  0.5262,  0.3566,  0.5955]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9133185903297719, distance: 0.3369144795410354 entropy 0.9217694997787476
epoch: 0, step: 16
	action: tensor([[-0.1659,  0.5132, -0.5278, -0.1186,  0.2065,  1.0422,  0.6606]],
       dtype=torch.float64)
	q_value: tensor([[-19.5793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5138114151203921, distance: 0.7979194880506125 entropy 0.9217694997787476
epoch: 0, step: 17
	action: tensor([[ 0.1451, -0.0539, -0.1797,  0.3925, -0.4982, -1.0589, -0.5255]],
       dtype=torch.float64)
	q_value: tensor([[-18.9820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5571008995820287, distance: 0.7615687102589038 entropy 0.9217694997787476
epoch: 0, step: 18
	action: tensor([[-0.1943,  0.9539, -0.3710,  0.9480,  0.7501,  0.4448, -0.8127]],
       dtype=torch.float64)
	q_value: tensor([[-23.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 19
	action: tensor([[ 0.0879, -0.1308, -0.5832,  0.1511, -0.3209,  0.3993, -0.9899]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30783843202827166, distance: 0.9520514834242876 entropy 0.9217694997787476
epoch: 0, step: 20
	action: tensor([[ 0.5174, -0.9827,  0.9612,  0.4912,  0.7860, -0.3773,  1.6449]],
       dtype=torch.float64)
	q_value: tensor([[-17.6762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04959620116678587, distance: 1.1723784269914272 entropy 0.9217694997787476
epoch: 0, step: 21
	action: tensor([[ 0.5093,  0.3167, -0.0152,  0.1558, -0.0449, -0.1451,  0.3598]],
       dtype=torch.float64)
	q_value: tensor([[-40.2280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8820237751601894, distance: 0.393055557038403 entropy 0.9217694997787476
epoch: 0, step: 22
	action: tensor([[-0.0132,  0.5918,  0.5583,  0.4664,  0.0077,  1.0143, -0.2669]],
       dtype=torch.float64)
	q_value: tensor([[-16.2727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8561558874772583, distance: 0.43401300068347176 entropy 0.9217694997787476
epoch: 0, step: 23
	action: tensor([[-0.2866, -0.3021,  0.2420, -0.0505, -0.7523,  0.1544, -1.0914]],
       dtype=torch.float64)
	q_value: tensor([[-19.7274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26519225329732143, distance: 1.2871671730651328 entropy 0.9217694997787476
epoch: 0, step: 24
	action: tensor([[-0.7505, -0.5359, -0.0202,  0.2711,  0.8170, -0.1504,  0.4130]],
       dtype=torch.float64)
	q_value: tensor([[-20.8853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9443107483588828, distance: 1.5956569632836117 entropy 0.9217694997787476
epoch: 0, step: 25
	action: tensor([[ 0.5207,  0.0907, -0.5937, -0.6211, -0.7153, -0.4464,  0.2851]],
       dtype=torch.float64)
	q_value: tensor([[-23.4841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5454823721450961, distance: 0.7714931226626651 entropy 0.9217694997787476
epoch: 0, step: 26
	action: tensor([[ 1.2190, -0.2319,  0.2456,  0.2425,  0.6278, -0.5774,  0.1204]],
       dtype=torch.float64)
	q_value: tensor([[-20.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5181621161482166, distance: 0.7943413390927211 entropy 0.9217694997787476
epoch: 0, step: 27
	action: tensor([[-0.0211, -1.2532, -0.8028,  0.0663, -0.2763, -0.1244,  0.4821]],
       dtype=torch.float64)
	q_value: tensor([[-29.1015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.802698758865217, distance: 1.5364494387649263 entropy 0.9217694997787476
epoch: 0, step: 28
	action: tensor([[-0.7621, -0.0508, -0.7380,  0.4928,  0.2772,  0.2590, -0.2109]],
       dtype=torch.float64)
	q_value: tensor([[-23.1162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5832801908017942, distance: 1.4399107774642808 entropy 0.9217694997787476
epoch: 0, step: 29
	action: tensor([[-0.2313,  1.1904,  0.9106, -0.8401,  0.5272, -0.6294,  0.2372]],
       dtype=torch.float64)
	q_value: tensor([[-17.3766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06539110531366998, distance: 1.1811667869214635 entropy 0.9217694997787476
epoch: 0, step: 30
	action: tensor([[ 0.0027, -0.4427, -0.7185, -0.0873,  0.7694,  0.2054,  0.2903]],
       dtype=torch.float64)
	q_value: tensor([[-22.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013912389906774791, distance: 1.1522770400239606 entropy 0.9217694997787476
epoch: 0, step: 31
	action: tensor([[-0.0982, -0.0604, -0.4195, -0.2920, -0.8702,  1.0742,  0.2961]],
       dtype=torch.float64)
	q_value: tensor([[-19.7068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0381805015185851, distance: 1.1222858358324856 entropy 0.9217694997787476
epoch: 0, step: 32
	action: tensor([[-0.2341,  0.3976,  0.8057,  0.0897,  0.5914, -0.1697, -0.2019]],
       dtype=torch.float64)
	q_value: tensor([[-20.3967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33337205325670427, distance: 0.9343260368389182 entropy 0.9217694997787476
epoch: 0, step: 33
	action: tensor([[-0.3039,  0.2131,  0.9205, -0.1020,  0.9644, -0.1476,  0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-19.3208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04425860948348426, distance: 1.1187341375159034 entropy 0.9217694997787476
epoch: 0, step: 34
	action: tensor([[ 0.1318,  0.2846,  0.6785,  0.2623, -1.3381,  0.7748,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-21.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7427620709528238, distance: 0.5803957124760573 entropy 0.9217694997787476
epoch: 0, step: 35
	action: tensor([[ 0.5942,  0.0984,  0.4894, -1.2144,  0.0077, -0.9476,  0.4796]],
       dtype=torch.float64)
	q_value: tensor([[-25.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0691167610221669, distance: 1.10408954803673 entropy 0.9217694997787476
epoch: 0, step: 36
	action: tensor([[-0.0187,  0.3495,  0.4099,  0.6797,  0.2444,  0.5470,  0.4237]],
       dtype=torch.float64)
	q_value: tensor([[-26.6535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8814671882030568, distance: 0.39398164268165065 entropy 0.9217694997787476
epoch: 0, step: 37
	action: tensor([[-0.2524, -0.5693, -0.5146, -0.0027,  0.4430,  0.8589, -1.1310]],
       dtype=torch.float64)
	q_value: tensor([[-18.4280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08417201871422364, distance: 1.1915322199076026 entropy 0.9217694997787476
epoch: 0, step: 38
	action: tensor([[ 0.1124,  0.5770, -0.4366,  0.9403, -0.3826, -0.1351,  0.8962]],
       dtype=torch.float64)
	q_value: tensor([[-20.8705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 39
	action: tensor([[ 0.3613, -0.2676,  0.1234, -0.3007, -0.8799,  0.1944, -0.4418]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21972303971114782, distance: 1.0108368948445914 entropy 0.9217694997787476
epoch: 0, step: 40
	action: tensor([[ 0.1830,  0.1794, -0.3535, -1.1241, -0.8208, -0.1813, -0.1840]],
       dtype=torch.float64)
	q_value: tensor([[-20.4497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3164641766643166, distance: 0.9461006338868992 entropy 0.9217694997787476
epoch: 0, step: 41
	action: tensor([[ 0.3609, -0.1507, -1.4190,  0.6259, -0.1885,  0.2768,  1.0851]],
       dtype=torch.float64)
	q_value: tensor([[-21.8795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2576285984313883, distance: 0.9859781933474694 entropy 0.9217694997787476
epoch: 0, step: 42
	action: tensor([[ 0.6156,  0.5563, -0.6088, -0.2652, -1.4481, -0.3293,  0.3618]],
       dtype=torch.float64)
	q_value: tensor([[-23.3923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9695792494120768, distance: 0.1995913225736804 entropy 0.9217694997787476
epoch: 0, step: 43
	action: tensor([[-0.1551, -0.1669,  0.5729,  0.1351, -0.2065,  0.4591,  0.5474]],
       dtype=torch.float64)
	q_value: tensor([[-24.3326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28058459270701586, distance: 0.9706140208912345 entropy 0.9217694997787476
epoch: 0, step: 44
	action: tensor([[ 0.1478,  0.6319,  0.1864,  0.1369, -0.6293, -0.9524,  0.7243]],
       dtype=torch.float64)
	q_value: tensor([[-18.8805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7930853090234292, distance: 0.5205379011561839 entropy 0.9217694997787476
epoch: 0, step: 45
	action: tensor([[ 0.2278,  1.3388, -1.0471, -0.9827, -0.2623, -0.6499,  0.4639]],
       dtype=torch.float64)
	q_value: tensor([[-21.5722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9743382016515518, distance: 0.18331594300799506 entropy 0.9217694997787476
epoch: 0, step: 46
	action: tensor([[-0.1361, -0.5278,  0.4441,  0.8264,  0.1008,  0.5396,  1.0105]],
       dtype=torch.float64)
	q_value: tensor([[-24.1618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5676604094621284, distance: 0.7524353601740892 entropy 0.9217694997787476
epoch: 0, step: 47
	action: tensor([[-0.5645,  0.1411,  0.2740, -0.6653,  0.5434, -0.0054,  1.1747]],
       dtype=torch.float64)
	q_value: tensor([[-25.6688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6156940569531499, distance: 1.4545754622275484 entropy 0.9217694997787476
epoch: 0, step: 48
	action: tensor([[ 0.4140,  0.7482, -0.4752, -0.4697, -0.3248,  0.6013,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-23.2160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.89568963597356, distance: 0.3695901802313275 entropy 0.9217694997787476
epoch: 0, step: 49
	action: tensor([[-1.1004, -0.2504, -0.3954,  0.3147,  0.8366, -1.3574,  1.0889]],
       dtype=torch.float64)
	q_value: tensor([[-18.9257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1825552775540977, distance: 1.6905939840556712 entropy 0.9217694997787476
epoch: 0, step: 50
	action: tensor([[ 0.1713, -0.5442, -0.2030,  0.3118, -0.2324, -0.3610,  0.1205]],
       dtype=torch.float64)
	q_value: tensor([[-33.7019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04430499900285456, distance: 1.1187069867776285 entropy 0.9217694997787476
epoch: 0, step: 51
	action: tensor([[ 1.0098, -0.4542,  1.2640,  1.3686, -0.6657, -0.2956,  0.4741]],
       dtype=torch.float64)
	q_value: tensor([[-19.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.320639389616699, distance: 0.9432066946477284 entropy 0.9217694997787476
epoch: 0, step: 52
	action: tensor([[-0.3472,  0.9831, -0.5521,  0.8293, -0.7166, -0.0703,  1.0652]],
       dtype=torch.float64)
	q_value: tensor([[-37.5317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 53
	action: tensor([[ 0.1914, -0.4476, -0.7131, -0.6430,  0.1417, -0.5451,  0.2896]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012616890054104601, distance: 1.1515406589057646 entropy 0.9217694997787476
epoch: 0, step: 54
	action: tensor([[ 0.5111, -0.1044, -1.9637, -0.8189, -0.1264, -0.0592, -0.8438]],
       dtype=torch.float64)
	q_value: tensor([[-19.6078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8367686828624302, distance: 0.4623367712733648 entropy 0.9217694997787476
epoch: 0, step: 55
	action: tensor([[ 1.1726,  0.9884,  0.4081, -0.6828, -0.5744,  0.4402, -0.5604]],
       dtype=torch.float64)
	q_value: tensor([[-25.5581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9887427424257308, distance: 0.12141518190587544 entropy 0.9217694997787476
epoch: 0, step: 56
	action: tensor([[-0.3023,  0.0100, -0.3794,  0.7156,  0.6722, -0.8043,  0.3371]],
       dtype=torch.float64)
	q_value: tensor([[-28.9611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07181609612974338, distance: 1.1847230295443478 entropy 0.9217694997787476
epoch: 0, step: 57
	action: tensor([[ 0.1205, -0.9676, -0.2872,  0.0426, -0.4739,  0.2187,  0.2590]],
       dtype=torch.float64)
	q_value: tensor([[-24.1127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4955402888674161, distance: 1.3994447326534036 entropy 0.9217694997787476
epoch: 0, step: 58
	action: tensor([[-0.6208,  0.3670, -1.1033, -0.2187, -0.5291,  1.3760,  0.0601]],
       dtype=torch.float64)
	q_value: tensor([[-19.9237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30414119131986883, distance: 1.3068297102675748 entropy 0.9217694997787476
epoch: 0, step: 59
	action: tensor([[-0.7305,  0.7612,  0.3171, -0.4204, -0.3216, -0.3462, -0.4542]],
       dtype=torch.float64)
	q_value: tensor([[-21.3370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3070888862326011, distance: 1.3083057625027361 entropy 0.9217694997787476
epoch: 0, step: 60
	action: tensor([[-0.8560, -0.8413, -0.4887,  0.4283, -0.2020, -0.1781, -0.7152]],
       dtype=torch.float64)
	q_value: tensor([[-17.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2150665689060438, distance: 1.70313896314953 entropy 0.9217694997787476
epoch: 0, step: 61
	action: tensor([[ 0.7924,  0.2205, -0.4354,  0.6152,  0.1373,  0.0902,  0.6999]],
       dtype=torch.float64)
	q_value: tensor([[-21.8056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9621156280824105, distance: 0.22273411413745925 entropy 0.9217694997787476
epoch: 0, step: 62
	action: tensor([[ 0.5929,  0.5896, -0.6055,  0.2439, -0.1959,  0.6395,  0.3703]],
       dtype=torch.float64)
	q_value: tensor([[-20.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8548200657757222, distance: 0.4360235945853571 entropy 0.9217694997787476
epoch: 0, step: 63
	action: tensor([[ 0.1531,  0.0149,  0.4985, -0.6426, -0.5822,  0.4300, -0.1908]],
       dtype=torch.float64)
	q_value: tensor([[-17.7452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16100544226220226, distance: 1.0481809565401279 entropy 0.9217694997787476
epoch: 0, step: 64
	action: tensor([[ 0.5916,  0.3171,  0.1048, -0.0175,  0.3598, -0.3955, -0.5375]],
       dtype=torch.float64)
	q_value: tensor([[-20.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8124077957006459, distance: 0.49563741879385154 entropy 0.9217694997787476
epoch: 0, step: 65
	action: tensor([[ 0.6500, -0.3515, -0.5871, -0.4644,  0.2096, -0.3335,  0.5199]],
       dtype=torch.float64)
	q_value: tensor([[-19.8694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09035173847328093, distance: 1.0914238509034853 entropy 0.9217694997787476
epoch: 0, step: 66
	action: tensor([[ 0.3697,  0.9694, -0.1224,  0.1415, -0.7007,  0.0248,  0.3918]],
       dtype=torch.float64)
	q_value: tensor([[-20.6008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 67
	action: tensor([[ 0.0545,  0.5201, -0.8093,  0.3050,  1.1179, -0.0263, -0.4623]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5439970277594077, distance: 0.7727526978910879 entropy 0.9217694997787476
epoch: 0, step: 68
	action: tensor([[ 0.6550, -0.1466, -0.7089, -1.2993, -0.0840, -0.5317,  0.3149]],
       dtype=torch.float64)
	q_value: tensor([[-21.9614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05311605015875609, distance: 1.1135380735848643 entropy 0.9217694997787476
epoch: 0, step: 69
	action: tensor([[ 0.5884, -0.1931,  1.1481,  0.0025, -0.4460,  0.4870,  1.0818]],
       dtype=torch.float64)
	q_value: tensor([[-23.7709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6575220403176969, distance: 0.6696887556727793 entropy 0.9217694997787476
epoch: 0, step: 70
	action: tensor([[ 0.3025, -0.5900, -0.0207,  0.1691, -0.6574,  0.0357,  0.4627]],
       dtype=torch.float64)
	q_value: tensor([[-28.1696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11814519587068595, distance: 1.0746207838872248 entropy 0.9217694997787476
epoch: 0, step: 71
	action: tensor([[-0.3857,  0.1294,  0.3799, -0.5979,  0.3165, -0.3239, -0.6799]],
       dtype=torch.float64)
	q_value: tensor([[-19.5245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4836732270151902, distance: 1.3938814010367144 entropy 0.9217694997787476
epoch: 0, step: 72
	action: tensor([[-0.0318, -0.0951, -0.7846,  0.2631, -0.6891,  0.1163,  0.1572]],
       dtype=torch.float64)
	q_value: tensor([[-17.4398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13065219623501667, distance: 1.0669731087863026 entropy 0.9217694997787476
epoch: 0, step: 73
	action: tensor([[-0.2616,  0.1553, -0.1224, -0.4510, -0.1628, -0.9604, -0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-15.7569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025314300556568137, distance: 1.1297672739644096 entropy 0.9217694997787476
epoch: 0, step: 74
	action: tensor([[-0.5905, -0.4182,  0.2632,  0.6690,  0.1153,  0.2110,  0.6884]],
       dtype=torch.float64)
	q_value: tensor([[-18.6142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13461367810730707, distance: 1.2189354302567883 entropy 0.9217694997787476
epoch: 0, step: 75
	action: tensor([[ 0.4118, -0.0934, -0.1245, -1.1550,  0.8332, -0.0963,  0.9437]],
       dtype=torch.float64)
	q_value: tensor([[-22.0868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03004431310936928, distance: 1.1614075571834448 entropy 0.9217694997787476
epoch: 0, step: 76
	action: tensor([[-0.1169, -0.2482,  0.9692, -0.6460, -0.1960, -0.7541, -0.5810]],
       dtype=torch.float64)
	q_value: tensor([[-24.4002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5781662386801496, distance: 1.437583460251039 entropy 0.9217694997787476
epoch: 0, step: 77
	action: tensor([[ 0.7432,  0.6418, -0.0123, -0.4307, -1.0291,  0.0058, -0.4633]],
       dtype=torch.float64)
	q_value: tensor([[-24.6913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9371395081651057, distance: 0.28690993073822046 entropy 0.9217694997787476
epoch: 0, step: 78
	action: tensor([[ 0.1740,  0.5020, -1.5282, -0.4955,  1.2690,  1.0454,  0.3134]],
       dtype=torch.float64)
	q_value: tensor([[-23.4684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8905459435066866, distance: 0.37859303860863425 entropy 0.9217694997787476
epoch: 0, step: 79
	action: tensor([[ 0.9114,  0.2299, -0.4639, -0.8929, -0.3393, -0.6208, -0.2948]],
       dtype=torch.float64)
	q_value: tensor([[-25.9332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3896277496715428, distance: 0.8940340996675921 entropy 0.9217694997787476
epoch: 0, step: 80
	action: tensor([[ 0.3431, -0.6910, -0.1505, -0.2505,  0.2148, -0.2781,  0.0464]],
       dtype=torch.float64)
	q_value: tensor([[-22.8929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3255365706782407, distance: 1.317505844203951 entropy 0.9217694997787476
epoch: 0, step: 81
	action: tensor([[ 0.8040,  0.4429, -0.4394,  0.6634,  0.0548,  0.3736, -0.4892]],
       dtype=torch.float64)
	q_value: tensor([[-19.9386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.905979378381593, distance: 0.3508877869573389 entropy 0.9217694997787476
epoch: 0, step: 82
	action: tensor([[-0.8979, -0.9288, -0.0025,  0.1745,  0.3730, -0.2775,  0.4496]],
       dtype=torch.float64)
	q_value: tensor([[-20.3031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2893693530187087, distance: 1.731468622266751 entropy 0.9217694997787476
epoch: 0, step: 83
	action: tensor([[ 0.4578,  0.7085,  0.1443,  0.3086, -1.0460, -0.8244, -0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-25.6371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9832220575767188, distance: 0.14822656456545436 entropy 0.9217694997787476
epoch: 0, step: 84
	action: tensor([[-0.1873,  0.1123,  0.1498, -0.0350,  0.2953, -0.6191,  0.7891]],
       dtype=torch.float64)
	q_value: tensor([[-22.6689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009824154753289793, distance: 1.149951623264537 entropy 0.9217694997787476
epoch: 0, step: 85
	action: tensor([[ 0.0458, -0.8090,  0.2999,  0.4639,  0.4301, -0.5175, -0.5819]],
       dtype=torch.float64)
	q_value: tensor([[-20.1514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2378033120474592, distance: 1.2731586161076376 entropy 0.9217694997787476
epoch: 0, step: 86
	action: tensor([[-0.0307,  0.8613, -0.0476, -1.0982,  0.5372, -0.0944,  0.0788]],
       dtype=torch.float64)
	q_value: tensor([[-25.4282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4415371265158733, distance: 0.8551727360121977 entropy 0.9217694997787476
epoch: 0, step: 87
	action: tensor([[-0.4070,  0.9680, -1.3963, -0.7700, -0.1821, -0.2407,  0.3546]],
       dtype=torch.float64)
	q_value: tensor([[-18.8564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8304539288443697, distance: 0.4711948753651011 entropy 0.9217694997787476
epoch: 0, step: 88
	action: tensor([[-0.5139, -0.9636, -1.6272,  0.4667, -0.3995, -0.3717, -0.2752]],
       dtype=torch.float64)
	q_value: tensor([[-21.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1253981151025005, distance: 1.6683103296371653 entropy 0.9217694997787476
epoch: 0, step: 89
	action: tensor([[ 0.4720, -0.3626, -0.4081, -0.8736,  0.5006,  0.0686,  0.3498]],
       dtype=torch.float64)
	q_value: tensor([[-25.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05808306851586931, distance: 1.17710871656592 entropy 0.9217694997787476
epoch: 0, step: 90
	action: tensor([[-0.4454,  1.0208, -0.7692, -0.3304, -0.6331,  0.2472,  0.9099]],
       dtype=torch.float64)
	q_value: tensor([[-19.5327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40687695134121094, distance: 0.8813108031448416 entropy 0.9217694997787476
epoch: 0, step: 91
	action: tensor([[ 0.6037,  0.0294, -0.3478, -0.1365, -1.0096,  0.1481, -0.4483]],
       dtype=torch.float64)
	q_value: tensor([[-20.8671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 92
	action: tensor([[ 1.1699,  0.9367,  0.1233,  0.2324,  0.0949,  0.0724, -0.6295]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 93
	action: tensor([[ 0.2168,  0.0090,  0.9377, -0.3369,  0.0980,  1.7439, -0.5651]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7878810742199817, distance: 0.5270434286930391 entropy 0.9217694997787476
epoch: 0, step: 94
	action: tensor([[-0.1885,  0.4117,  0.2178, -0.4138,  0.3016,  0.3009,  0.1891]],
       dtype=torch.float64)
	q_value: tensor([[-28.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2660098626407944, distance: 0.9803966202705594 entropy 0.9217694997787476
epoch: 0, step: 95
	action: tensor([[ 0.3957,  0.7550,  0.0322, -0.3894,  0.9194,  0.1053, -1.0828]],
       dtype=torch.float64)
	q_value: tensor([[-14.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8672043327240501, distance: 0.4170120898387508 entropy 0.9217694997787476
epoch: 0, step: 96
	action: tensor([[ 0.3540, -1.5770,  0.2376, -0.1167, -0.8827, -1.3977,  0.7238]],
       dtype=torch.float64)
	q_value: tensor([[-21.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5579744121820842, distance: 1.4283572831735212 entropy 0.9217694997787476
epoch: 0, step: 97
	action: tensor([[-1.0058, -0.4396, -0.6222, -0.5072, -0.2814,  0.0622, -0.4635]],
       dtype=torch.float64)
	q_value: tensor([[-35.3602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.062665014387496, distance: 1.6435050560669568 entropy 0.9217694997787476
epoch: 0, step: 98
	action: tensor([[ 0.4856,  0.0224,  1.1806, -0.7823, -0.8320, -0.3769, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[-17.8844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2182923475694194, distance: 1.0117631903506767 entropy 0.9217694997787476
epoch: 0, step: 99
	action: tensor([[-0.0414, -0.2007,  0.8793, -0.1304, -1.2234,  0.2474,  0.9334]],
       dtype=torch.float64)
	q_value: tensor([[-28.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048229649828189114, distance: 1.1164075862846101 entropy 0.9217694997787476
epoch: 0, step: 100
	action: tensor([[ 0.4713,  0.5464,  0.3893, -0.0371,  0.0734, -0.3975,  0.2828]],
       dtype=torch.float64)
	q_value: tensor([[-26.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.853364301716131, distance: 0.4382042134406092 entropy 0.9217694997787476
epoch: 0, step: 101
	action: tensor([[-0.0950, -0.5966,  0.7079, -0.9826,  0.6605, -0.4876,  1.0273]],
       dtype=torch.float64)
	q_value: tensor([[-18.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8710386105651855, distance: 1.5653017403152905 entropy 0.9217694997787476
epoch: 0, step: 102
	action: tensor([[ 0.4359,  0.0969,  0.0204,  0.2166, -0.7514,  0.4625, -0.0651]],
       dtype=torch.float64)
	q_value: tensor([[-29.0770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8199430683955742, distance: 0.4855809223895246 entropy 0.9217694997787476
epoch: 0, step: 103
	action: tensor([[-0.2483,  1.2936, -0.7128,  0.2647,  0.0381, -0.8483,  0.3699]],
       dtype=torch.float64)
	q_value: tensor([[-18.4241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 104
	action: tensor([[-0.0885,  0.3404,  0.9167, -0.9226, -0.3811, -0.4281, -0.5375]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24321408789526755, distance: 1.2759382435545064 entropy 0.9217694997787476
epoch: 0, step: 105
	action: tensor([[-0.8900, -0.0292,  0.2634,  0.3301, -0.3258, -0.3977,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-23.3551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6762748306188477, distance: 1.4815943231989797 entropy 0.9217694997787476
epoch: 0, step: 106
	action: tensor([[ 0.2594, -0.5981, -1.8618, -0.5688, -0.3963,  0.4960, -0.2876]],
       dtype=torch.float64)
	q_value: tensor([[-18.2788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14318340142207175, distance: 1.059255259034891 entropy 0.9217694997787476
epoch: 0, step: 107
	action: tensor([[ 0.2322,  0.4094,  0.1686,  1.0642, -0.7864,  0.3983,  0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-23.8357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7379958060787197, distance: 0.5857480006071475 entropy 0.9217694997787476
epoch: 0, step: 108
	action: tensor([[-0.1992, -0.0447,  1.1478, -0.0564, -0.7293, -0.3623, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-20.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09526330272614425, distance: 1.1976115109457715 entropy 0.9217694997787476
epoch: 0, step: 109
	action: tensor([[ 0.2436,  1.1558,  0.1893, -0.3389, -0.3857,  0.3064,  1.1618]],
       dtype=torch.float64)
	q_value: tensor([[-23.4678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 110
	action: tensor([[-0.3338, -0.4294,  1.5884,  0.3364,  0.0609, -0.2413, -0.1096]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23424298314662728, distance: 1.2713262863791253 entropy 0.9217694997787476
epoch: 0, step: 111
	action: tensor([[-0.4278, -0.8202, -0.2632, -0.5471,  0.1387, -0.3760, -0.6162]],
       dtype=torch.float64)
	q_value: tensor([[-28.8347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8157492600204796, distance: 1.5420009141942772 entropy 0.9217694997787476
epoch: 0, step: 112
	action: tensor([[-0.1181, -0.7820, -0.1093, -0.6096, -0.7432, -0.1812,  0.3260]],
       dtype=torch.float64)
	q_value: tensor([[-20.0779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7086337794390345, distance: 1.4958263773718852 entropy 0.9217694997787476
epoch: 0, step: 113
	action: tensor([[-0.1668,  1.0343,  0.2527, -0.2294, -0.2466,  0.0950,  0.0779]],
       dtype=torch.float64)
	q_value: tensor([[-20.7413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 114
	action: tensor([[ 0.7134, -0.2635, -1.0216, -0.0700,  1.0340, -1.3693, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24253483061488168, distance: 0.9959511260681491 entropy 0.9217694997787476
epoch: 0, step: 115
	action: tensor([[-1.2175,  0.3097, -1.5048,  0.0193, -0.5688,  0.5214, -0.4584]],
       dtype=torch.float64)
	q_value: tensor([[-33.5410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1517546887592798, distance: 1.6786226245696803 entropy 0.9217694997787476
epoch: 0, step: 116
	action: tensor([[ 0.4042, -0.4790,  0.2677, -0.0260, -0.7449,  1.9087, -0.3397]],
       dtype=torch.float64)
	q_value: tensor([[-20.2885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5736011045460112, distance: 0.7472479439970151 entropy 0.9217694997787476
epoch: 0, step: 117
	action: tensor([[ 0.2316, -0.9938,  0.1682,  1.0575,  0.5494, -0.6832, -0.2587]],
       dtype=torch.float64)
	q_value: tensor([[-30.8755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12974398257793862, distance: 1.0675303003881433 entropy 0.9217694997787476
epoch: 0, step: 118
	action: tensor([[-0.5302,  0.6241, -0.0295, -1.4472,  0.4340,  0.0458, -1.1952]],
       dtype=torch.float64)
	q_value: tensor([[-32.2624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20799411530761613, distance: 1.2577348727073272 entropy 0.9217694997787476
epoch: 0, step: 119
	action: tensor([[ 0.8810,  0.1991,  0.3335, -1.2592, -0.5660,  0.3457,  1.0179]],
       dtype=torch.float64)
	q_value: tensor([[-21.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3970123848972691, distance: 0.8886093719938446 entropy 0.9217694997787476
epoch: 0, step: 120
	action: tensor([[ 0.7197,  0.9233,  0.2108,  0.4685, -0.3093, -0.8220, -0.5031]],
       dtype=torch.float64)
	q_value: tensor([[-28.3251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9355506537088178, distance: 0.290513254493546 entropy 0.9217694997787476
epoch: 0, step: 121
	action: tensor([[ 0.1030,  0.2196, -0.6310,  0.0776,  0.0108,  0.3949,  1.2399]],
       dtype=torch.float64)
	q_value: tensor([[-24.0867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5413081055061173, distance: 0.7750277025428136 entropy 0.9217694997787476
epoch: 0, step: 122
	action: tensor([[-0.0940,  0.2203,  1.4810, -0.0790, -1.0040, -0.0163,  0.3640]],
       dtype=torch.float64)
	q_value: tensor([[-20.0307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2807812830401861, distance: 0.9704813274266059 entropy 0.9217694997787476
epoch: 0, step: 123
	action: tensor([[-0.1578,  0.9998,  0.0686, -0.0116, -0.6295, -0.7170, -0.7732]],
       dtype=torch.float64)
	q_value: tensor([[-27.1037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.643678101631883, distance: 0.6830900352640721 entropy 0.9217694997787476
epoch: 0, step: 124
	action: tensor([[-0.7620,  1.2151, -1.5830,  0.0710,  1.3574,  0.0667, -0.1053]],
       dtype=torch.float64)
	q_value: tensor([[-20.8962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 125
	action: tensor([[-0.8950,  0.3192,  0.3592, -0.0728, -0.1644,  0.0589, -0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5952359545148824, distance: 1.4453371246351012 entropy 0.9217694997787476
epoch: 0, step: 126
	action: tensor([[ 0.3356,  1.4216, -0.1344, -0.0741, -0.2933, -0.7476, -0.9564]],
       dtype=torch.float64)
	q_value: tensor([[-16.1133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 127
	action: tensor([[-0.0700, -0.7188, -0.5858, -0.7018, -0.0918, -0.0478,  0.1912]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3866683619116038, distance: 1.3475441303853402 entropy 0.9217694997787476
LOSS epoch 0 actor 242.94912880913762 critic 329.89916719439964 
epoch: 1, step: 0
	action: tensor([[-0.8169, -1.2791,  0.0638, -0.0185, -0.2260, -1.2727,  0.3402]],
       dtype=torch.float64)
	q_value: tensor([[-23.0363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7409043310115175, distance: 1.5098859578293768 entropy 0.9217694997787476
epoch: 1, step: 1
	action: tensor([[ 0.2940, -0.4308, -0.0596, -1.0731, -0.2639,  1.0892, -0.0639]],
       dtype=torch.float64)
	q_value: tensor([[-40.0962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0850479279655314, distance: 1.192013445840418 entropy 0.9217694997787476
epoch: 1, step: 2
	action: tensor([[ 0.1656,  1.2044, -0.6256,  0.2319, -0.7871, -0.5148,  0.1547]],
       dtype=torch.float64)
	q_value: tensor([[-29.2868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 3
	action: tensor([[ 0.2565, -0.1639, -0.2832, -0.1006, -0.0176,  0.4581,  0.4201]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4419219813933051, distance: 0.8548780215960348 entropy 0.9217694997787476
epoch: 1, step: 4
	action: tensor([[-0.0563, -0.3541,  0.1517,  0.0764,  0.0679,  0.3544,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-19.8067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11789772049659464, distance: 1.0747715590057578 entropy 0.9217694997787476
epoch: 1, step: 5
	action: tensor([[ 0.5783,  0.5176,  0.1397,  0.4919, -0.8665, -0.5509, -0.3663]],
       dtype=torch.float64)
	q_value: tensor([[-20.3188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9880579749962531, distance: 0.12505345106872626 entropy 0.9217694997787476
epoch: 1, step: 6
	action: tensor([[ 0.8751,  0.0608, -1.0070,  0.6079, -0.6314, -0.3125, -0.5104]],
       dtype=torch.float64)
	q_value: tensor([[-27.6044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.901898473776269, distance: 0.3584219310064312 entropy 0.9217694997787476
epoch: 1, step: 7
	action: tensor([[-0.0029, -0.1987, -0.5470,  0.0614, -0.4299, -0.1752,  0.3814]],
       dtype=torch.float64)
	q_value: tensor([[-28.9661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19221189390986915, distance: 1.0285026628023437 entropy 0.9217694997787476
epoch: 1, step: 8
	action: tensor([[ 0.9142, -0.0121, -0.6767, -0.4182,  0.1822,  0.4975,  1.1513]],
       dtype=torch.float64)
	q_value: tensor([[-20.6371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6927612680662969, distance: 0.6342999472195501 entropy 0.9217694997787476
epoch: 1, step: 9
	action: tensor([[-0.5713,  0.5604, -0.0176, -1.2495, -0.3015, -0.1914, -0.5029]],
       dtype=torch.float64)
	q_value: tensor([[-29.5839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27289556339609566, distance: 1.2910797803045793 entropy 0.9217694997787476
epoch: 1, step: 10
	action: tensor([[-0.5413,  0.2749, -0.3720,  0.0835,  0.3627,  0.0157,  0.5120]],
       dtype=torch.float64)
	q_value: tensor([[-26.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12063665435994397, distance: 1.2114042835820888 entropy 0.9217694997787476
epoch: 1, step: 11
	action: tensor([[ 0.3150, -0.0546,  0.2300, -0.5063,  0.6809,  0.5319, -0.0365]],
       dtype=torch.float64)
	q_value: tensor([[-20.2649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.404997756422496, distance: 0.8827058299101976 entropy 0.9217694997787476
epoch: 1, step: 12
	action: tensor([[-0.1733, -0.2752, -0.4217,  0.5151,  0.6801, -0.9363, -0.3470]],
       dtype=torch.float64)
	q_value: tensor([[-21.9033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17843111149859792, distance: 1.242249392744649 entropy 0.9217694997787476
epoch: 1, step: 13
	action: tensor([[-0.0816,  0.5229, -0.6303,  0.6229, -0.0636,  0.0481,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[-30.6371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3394252035492853, distance: 0.9300744050416273 entropy 0.9217694997787476
epoch: 1, step: 14
	action: tensor([[ 1.4279, -0.0959, -0.8470,  0.0604, -0.6418,  0.9557, -0.6682]],
       dtype=torch.float64)
	q_value: tensor([[-19.3449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7166769901759731, distance: 0.6091126467441149 entropy 0.9217694997787476
epoch: 1, step: 15
	action: tensor([[-0.7966,  0.5309,  1.0994,  0.0363, -0.2513, -0.4461, -0.1510]],
       dtype=torch.float64)
	q_value: tensor([[-35.1389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41015235209079237, distance: 1.3589069234973843 entropy 0.9217694997787476
epoch: 1, step: 16
	action: tensor([[ 0.3164,  0.5073,  0.1869,  0.2286, -0.1049,  0.6508,  0.4239]],
       dtype=torch.float64)
	q_value: tensor([[-27.5736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8641910489751179, distance: 0.42171678858171424 entropy 0.9217694997787476
epoch: 1, step: 17
	action: tensor([[ 1.3094, -0.4764, -0.6970, -0.5019, -0.0543, -0.8355, -0.2803]],
       dtype=torch.float64)
	q_value: tensor([[-21.5895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3229543758373208, distance: 1.3162219432847622 entropy 0.9217694997787476
epoch: 1, step: 18
	action: tensor([[-0.5555, -0.6142, -0.3708,  0.1118,  0.1370,  1.0260,  0.2121]],
       dtype=torch.float64)
	q_value: tensor([[-35.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4437613909445097, distance: 1.3750054043714912 entropy 0.9217694997787476
epoch: 1, step: 19
	action: tensor([[ 0.2985, -0.7786,  0.6001, -0.2149,  0.1781, -0.8604,  0.4618]],
       dtype=torch.float64)
	q_value: tensor([[-25.5859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5246126839853769, distance: 1.4129814408751094 entropy 0.9217694997787476
epoch: 1, step: 20
	action: tensor([[ 0.9528,  0.2994,  0.7642, -0.2504, -0.3836, -0.7927,  0.8222]],
       dtype=torch.float64)
	q_value: tensor([[-34.7388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7409129929407012, distance: 0.5824779778439042 entropy 0.9217694997787476
epoch: 1, step: 21
	action: tensor([[ 0.0455, -0.1759, -0.0306, -0.1956, -0.2747,  0.0623,  0.0550]],
       dtype=torch.float64)
	q_value: tensor([[-33.5628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1339440576724087, distance: 1.0649510993786626 entropy 0.9217694997787476
epoch: 1, step: 22
	action: tensor([[ 0.0017, -0.7437,  0.4863,  0.2117, -0.1787,  0.3475,  0.6980]],
       dtype=torch.float64)
	q_value: tensor([[-18.5218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04614218050267849, distance: 1.1704478005906245 entropy 0.9217694997787476
epoch: 1, step: 23
	action: tensor([[ 0.0725,  0.0207,  0.4993, -0.7035, -0.1787,  0.1586, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-29.0602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03429339235002615, distance: 1.1638005774857372 entropy 0.9217694997787476
epoch: 1, step: 24
	action: tensor([[-0.0412,  0.8276,  0.8002, -0.8934, -0.2494,  0.4124,  1.4338]],
       dtype=torch.float64)
	q_value: tensor([[-22.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.230608733449442, distance: 1.0037610040531153 entropy 0.9217694997787476
epoch: 1, step: 25
	action: tensor([[ 0.8284,  0.4460,  0.7267, -0.2331,  0.1723, -1.0693,  1.7769]],
       dtype=torch.float64)
	q_value: tensor([[-33.1519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8187658734271799, distance: 0.48716567714039094 entropy 0.9217694997787476
epoch: 1, step: 26
	action: tensor([[ 0.3507, -0.5059, -0.6151, -0.0184, -0.1664, -0.3906,  0.2991]],
       dtype=torch.float64)
	q_value: tensor([[-40.8699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05332046493909204, distance: 1.113417870931198 entropy 0.9217694997787476
epoch: 1, step: 27
	action: tensor([[ 0.7601, -0.1506, -0.1141,  0.1384,  0.8079, -0.2126, -0.1093]],
       dtype=torch.float64)
	q_value: tensor([[-24.1618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6486709805227759, distance: 0.6782873304731657 entropy 0.9217694997787476
epoch: 1, step: 28
	action: tensor([[-0.0531, -1.1040, -0.3865, -0.4692,  0.1487, -0.6026,  0.3789]],
       dtype=torch.float64)
	q_value: tensor([[-28.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7912902112490574, distance: 1.5315799403157344 entropy 0.9217694997787476
epoch: 1, step: 29
	action: tensor([[ 0.4052, -0.0269, -0.1964, -0.3690,  0.4565,  0.2415,  0.2852]],
       dtype=torch.float64)
	q_value: tensor([[-30.4152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5049464779907199, distance: 0.8051610741438967 entropy 0.9217694997787476
epoch: 1, step: 30
	action: tensor([[ 0.3654,  0.2429, -0.5459,  0.2891,  0.4298, -0.5779, -1.3790]],
       dtype=torch.float64)
	q_value: tensor([[-20.6482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7195097378046383, distance: 0.6060599525333795 entropy 0.9217694997787476
epoch: 1, step: 31
	action: tensor([[-0.9036,  0.2775,  0.6278, -0.1737, -0.6930, -1.2209,  0.3720]],
       dtype=torch.float64)
	q_value: tensor([[-31.3487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6322426592998156, distance: 1.4620056646522406 entropy 0.9217694997787476
epoch: 1, step: 32
	action: tensor([[-0.0593, -1.2441,  0.2065, -0.6437, -0.7819, -0.5549, -0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-31.6065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9272139406827107, distance: 1.5886259686924367 entropy 0.9217694997787476
epoch: 1, step: 33
	action: tensor([[ 0.1071, -0.3952,  0.5672,  0.5185,  0.1810,  0.0673,  1.0959]],
       dtype=torch.float64)
	q_value: tensor([[-32.3622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4550954711253429, distance: 0.8447280235958146 entropy 0.9217694997787476
epoch: 1, step: 34
	action: tensor([[ 0.0344,  1.2542,  0.0923,  0.1463, -0.9848,  0.0877,  0.9859]],
       dtype=torch.float64)
	q_value: tensor([[-30.9773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 35
	action: tensor([[ 1.0527,  0.0895,  0.3386,  0.5241,  0.4463, -0.0589, -0.1952]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9135912059149394, distance: 0.33638425934502975 entropy 0.9217694997787476
epoch: 1, step: 36
	action: tensor([[-0.4360,  0.3738, -0.3171, -0.5570, -0.2323,  0.0967, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-30.4173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006776737130099186, distance: 1.140460202416832 entropy 0.9217694997787476
epoch: 1, step: 37
	action: tensor([[ 0.1748, -0.6007, -0.2409,  0.0034, -1.0398, -0.3826, -0.7704]],
       dtype=torch.float64)
	q_value: tensor([[-19.5072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02140586475218975, distance: 1.1565272415333796 entropy 0.9217694997787476
epoch: 1, step: 38
	action: tensor([[-0.8343,  0.4669,  0.2975, -0.3447, -0.2890,  0.3600,  0.0958]],
       dtype=torch.float64)
	q_value: tensor([[-28.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4866645948337016, distance: 1.3952858586770676 entropy 0.9217694997787476
epoch: 1, step: 39
	action: tensor([[ 0.0495, -0.6252,  0.1458,  0.7538,  0.1071, -0.4354,  0.3445]],
       dtype=torch.float64)
	q_value: tensor([[-22.1239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18490773515806425, distance: 1.0331421474845555 entropy 0.9217694997787476
epoch: 1, step: 40
	action: tensor([[ 0.6892,  0.3569, -0.4082,  0.2858,  0.6463, -0.0639,  0.1302]],
       dtype=torch.float64)
	q_value: tensor([[-29.9499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.945690984776616, distance: 0.26668135752357935 entropy 0.9217694997787476
epoch: 1, step: 41
	action: tensor([[ 0.9763, -0.4960,  0.1177, -0.0243,  0.2500,  0.4727, -0.5915]],
       dtype=torch.float64)
	q_value: tensor([[-24.9137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3917698336966384, distance: 0.8924639272355382 entropy 0.9217694997787476
epoch: 1, step: 42
	action: tensor([[ 0.3741,  0.3026, -0.7407, -0.0804, -0.3361, -0.6031,  0.5660]],
       dtype=torch.float64)
	q_value: tensor([[-29.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8229366627149075, distance: 0.48152741278641625 entropy 0.9217694997787476
epoch: 1, step: 43
	action: tensor([[-0.1842, -0.0262,  0.5687,  0.6595, -0.3903,  0.1621, -0.7537]],
       dtype=torch.float64)
	q_value: tensor([[-23.6608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6050678273082996, distance: 0.7191474593810551 entropy 0.9217694997787476
epoch: 1, step: 44
	action: tensor([[-0.2723, -0.0101,  0.5659, -0.6810, -0.7316, -0.2267,  0.9013]],
       dtype=torch.float64)
	q_value: tensor([[-25.8465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4789250392142337, distance: 1.391649199819553 entropy 0.9217694997787476
epoch: 1, step: 45
	action: tensor([[-0.9572, -0.6713,  0.6601,  0.8771,  0.4495,  0.9442, -0.6878]],
       dtype=torch.float64)
	q_value: tensor([[-28.4241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02123973829871728, distance: 1.132126242720638 entropy 0.9217694997787476
epoch: 1, step: 46
	action: tensor([[ 0.2784,  1.2237,  0.0905,  0.3940, -0.5857,  0.5043,  0.3621]],
       dtype=torch.float64)
	q_value: tensor([[-32.4957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 47
	action: tensor([[ 0.1576,  0.5478,  0.5868, -0.2802,  0.2263,  0.5711, -0.1477]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7346860099300575, distance: 0.5894361517173019 entropy 0.9217694997787476
epoch: 1, step: 48
	action: tensor([[ 0.7558, -0.7295,  0.4039,  1.0267,  0.4172,  0.1740, -0.3726]],
       dtype=torch.float64)
	q_value: tensor([[-22.6160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7500056694553248, distance: 0.5721656391545931 entropy 0.9217694997787476
epoch: 1, step: 49
	action: tensor([[ 0.4587,  0.4291, -0.3118,  0.0376, -0.4040, -0.6853,  0.9707]],
       dtype=torch.float64)
	q_value: tensor([[-36.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8795535298617599, distance: 0.3971492362562226 entropy 0.9217694997787476
epoch: 1, step: 50
	action: tensor([[ 0.3639, -0.2771, -0.0914,  0.6620, -0.5620,  0.3294,  0.3797]],
       dtype=torch.float64)
	q_value: tensor([[-26.2925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7332545415102163, distance: 0.5910241271517337 entropy 0.9217694997787476
epoch: 1, step: 51
	action: tensor([[ 0.2857,  0.6401, -0.1551,  0.0119, -1.4379,  0.5321, -1.7177]],
       dtype=torch.float64)
	q_value: tensor([[-23.7439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7049170028446594, distance: 0.6216254494375449 entropy 0.9217694997787476
epoch: 1, step: 52
	action: tensor([[-0.1804,  0.4956,  0.9024, -0.4549, -0.0820,  0.1863, -0.9128]],
       dtype=torch.float64)
	q_value: tensor([[-37.5137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17469565316316626, distance: 1.039593988554901 entropy 0.9217694997787476
epoch: 1, step: 53
	action: tensor([[-0.8687,  0.2486, -0.6608, -0.2190, -0.8267,  1.4010, -0.7645]],
       dtype=torch.float64)
	q_value: tensor([[-27.4166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6344496017749244, distance: 1.4629937139677283 entropy 0.9217694997787476
epoch: 1, step: 54
	action: tensor([[-0.4306, -0.0677,  0.3547,  0.5213, -0.9577, -0.2121,  0.8949]],
       dtype=torch.float64)
	q_value: tensor([[-30.7568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1070134570651603, distance: 1.0813820350589198 entropy 0.9217694997787476
epoch: 1, step: 55
	action: tensor([[ 0.2999, -0.3707, -0.1966, -0.2287, -0.2789, -0.6391, -0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-28.2766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01620973335536524, distance: 1.135031603305234 entropy 0.9217694997787476
epoch: 1, step: 56
	action: tensor([[-0.6512, -0.0041, -0.4457,  0.1285,  0.3230,  1.2124, -0.1727]],
       dtype=torch.float64)
	q_value: tensor([[-24.3957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10364330645028219, distance: 1.2021843224868618 entropy 0.9217694997787476
epoch: 1, step: 57
	action: tensor([[ 0.0637, -0.8267,  0.6853, -0.2334, -0.8327, -0.0055,  0.1414]],
       dtype=torch.float64)
	q_value: tensor([[-24.3919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.621381964516794, distance: 1.4571335647359507 entropy 0.9217694997787476
epoch: 1, step: 58
	action: tensor([[ 0.6885, -0.6639, -0.2098,  0.4420, -0.6141, -0.3410,  0.4966]],
       dtype=torch.float64)
	q_value: tensor([[-29.6825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974317038433627, distance: 0.9591818831867908 entropy 0.9217694997787476
epoch: 1, step: 59
	action: tensor([[ 0.1430,  0.9265, -0.0233,  0.1479,  0.7849, -0.9356,  0.2722]],
       dtype=torch.float64)
	q_value: tensor([[-29.3871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6272985422177293, distance: 0.6986139267859388 entropy 0.9217694997787476
epoch: 1, step: 60
	action: tensor([[-1.2843,  0.4060, -1.0926, -0.3178, -0.4793, -0.5475, -1.3840]],
       dtype=torch.float64)
	q_value: tensor([[-27.7432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5356573208077939, distance: 1.4180901828606804 entropy 0.9217694997787476
epoch: 1, step: 61
	action: tensor([[ 0.6062, -0.4719,  0.1231,  0.8736,  0.4695, -0.0050,  0.0526]],
       dtype=torch.float64)
	q_value: tensor([[-31.5257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8217907891295858, distance: 0.48308301353295274 entropy 0.9217694997787476
epoch: 1, step: 62
	action: tensor([[-1.1839, -0.1540,  0.6918,  0.4066,  0.6004,  0.3715,  0.2650]],
       dtype=torch.float64)
	q_value: tensor([[-30.7437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7153419436545498, distance: 1.498759837914645 entropy 0.9217694997787476
epoch: 1, step: 63
	action: tensor([[ 0.8330,  1.0554, -0.5332,  0.0746, -0.3703, -1.1249, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[-29.8161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9440193104717804, distance: 0.27075458203191904 entropy 0.9217694997787476
epoch: 1, step: 64
	action: tensor([[ 0.4150,  0.4843,  0.3599, -0.3116, -0.2604, -0.2394, -0.4426]],
       dtype=torch.float64)
	q_value: tensor([[-30.5151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7537116732060339, distance: 0.5679088118877526 entropy 0.9217694997787476
epoch: 1, step: 65
	action: tensor([[ 0.1111, -0.1739, -0.2814, -0.4820,  0.0661,  0.0536, -0.4963]],
       dtype=torch.float64)
	q_value: tensor([[-24.1351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11766759022793583, distance: 1.074911747589101 entropy 0.9217694997787476
epoch: 1, step: 66
	action: tensor([[-0.6912,  0.3296,  0.6297,  0.4512, -0.5168, -0.9287, -0.8861]],
       dtype=torch.float64)
	q_value: tensor([[-19.1057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08398067358496308, distance: 1.1914270687216155 entropy 0.9217694997787476
epoch: 1, step: 67
	action: tensor([[-0.6723,  0.2597,  0.0660, -0.5103, -1.5121, -0.0877, -0.2123]],
       dtype=torch.float64)
	q_value: tensor([[-30.7878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5189084284239156, distance: 1.410335667137482 entropy 0.9217694997787476
epoch: 1, step: 68
	action: tensor([[-0.3722,  0.5858, -0.9910,  0.4972,  0.7378, -0.1045,  0.7950]],
       dtype=torch.float64)
	q_value: tensor([[-31.0039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029496219612520802, distance: 1.1273410180180767 entropy 0.9217694997787476
epoch: 1, step: 69
	action: tensor([[ 0.6820,  0.5637,  0.4539,  0.3122, -0.0818, -0.7296, -0.0987]],
       dtype=torch.float64)
	q_value: tensor([[-27.6521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9723443934175033, distance: 0.1903041624335476 entropy 0.9217694997787476
epoch: 1, step: 70
	action: tensor([[ 0.6251,  0.3288,  0.1239,  0.3845,  0.2537, -0.5607,  0.7889]],
       dtype=torch.float64)
	q_value: tensor([[-28.3224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9290241960361414, distance: 0.30486800833074096 entropy 0.9217694997787476
epoch: 1, step: 71
	action: tensor([[-0.8115, -0.9323, -0.6969, -0.3708, -0.2749, -0.2597,  0.2042]],
       dtype=torch.float64)
	q_value: tensor([[-27.8575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9134615263763808, distance: 1.582947679158123 entropy 0.9217694997787476
epoch: 1, step: 72
	action: tensor([[-0.1174,  0.3645,  0.0169, -0.3764, -0.0524, -0.5103, -0.4308]],
       dtype=torch.float64)
	q_value: tensor([[-28.2421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18069950096427378, distance: 1.0358057151910738 entropy 0.9217694997787476
epoch: 1, step: 73
	action: tensor([[ 2.2223,  0.7739, -0.3730,  0.9720, -0.0702, -0.2061, -0.8111]],
       dtype=torch.float64)
	q_value: tensor([[-20.3741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 74
	action: tensor([[-0.4351,  0.5674, -0.1473,  1.0667, -0.8873,  0.5915,  1.0460]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 75
	action: tensor([[ 0.0513,  0.8353, -1.1329, -0.1265, -0.0831,  0.3579, -0.1458]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.649689072841497, distance: 0.6773038362413881 entropy 0.9217694997787476
epoch: 1, step: 76
	action: tensor([[ 0.8741,  0.4330, -0.1026, -0.0270,  0.3646,  0.8706, -0.0888]],
       dtype=torch.float64)
	q_value: tensor([[-23.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08890237812499313 entropy 0.9217694997787476
epoch: 1, step: 77
	action: tensor([[-0.3377, -0.4811,  0.0261,  0.4143,  0.1762, -0.3487, -0.0530]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3202697955961977, distance: 1.3148858058179915 entropy 0.9217694997787476
epoch: 1, step: 78
	action: tensor([[ 0.3605, -1.1121,  0.1864, -1.0547,  0.5101, -0.4776,  0.6678]],
       dtype=torch.float64)
	q_value: tensor([[-24.3000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9658858334785285, distance: 1.60448565942962 entropy 0.9217694997787476
epoch: 1, step: 79
	action: tensor([[ 0.2533,  0.1263, -0.4273, -0.0677, -0.9508,  0.2209, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-35.3153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5491225894546801, distance: 0.7683974799638995 entropy 0.9217694997787476
epoch: 1, step: 80
	action: tensor([[ 0.3220,  0.0391, -0.7483, -0.7027,  1.1876,  0.5748, -0.4259]],
       dtype=torch.float64)
	q_value: tensor([[-23.1691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.66112101693239, distance: 0.6661607043679147 entropy 0.9217694997787476
epoch: 1, step: 81
	action: tensor([[ 0.6915,  0.4294,  0.4549,  0.3482,  0.1742,  0.0599, -0.2596]],
       dtype=torch.float64)
	q_value: tensor([[-27.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9908722057664746, distance: 0.10933006691504513 entropy 0.9217694997787476
epoch: 1, step: 82
	action: tensor([[-0.2402, -0.1124,  0.6973,  0.6875,  0.4862,  0.5206, -0.3505]],
       dtype=torch.float64)
	q_value: tensor([[-24.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6700956594744678, distance: 0.6572804396092502 entropy 0.9217694997787476
epoch: 1, step: 83
	action: tensor([[-0.8085, -0.2319, -0.7592,  0.0854,  0.2174,  0.4416, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-26.9616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8088215299710246, distance: 1.539056461664926 entropy 0.9217694997787476
epoch: 1, step: 84
	action: tensor([[ 0.7944,  0.1770, -0.4646,  0.1312, -0.4831, -0.2512, -0.9802]],
       dtype=torch.float64)
	q_value: tensor([[-22.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8804573370500223, distance: 0.3956563646156494 entropy 0.9217694997787476
epoch: 1, step: 85
	action: tensor([[ 0.4407, -0.5063,  0.2597, -0.6727, -0.9113, -0.2943,  0.6204]],
       dtype=torch.float64)
	q_value: tensor([[-27.6854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.312793594893201, distance: 1.3111576639746008 entropy 0.9217694997787476
epoch: 1, step: 86
	action: tensor([[ 0.5460,  0.7394,  0.1860, -1.3406,  0.6865,  0.1261,  0.4304]],
       dtype=torch.float64)
	q_value: tensor([[-30.2814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6345121963650089, distance: 0.6918200395426923 entropy 0.9217694997787476
epoch: 1, step: 87
	action: tensor([[ 0.3437, -0.0823,  0.7115, -0.1493,  0.4025,  1.8959, -0.1050]],
       dtype=torch.float64)
	q_value: tensor([[-29.0779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8259040893802301, distance: 0.47747537476384433 entropy 0.9217694997787476
epoch: 1, step: 88
	action: tensor([[ 0.3667,  0.0388, -0.5649,  0.5641,  0.9180,  0.2625,  1.6391]],
       dtype=torch.float64)
	q_value: tensor([[-35.5754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7667891337065714, distance: 0.552625708071316 entropy 0.9217694997787476
epoch: 1, step: 89
	action: tensor([[ 0.5681,  0.0073,  0.3641, -0.6520,  0.3294, -0.1244, -0.0753]],
       dtype=torch.float64)
	q_value: tensor([[-35.3607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29933989446275655, distance: 0.9578784182018081 entropy 0.9217694997787476
epoch: 1, step: 90
	action: tensor([[ 0.5466, -1.2002,  0.0468, -0.3537, -0.3493,  0.2623, -0.8844]],
       dtype=torch.float64)
	q_value: tensor([[-24.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8070743123645339, distance: 1.5383129620472535 entropy 0.9217694997787476
epoch: 1, step: 91
	action: tensor([[ 1.1771,  0.0892,  1.5834,  0.4658,  0.5837, -0.3320, -1.4135]],
       dtype=torch.float64)
	q_value: tensor([[-31.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6913314964331936, distance: 0.6357741289045362 entropy 0.9217694997787476
epoch: 1, step: 92
	action: tensor([[ 0.3386, -0.3624, -0.6442, -0.4864, -0.7891,  0.7205, -0.6046]],
       dtype=torch.float64)
	q_value: tensor([[-46.2250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09647275383558152, distance: 1.087745561791611 entropy 0.9217694997787476
epoch: 1, step: 93
	action: tensor([[ 0.2344, -0.2425,  0.6694, -0.7390, -0.4142,  0.4222,  0.4533]],
       dtype=torch.float64)
	q_value: tensor([[-26.4567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07295561105911919, distance: 1.1853526389310987 entropy 0.9217694997787476
epoch: 1, step: 94
	action: tensor([[-0.8454, -0.5253, -1.2874,  0.5948, -0.4586, -0.8621, -0.4429]],
       dtype=torch.float64)
	q_value: tensor([[-27.6073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9817451059460607, distance: 1.6109445445491948 entropy 0.9217694997787476
epoch: 1, step: 95
	action: tensor([[ 1.1287,  0.1289,  0.7664,  0.8282, -0.7243,  0.9630,  1.2087]],
       dtype=torch.float64)
	q_value: tensor([[-32.3510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.822900419246467, distance: 0.48157669269626213 entropy 0.9217694997787476
epoch: 1, step: 96
	action: tensor([[ 1.1080, -0.6044, -0.4710,  0.1023, -0.5743, -0.6339, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-39.0997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03306945343119927, distance: 1.1252637629812685 entropy 0.9217694997787476
epoch: 1, step: 97
	action: tensor([[ 0.7064, -0.6609,  0.0667,  0.4039, -0.1736,  0.1723, -0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-32.9526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4492108329102019, distance: 0.8492770498950585 entropy 0.9217694997787476
epoch: 1, step: 98
	action: tensor([[-0.1095,  0.8015,  0.2097, -0.2510,  0.4642, -0.8295, -0.3273]],
       dtype=torch.float64)
	q_value: tensor([[-27.0766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35162799187016547, distance: 0.9214437354102393 entropy 0.9217694997787476
epoch: 1, step: 99
	action: tensor([[ 1.0706,  0.9438,  0.1570, -0.7568,  0.9480, -1.4575,  0.7156]],
       dtype=torch.float64)
	q_value: tensor([[-24.3596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9200238249908821, distance: 0.3236212328341663 entropy 0.9217694997787476
epoch: 1, step: 100
	action: tensor([[ 0.6832,  1.1189, -0.1640,  1.0365, -0.7982, -0.2765, -0.7642]],
       dtype=torch.float64)
	q_value: tensor([[-39.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 101
	action: tensor([[ 1.1352,  0.8160, -0.0806,  0.7165, -0.0969, -0.8257, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 102
	action: tensor([[-0.8549, -0.1811, -0.3165,  0.3723, -0.9852,  0.5155,  0.3000]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8386682581504428, distance: 1.551702224091294 entropy 0.9217694997787476
epoch: 1, step: 103
	action: tensor([[ 1.6178,  0.3956,  0.3610, -0.5649, -0.7391, -0.1483,  0.4536]],
       dtype=torch.float64)
	q_value: tensor([[-24.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.508599043910695, distance: 0.8021852862130923 entropy 0.9217694997787476
epoch: 1, step: 104
	action: tensor([[ 0.5865, -0.4398,  0.6749, -0.7186, -1.0287,  0.7143,  0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-36.3381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09619789791474675, distance: 1.0879109970762602 entropy 0.9217694997787476
epoch: 1, step: 105
	action: tensor([[ 0.4035, -0.2152,  0.0152,  0.8571, -0.3342, -0.8210, -0.2444]],
       dtype=torch.float64)
	q_value: tensor([[-34.1455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7325528533300352, distance: 0.5918009768728932 entropy 0.9217694997787476
epoch: 1, step: 106
	action: tensor([[ 0.9204,  0.1595, -0.2854,  0.4232,  0.5147, -0.2058,  0.4474]],
       dtype=torch.float64)
	q_value: tensor([[-31.2297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9505900986920233, distance: 0.25436870939309586 entropy 0.9217694997787476
epoch: 1, step: 107
	action: tensor([[ 0.6245,  0.8354, -0.2087, -0.0074,  0.0439, -0.3574,  0.7394]],
       dtype=torch.float64)
	q_value: tensor([[-27.9759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9580572154500377, distance: 0.23436098953944426 entropy 0.9217694997787476
epoch: 1, step: 108
	action: tensor([[-1.1006,  0.1771, -0.9586, -0.5622, -0.6833, -1.0244, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[-23.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14561690821297524, distance: 1.2248316515030748 entropy 0.9217694997787476
epoch: 1, step: 109
	action: tensor([[-0.4724,  0.4343, -0.5926, -0.0524, -0.0255, -0.9394,  1.1426]],
       dtype=torch.float64)
	q_value: tensor([[-29.4615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18832234221620925, distance: 1.03097584248879 entropy 0.9217694997787476
epoch: 1, step: 110
	action: tensor([[ 0.3157, -0.1316, -0.1689,  0.1886,  0.2764, -0.6411,  1.0429]],
       dtype=torch.float64)
	q_value: tensor([[-28.2394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39361542835373964, distance: 0.8911088661551697 entropy 0.9217694997787476
epoch: 1, step: 111
	action: tensor([[ 0.2910,  0.0687,  0.5927,  0.0589,  1.4737, -0.7720, -0.3390]],
       dtype=torch.float64)
	q_value: tensor([[-28.6444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5682422807297476, distance: 0.7519288510113948 entropy 0.9217694997787476
epoch: 1, step: 112
	action: tensor([[ 0.0659, -0.4930, -0.3965,  0.2133, -0.5863,  0.1153, -0.3466]],
       dtype=torch.float64)
	q_value: tensor([[-34.8069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008661495342566994, distance: 1.1492894351346203 entropy 0.9217694997787476
epoch: 1, step: 113
	action: tensor([[ 0.0590, -0.0764,  0.0472, -0.5173, -0.9398, -0.1000,  0.6358]],
       dtype=torch.float64)
	q_value: tensor([[-21.3661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05444579302572283, distance: 1.112755908287437 entropy 0.9217694997787476
epoch: 1, step: 114
	action: tensor([[ 0.0128, -1.0799, -0.1173,  0.0174, -0.5204, -0.0874,  0.4979]],
       dtype=torch.float64)
	q_value: tensor([[-25.6983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6612554317256685, distance: 1.474941856023056 entropy 0.9217694997787476
epoch: 1, step: 115
	action: tensor([[ 0.4788,  0.5624, -1.4049, -0.3924, -0.3754,  0.5864,  0.9407]],
       dtype=torch.float64)
	q_value: tensor([[-28.3309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9217326101665485, distance: 0.3201452912864803 entropy 0.9217694997787476
epoch: 1, step: 116
	action: tensor([[ 0.2165,  0.6434, -0.0419,  0.6767, -0.3824,  0.1958, -0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-30.6561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 117
	action: tensor([[ 0.9789,  0.1000,  0.7248,  0.5997, -0.0277, -0.0697, -0.5911]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8841638243104463, distance: 0.38947429399638306 entropy 0.9217694997787476
epoch: 1, step: 118
	action: tensor([[-0.5101,  1.2885, -0.3660, -0.2403, -0.2787, -0.4616,  0.8765]],
       dtype=torch.float64)
	q_value: tensor([[-32.6328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 119
	action: tensor([[ 0.7824,  0.8840,  0.8457,  0.6194,  0.5468, -0.4789,  0.7543]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 120
	action: tensor([[ 0.3887,  0.0671,  0.2855,  0.9699, -0.5105, -0.5871, -0.1210]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9440274565867727, distance: 0.27073488168326243 entropy 0.9217694997787476
epoch: 1, step: 121
	action: tensor([[ 0.2431, -0.0292, -0.8094, -0.0444, -0.5397,  0.5799,  0.4093]],
       dtype=torch.float64)
	q_value: tensor([[-29.8684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4425925209403716, distance: 0.8543642925188453 entropy 0.9217694997787476
epoch: 1, step: 122
	action: tensor([[ 0.3417,  0.3376,  0.3679,  0.4339, -0.1508, -0.5495, -1.0746]],
       dtype=torch.float64)
	q_value: tensor([[-21.7383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8572529576787349, distance: 0.43235476781718823 entropy 0.9217694997787476
epoch: 1, step: 123
	action: tensor([[-0.1133,  0.6647, -1.3554, -1.4296, -0.3471, -0.7322,  0.6995]],
       dtype=torch.float64)
	q_value: tensor([[-29.3617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8564872769867611, distance: 0.43351277065606453 entropy 0.9217694997787476
epoch: 1, step: 124
	action: tensor([[ 0.0702,  0.5019,  0.3603, -0.4543, -0.6367,  0.0988,  0.1639]],
       dtype=torch.float64)
	q_value: tensor([[-34.2312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47302330555244754, distance: 0.8307156604346414 entropy 0.9217694997787476
epoch: 1, step: 125
	action: tensor([[ 0.7089,  0.0944,  0.0768, -1.4295, -0.7145, -0.0043,  0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-24.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07615383217032412, distance: 1.099908413520215 entropy 0.9217694997787476
epoch: 1, step: 126
	action: tensor([[ 0.2921, -0.0857, -0.6452,  0.1524,  0.6034, -1.2778,  0.4025]],
       dtype=torch.float64)
	q_value: tensor([[-33.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3314223108329709, distance: 0.935691389918502 entropy 0.9217694997787476
epoch: 1, step: 127
	action: tensor([[-0.0712,  1.4026,  0.4018,  0.5927, -0.1953, -0.2000, -0.2486]],
       dtype=torch.float64)
	q_value: tensor([[-34.4550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
LOSS epoch 1 actor 419.16229763232633 critic 396.52946452510724 
epoch: 2, step: 0
	action: tensor([[ 0.0988, -0.1966,  0.6611,  1.0017, -0.0587, -0.2973, -0.5601]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7814167350260051, distance: 0.5350140012851239 entropy 0.8164090514183044
epoch: 2, step: 1
	action: tensor([[ 0.2120, -0.4440, -0.5574, -0.2047,  0.4500,  0.4009,  0.5068]],
       dtype=torch.float64)
	q_value: tensor([[-30.3810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1418847073441145, distance: 1.0600577222775545 entropy 0.8164090514183044
epoch: 2, step: 2
	action: tensor([[-0.4923, -0.4174,  0.2951,  0.4562, -0.1824, -0.1610, -0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-22.9618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.295103163158033, distance: 1.3022935072060948 entropy 0.8164090514183044
epoch: 2, step: 3
	action: tensor([[ 0.4330,  0.1816,  0.4212, -0.9696, -0.0214, -0.5917,  0.4832]],
       dtype=torch.float64)
	q_value: tensor([[-23.3347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11708610345930903, distance: 1.0752658907691062 entropy 0.8164090514183044
epoch: 2, step: 4
	action: tensor([[ 0.6035, -0.0934, -0.7775, -0.3412,  0.0920, -0.0504,  0.3487]],
       dtype=torch.float64)
	q_value: tensor([[-27.2669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5103913106476623, distance: 0.8007210609941277 entropy 0.8164090514183044
epoch: 2, step: 5
	action: tensor([[ 0.7218, -0.6103,  0.0641, -0.4823,  0.1334,  0.4479,  0.2784]],
       dtype=torch.float64)
	q_value: tensor([[-22.2687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.052320757899162684, distance: 1.1738990790878001 entropy 0.8164090514183044
epoch: 2, step: 6
	action: tensor([[-0.4654,  0.0800,  0.2269, -0.6153, -0.1020, -0.0116, -0.4667]],
       dtype=torch.float64)
	q_value: tensor([[-25.5466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.503551018433017, distance: 1.403187728191019 entropy 0.8164090514183044
epoch: 2, step: 7
	action: tensor([[ 0.0840,  1.6868, -0.5043, -0.8313,  0.1387, -1.2395,  1.0987]],
       dtype=torch.float64)
	q_value: tensor([[-20.2567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8967604626627754, distance: 0.36768822162066384 entropy 0.8164090514183044
epoch: 2, step: 8
	action: tensor([[ 0.6497,  1.2779,  0.2055, -0.3480,  0.3335,  0.0603, -0.4524]],
       dtype=torch.float64)
	q_value: tensor([[-31.7413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 9
	action: tensor([[ 0.3909, -0.4524, -0.3726, -0.5027,  0.0308,  0.0125,  0.0617]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031018409163147576, distance: 1.1619565894736008 entropy 0.8164090514183044
epoch: 2, step: 10
	action: tensor([[ 0.7970,  0.3244, -0.6728, -0.8600, -0.8808, -0.0963, -0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-21.2569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.634078561076703, distance: 0.6922303249866484 entropy 0.8164090514183044
epoch: 2, step: 11
	action: tensor([[ 0.9937, -0.4735, -0.0470,  0.8584,  0.3037, -0.5135, -0.1851]],
       dtype=torch.float64)
	q_value: tensor([[-28.5794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6993119716843315, distance: 0.6275014907821256 entropy 0.8164090514183044
epoch: 2, step: 12
	action: tensor([[-0.4850,  0.0679, -0.1137, -0.0797, -0.4217, -0.4999, -0.5644]],
       dtype=torch.float64)
	q_value: tensor([[-34.3824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19822737470862672, distance: 1.2526401043548285 entropy 0.8164090514183044
epoch: 2, step: 13
	action: tensor([[ 2.1376e-01,  4.1492e-01, -4.3580e-01, -4.1606e-01, -2.6512e-01,
          5.3643e-01,  1.9693e-04]], dtype=torch.float64)
	q_value: tensor([[-21.4719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6731911910295171, distance: 0.6541895017008906 entropy 0.8164090514183044
epoch: 2, step: 14
	action: tensor([[-0.6226, -0.0641, -0.0487,  0.0199, -0.7422,  0.7007,  0.4109]],
       dtype=torch.float64)
	q_value: tensor([[-20.8985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4548352077752862, distance: 1.3802685566344506 entropy 0.8164090514183044
epoch: 2, step: 15
	action: tensor([[-0.1586, -0.9131,  0.9809,  0.0197, -0.5022,  0.6102,  0.7192]],
       dtype=torch.float64)
	q_value: tensor([[-23.5064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32539986179931724, distance: 1.3174379021361187 entropy 0.8164090514183044
epoch: 2, step: 16
	action: tensor([[-0.2147,  0.2079,  0.2973,  0.0181,  0.1618,  0.5824,  0.4069]],
       dtype=torch.float64)
	q_value: tensor([[-34.3178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42272151306115513, distance: 0.8694595456465816 entropy 0.8164090514183044
epoch: 2, step: 17
	action: tensor([[-0.0790,  0.3818, -0.3846, -0.6240,  0.3062, -0.6897, -0.1384]],
       dtype=torch.float64)
	q_value: tensor([[-20.3053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27677537844330224, distance: 0.9731802680800407 entropy 0.8164090514183044
epoch: 2, step: 18
	action: tensor([[-0.1155, -0.1386, -0.2610,  0.4954,  0.0987, -0.7176,  1.4049]],
       dtype=torch.float64)
	q_value: tensor([[-21.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1332903087142273, distance: 1.0653529667998585 entropy 0.8164090514183044
epoch: 2, step: 19
	action: tensor([[-0.3123, -1.0120, -0.3699, -0.8336, -0.7372, -0.4538, -0.1194]],
       dtype=torch.float64)
	q_value: tensor([[-32.0264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5273892751536737, distance: 1.4142675010041679 entropy 0.8164090514183044
epoch: 2, step: 20
	action: tensor([[-0.1981,  0.7175, -0.2198,  0.8163, -0.1722,  0.1003, -0.2061]],
       dtype=torch.float64)
	q_value: tensor([[-28.6367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 21
	action: tensor([[ 0.3153, -0.9353, -0.6185, -0.4478,  0.4750, -0.3377, -0.4449]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5500332939791246, distance: 1.4247124079386977 entropy 0.8164090514183044
epoch: 2, step: 22
	action: tensor([[ 0.0810, -0.4555,  0.8023, -0.5343, -0.5173,  0.2753,  0.1555]],
       dtype=torch.float64)
	q_value: tensor([[-26.9143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32678085537795165, distance: 1.318124072272619 entropy 0.8164090514183044
epoch: 2, step: 23
	action: tensor([[ 0.4835, -0.3127, -0.5825, -0.0272, -0.0454,  0.0784, -0.7948]],
       dtype=torch.float64)
	q_value: tensor([[-27.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36381369263606766, distance: 0.9127437168775562 entropy 0.8164090514183044
epoch: 2, step: 24
	action: tensor([[ 0.1680,  0.6958, -0.0642,  1.1088, -0.1451, -0.2072, -1.0041]],
       dtype=torch.float64)
	q_value: tensor([[-22.8301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 25
	action: tensor([[-0.6599, -0.0925,  0.5072, -0.6903,  0.0063,  1.1412, -0.4812]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5387765153018313, distance: 1.419529649589687 entropy 0.8164090514183044
epoch: 2, step: 26
	action: tensor([[-0.0280, -0.1843, -0.0519, -1.0204,  0.3058, -0.1270,  0.4700]],
       dtype=torch.float64)
	q_value: tensor([[-28.0013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3311443325390624, distance: 1.3202897965879064 entropy 0.8164090514183044
epoch: 2, step: 27
	action: tensor([[ 0.6061, -0.2677,  0.4099, -0.4558, -0.2070,  0.6205, -0.5889]],
       dtype=torch.float64)
	q_value: tensor([[-23.3512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38121706934669397, distance: 0.9001727374174849 entropy 0.8164090514183044
epoch: 2, step: 28
	action: tensor([[ 0.7636,  0.3165,  0.0152, -0.5101, -0.0323,  0.2397,  0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-26.8236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7951254399830274, distance: 0.5179653526551617 entropy 0.8164090514183044
epoch: 2, step: 29
	action: tensor([[-0.7576,  0.0462, -0.4627, -0.6227,  0.8853,  1.2449,  0.3324]],
       dtype=torch.float64)
	q_value: tensor([[-22.9898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.053292719621280904, distance: 1.174441081903833 entropy 0.8164090514183044
epoch: 2, step: 30
	action: tensor([[ 0.2138, -0.1558, -1.1719,  0.3019,  0.4337, -0.2090,  0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-29.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30805665779485103, distance: 0.9519013894629325 entropy 0.8164090514183044
epoch: 2, step: 31
	action: tensor([[ 0.0887,  1.1810,  0.9087, -0.0080, -0.9115, -0.4022,  0.1767]],
       dtype=torch.float64)
	q_value: tensor([[-25.2753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 32
	action: tensor([[ 0.8159, -0.8753,  0.0758,  0.0334,  0.6952, -0.6191, -0.6025]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30870983003400054, distance: 1.3091167375900163 entropy 0.8164090514183044
epoch: 2, step: 33
	action: tensor([[-0.2279,  0.9483, -0.1878, -0.6140,  0.3033, -0.5375, -0.4551]],
       dtype=torch.float64)
	q_value: tensor([[-34.4880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44212279357820283, distance: 0.8547242031662537 entropy 0.8164090514183044
epoch: 2, step: 34
	action: tensor([[ 0.1893,  0.5067,  0.1725,  0.0930, -0.7116,  0.3160, -0.9953]],
       dtype=torch.float64)
	q_value: tensor([[-22.5029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7289102936719141, distance: 0.5958174340263304 entropy 0.8164090514183044
epoch: 2, step: 35
	action: tensor([[ 0.1741,  0.5969, -0.0549, -1.0773,  0.6856,  0.3118,  0.5667]],
       dtype=torch.float64)
	q_value: tensor([[-26.5788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5790537805797578, distance: 0.7424547669201128 entropy 0.8164090514183044
epoch: 2, step: 36
	action: tensor([[ 0.4563,  0.7161,  0.4743,  0.3584, -0.1366,  0.4062, -0.8485]],
       dtype=torch.float64)
	q_value: tensor([[-25.7256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 37
	action: tensor([[ 0.0743, -0.1773, -0.3072,  0.1943, -0.2421, -0.2800,  0.1118]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925596740341826, distance: 0.9625019085098634 entropy 0.8164090514183044
epoch: 2, step: 38
	action: tensor([[ 0.4740,  0.2710,  0.0636,  0.7817, -0.3865,  0.4896,  0.3434]],
       dtype=torch.float64)
	q_value: tensor([[-19.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8861148528869819, distance: 0.3861804078197315 entropy 0.8164090514183044
epoch: 2, step: 39
	action: tensor([[ 0.4761,  1.6268, -0.6429, -0.6573, -0.3121, -0.1827,  0.4599]],
       dtype=torch.float64)
	q_value: tensor([[-23.7482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 40
	action: tensor([[-0.0957,  0.2803, -0.7109, -0.9238,  0.2525,  0.3165,  0.3010]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46923887745594994, distance: 0.8336931732033522 entropy 0.8164090514183044
epoch: 2, step: 41
	action: tensor([[ 0.0676, -0.3774,  0.0875, -0.2617,  0.1412,  0.4075, -0.3213]],
       dtype=torch.float64)
	q_value: tensor([[-22.8052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046456220915814805, distance: 1.1174472005566696 entropy 0.8164090514183044
epoch: 2, step: 42
	action: tensor([[ 0.9205,  0.4386, -0.2253, -0.0741, -1.2326, -0.9584,  0.1664]],
       dtype=torch.float64)
	q_value: tensor([[-19.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8767939302062355, distance: 0.4016730974412933 entropy 0.8164090514183044
epoch: 2, step: 43
	action: tensor([[ 0.6622, -0.0201, -0.2017,  0.3407, -0.5354,  0.2216,  0.4364]],
       dtype=torch.float64)
	q_value: tensor([[-31.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8709576538151345, distance: 0.4110766593717332 entropy 0.8164090514183044
epoch: 2, step: 44
	action: tensor([[ 0.1333,  0.1829, -0.9179,  0.9265,  0.0027,  0.9451, -0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-22.5523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30301493872072316, distance: 0.9553630232739164 entropy 0.8164090514183044
epoch: 2, step: 45
	action: tensor([[-0.2401,  0.0651, -1.0623,  0.7032,  0.6284, -0.1303,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-25.0694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13348004379626133, distance: 1.2183263364372514 entropy 0.8164090514183044
epoch: 2, step: 46
	action: tensor([[-0.1099, -0.3103,  0.0034, -0.5860,  0.3244, -0.6956, -0.6419]],
       dtype=torch.float64)
	q_value: tensor([[-26.7780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4352978459218275, distance: 1.3709692369455513 entropy 0.8164090514183044
epoch: 2, step: 47
	action: tensor([[ 0.5803,  0.5809,  0.6006, -0.9858,  0.0317, -0.3744,  0.0829]],
       dtype=torch.float64)
	q_value: tensor([[-25.0500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.539038681888627, distance: 0.7769426001217005 entropy 0.8164090514183044
epoch: 2, step: 48
	action: tensor([[ 0.4171,  0.0138, -0.0541,  0.1196,  0.5989,  0.3471, -0.1459]],
       dtype=torch.float64)
	q_value: tensor([[-28.2105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.778526782252203, distance: 0.538539175387822 entropy 0.8164090514183044
epoch: 2, step: 49
	action: tensor([[ 0.8446,  0.3388, -0.2019,  0.2115, -0.5678, -0.2953, -0.3172]],
       dtype=torch.float64)
	q_value: tensor([[-21.2025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9538969896076716, distance: 0.24570915337848517 entropy 0.8164090514183044
epoch: 2, step: 50
	action: tensor([[ 1.1092,  0.4967,  0.4660, -0.6557,  0.1992,  0.4905,  0.4966]],
       dtype=torch.float64)
	q_value: tensor([[-24.4802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8925672051420036, distance: 0.375081056203484 entropy 0.8164090514183044
epoch: 2, step: 51
	action: tensor([[ 0.8329, -0.4741, -0.3697,  0.0912,  1.3985, -0.1595, -0.2607]],
       dtype=torch.float64)
	q_value: tensor([[-29.3315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34374091142840113, distance: 0.9270312167636441 entropy 0.8164090514183044
epoch: 2, step: 52
	action: tensor([[ 0.3800, -0.5784, -0.7297,  0.0989, -0.4544,  0.3785,  0.4342]],
       dtype=torch.float64)
	q_value: tensor([[-34.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11311197113278015, distance: 1.0776831429528824 entropy 0.8164090514183044
epoch: 2, step: 53
	action: tensor([[ 0.4814,  0.1875, -0.2679, -0.6670,  0.1120,  0.9293, -0.5242]],
       dtype=torch.float64)
	q_value: tensor([[-23.0906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7414122299588464, distance: 0.5819165162892085 entropy 0.8164090514183044
epoch: 2, step: 54
	action: tensor([[-0.0512, -0.1915,  0.2436, -0.1140, -0.9453, -1.1739,  0.3765]],
       dtype=torch.float64)
	q_value: tensor([[-24.5669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15721871830263723, distance: 1.0505437271694849 entropy 0.8164090514183044
epoch: 2, step: 55
	action: tensor([[-0.6384,  0.1870,  0.0902, -0.2888,  0.2183,  1.1353,  0.5966]],
       dtype=torch.float64)
	q_value: tensor([[-30.9908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020509730181037744, distance: 1.1560197882758216 entropy 0.8164090514183044
epoch: 2, step: 56
	action: tensor([[ 0.4637, -0.8531, -0.4808,  0.7900, -0.0496, -0.4499,  0.6158]],
       dtype=torch.float64)
	q_value: tensor([[-26.6089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11850096727291048, distance: 1.0744039920132011 entropy 0.8164090514183044
epoch: 2, step: 57
	action: tensor([[ 0.3577,  0.3802, -0.4624,  0.6268, -0.9924,  0.4900,  0.6440]],
       dtype=torch.float64)
	q_value: tensor([[-32.5432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6629612238600415, distance: 0.6643495234644322 entropy 0.8164090514183044
epoch: 2, step: 58
	action: tensor([[ 0.5026, -0.1381,  0.1851, -0.0884, -0.3416,  0.6010,  0.9193]],
       dtype=torch.float64)
	q_value: tensor([[-25.6418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6709620360097601, distance: 0.6564168154646504 entropy 0.8164090514183044
epoch: 2, step: 59
	action: tensor([[ 0.3048,  0.3280, -1.0298, -0.7435,  0.2348,  0.6624,  0.9137]],
       dtype=torch.float64)
	q_value: tensor([[-26.5218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.835643412570137, distance: 0.46392764335484865 entropy 0.8164090514183044
epoch: 2, step: 60
	action: tensor([[ 0.6566, -0.3095, -0.6475,  0.6083,  0.0781, -0.4978,  0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-27.8246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5818595397355195, distance: 0.7399762648966307 entropy 0.8164090514183044
epoch: 2, step: 61
	action: tensor([[ 0.8015, -1.0782,  0.1775, -0.0160, -0.5952,  0.2874,  0.6743]],
       dtype=torch.float64)
	q_value: tensor([[-28.0776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3501006086661338, distance: 1.3296574149700138 entropy 0.8164090514183044
epoch: 2, step: 62
	action: tensor([[ 0.4046,  0.6804,  0.2985,  0.1737, -0.4290, -0.5774, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-32.5888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9138496690686612, distance: 0.3358807917866197 entropy 0.8164090514183044
epoch: 2, step: 63
	action: tensor([[ 0.5288,  1.4454, -0.6612, -0.4081, -0.3757, -0.8670,  0.6655]],
       dtype=torch.float64)
	q_value: tensor([[-23.9179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9035102459684061, distance: 0.35546536628238523 entropy 0.8164090514183044
epoch: 2, step: 64
	action: tensor([[ 0.3405, -0.1515, -0.4458,  0.2251,  0.1144,  1.2566,  0.8179]],
       dtype=torch.float64)
	q_value: tensor([[-29.6233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7370855687212545, distance: 0.5867646015845939 entropy 0.8164090514183044
epoch: 2, step: 65
	action: tensor([[ 0.1151,  0.9334,  0.0510, -0.5740, -0.6189,  0.4249,  0.2973]],
       dtype=torch.float64)
	q_value: tensor([[-28.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6708172089891357, distance: 0.6565612614608326 entropy 0.8164090514183044
epoch: 2, step: 66
	action: tensor([[ 0.6049, -0.0081, -0.1022, -0.1419, -0.0634,  0.3802,  0.5266]],
       dtype=torch.float64)
	q_value: tensor([[-26.2655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7254945526212584, distance: 0.5995593455141854 entropy 0.8164090514183044
epoch: 2, step: 67
	action: tensor([[-0.3034,  0.0578, -0.5927, -0.3707, -0.6590, -0.3452, -0.8109]],
       dtype=torch.float64)
	q_value: tensor([[-22.0229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10598358157864707, distance: 1.08200543075329 entropy 0.8164090514183044
epoch: 2, step: 68
	action: tensor([[-0.8193, -0.2862, -0.0036, -0.3071,  0.8875,  1.3193,  0.1413]],
       dtype=torch.float64)
	q_value: tensor([[-23.7318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30650973814174853, distance: 1.3080158867558074 entropy 0.8164090514183044
epoch: 2, step: 69
	action: tensor([[-0.3610,  0.4453,  0.2461, -0.2643,  0.5570,  0.1747, -0.1273]],
       dtype=torch.float64)
	q_value: tensor([[-29.5576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09528990910408985, distance: 1.0884573351727764 entropy 0.8164090514183044
epoch: 2, step: 70
	action: tensor([[-0.0091, -0.4794,  0.5977,  0.6191, -0.0292,  0.1134, -0.2273]],
       dtype=torch.float64)
	q_value: tensor([[-18.7417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42066931653318995, distance: 0.8710036174743789 entropy 0.8164090514183044
epoch: 2, step: 71
	action: tensor([[ 0.3055, -0.9868, -0.5331, -1.0944,  0.1010, -0.0369, -0.4975]],
       dtype=torch.float64)
	q_value: tensor([[-26.6656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5925751675859303, distance: 1.4441312406127322 entropy 0.8164090514183044
epoch: 2, step: 72
	action: tensor([[ 0.2592, -1.0973, -0.4768,  0.3740,  0.0842, -0.0116,  0.4584]],
       dtype=torch.float64)
	q_value: tensor([[-27.2890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4000955663402539, distance: 1.3540525937799743 entropy 0.8164090514183044
epoch: 2, step: 73
	action: tensor([[ 0.0171,  0.6140,  0.2395,  0.4637, -0.5587,  0.6659, -0.8416]],
       dtype=torch.float64)
	q_value: tensor([[-28.6232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 74
	action: tensor([[-0.0627, -0.0221,  0.1602,  0.1901,  0.1466,  0.2142,  0.6501]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39345895442892465, distance: 0.8912238314051879 entropy 0.8164090514183044
epoch: 2, step: 75
	action: tensor([[-0.1177,  0.4604,  0.4791,  0.3217,  0.5187, -0.2262, -0.2041]],
       dtype=torch.float64)
	q_value: tensor([[-21.2930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5389875567279414, distance: 0.7769856842291988 entropy 0.8164090514183044
epoch: 2, step: 76
	action: tensor([[ 0.8052, -0.4603, -0.0941,  0.1172, -0.5265, -0.8353, -0.9683]],
       dtype=torch.float64)
	q_value: tensor([[-22.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21253710739320386, distance: 1.0154808600712093 entropy 0.8164090514183044
epoch: 2, step: 77
	action: tensor([[-0.3524,  0.8983, -0.1224, -0.7046,  0.6878, -0.9886,  0.1619]],
       dtype=torch.float64)
	q_value: tensor([[-33.6664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12603444522960106, distance: 1.069803098512207 entropy 0.8164090514183044
epoch: 2, step: 78
	action: tensor([[-0.7237, -0.2596,  0.1189,  0.5360, -0.2590,  0.1639,  0.4856]],
       dtype=torch.float64)
	q_value: tensor([[-25.2087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37327101744834645, distance: 1.3410186584399906 entropy 0.8164090514183044
epoch: 2, step: 79
	action: tensor([[-0.3725, -0.6157, -0.1466, -0.2968, -0.1848, -0.8813,  0.1075]],
       dtype=torch.float64)
	q_value: tensor([[-24.3590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.603033794188264, distance: 1.4488653699596696 entropy 0.8164090514183044
epoch: 2, step: 80
	action: tensor([[-0.2679,  0.1563,  0.2412,  0.2423,  0.0250, -0.4972, -0.2370]],
       dtype=torch.float64)
	q_value: tensor([[-27.4766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12015631807526927, distance: 1.0733947162287378 entropy 0.8164090514183044
epoch: 2, step: 81
	action: tensor([[ 0.6373,  0.7059,  0.0284, -0.0831,  0.4268, -0.7175,  0.4723]],
       dtype=torch.float64)
	q_value: tensor([[-21.1574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9255371935661502, distance: 0.31226721750746306 entropy 0.8164090514183044
epoch: 2, step: 82
	action: tensor([[ 0.6352, -0.2687,  0.4788, -0.4804,  0.1245,  0.0187,  1.2759]],
       dtype=torch.float64)
	q_value: tensor([[-26.1207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2031974040327993, distance: 1.0214851642298541 entropy 0.8164090514183044
epoch: 2, step: 83
	action: tensor([[ 0.5801, -0.2564,  1.0955,  0.7507,  0.3304, -0.5038,  0.1394]],
       dtype=torch.float64)
	q_value: tensor([[-31.0445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7219193874789214, distance: 0.6034510523107148 entropy 0.8164090514183044
epoch: 2, step: 84
	action: tensor([[ 0.3962, -0.7576,  0.1151, -0.9680,  0.6895, -0.0114, -0.4766]],
       dtype=torch.float64)
	q_value: tensor([[-36.2061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7048271568071156, distance: 1.4941591911223013 entropy 0.8164090514183044
epoch: 2, step: 85
	action: tensor([[ 0.4216, -0.4472,  0.2625, -0.1765, -0.5547, -0.6416,  0.0543]],
       dtype=torch.float64)
	q_value: tensor([[-26.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05606791412679346, distance: 1.1759872610021767 entropy 0.8164090514183044
epoch: 2, step: 86
	action: tensor([[ 0.6710, -0.2919, -0.3649,  0.3525, -0.5458,  0.3626,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-26.7458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6715114642775568, distance: 0.6558685434195252 entropy 0.8164090514183044
epoch: 2, step: 87
	action: tensor([[ 0.1185,  0.1575, -0.0657,  0.1939, -0.0056, -0.4637,  1.0085]],
       dtype=torch.float64)
	q_value: tensor([[-22.8233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5156180691804826, distance: 0.7964355924819607 entropy 0.8164090514183044
epoch: 2, step: 88
	action: tensor([[ 0.4958, -0.1942,  0.6443, -0.2509, -0.7010,  0.1123,  0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-24.7802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37002575134910665, distance: 0.9082765295568123 entropy 0.8164090514183044
epoch: 2, step: 89
	action: tensor([[-0.3483,  0.2870, -1.0382, -1.1348,  0.1028, -0.0330, -0.0938]],
       dtype=torch.float64)
	q_value: tensor([[-27.1765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46442615742617377, distance: 0.8374644340694263 entropy 0.8164090514183044
epoch: 2, step: 90
	action: tensor([[ 0.4380, -1.0974,  0.1661,  0.9118,  0.6107, -0.8556,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-24.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.038338775456017826, distance: 1.1660743155986735 entropy 0.8164090514183044
epoch: 2, step: 91
	action: tensor([[-0.2275, -0.6766,  0.5893, -0.5105,  0.1488,  0.6199,  0.7911]],
       dtype=torch.float64)
	q_value: tensor([[-40.6512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6173552050363715, distance: 1.4553230185143493 entropy 0.8164090514183044
epoch: 2, step: 92
	action: tensor([[-0.1570,  0.6696, -0.0188, -0.1130,  0.0856,  0.2697,  0.0407]],
       dtype=torch.float64)
	q_value: tensor([[-29.6541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5259023009012667, distance: 0.7879354079240483 entropy 0.8164090514183044
epoch: 2, step: 93
	action: tensor([[-0.1184,  0.2532, -0.3247,  0.1580, -1.1899,  0.1476, -0.2269]],
       dtype=torch.float64)
	q_value: tensor([[-17.9627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3323821120430932, distance: 0.9350195156609793 entropy 0.8164090514183044
epoch: 2, step: 94
	action: tensor([[ 0.3142,  0.1608,  0.3207,  0.0604, -0.6110,  0.4169,  0.1454]],
       dtype=torch.float64)
	q_value: tensor([[-24.2609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7507021794985032, distance: 0.5713680269059864 entropy 0.8164090514183044
epoch: 2, step: 95
	action: tensor([[-0.0980, -0.3174,  0.4177, -0.1825, -0.1526,  0.2068, -0.1606]],
       dtype=torch.float64)
	q_value: tensor([[-23.2373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10907996012478494, distance: 1.2051417229762873 entropy 0.8164090514183044
epoch: 2, step: 96
	action: tensor([[ 1.0403, -0.0426,  0.1673, -0.8680,  0.2991, -0.5303,  0.4339]],
       dtype=torch.float64)
	q_value: tensor([[-21.1313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16275207323668373, distance: 1.0470893289839083 entropy 0.8164090514183044
epoch: 2, step: 97
	action: tensor([[ 0.1862,  0.5623, -0.8243,  0.2271,  0.2283, -0.1990,  0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-30.8697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7221459185417451, distance: 0.6032052094628505 entropy 0.8164090514183044
epoch: 2, step: 98
	action: tensor([[-0.3878,  0.1185, -0.4620,  0.1463,  0.0178, -0.4210, -0.2135]],
       dtype=torch.float64)
	q_value: tensor([[-20.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023723490595903618, distance: 1.1578386098869615 entropy 0.8164090514183044
epoch: 2, step: 99
	action: tensor([[ 1.0167,  0.1772, -0.6782, -0.3048, -0.0343,  0.3218,  0.3831]],
       dtype=torch.float64)
	q_value: tensor([[-19.2812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7870151363006164, distance: 0.5281181137006263 entropy 0.8164090514183044
epoch: 2, step: 100
	action: tensor([[-0.4635,  0.0812,  0.9973,  0.4435, -0.5240, -0.0902, -0.2648]],
       dtype=torch.float64)
	q_value: tensor([[-25.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20030397142899403, distance: 1.0233381452472488 entropy 0.8164090514183044
epoch: 2, step: 101
	action: tensor([[-0.9956,  0.0386, -0.0231,  0.6586, -0.4218, -0.2056, -0.3527]],
       dtype=torch.float64)
	q_value: tensor([[-27.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5035828917198106, distance: 1.4032026009712961 entropy 0.8164090514183044
epoch: 2, step: 102
	action: tensor([[ 0.5543,  0.5218, -0.2805,  0.3997,  1.2529,  0.1193, -0.3071]],
       dtype=torch.float64)
	q_value: tensor([[-23.7332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9119051934386548, distance: 0.3396501779067972 entropy 0.8164090514183044
epoch: 2, step: 103
	action: tensor([[-0.0069, -0.3087,  0.1662, -0.5459,  0.1008,  0.5089, -0.5936]],
       dtype=torch.float64)
	q_value: tensor([[-28.5762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09673937547511646, distance: 1.1984182423107954 entropy 0.8164090514183044
epoch: 2, step: 104
	action: tensor([[-2.5175e-01,  2.6105e-01,  6.2789e-01,  3.9274e-04,  2.4243e-01,
         -2.6681e-01,  7.8914e-01]], dtype=torch.float64)
	q_value: tensor([[-21.3982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08782779282350595, distance: 1.0929369557059356 entropy 0.8164090514183044
epoch: 2, step: 105
	action: tensor([[ 0.4736,  0.0750, -0.5072,  0.8651,  0.5228, -0.3338, -0.0383]],
       dtype=torch.float64)
	q_value: tensor([[-25.2452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7691239042797726, distance: 0.5498524671712746 entropy 0.8164090514183044
epoch: 2, step: 106
	action: tensor([[ 0.1718,  1.3574, -0.9243,  0.3285,  0.6682,  0.6342,  0.6912]],
       dtype=torch.float64)
	q_value: tensor([[-27.6676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 107
	action: tensor([[-0.7097,  0.3000,  0.6279, -0.0410,  0.6024,  0.4651,  0.7569]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17436221249300288, distance: 1.2401029125851317 entropy 0.8164090514183044
epoch: 2, step: 108
	action: tensor([[-0.5157,  1.1073, -0.3255,  0.4142, -0.9020, -0.1786, -0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-25.9915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 109
	action: tensor([[-0.0818,  0.0228, -1.3761, -0.1760,  0.0022,  0.0549,  0.4037]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3362520346153697, distance: 0.9323056039003529 entropy 0.8164090514183044
epoch: 2, step: 110
	action: tensor([[ 0.3906,  0.9806, -0.1325,  0.1656,  0.1896,  0.1406, -0.1753]],
       dtype=torch.float64)
	q_value: tensor([[-23.3467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 111
	action: tensor([[-0.1243,  0.2481, -0.3104,  0.0150, -0.5696, -0.2793,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3954926403916236, distance: 0.8897284740595254 entropy 0.8164090514183044
epoch: 2, step: 112
	action: tensor([[-0.2461, -0.1944,  0.1565, -0.0202, -0.5683, -1.0278,  1.4485]],
       dtype=torch.float64)
	q_value: tensor([[-19.4512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10934675332795285, distance: 1.2052866648643885 entropy 0.8164090514183044
epoch: 2, step: 113
	action: tensor([[ 0.5589,  0.6073, -0.5033, -0.1885, -0.3256,  0.1284, -0.3907]],
       dtype=torch.float64)
	q_value: tensor([[-34.6110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9394028604219075, distance: 0.2816973460511865 entropy 0.8164090514183044
epoch: 2, step: 114
	action: tensor([[ 0.1145, -1.6786, -0.1122,  0.5119,  0.7485,  0.1749,  0.2509]],
       dtype=torch.float64)
	q_value: tensor([[-21.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 115
	action: tensor([[-0.2893,  0.3010, -0.0592,  1.3427,  0.0791,  0.0491, -0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48446264639322767, distance: 0.821649815753772 entropy 0.8164090514183044
epoch: 2, step: 116
	action: tensor([[-0.6017, -0.2912, -0.0917, -0.5731,  0.7061, -0.4647, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-26.6313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8786942401395967, distance: 1.5685008024061746 entropy 0.8164090514183044
epoch: 2, step: 117
	action: tensor([[ 0.4996, -0.1573,  0.2655, -0.4483, -0.1316, -0.2465,  0.8631]],
       dtype=torch.float64)
	q_value: tensor([[-23.7187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19391771475971442, distance: 1.0274161349903168 entropy 0.8164090514183044
epoch: 2, step: 118
	action: tensor([[-1.0813,  0.1090, -0.0791, -0.1405, -0.1680, -0.2338,  0.4321]],
       dtype=torch.float64)
	q_value: tensor([[-25.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.028621998068759, distance: 1.629886109576114 entropy 0.8164090514183044
epoch: 2, step: 119
	action: tensor([[ 1.4472,  1.2841, -0.8469, -0.4158,  0.1386,  0.2858, -0.1351]],
       dtype=torch.float64)
	q_value: tensor([[-22.9439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 120
	action: tensor([[ 0.2599, -0.4422, -0.6193,  0.4935,  0.0438,  0.1752,  0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037190541033099, distance: 0.954880333030054 entropy 0.8164090514183044
epoch: 2, step: 121
	action: tensor([[ 0.6534,  0.2014, -0.0604,  0.0699, -0.2689, -0.2687,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-22.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8311029738378534, distance: 0.4702921121202094 entropy 0.8164090514183044
epoch: 2, step: 122
	action: tensor([[-1.1946,  0.0954, -0.7017,  0.5528, -0.3481,  0.5640, -0.6438]],
       dtype=torch.float64)
	q_value: tensor([[-21.4209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9462896508040297, distance: 1.5964687796099237 entropy 0.8164090514183044
epoch: 2, step: 123
	action: tensor([[ 0.0088,  0.4859,  0.0758,  0.6936, -0.3256, -0.8680, -0.4477]],
       dtype=torch.float64)
	q_value: tensor([[-24.2880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7483811696737332, distance: 0.5740216369704535 entropy 0.8164090514183044
epoch: 2, step: 124
	action: tensor([[ 0.2041, -0.0367, -0.0330,  0.3059,  0.7005, -0.3907,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-27.1287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.477911660722495, distance: 0.8268537302279372 entropy 0.8164090514183044
epoch: 2, step: 125
	action: tensor([[ 0.3930,  0.4148, -1.1153, -0.2000,  1.3354, -0.8439, -0.3931]],
       dtype=torch.float64)
	q_value: tensor([[-24.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7341363955513633, distance: 0.5900463626236923 entropy 0.8164090514183044
epoch: 2, step: 126
	action: tensor([[ 0.5678, -1.4898, -0.4961,  0.1961,  0.8037,  0.2687,  0.7882]],
       dtype=torch.float64)
	q_value: tensor([[-33.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3819409519117005, distance: 1.345245155190203 entropy 0.8164090514183044
epoch: 2, step: 127
	action: tensor([[ 0.2865,  1.3426,  0.4917, -0.9573, -0.5704,  0.3311, -0.3422]],
       dtype=torch.float64)
	q_value: tensor([[-33.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6351443402335111, distance: 0.6912214982839711 entropy 0.8164090514183044
LOSS epoch 2 actor 325.77685957825895 critic 225.0122299438243 
epoch: 3, step: 0
	action: tensor([[ 0.2870,  0.2672, -0.0467, -0.6376, -0.1391,  0.3494, -0.5831]],
       dtype=torch.float64)
	q_value: tensor([[-30.3252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5643124493123797, distance: 0.7553431039223084 entropy 0.8164090514183044
epoch: 3, step: 1
	action: tensor([[-0.3489, -0.2045, -0.3924,  0.0469,  0.1124,  0.5140, -0.1753]],
       dtype=torch.float64)
	q_value: tensor([[-20.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10936263246490396, distance: 1.2052952910416732 entropy 0.8164090514183044
epoch: 3, step: 2
	action: tensor([[-0.4329,  0.0683, -0.7605,  0.1938, -0.1484, -1.7773, -0.4598]],
       dtype=torch.float64)
	q_value: tensor([[-16.8925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26212621422428783, distance: 0.9829869088447372 entropy 0.8164090514183044
epoch: 3, step: 3
	action: tensor([[-0.5642,  0.3114, -0.1076, -0.7685,  0.2763, -0.3310,  0.1001]],
       dtype=torch.float64)
	q_value: tensor([[-32.1342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37359152608267787, distance: 1.3411751399276204 entropy 0.8164090514183044
epoch: 3, step: 4
	action: tensor([[ 0.1655, -0.3952,  0.4118,  1.0688,  0.3758,  0.3513,  0.2530]],
       dtype=torch.float64)
	q_value: tensor([[-17.7558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.881034770154985, distance: 0.3946996281774002 entropy 0.8164090514183044
epoch: 3, step: 5
	action: tensor([[0.5956, 0.0646, 1.0854, 0.2541, 0.6649, 0.6858, 0.7491]],
       dtype=torch.float64)
	q_value: tensor([[-27.5612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8435974211839801, distance: 0.452562586731322 entropy 0.8164090514183044
epoch: 3, step: 6
	action: tensor([[ 0.1852,  0.4266, -0.5845,  0.0069,  0.6261,  0.4171,  0.3257]],
       dtype=torch.float64)
	q_value: tensor([[-29.3821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7599368747390727, distance: 0.5606856242395132 entropy 0.8164090514183044
epoch: 3, step: 7
	action: tensor([[-0.1329, -0.3100,  0.5400, -0.1507,  1.2154, -0.4715,  0.8531]],
       dtype=torch.float64)
	q_value: tensor([[-19.0043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27809484675172236, distance: 1.2937138726399762 entropy 0.8164090514183044
epoch: 3, step: 8
	action: tensor([[ 1.0671,  0.1841, -0.1983, -1.1020, -0.5229,  0.2017,  0.1835]],
       dtype=torch.float64)
	q_value: tensor([[-30.5510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33136125889754775, distance: 0.9357341108073173 entropy 0.8164090514183044
epoch: 3, step: 9
	action: tensor([[ 0.0304,  0.3198, -1.1285, -0.5033, -0.3854,  0.0573, -1.4070]],
       dtype=torch.float64)
	q_value: tensor([[-28.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6808601902426363, distance: 0.6464682261033535 entropy 0.8164090514183044
epoch: 3, step: 10
	action: tensor([[ 0.3045,  0.2196,  0.3263,  0.4207, -0.2222,  0.6213,  0.1962]],
       dtype=torch.float64)
	q_value: tensor([[-24.7375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8946139338019916, distance: 0.37149099435696037 entropy 0.8164090514183044
epoch: 3, step: 11
	action: tensor([[ 0.4110, -1.3072, -0.3735, -0.1033,  0.2624, -0.0745, -0.4246]],
       dtype=torch.float64)
	q_value: tensor([[-20.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7557255612208214, distance: 1.5162995610810897 entropy 0.8164090514183044
epoch: 3, step: 12
	action: tensor([[-0.0204, -0.4900,  0.5498, -0.3705, -0.7568, -0.6162,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-26.1197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4465637253145258, distance: 1.3763391971183891 entropy 0.8164090514183044
epoch: 3, step: 13
	action: tensor([[-0.2743,  0.4910,  0.2921, -0.1624,  0.4219, -0.3171, -0.1280]],
       dtype=torch.float64)
	q_value: tensor([[-25.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1335881482138621, distance: 1.065169900046161 entropy 0.8164090514183044
epoch: 3, step: 14
	action: tensor([[ 1.7806,  0.2904, -0.3012, -0.2900, -0.5727, -0.3717, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-18.1147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2811607133383741, distance: 0.9702253005815407 entropy 0.8164090514183044
epoch: 3, step: 15
	action: tensor([[ 0.5257,  1.3146,  0.3274, -0.4583,  0.1160,  0.5846, -0.1460]],
       dtype=torch.float64)
	q_value: tensor([[-29.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 16
	action: tensor([[ 0.9704,  0.4056, -0.5345, -0.4092, -0.1651,  0.6155, -0.0537]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.923396398675035, distance: 0.31672422851752746 entropy 0.8164090514183044
epoch: 3, step: 17
	action: tensor([[-0.1132, -0.7211,  0.2269, -0.3058,  0.4307,  0.1903, -0.1941]],
       dtype=torch.float64)
	q_value: tensor([[-23.5273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.557398446617628, distance: 1.4280932349803104 entropy 0.8164090514183044
epoch: 3, step: 18
	action: tensor([[ 0.5279,  1.3450,  0.8313,  0.0392,  0.4812,  0.5808, -0.1088]],
       dtype=torch.float64)
	q_value: tensor([[-20.8157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 19
	action: tensor([[ 1.3936, -0.5151, -0.9266, -0.4612,  0.3944,  0.0345, -0.8338]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21802900904108635, distance: 1.2629481152898587 entropy 0.8164090514183044
epoch: 3, step: 20
	action: tensor([[-0.6377, -0.1893,  0.1106, -0.8065,  0.6071, -0.8499, -0.2619]],
       dtype=torch.float64)
	q_value: tensor([[-30.2440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8549283124141511, distance: 1.5585482731813995 entropy 0.8164090514183044
epoch: 3, step: 21
	action: tensor([[-0.6254, -0.4269,  0.6456,  0.3152,  0.0707,  0.2135, -0.4656]],
       dtype=torch.float64)
	q_value: tensor([[-23.6493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4271917864579826, distance: 1.3670923784033946 entropy 0.8164090514183044
epoch: 3, step: 22
	action: tensor([[ 0.0527, -0.4365,  0.4046, -0.3940, -0.8160, -0.1732,  0.6683]],
       dtype=torch.float64)
	q_value: tensor([[-23.1964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3626296691856876, distance: 1.335812840098379 entropy 0.8164090514183044
epoch: 3, step: 23
	action: tensor([[ 0.6311,  1.5467, -0.2560,  0.1236, -1.0685,  0.1652,  0.4114]],
       dtype=torch.float64)
	q_value: tensor([[-25.0832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 24
	action: tensor([[ 0.6189,  0.0954, -1.2388, -0.7090, -0.0805, -1.0818, -0.2368]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6618138781397485, distance: 0.665479350410257 entropy 0.8164090514183044
epoch: 3, step: 25
	action: tensor([[ 0.5260,  0.1174, -0.0141,  0.4363, -0.0458, -0.5900,  1.0425]],
       dtype=torch.float64)
	q_value: tensor([[-27.5945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8240402394194563, distance: 0.4800244674350671 entropy 0.8164090514183044
epoch: 3, step: 26
	action: tensor([[ 0.0429,  0.5136,  0.7390,  0.6054, -0.3185,  0.2315, -0.2762]],
       dtype=torch.float64)
	q_value: tensor([[-25.6396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8947478634631175, distance: 0.37125486503660543 entropy 0.8164090514183044
epoch: 3, step: 27
	action: tensor([[ 0.3636,  0.2849, -1.3512,  0.3053, -1.1727,  0.6340, -0.0406]],
       dtype=torch.float64)
	q_value: tensor([[-23.3592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5261740695455707, distance: 0.7877095400973176 entropy 0.8164090514183044
epoch: 3, step: 28
	action: tensor([[ 0.4155,  0.4455,  0.1808,  0.7050,  0.0570, -0.5408,  0.0854]],
       dtype=torch.float64)
	q_value: tensor([[-25.7199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9069742117330182, distance: 0.3490264762353776 entropy 0.8164090514183044
epoch: 3, step: 29
	action: tensor([[ 0.5415, -0.0482,  0.1603, -0.2532, -0.5953,  0.8110, -0.2296]],
       dtype=torch.float64)
	q_value: tensor([[-22.9516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6864776180226704, distance: 0.640753472208823 entropy 0.8164090514183044
epoch: 3, step: 30
	action: tensor([[-0.3845, -0.1321,  0.7306,  0.0831, -0.6044, -0.0307, -0.0990]],
       dtype=torch.float64)
	q_value: tensor([[-24.2442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1577094263256389, distance: 1.23127901567904 entropy 0.8164090514183044
epoch: 3, step: 31
	action: tensor([[ 0.6503, -0.1306, -0.5842,  0.4062, -0.1709,  0.1968,  0.5563]],
       dtype=torch.float64)
	q_value: tensor([[-22.5637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7534164120625599, distance: 0.5682491267973813 entropy 0.8164090514183044
epoch: 3, step: 32
	action: tensor([[-0.0526, -0.8595, -0.6384,  0.0533, -1.3001, -0.1150,  0.1633]],
       dtype=torch.float64)
	q_value: tensor([[-20.9780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4571018307990584, distance: 1.3813433625151923 entropy 0.8164090514183044
epoch: 3, step: 33
	action: tensor([[ 1.4493, -0.8485, -0.3591,  0.4665, -0.2417, -0.4713,  0.2801]],
       dtype=torch.float64)
	q_value: tensor([[-26.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11974210681676967, distance: 1.2109206856802657 entropy 0.8164090514183044
epoch: 3, step: 34
	action: tensor([[ 0.9508,  1.0109, -0.6708,  0.1938, -0.8439, -0.5395,  0.1218]],
       dtype=torch.float64)
	q_value: tensor([[-33.5371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 35
	action: tensor([[-0.7461, -0.1496, -0.2035,  0.4559, -0.1510, -0.1062, -1.3004]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5543463831835975, distance: 1.4266932179613956 entropy 0.8164090514183044
epoch: 3, step: 36
	action: tensor([[-0.3162,  0.1348, -0.9260,  0.0099,  0.6100, -0.8456,  0.5695]],
       dtype=torch.float64)
	q_value: tensor([[-24.5644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026481926961710145, distance: 1.1290903678568065 entropy 0.8164090514183044
epoch: 3, step: 37
	action: tensor([[ 0.3558,  0.8841,  1.0778, -0.0693,  0.5595,  0.0326, -0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-24.4642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8348759511648166, distance: 0.4650095345505127 entropy 0.8164090514183044
epoch: 3, step: 38
	action: tensor([[ 0.2522,  0.0028, -0.2340, -0.0941, -0.6476, -0.3913, -0.7627]],
       dtype=torch.float64)
	q_value: tensor([[-25.1786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49984624003883926, distance: 0.8092979909499826 entropy 0.8164090514183044
epoch: 3, step: 39
	action: tensor([[ 0.0484, -0.1649, -0.6620, -0.4976,  0.1066, -0.5204, -0.1170]],
       dtype=torch.float64)
	q_value: tensor([[-21.8803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17823323821674086, distance: 1.0373635376846213 entropy 0.8164090514183044
epoch: 3, step: 40
	action: tensor([[-0.5172, -0.4210,  0.0545, -0.2136, -0.6301,  0.2002,  0.5264]],
       dtype=torch.float64)
	q_value: tensor([[-19.3577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7060780985268036, distance: 1.494707271118377 entropy 0.8164090514183044
epoch: 3, step: 41
	action: tensor([[-0.0719,  0.9138, -0.7329, -0.6525, -0.0712, -0.1674,  0.4757]],
       dtype=torch.float64)
	q_value: tensor([[-21.4609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.780878319449779, distance: 0.5356725208251961 entropy 0.8164090514183044
epoch: 3, step: 42
	action: tensor([[-0.0021,  0.5259, -0.1814,  0.6761, -0.1940, -0.2503, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-20.9362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6131311378456894, distance: 0.7117682006855989 entropy 0.8164090514183044
epoch: 3, step: 43
	action: tensor([[ 0.0372,  0.6106, -0.2281,  0.2982, -0.0219,  0.0646, -0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-18.8405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6388177844060401, distance: 0.6877330130378612 entropy 0.8164090514183044
epoch: 3, step: 44
	action: tensor([[-0.0536,  0.4159, -0.2874,  0.0092, -0.3189, -0.4131,  0.1034]],
       dtype=torch.float64)
	q_value: tensor([[-16.0510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5470784056629661, distance: 0.7701373867588619 entropy 0.8164090514183044
epoch: 3, step: 45
	action: tensor([[-0.1033,  0.5403, -0.2986, -0.5530, -0.2311, -0.4881, -0.2171]],
       dtype=torch.float64)
	q_value: tensor([[-17.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48252625063622334, distance: 0.8231914576836618 entropy 0.8164090514183044
epoch: 3, step: 46
	action: tensor([[-0.1061, -0.1029, -0.0488, -0.2474, -0.4772, -0.3137, -0.5449]],
       dtype=torch.float64)
	q_value: tensor([[-18.5593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00898003663691449, distance: 1.1391945401099735 entropy 0.8164090514183044
epoch: 3, step: 47
	action: tensor([[ 0.4843,  0.2405,  0.3008, -0.6091,  0.0640,  0.3105,  0.4462]],
       dtype=torch.float64)
	q_value: tensor([[-19.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6103107080546543, distance: 0.7143580274378398 entropy 0.8164090514183044
epoch: 3, step: 48
	action: tensor([[ 0.4377, -0.2545, -0.5835,  0.4922,  0.6273,  0.5715, -0.5699]],
       dtype=torch.float64)
	q_value: tensor([[-21.3155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7291240486214592, distance: 0.5955824858925759 entropy 0.8164090514183044
epoch: 3, step: 49
	action: tensor([[-0.1072, -0.0218,  0.5069, -0.7986, -0.0199, -0.4378,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-23.3167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40948300459704456, distance: 1.3585843736310825 entropy 0.8164090514183044
epoch: 3, step: 50
	action: tensor([[ 0.9405, -0.3978,  0.4913,  0.1361,  0.1959, -0.5198, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[-21.7186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35867589592627835, distance: 0.9164219331862897 entropy 0.8164090514183044
epoch: 3, step: 51
	action: tensor([[ 1.1871, -0.2281, -0.7001,  0.0842,  0.6356, -0.1254, -1.0502]],
       dtype=torch.float64)
	q_value: tensor([[-28.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4715743822896643, distance: 0.8318569035280602 entropy 0.8164090514183044
epoch: 3, step: 52
	action: tensor([[ 0.9597,  0.9400, -0.3573, -0.9932, -0.6648, -0.3583, -0.5564]],
       dtype=torch.float64)
	q_value: tensor([[-30.1885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8305237179769851, distance: 0.4710978879619071 entropy 0.8164090514183044
epoch: 3, step: 53
	action: tensor([[ 0.1140, -0.2132, -0.1410, -0.0011, -0.7421, -0.2772, -0.2955]],
       dtype=torch.float64)
	q_value: tensor([[-29.1127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2674867999817152, distance: 0.9794097449386254 entropy 0.8164090514183044
epoch: 3, step: 54
	action: tensor([[ 0.6576,  0.3456,  0.5973,  1.3273, -0.0097,  0.2988,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-20.3241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7902908723338471, distance: 0.5240411127130196 entropy 0.8164090514183044
epoch: 3, step: 55
	action: tensor([[ 0.1385,  0.3431, -0.2622, -0.3655,  0.2233, -0.0675,  0.4829]],
       dtype=torch.float64)
	q_value: tensor([[-29.4251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5395987769618398, distance: 0.7764704412088751 entropy 0.8164090514183044
epoch: 3, step: 56
	action: tensor([[ 0.0447,  0.7783, -0.3759,  0.3119,  0.4126, -0.2764, -0.0707]],
       dtype=torch.float64)
	q_value: tensor([[-17.4025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6192172182957709, distance: 0.7061473673969061 entropy 0.8164090514183044
epoch: 3, step: 57
	action: tensor([[ 0.9181,  0.4032,  1.5898, -0.4588, -0.0821,  0.7613, -0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-18.6877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9761895151153377, distance: 0.1765797156280514 entropy 0.8164090514183044
epoch: 3, step: 58
	action: tensor([[ 0.7233,  0.3050, -0.0364,  0.1128,  0.0599,  0.7134, -0.1709]],
       dtype=torch.float64)
	q_value: tensor([[-34.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09688373705843133 entropy 0.8164090514183044
epoch: 3, step: 59
	action: tensor([[ 0.1152,  0.1642, -0.0306,  0.2837,  0.2980,  0.6337,  0.7994]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7480645702568645, distance: 0.5743826548327545 entropy 0.8164090514183044
epoch: 3, step: 60
	action: tensor([[ 0.6210,  0.2577, -0.3497, -0.5839, -0.7217,  0.3696,  0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-20.8370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6951597161947458, distance: 0.6318192765367481 entropy 0.8164090514183044
epoch: 3, step: 61
	action: tensor([[-0.3432,  0.2495, -0.5603,  0.0616, -0.3005,  0.2377, -0.1522]],
       dtype=torch.float64)
	q_value: tensor([[-23.8926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08407823849382334, distance: 1.0951809529368282 entropy 0.8164090514183044
epoch: 3, step: 62
	action: tensor([[ 1.0731, -0.2204, -0.9599,  0.6082,  0.2825, -0.7321,  0.3349]],
       dtype=torch.float64)
	q_value: tensor([[-15.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6226057160521185, distance: 0.7029984254032279 entropy 0.8164090514183044
epoch: 3, step: 63
	action: tensor([[ 0.8651, -0.0369,  0.2006, -0.5927, -0.3901, -0.5936, -0.5083]],
       dtype=torch.float64)
	q_value: tensor([[-31.1201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26945216494988133, distance: 0.9780949628409996 entropy 0.8164090514183044
epoch: 3, step: 64
	action: tensor([[-1.1942,  0.4038,  0.3260, -0.0055, -1.3530, -0.0682,  0.7831]],
       dtype=torch.float64)
	q_value: tensor([[-26.5239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8226732468977918, distance: 1.544938168596565 entropy 0.8164090514183044
epoch: 3, step: 65
	action: tensor([[ 0.7088,  0.0683,  0.6877, -0.3897, -0.9766,  0.0122, -0.6749]],
       dtype=torch.float64)
	q_value: tensor([[-28.7032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5900593149826728, distance: 0.732684838113857 entropy 0.8164090514183044
epoch: 3, step: 66
	action: tensor([[ 0.2461,  0.2282,  0.3602, -0.1187, -0.4547,  0.0184,  0.4456]],
       dtype=torch.float64)
	q_value: tensor([[-30.1470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6104691376993214, distance: 0.7142128002066707 entropy 0.8164090514183044
epoch: 3, step: 67
	action: tensor([[ 0.6728,  0.2582, -1.1906, -0.7113, -0.2336, -0.3235,  0.2336]],
       dtype=torch.float64)
	q_value: tensor([[-19.6737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7482474349417405, distance: 0.5741741621850295 entropy 0.8164090514183044
epoch: 3, step: 68
	action: tensor([[-0.1523, -0.1220, -0.0699,  0.3312,  0.6757,  0.9301,  0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-23.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5537710491744463, distance: 0.7644262022656081 entropy 0.8164090514183044
epoch: 3, step: 69
	action: tensor([[ 1.2741, -0.7721,  0.0651, -0.6919, -0.5021, -0.3368,  0.4091]],
       dtype=torch.float64)
	q_value: tensor([[-21.4901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6286923679852781, distance: 1.4604147947081765 entropy 0.8164090514183044
epoch: 3, step: 70
	action: tensor([[ 0.5672, -0.0855,  0.4380, -2.0270,  0.2853, -0.1179, -0.2331]],
       dtype=torch.float64)
	q_value: tensor([[-31.3193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24442621505401996, distance: 1.27656010853137 entropy 0.8164090514183044
epoch: 3, step: 71
	action: tensor([[-0.5674,  0.1159, -0.4906,  0.1332, -0.1811, -0.3600,  0.3172]],
       dtype=torch.float64)
	q_value: tensor([[-31.8163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2689346119879934, distance: 1.289069446959321 entropy 0.8164090514183044
epoch: 3, step: 72
	action: tensor([[ 0.2831,  0.4493,  0.0900, -0.3921,  0.0115,  0.6600, -0.3210]],
       dtype=torch.float64)
	q_value: tensor([[-18.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7613952355159197, distance: 0.5589799738129363 entropy 0.8164090514183044
epoch: 3, step: 73
	action: tensor([[-0.0623, -0.5722,  0.2587, -0.2566, -0.1519, -0.0327, -0.4585]],
       dtype=torch.float64)
	q_value: tensor([[-20.2144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4236626489611668, distance: 1.365401069885 entropy 0.8164090514183044
epoch: 3, step: 74
	action: tensor([[ 0.7169, -0.2096, -0.5823,  0.2054,  0.0560,  0.2114,  0.1953]],
       dtype=torch.float64)
	q_value: tensor([[-20.5562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6784217784521995, distance: 0.6489332211914385 entropy 0.8164090514183044
epoch: 3, step: 75
	action: tensor([[ 0.0149,  0.8464, -0.1987, -0.5290, -0.0157,  0.9930,  0.3752]],
       dtype=torch.float64)
	q_value: tensor([[-19.9387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.69472707323353, distance: 0.6322674706528674 entropy 0.8164090514183044
epoch: 3, step: 76
	action: tensor([[ 0.8800,  1.1232, -0.7615,  0.5693, -0.1181,  0.3430, -0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-23.2121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 77
	action: tensor([[ 0.8904,  0.5833, -0.2482, -0.1029, -0.0311,  0.2624,  1.3052]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.989524615832168, distance: 0.11712286106937587 entropy 0.8164090514183044
epoch: 3, step: 78
	action: tensor([[-0.0638,  0.0985,  0.5483, -0.9279,  0.9693,  0.4114, -0.1787]],
       dtype=torch.float64)
	q_value: tensor([[-25.7037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1500821227238287, distance: 1.2272163130725107 entropy 0.8164090514183044
epoch: 3, step: 79
	action: tensor([[ 0.9464,  0.1374,  0.2951, -0.0084, -0.6732, -0.5915,  0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-22.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7149593613222118, distance: 0.61095621139204 entropy 0.8164090514183044
epoch: 3, step: 80
	action: tensor([[0.5790, 0.3018, 0.4449, 0.8709, 0.3042, 0.2903, 0.0784]],
       dtype=torch.float64)
	q_value: tensor([[-26.1387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9669362963059122, distance: 0.20808100818340833 entropy 0.8164090514183044
epoch: 3, step: 81
	action: tensor([[ 0.3234,  0.0954,  0.3222,  0.5533, -0.3722, -0.3671,  1.2853]],
       dtype=torch.float64)
	q_value: tensor([[-24.5575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8365223608520784, distance: 0.4626854812931989 entropy 0.8164090514183044
epoch: 3, step: 82
	action: tensor([[ 0.8383,  0.5635,  0.2702, -1.5544,  0.0434,  0.2293, -0.5888]],
       dtype=torch.float64)
	q_value: tensor([[-26.7717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5110247187653623, distance: 0.8002029458407842 entropy 0.8164090514183044
epoch: 3, step: 83
	action: tensor([[ 0.1580, -0.0347,  0.4699, -0.0358, -0.9013, -0.7366,  0.2572]],
       dtype=torch.float64)
	q_value: tensor([[-29.6626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31814420597669446, distance: 0.9449372313119764 entropy 0.8164090514183044
epoch: 3, step: 84
	action: tensor([[ 0.3538, -0.4018, -0.7442,  0.5028, -0.3762,  0.1049,  0.8483]],
       dtype=torch.float64)
	q_value: tensor([[-24.9715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35735408378576194, distance: 0.9173658507076526 entropy 0.8164090514183044
epoch: 3, step: 85
	action: tensor([[ 0.4761,  1.7432,  0.5593, -0.3972,  0.1662,  0.6042,  0.4705]],
       dtype=torch.float64)
	q_value: tensor([[-23.1076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 86
	action: tensor([[ 0.1972,  0.4772,  0.3738,  0.2019,  0.6294,  0.6553, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9090379672892327, distance: 0.3451332266727161 entropy 0.8164090514183044
epoch: 3, step: 87
	action: tensor([[ 0.9870,  0.2279, -0.5223,  0.7883,  1.1473,  0.6816, -0.3573]],
       dtype=torch.float64)
	q_value: tensor([[-19.9283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9763240476982047, distance: 0.1760801588934878 entropy 0.8164090514183044
epoch: 3, step: 88
	action: tensor([[-0.1074, -0.5777, -1.0817, -0.6605,  1.0512,  0.5823,  0.1677]],
       dtype=torch.float64)
	q_value: tensor([[-30.3682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10747136696781867, distance: 1.0811047413851074 entropy 0.8164090514183044
epoch: 3, step: 89
	action: tensor([[ 0.3245, -0.2123,  0.8826, -0.4997, -0.3830,  0.2432,  0.1669]],
       dtype=torch.float64)
	q_value: tensor([[-26.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14372949135742596, distance: 1.058917648375577 entropy 0.8164090514183044
epoch: 3, step: 90
	action: tensor([[-0.2120,  0.0298, -0.4514, -1.2951,  0.0309, -0.5187, -0.2134]],
       dtype=torch.float64)
	q_value: tensor([[-25.1486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006967097770323338, distance: 1.1483237138597087 entropy 0.8164090514183044
epoch: 3, step: 91
	action: tensor([[ 1.0101,  0.1830, -0.3490, -0.2186,  0.2452, -0.9215,  0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-21.9708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5872692720031415, distance: 0.7351739247331791 entropy 0.8164090514183044
epoch: 3, step: 92
	action: tensor([[ 0.9462, -0.3560, -0.3199,  1.3272, -0.3174,  0.7531,  0.0789]],
       dtype=torch.float64)
	q_value: tensor([[-27.0088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9373547279912557, distance: 0.2864183528996245 entropy 0.8164090514183044
epoch: 3, step: 93
	action: tensor([[ 0.4336, -0.3381, -0.2125,  0.6055,  0.7814,  0.2546,  0.2238]],
       dtype=torch.float64)
	q_value: tensor([[-30.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7276653164270458, distance: 0.5971840101568818 entropy 0.8164090514183044
epoch: 3, step: 94
	action: tensor([[ 0.0352,  0.5093, -0.1817, -0.2440, -0.4096,  0.8277,  0.1994]],
       dtype=torch.float64)
	q_value: tensor([[-24.5984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.58638712397265, distance: 0.7359591655952389 entropy 0.8164090514183044
epoch: 3, step: 95
	action: tensor([[ 0.1158, -0.5481,  0.5479,  0.4484, -1.3370, -0.3951,  0.3243]],
       dtype=torch.float64)
	q_value: tensor([[-21.2746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2765968679229398, distance: 0.9733003636854588 entropy 0.8164090514183044
epoch: 3, step: 96
	action: tensor([[-0.0706,  0.6231,  0.6271,  0.5958,  0.8649, -0.0085, -0.1026]],
       dtype=torch.float64)
	q_value: tensor([[-29.5260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.795653031337832, distance: 0.5172979925829458 entropy 0.8164090514183044
epoch: 3, step: 97
	action: tensor([[-0.1996,  1.0767, -1.0469,  0.1337, -0.4153, -0.3982,  1.1108]],
       dtype=torch.float64)
	q_value: tensor([[-22.9240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 98
	action: tensor([[-0.6947,  1.8581,  0.0419,  0.6849,  0.3350,  0.1672,  0.2075]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 99
	action: tensor([[ 0.3246,  0.3958,  0.2492,  0.2312, -0.1843,  1.0883,  0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8761900507015747, distance: 0.40265627000539306 entropy 0.8164090514183044
epoch: 3, step: 100
	action: tensor([[-0.4031,  0.6163, -0.8046,  0.4501,  0.1386,  0.0410, -0.1117]],
       dtype=torch.float64)
	q_value: tensor([[-23.2579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10554437728523569, distance: 1.0822711770357978 entropy 0.8164090514183044
epoch: 3, step: 101
	action: tensor([[-0.1920,  0.1382,  0.5088, -0.6293, -0.9347, -0.3109,  0.6315]],
       dtype=torch.float64)
	q_value: tensor([[-17.6767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1952421540235516, distance: 1.251078739962961 entropy 0.8164090514183044
epoch: 3, step: 102
	action: tensor([[ 1.3786,  0.8799, -1.0412, -0.6017, -0.3586, -0.5619,  0.7061]],
       dtype=torch.float64)
	q_value: tensor([[-25.9803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4748367495778155, distance: 0.8292850899609634 entropy 0.8164090514183044
epoch: 3, step: 103
	action: tensor([[ 0.6786,  0.3836,  1.4217,  0.0537, -0.2741,  0.9668, -0.8323]],
       dtype=torch.float64)
	q_value: tensor([[-30.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9824429400382181, distance: 0.15162911125658834 entropy 0.8164090514183044
epoch: 3, step: 104
	action: tensor([[-0.1898,  0.4547,  0.3108, -0.8179, -0.3965,  1.0629, -1.0710]],
       dtype=torch.float64)
	q_value: tensor([[-34.5338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23109540888018076, distance: 1.0034434912899088 entropy 0.8164090514183044
epoch: 3, step: 105
	action: tensor([[ 0.4918, -0.4126, -0.4957, -0.2754, -0.8722, -1.0814,  0.2000]],
       dtype=torch.float64)
	q_value: tensor([[-29.0168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3335630007440744, distance: 0.9341922140765622 entropy 0.8164090514183044
epoch: 3, step: 106
	action: tensor([[ 1.2386, -0.2107, -0.7324, -0.2822, -0.1206, -0.5446,  0.7318]],
       dtype=torch.float64)
	q_value: tensor([[-27.6159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13719827853347055, distance: 1.0629484298365657 entropy 0.8164090514183044
epoch: 3, step: 107
	action: tensor([[ 0.2648, -0.2739,  0.3190,  0.0114, -0.4603, -0.3547, -0.5362]],
       dtype=torch.float64)
	q_value: tensor([[-28.1409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2454872231187547, distance: 0.9940082581896961 entropy 0.8164090514183044
epoch: 3, step: 108
	action: tensor([[ 0.7279,  0.0255,  0.4272, -0.1726, -0.1449,  0.0599,  0.2654]],
       dtype=torch.float64)
	q_value: tensor([[-22.5782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6893216532272843, distance: 0.6378406387490335 entropy 0.8164090514183044
epoch: 3, step: 109
	action: tensor([[-0.0497, -0.3525,  0.7835,  0.1230, -0.9394, -0.3756,  0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-21.7887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0019448935322829985, distance: 1.1454565273185597 entropy 0.8164090514183044
epoch: 3, step: 110
	action: tensor([[ 0.3903,  0.5907,  0.0007, -0.1313,  0.2123, -0.0446, -0.5662]],
       dtype=torch.float64)
	q_value: tensor([[-26.2454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8482745581231336, distance: 0.44574440310990054 entropy 0.8164090514183044
epoch: 3, step: 111
	action: tensor([[ 0.0913, -0.1509, -0.2011,  0.0288, -0.3341,  0.1188,  0.2237]],
       dtype=torch.float64)
	q_value: tensor([[-19.0590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3153571466218742, distance: 0.9468664592210011 entropy 0.8164090514183044
epoch: 3, step: 112
	action: tensor([[ 0.4974,  0.3419,  0.3540, -0.1943, -0.0815, -0.0505, -0.2969]],
       dtype=torch.float64)
	q_value: tensor([[-16.7294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7828979074024937, distance: 0.5331982287425349 entropy 0.8164090514183044
epoch: 3, step: 113
	action: tensor([[-1.0178, -0.2415, -0.3575, -0.4239,  0.5867,  0.0774,  0.7577]],
       dtype=torch.float64)
	q_value: tensor([[-20.6335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1056755811851384, distance: 1.6605517844896784 entropy 0.8164090514183044
epoch: 3, step: 114
	action: tensor([[-0.1572,  0.2950,  0.0644, -0.8430, -0.2953,  0.3268,  0.4169]],
       dtype=torch.float64)
	q_value: tensor([[-24.8425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012251193150787598, distance: 1.1373128606503258 entropy 0.8164090514183044
epoch: 3, step: 115
	action: tensor([[ 0.4255, -0.3933,  0.6875, -0.7261,  0.1163,  0.0601,  0.8113]],
       dtype=torch.float64)
	q_value: tensor([[-21.1713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17075131313526315, distance: 1.2381949261954213 entropy 0.8164090514183044
epoch: 3, step: 116
	action: tensor([[ 0.7226,  0.3776, -0.5099, -0.3622,  0.3425,  0.0773, -0.1085]],
       dtype=torch.float64)
	q_value: tensor([[-26.6312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8718854523368359, distance: 0.4095961982986931 entropy 0.8164090514183044
epoch: 3, step: 117
	action: tensor([[ 1.1688,  0.2692,  1.2797,  0.6129, -0.2369, -0.5632, -0.4843]],
       dtype=torch.float64)
	q_value: tensor([[-20.1928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7688165959468718, distance: 0.5502182868839433 entropy 0.8164090514183044
epoch: 3, step: 118
	action: tensor([[-0.4298, -0.2845,  0.2363, -0.1299, -0.8808, -0.2130,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-36.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45330761066929726, distance: 1.379543715766171 entropy 0.8164090514183044
epoch: 3, step: 119
	action: tensor([[-0.1044, -0.4654, -0.0246, -0.6692, -0.3595,  0.7293,  0.4524]],
       dtype=torch.float64)
	q_value: tensor([[-21.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37426972854851237, distance: 1.3415061976043505 entropy 0.8164090514183044
epoch: 3, step: 120
	action: tensor([[-0.4158, -0.0388,  0.1276,  1.0984, -0.8194,  0.7506, -0.6940]],
       dtype=torch.float64)
	q_value: tensor([[-23.4421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3270142951800792, distance: 0.9387708870917022 entropy 0.8164090514183044
epoch: 3, step: 121
	action: tensor([[ 0.4236,  1.3768, -1.0306, -0.2475, -0.5437, -0.5027,  0.6047]],
       dtype=torch.float64)
	q_value: tensor([[-25.3897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 122
	action: tensor([[ 0.3317,  0.3834,  0.2831, -0.6200, -1.0666,  0.2792,  0.3934]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5450844535856457, distance: 0.7718307600300575 entropy 0.8164090514183044
epoch: 3, step: 123
	action: tensor([[-0.0092,  0.6844, -0.1664,  0.3180, -0.4365, -0.3969, -0.3474]],
       dtype=torch.float64)
	q_value: tensor([[-27.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6605644875201878, distance: 0.6667074864877884 entropy 0.8164090514183044
epoch: 3, step: 124
	action: tensor([[ 0.5038,  1.2767, -0.5199, -0.5822,  0.0417, -0.0064,  0.8195]],
       dtype=torch.float64)
	q_value: tensor([[-19.0826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 125
	action: tensor([[ 1.2466, -0.1827, -0.7069,  0.1637, -0.0523, -0.0516,  0.8543]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.584487763340457, distance: 0.7376470370864763 entropy 0.8164090514183044
epoch: 3, step: 126
	action: tensor([[ 0.0682,  0.5549,  0.2420,  0.1458,  0.8690, -0.8058, -0.8127]],
       dtype=torch.float64)
	q_value: tensor([[-27.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5155133143161525, distance: 0.7965217084029345 entropy 0.8164090514183044
epoch: 3, step: 127
	action: tensor([[ 0.1287,  0.8102,  0.0300, -1.0169, -0.6169, -0.1676,  0.4938]],
       dtype=torch.float64)
	q_value: tensor([[-25.6783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5508566200230933, distance: 0.7669184656563853 entropy 0.8164090514183044
LOSS epoch 3 actor 346.3095410775203 critic 1321.7696880605195 
epoch: 4, step: 0
	action: tensor([[ 0.7923, -0.0781, -0.3206, -0.9240,  0.1808,  0.4534,  0.2708]],
       dtype=torch.float64)
	q_value: tensor([[-21.1950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3244929159305777, distance: 0.9405278222974273 entropy 0.7110485434532166
epoch: 4, step: 1
	action: tensor([[ 1.2104,  0.1780,  0.3467, -0.6965,  0.2621,  0.5395, -0.4695]],
       dtype=torch.float64)
	q_value: tensor([[-19.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6309937185059853, distance: 0.69514207001829 entropy 0.7110485434532166
epoch: 4, step: 2
	action: tensor([[ 0.5710,  0.4715,  0.3958, -0.4260, -0.6834, -0.5648, -0.0626]],
       dtype=torch.float64)
	q_value: tensor([[-22.9182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7632759770052788, distance: 0.5567726065776247 entropy 0.7110485434532166
epoch: 4, step: 3
	action: tensor([[ 0.6646, -0.1394, -0.3497, -0.0557, -0.5076,  0.3489,  1.2272]],
       dtype=torch.float64)
	q_value: tensor([[-20.6456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6292191683553299, distance: 0.6968115334260795 entropy 0.7110485434532166
epoch: 4, step: 4
	action: tensor([[-0.1100,  0.3962, -0.1548, -0.0764, -0.3558,  0.3085, -0.2153]],
       dtype=torch.float64)
	q_value: tensor([[-21.7849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43773391328802735, distance: 0.8580797203926652 entropy 0.7110485434532166
epoch: 4, step: 5
	action: tensor([[-0.1906, -0.2898,  0.4438,  0.6483, -0.0345,  0.4218,  0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-14.2448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5003631438937409, distance: 0.8088796821963203 entropy 0.7110485434532166
epoch: 4, step: 6
	action: tensor([[-0.6069, -0.8636, -0.2256, -0.1544,  0.5835, -0.5305,  0.3268]],
       dtype=torch.float64)
	q_value: tensor([[-19.2217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0858693393061563, distance: 1.6527236563939007 entropy 0.7110485434532166
epoch: 4, step: 7
	action: tensor([[ 0.0337,  0.2465, -0.0540,  0.4629,  0.1842,  0.2421, -0.3389]],
       dtype=torch.float64)
	q_value: tensor([[-22.6412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.630324977832967, distance: 0.6957716790138186 entropy 0.7110485434532166
epoch: 4, step: 8
	action: tensor([[ 0.8526,  0.5715,  0.0124, -0.0917,  0.3298,  0.0522,  0.0749]],
       dtype=torch.float64)
	q_value: tensor([[-14.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.983685298922647, distance: 0.14616596350849484 entropy 0.7110485434532166
epoch: 4, step: 9
	action: tensor([[-0.2268,  0.2075, -0.2934, -0.6140, -1.1822,  0.0679,  0.3178]],
       dtype=torch.float64)
	q_value: tensor([[-17.5750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07397485945071403, distance: 1.1012047651125974 entropy 0.7110485434532166
epoch: 4, step: 10
	action: tensor([[ 0.2322, -0.7768,  0.9670,  0.7388,  0.4464, -0.5510,  0.5744]],
       dtype=torch.float64)
	q_value: tensor([[-20.5428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18713336494251975, distance: 1.031730673092224 entropy 0.7110485434532166
epoch: 4, step: 11
	action: tensor([[-0.0588,  0.3250, -0.2120,  0.2660,  0.7844,  0.2578, -0.4765]],
       dtype=torch.float64)
	q_value: tensor([[-28.9605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5851131954843678, distance: 0.7370916720886516 entropy 0.7110485434532166
epoch: 4, step: 12
	action: tensor([[ 0.2709,  0.1718,  0.2183, -0.4563, -1.2553,  0.2291, -0.6718]],
       dtype=torch.float64)
	q_value: tensor([[-16.0439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42011112461390265, distance: 0.8714231275568208 entropy 0.7110485434532166
epoch: 4, step: 13
	action: tensor([[ 0.4036,  0.1403, -0.4383, -0.3954,  0.2600, -0.1386,  0.5365]],
       dtype=torch.float64)
	q_value: tensor([[-23.7620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5799785586252195, distance: 0.7416387679243549 entropy 0.7110485434532166
epoch: 4, step: 14
	action: tensor([[ 0.7983,  0.7446,  0.5664,  0.1220,  0.5253, -0.5141, -0.2662]],
       dtype=torch.float64)
	q_value: tensor([[-15.9086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9774242635415432, distance: 0.17194029066842956 entropy 0.7110485434532166
epoch: 4, step: 15
	action: tensor([[ 1.5353,  0.5719, -0.3292,  0.8305, -0.1846,  0.2007,  0.8500]],
       dtype=torch.float64)
	q_value: tensor([[-21.4373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 16
	action: tensor([[-0.0188,  0.1093,  0.7410, -0.4763, -0.0435,  0.2182, -0.3818]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09492498817018602, distance: 1.0886768313549922 entropy 0.7110485434532166
epoch: 4, step: 17
	action: tensor([[ 0.7019,  0.8566,  0.0867, -0.2781,  1.2344, -0.3665,  0.6269]],
       dtype=torch.float64)
	q_value: tensor([[-18.1368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9533551806203746, distance: 0.24714874024064495 entropy 0.7110485434532166
epoch: 4, step: 18
	action: tensor([[-0.1454, -0.4036,  0.5721, -0.8107,  0.1806, -0.7043,  0.3143]],
       dtype=torch.float64)
	q_value: tensor([[-22.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.741498602351522, distance: 1.5101436415443692 entropy 0.7110485434532166
epoch: 4, step: 19
	action: tensor([[-0.4439, -0.2636, -0.8291, -0.2783,  0.6780,  0.7551, -0.2671]],
       dtype=torch.float64)
	q_value: tensor([[-21.1604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19612434301928716, distance: 1.251540355341499 entropy 0.7110485434532166
epoch: 4, step: 20
	action: tensor([[ 0.3405,  0.1865, -0.1871, -0.2221,  0.4332, -0.4809, -0.2197]],
       dtype=torch.float64)
	q_value: tensor([[-18.0340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5126062434341347, distance: 0.7989078234584016 entropy 0.7110485434532166
epoch: 4, step: 21
	action: tensor([[-0.0361,  0.5337,  0.2523, -0.4067,  0.1489,  0.9504, -0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-16.0385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6292008675296608, distance: 0.6968287296616428 entropy 0.7110485434532166
epoch: 4, step: 22
	action: tensor([[ 0.2631,  0.8169,  0.4186,  0.2762,  0.3895, -0.2432, -0.9239]],
       dtype=torch.float64)
	q_value: tensor([[-18.0631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 23
	action: tensor([[-0.1253, -0.6587,  0.6101,  0.7241,  0.1697,  0.1113,  0.5988]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300473006881247, distance: 0.9571035609633988 entropy 0.7110485434532166
epoch: 4, step: 24
	action: tensor([[-0.2800, -0.3776, -1.1943, -0.3350,  0.3908, -0.1288, -0.5889]],
       dtype=torch.float64)
	q_value: tensor([[-23.5125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17444007676788198, distance: 1.240144023458987 entropy 0.7110485434532166
epoch: 4, step: 25
	action: tensor([[-0.1697, -0.1199,  0.2195,  0.2341,  0.2191,  0.1306,  0.5631]],
       dtype=torch.float64)
	q_value: tensor([[-18.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20352720058296092, distance: 1.0212737460247585 entropy 0.7110485434532166
epoch: 4, step: 26
	action: tensor([[-0.2657, -0.7282,  0.3706,  0.0755,  0.2297, -0.3773, -0.1887]],
       dtype=torch.float64)
	q_value: tensor([[-16.9623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6921513316441883, distance: 1.4885940998267306 entropy 0.7110485434532166
epoch: 4, step: 27
	action: tensor([[ 0.1232, -0.1894, -0.1798, -0.2060, -0.1587,  0.0044,  0.2422]],
       dtype=torch.float64)
	q_value: tensor([[-19.9254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12853567197480653, distance: 1.0682711517705386 entropy 0.7110485434532166
epoch: 4, step: 28
	action: tensor([[ 0.5830,  0.8428,  0.6316, -0.1751,  0.0664, -0.3390, -0.2613]],
       dtype=torch.float64)
	q_value: tensor([[-14.2647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8906857280075335, distance: 0.37835120953440826 entropy 0.7110485434532166
epoch: 4, step: 29
	action: tensor([[ 0.0640, -0.5041, -0.0717,  0.0079,  0.9056,  0.0952, -0.1059]],
       dtype=torch.float64)
	q_value: tensor([[-19.8971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058425554344889497, distance: 1.177299207487216 entropy 0.7110485434532166
epoch: 4, step: 30
	action: tensor([[-0.2659,  0.1035, -0.0943, -0.5706, -0.1717, -0.7179, -0.2077]],
       dtype=torch.float64)
	q_value: tensor([[-18.1645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1528284466785501, distance: 1.2286806971604465 entropy 0.7110485434532166
epoch: 4, step: 31
	action: tensor([[-0.2215,  0.0821, -0.0037, -0.0791, -0.2108,  0.1053,  0.5722]],
       dtype=torch.float64)
	q_value: tensor([[-15.9866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06851890352715473, distance: 1.1044440405406406 entropy 0.7110485434532166
epoch: 4, step: 32
	action: tensor([[ 0.4747, -0.2696,  0.9842, -0.9398,  0.3558, -0.8641,  0.5700]],
       dtype=torch.float64)
	q_value: tensor([[-15.0631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08388258101435997, distance: 1.1913731596479555 entropy 0.7110485434532166
epoch: 4, step: 33
	action: tensor([[ 0.7766,  0.4202, -0.0199, -0.1815, -0.1782, -0.6292,  0.4210]],
       dtype=torch.float64)
	q_value: tensor([[-26.1359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8416395400759153, distance: 0.45538641477545816 entropy 0.7110485434532166
epoch: 4, step: 34
	action: tensor([[ 0.5716, -0.2431,  0.6814, -0.0586,  0.0220,  0.2184,  0.7460]],
       dtype=torch.float64)
	q_value: tensor([[-18.6606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5193588765800448, distance: 0.7933542567839117 entropy 0.7110485434532166
epoch: 4, step: 35
	action: tensor([[ 0.2262, -0.7170, -0.3601, -0.5276,  0.2455, -0.0790,  0.5857]],
       dtype=torch.float64)
	q_value: tensor([[-20.8133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4430813325017364, distance: 1.3746815301566675 entropy 0.7110485434532166
epoch: 4, step: 36
	action: tensor([[ 0.8485,  1.2074, -0.2279,  0.6786, -0.4691, -0.1308,  0.5194]],
       dtype=torch.float64)
	q_value: tensor([[-18.7407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 37
	action: tensor([[-0.5226, -0.2710, -0.7399, -0.9792,  0.2963, -0.4554, -0.4048]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12755696500500657, distance: 1.2151389424991839 entropy 0.7110485434532166
epoch: 4, step: 38
	action: tensor([[ 0.3089, -0.0540, -0.1739,  0.0043,  0.6216, -0.0405, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[-17.0823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5134998309196335, distance: 0.7981751288587733 entropy 0.7110485434532166
epoch: 4, step: 39
	action: tensor([[ 0.6846,  0.9970, -0.1077, -1.1703, -0.9480, -0.0095,  0.1041]],
       dtype=torch.float64)
	q_value: tensor([[-15.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9051687039875612, distance: 0.35239727074125 entropy 0.7110485434532166
epoch: 4, step: 40
	action: tensor([[ 0.4342, -0.4807,  0.7216,  0.4739, -0.6966, -0.1136,  0.1874]],
       dtype=torch.float64)
	q_value: tensor([[-25.5550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4828694013743028, distance: 0.8229184722404601 entropy 0.7110485434532166
epoch: 4, step: 41
	action: tensor([[ 0.4164,  0.7964, -0.4439,  0.0310, -0.0500, -0.2954,  0.9096]],
       dtype=torch.float64)
	q_value: tensor([[-22.4518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9191214657171655, distance: 0.3254417970399897 entropy 0.7110485434532166
epoch: 4, step: 42
	action: tensor([[-0.7986,  0.1972,  0.5483,  0.1216,  0.7323,  0.5078,  0.5659]],
       dtype=torch.float64)
	q_value: tensor([[-17.7391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17728935952191627, distance: 1.2416474549644279 entropy 0.7110485434532166
epoch: 4, step: 43
	action: tensor([[-0.5606,  0.1443,  0.2689, -0.6119, -0.3785,  0.6350, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-20.4383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4782770004057266, distance: 1.3913442683727497 entropy 0.7110485434532166
epoch: 4, step: 44
	action: tensor([[ 0.4653,  1.3984, -0.4385,  0.0338,  0.3106,  0.7138,  0.9139]],
       dtype=torch.float64)
	q_value: tensor([[-18.5110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 45
	action: tensor([[ 0.4843, -0.0789,  1.4085,  0.6047,  0.7496,  0.1251, -0.1297]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7629227579815749, distance: 0.557187835602489 entropy 0.7110485434532166
epoch: 4, step: 46
	action: tensor([[ 1.2985,  0.6969, -0.4337,  0.4924,  0.0765, -0.1662,  0.6354]],
       dtype=torch.float64)
	q_value: tensor([[-26.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 47
	action: tensor([[ 0.1494,  0.1430, -0.2084,  0.4040, -0.5649, -0.4618,  0.4913]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6517289994716039, distance: 0.6753289229499415 entropy 0.7110485434532166
epoch: 4, step: 48
	action: tensor([[ 1.0543,  0.0793, -0.4167, -0.2007, -1.8880, -0.2855,  0.8105]],
       dtype=torch.float64)
	q_value: tensor([[-17.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.689256294584085, distance: 0.6379077277606285 entropy 0.7110485434532166
epoch: 4, step: 49
	action: tensor([[ 0.7083, -0.0779,  0.5994, -0.4836,  0.3531, -0.4194, -0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-28.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3201554733874059, distance: 0.9435425632339255 entropy 0.7110485434532166
epoch: 4, step: 50
	action: tensor([[ 0.9729,  0.7455, -0.2003,  0.4948,  0.4803, -0.6802, -0.0694]],
       dtype=torch.float64)
	q_value: tensor([[-20.6281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9667591609983004, distance: 0.20863764967699888 entropy 0.7110485434532166
epoch: 4, step: 51
	action: tensor([[-0.7239,  0.4597,  0.2785,  0.1593, -0.4429, -0.1279,  0.9184]],
       dtype=torch.float64)
	q_value: tensor([[-22.2039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15213947391039695, distance: 1.2283134898537735 entropy 0.7110485434532166
epoch: 4, step: 52
	action: tensor([[ 0.6601,  0.7298,  0.0327,  0.5822, -0.8247,  0.0455,  0.3876]],
       dtype=torch.float64)
	q_value: tensor([[-18.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 53
	action: tensor([[ 0.2359,  1.2177,  0.0828,  0.7203, -0.9059, -0.0293, -0.0727]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 54
	action: tensor([[ 0.9511, -0.0631,  0.0321, -0.1026, -0.2448, -0.2796,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5605331370572952, distance: 0.7586120905189666 entropy 0.7110485434532166
epoch: 4, step: 55
	action: tensor([[ 0.4455, -0.0957, -0.5627, -0.0203, -0.0724, -0.2235, -0.5780]],
       dtype=torch.float64)
	q_value: tensor([[-19.1425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5556331883617296, distance: 0.7628295374317439 entropy 0.7110485434532166
epoch: 4, step: 56
	action: tensor([[-0.1366, -0.0865, -0.7147, -0.0495,  0.1006, -0.7372, -0.3273]],
       dtype=torch.float64)
	q_value: tensor([[-16.1141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.149197271929425, distance: 1.055531334164928 entropy 0.7110485434532166
epoch: 4, step: 57
	action: tensor([[ 0.1541, -0.0743,  0.1087, -0.4366, -0.1451, -0.3994,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[-17.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07757824446519046, distance: 1.0990601513892586 entropy 0.7110485434532166
epoch: 4, step: 58
	action: tensor([[ 0.3529,  0.4583, -0.2883, -1.4190,  0.8055,  0.6001, -0.2217]],
       dtype=torch.float64)
	q_value: tensor([[-15.6215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6075304534720151, distance: 0.7169018069667017 entropy 0.7110485434532166
epoch: 4, step: 59
	action: tensor([[ 0.3266, -0.4074, -0.6303,  0.2500, -1.1545, -0.5668,  0.2867]],
       dtype=torch.float64)
	q_value: tensor([[-20.3161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4217117018041323, distance: 0.8702196696018334 entropy 0.7110485434532166
epoch: 4, step: 60
	action: tensor([[ 0.4910,  0.0696, -0.0360,  0.4396, -0.0171, -0.0370, -0.2712]],
       dtype=torch.float64)
	q_value: tensor([[-21.4193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8617699887921376, distance: 0.42545914684480973 entropy 0.7110485434532166
epoch: 4, step: 61
	action: tensor([[-0.1571,  0.3598, -0.1281,  0.5922,  0.7815, -0.6761,  0.9340]],
       dtype=torch.float64)
	q_value: tensor([[-16.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30697158448341966, distance: 0.9526474607937492 entropy 0.7110485434532166
epoch: 4, step: 62
	action: tensor([[ 0.0730,  0.0256,  0.2076, -0.2483, -0.4909, -0.7811,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[-22.1372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15890201343419863, distance: 1.0494940722584616 entropy 0.7110485434532166
epoch: 4, step: 63
	action: tensor([[ 0.4672, -0.6300, -0.3239,  0.4226,  0.1064,  0.3415,  0.1177]],
       dtype=torch.float64)
	q_value: tensor([[-17.7838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3949150226858492, distance: 0.8901534483834439 entropy 0.7110485434532166
epoch: 4, step: 64
	action: tensor([[ 0.0380, -0.1167, -0.4791,  0.6519, -0.4248,  0.0008,  0.0960]],
       dtype=torch.float64)
	q_value: tensor([[-18.3861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3830391358175472, distance: 0.8988464372920194 entropy 0.7110485434532166
epoch: 4, step: 65
	action: tensor([[-0.3640,  0.0436, -0.3251, -0.3461, -0.2552, -0.0412,  0.1453]],
       dtype=torch.float64)
	q_value: tensor([[-15.6325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11761598249404748, distance: 1.2097705140505983 entropy 0.7110485434532166
epoch: 4, step: 66
	action: tensor([[-0.0534, -0.5582,  0.0694,  0.8578,  0.7261,  0.4423,  0.5998]],
       dtype=torch.float64)
	q_value: tensor([[-13.6184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5027837448821812, distance: 0.8069179052009858 entropy 0.7110485434532166
epoch: 4, step: 67
	action: tensor([[ 0.4973,  0.2211,  0.3254, -0.1824, -0.5702, -0.1006,  0.3035]],
       dtype=torch.float64)
	q_value: tensor([[-22.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7051838598484866, distance: 0.6213443037465486 entropy 0.7110485434532166
epoch: 4, step: 68
	action: tensor([[ 0.8307,  0.2433,  0.1709,  1.1172,  0.1622,  0.3232, -0.5174]],
       dtype=torch.float64)
	q_value: tensor([[-17.7439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8924675777480398, distance: 0.3752549308982168 entropy 0.7110485434532166
epoch: 4, step: 69
	action: tensor([[ 0.2977, -0.6439, -1.1150, -0.2057,  0.1875, -0.2902,  0.1184]],
       dtype=torch.float64)
	q_value: tensor([[-23.2057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024332318663968344, distance: 1.158182853168254 entropy 0.7110485434532166
epoch: 4, step: 70
	action: tensor([[ 0.3722, -0.2704,  0.0026,  0.5591, -0.6588,  0.0759,  0.4437]],
       dtype=torch.float64)
	q_value: tensor([[-18.8065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.692940453267712, distance: 0.6341149546903467 entropy 0.7110485434532166
epoch: 4, step: 71
	action: tensor([[ 0.5340, -0.3694, -0.1114,  0.1066,  1.1017, -0.1173, -0.0646]],
       dtype=torch.float64)
	q_value: tensor([[-18.3355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40014551460558834, distance: 0.886297753671162 entropy 0.7110485434532166
epoch: 4, step: 72
	action: tensor([[ 0.0493,  0.4192, -0.5148,  0.0454, -0.1355, -0.7296,  0.2655]],
       dtype=torch.float64)
	q_value: tensor([[-21.5417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6181616329836125, distance: 0.7071254616691938 entropy 0.7110485434532166
epoch: 4, step: 73
	action: tensor([[ 0.3691,  0.5265, -1.2905, -0.8088, -0.3861, -0.1507, -0.3090]],
       dtype=torch.float64)
	q_value: tensor([[-15.9871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388576431623936, distance: 0.28296178140225253 entropy 0.7110485434532166
epoch: 4, step: 74
	action: tensor([[ 0.4648,  0.7385, -0.0928, -0.6992,  0.4599,  0.2874, -0.2958]],
       dtype=torch.float64)
	q_value: tensor([[-19.5981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9102047685307648, distance: 0.3429125109862989 entropy 0.7110485434532166
epoch: 4, step: 75
	action: tensor([[ 1.0400,  0.5334, -0.3831,  0.7047,  0.2245, -0.9339, -0.2313]],
       dtype=torch.float64)
	q_value: tensor([[-17.2440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08816487498841538 entropy 0.7110485434532166
epoch: 4, step: 76
	action: tensor([[-0.0568, -0.4165, -0.3080, -0.1373,  0.0597,  0.0614,  0.3157]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13559192229698303, distance: 1.2194607895115666 entropy 0.7110485434532166
epoch: 4, step: 77
	action: tensor([[-0.2304, -0.5679, -0.5199, -0.4737,  0.0383,  0.2386, -0.6579]],
       dtype=torch.float64)
	q_value: tensor([[-15.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4030548385381163, distance: 1.3554828158306538 entropy 0.7110485434532166
epoch: 4, step: 78
	action: tensor([[ 0.0501,  0.2475, -0.1455,  0.6181,  0.7523, -0.2724, -0.5357]],
       dtype=torch.float64)
	q_value: tensor([[-16.2803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5653208188538823, distance: 0.7544685021641186 entropy 0.7110485434532166
epoch: 4, step: 79
	action: tensor([[-0.1566,  0.5843,  0.0779,  0.3926,  0.4540,  0.4443,  0.3412]],
       dtype=torch.float64)
	q_value: tensor([[-18.8181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 80
	action: tensor([[ 0.1767,  0.4444, -0.2136, -0.0447,  0.3655, -0.0081, -0.6738]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6867395527267848, distance: 0.6404857550531483 entropy 0.7110485434532166
epoch: 4, step: 81
	action: tensor([[ 0.2040, -0.3543,  0.6316,  0.3690,  0.6500, -0.6927, -0.2161]],
       dtype=torch.float64)
	q_value: tensor([[-14.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2712357238164238, distance: 0.9769002731093073 entropy 0.7110485434532166
epoch: 4, step: 82
	action: tensor([[ 0.6896, -0.2512, -0.7139, -0.7582, -0.7748,  0.2460, -0.3148]],
       dtype=torch.float64)
	q_value: tensor([[-22.8344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17900298744609167, distance: 1.0368775744529817 entropy 0.7110485434532166
epoch: 4, step: 83
	action: tensor([[ 0.2858,  0.5417, -0.2532, -0.0636,  0.3343, -0.3633, -0.4973]],
       dtype=torch.float64)
	q_value: tensor([[-20.3020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7660300964812435, distance: 0.5535242998382882 entropy 0.7110485434532166
epoch: 4, step: 84
	action: tensor([[-0.3060,  0.6614, -0.0724, -0.0320, -0.2626, -0.2257,  0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-15.7121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36517910720865443, distance: 0.9117637028630166 entropy 0.7110485434532166
epoch: 4, step: 85
	action: tensor([[ 0.4585, -0.2071, -0.1108, -0.1767, -0.4286,  1.0579, -0.6330]],
       dtype=torch.float64)
	q_value: tensor([[-14.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6222373375199295, distance: 0.7033414437983088 entropy 0.7110485434532166
epoch: 4, step: 86
	action: tensor([[ 0.2636,  0.3592, -0.8190, -0.9718,  0.5313,  0.4353, -0.5905]],
       dtype=torch.float64)
	q_value: tensor([[-20.9041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8023484320358389, distance: 0.5087528149853303 entropy 0.7110485434532166
epoch: 4, step: 87
	action: tensor([[-0.7230,  0.0996,  0.0763, -0.8320, -0.7051,  0.9152, -0.2096]],
       dtype=torch.float64)
	q_value: tensor([[-17.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7454513218788776, distance: 1.5118564741008573 entropy 0.7110485434532166
epoch: 4, step: 88
	action: tensor([[ 1.5462,  0.3021, -0.6049,  0.3925,  0.2495, -0.2045,  0.2244]],
       dtype=torch.float64)
	q_value: tensor([[-22.1011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7796505617150997, distance: 0.5371711342827588 entropy 0.7110485434532166
epoch: 4, step: 89
	action: tensor([[-0.3741, -0.2499, -0.5617, -0.1302, -0.2547,  0.3317, -0.2758]],
       dtype=torch.float64)
	q_value: tensor([[-24.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3163454699525112, distance: 1.3129301903084851 entropy 0.7110485434532166
epoch: 4, step: 90
	action: tensor([[ 0.4292,  0.1928, -0.5904,  0.4656, -0.5539,  0.1387,  0.4400]],
       dtype=torch.float64)
	q_value: tensor([[-14.1040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7426425159549541, distance: 0.5805305704029046 entropy 0.7110485434532166
epoch: 4, step: 91
	action: tensor([[-0.7433, -0.6148,  0.6585, -0.1436,  0.1506, -0.8256,  0.2226]],
       dtype=torch.float64)
	q_value: tensor([[-16.3911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1727608850669866, distance: 1.6867963804604158 entropy 0.7110485434532166
epoch: 4, step: 92
	action: tensor([[-0.1883,  1.3209, -0.1956,  0.1127,  0.6643,  0.0639, -0.6876]],
       dtype=torch.float64)
	q_value: tensor([[-23.9670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 93
	action: tensor([[-0.0267, -0.5299,  0.0136, -0.0356,  0.4680,  0.8948,  0.6196]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21081512049178897, distance: 1.016590556690774 entropy 0.7110485434532166
epoch: 4, step: 94
	action: tensor([[ 0.8875,  0.7914,  0.3499, -0.2138, -0.1287,  0.2672,  0.5520]],
       dtype=torch.float64)
	q_value: tensor([[-20.9326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9787935294441853, distance: 0.16664446377214862 entropy 0.7110485434532166
epoch: 4, step: 95
	action: tensor([[-0.6896,  0.4255, -0.6660,  0.0731,  0.0231, -0.7264,  0.7290]],
       dtype=torch.float64)
	q_value: tensor([[-20.1433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13916325300876253, distance: 1.2213768294919132 entropy 0.7110485434532166
epoch: 4, step: 96
	action: tensor([[ 0.9815,  0.3305, -0.2997,  0.0537,  0.4202,  0.5030,  0.4211]],
       dtype=torch.float64)
	q_value: tensor([[-18.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9783696743761519, distance: 0.16830159117856397 entropy 0.7110485434532166
epoch: 4, step: 97
	action: tensor([[ 0.1839,  0.4874, -0.2020,  0.2344, -0.0401,  0.0235, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-19.6054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7560468018071104, distance: 0.5652101487117508 entropy 0.7110485434532166
epoch: 4, step: 98
	action: tensor([[ 0.0873,  0.5382, -0.3563,  0.7984,  0.8216,  0.4694,  0.3135]],
       dtype=torch.float64)
	q_value: tensor([[-13.4584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 99
	action: tensor([[ 0.2608, -0.4960, -0.2307,  0.1579,  0.2189,  0.3154,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23772197555478236, distance: 0.9991102009424095 entropy 0.7110485434532166
epoch: 4, step: 100
	action: tensor([[-0.0075,  0.0537, -0.2746, -0.2325, -0.1659,  0.1379,  0.1361]],
       dtype=torch.float64)
	q_value: tensor([[-16.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.251379572946541, distance: 0.9901193085577484 entropy 0.7110485434532166
epoch: 4, step: 101
	action: tensor([[ 0.5892,  0.7921, -0.2420,  1.4648, -0.4799, -0.2031, -0.3861]],
       dtype=torch.float64)
	q_value: tensor([[-13.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 102
	action: tensor([[-0.9009,  0.4510,  0.0312, -0.3759,  0.4747, -0.5311,  0.3453]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7586073522335084, distance: 1.5175434533385799 entropy 0.7110485434532166
epoch: 4, step: 103
	action: tensor([[-0.1063, -0.1382,  0.4575,  0.6153,  0.6120,  0.0599,  0.0803]],
       dtype=torch.float64)
	q_value: tensor([[-17.3403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.539294265421175, distance: 0.7767271793701772 entropy 0.7110485434532166
epoch: 4, step: 104
	action: tensor([[ 0.1470, -0.6011, -0.2754, -0.0086,  0.1793, -0.8026, -0.2643]],
       dtype=torch.float64)
	q_value: tensor([[-19.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2878347693015999, distance: 1.298633991690632 entropy 0.7110485434532166
epoch: 4, step: 105
	action: tensor([[ 0.8647,  0.9087,  0.0513, -0.8171,  0.1314, -0.1149,  0.2295]],
       dtype=torch.float64)
	q_value: tensor([[-20.4350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9549852038877709, distance: 0.24279198006836836 entropy 0.7110485434532166
epoch: 4, step: 106
	action: tensor([[ 0.2288,  0.3733, -0.6407, -0.0031,  0.6230,  0.1779,  0.5153]],
       dtype=torch.float64)
	q_value: tensor([[-20.9099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7485566500220826, distance: 0.5738214391451542 entropy 0.7110485434532166
epoch: 4, step: 107
	action: tensor([[-0.2448, -0.1972, -0.9321,  0.3075,  0.7121, -0.2340,  0.1772]],
       dtype=torch.float64)
	q_value: tensor([[-16.6277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20697482811820067, distance: 1.257204131881305 entropy 0.7110485434532166
epoch: 4, step: 108
	action: tensor([[ 0.7823, -0.2990, -0.1594,  0.0066, -0.1663,  0.5181, -0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-19.4256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6243887290014527, distance: 0.7013357885329123 entropy 0.7110485434532166
epoch: 4, step: 109
	action: tensor([[-0.1066, -0.4059,  0.2093, -1.3807, -0.3785,  1.1784,  0.3676]],
       dtype=torch.float64)
	q_value: tensor([[-17.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46817958749699096, distance: 1.3865843179905857 entropy 0.7110485434532166
epoch: 4, step: 110
	action: tensor([[ 0.1998,  0.1821,  0.2129, -1.1146, -0.2425,  0.2585,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-26.0635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07190794345943374, distance: 1.102433041200826 entropy 0.7110485434532166
epoch: 4, step: 111
	action: tensor([[-0.2568,  1.0354, -0.0491, -0.5718, -0.1707,  0.2635,  0.3947]],
       dtype=torch.float64)
	q_value: tensor([[-19.0575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43741168806717456, distance: 0.8583255606910997 entropy 0.7110485434532166
epoch: 4, step: 112
	action: tensor([[ 0.0120,  0.1764,  0.1554,  0.0749,  0.1092, -0.1511,  1.0374]],
       dtype=torch.float64)
	q_value: tensor([[-17.7784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4333724247052231, distance: 0.861401346370072 entropy 0.7110485434532166
epoch: 4, step: 113
	action: tensor([[ 0.6696,  0.3259,  0.5908,  0.2341, -0.5805, -0.1689, -0.4128]],
       dtype=torch.float64)
	q_value: tensor([[-17.9720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9450314918060827, distance: 0.26829567304147295 entropy 0.7110485434532166
epoch: 4, step: 114
	action: tensor([[ 1.4025,  0.4869, -0.0128,  0.4367,  0.0245,  0.0458, -0.3452]],
       dtype=torch.float64)
	q_value: tensor([[-21.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8434526554711096, distance: 0.452771983509659 entropy 0.7110485434532166
epoch: 4, step: 115
	action: tensor([[ 0.5641,  1.1495, -0.8547, -0.3283, -0.2253,  0.0926, -0.2473]],
       dtype=torch.float64)
	q_value: tensor([[-23.1384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 116
	action: tensor([[ 0.6753, -0.0824, -0.3184,  0.2025,  0.1912, -0.5763,  0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6288975523948712, distance: 0.6971136757094721 entropy 0.7110485434532166
epoch: 4, step: 117
	action: tensor([[-0.0879,  0.4048,  0.4752,  0.2196,  0.3308,  0.5183, -1.1622]],
       dtype=torch.float64)
	q_value: tensor([[-18.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7145602718491204, distance: 0.6113837660508732 entropy 0.7110485434532166
epoch: 4, step: 118
	action: tensor([[-0.1683,  0.2679,  0.0319, -0.5074,  0.3083, -1.0984,  0.2249]],
       dtype=torch.float64)
	q_value: tensor([[-19.9194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03923375109896177, distance: 1.1665767447443862 entropy 0.7110485434532166
epoch: 4, step: 119
	action: tensor([[-0.4629, -0.1259,  0.2614, -0.6141,  0.5482, -1.0992, -0.3691]],
       dtype=torch.float64)
	q_value: tensor([[-18.7990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6929061744036036, distance: 1.4889260823077406 entropy 0.7110485434532166
epoch: 4, step: 120
	action: tensor([[ 1.7072, -0.1831, -1.1854,  0.9755, -0.4888, -0.3676,  0.4023]],
       dtype=torch.float64)
	q_value: tensor([[-21.1504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7498668385352762, distance: 0.5723244892709286 entropy 0.7110485434532166
epoch: 4, step: 121
	action: tensor([[-0.2284,  0.1251, -0.2673, -0.6131, -0.4394,  0.1574,  0.2256]],
       dtype=torch.float64)
	q_value: tensor([[-28.8149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.036662478730579484, distance: 1.1231711286518364 entropy 0.7110485434532166
epoch: 4, step: 122
	action: tensor([[ 0.7215,  0.2211,  0.5538, -0.7126, -0.2222,  0.3327, -0.3756]],
       dtype=torch.float64)
	q_value: tensor([[-15.7047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6498920949958813, distance: 0.6771075426030644 entropy 0.7110485434532166
epoch: 4, step: 123
	action: tensor([[-0.1254, -0.7609, -0.0215,  0.6308, -0.3006, -1.0803,  0.7658]],
       dtype=torch.float64)
	q_value: tensor([[-21.0150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21645317089547378, distance: 1.2621308745143005 entropy 0.7110485434532166
epoch: 4, step: 124
	action: tensor([[ 0.9355, -0.2584, -0.4904,  0.0245,  0.4141,  0.1393,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[-27.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5696385312814551, distance: 0.7507120444582316 entropy 0.7110485434532166
epoch: 4, step: 125
	action: tensor([[ 0.5875,  0.1936, -0.6169, -0.8592, -0.4870,  0.7093, -0.5662]],
       dtype=torch.float64)
	q_value: tensor([[-19.1747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6668314977306732, distance: 0.6605240901670282 entropy 0.7110485434532166
epoch: 4, step: 126
	action: tensor([[ 1.1788,  0.5790, -0.3351, -0.4328, -0.0222,  0.3600,  0.1271]],
       dtype=torch.float64)
	q_value: tensor([[-20.8130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8744937341383212, distance: 0.4054052770235013 entropy 0.7110485434532166
epoch: 4, step: 127
	action: tensor([[ 0.7128,  0.0783, -0.0119,  0.1829, -0.1237, -0.4935,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[-20.4560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7817104689636634, distance: 0.5346544023920602 entropy 0.7110485434532166
LOSS epoch 4 actor 248.11040783841025 critic 1360.6115756294766 
epoch: 5, step: 0
	action: tensor([[ 0.6793, -0.4171, -0.2180,  0.1624,  0.2584, -0.6330,  0.4989]],
       dtype=torch.float64)
	q_value: tensor([[-14.2682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25635550633526816, distance: 0.9868232577924606 entropy 0.7110485434532166
epoch: 5, step: 1
	action: tensor([[ 0.1975,  0.3047,  0.4017,  0.6495,  0.0235, -0.7541,  0.2505]],
       dtype=torch.float64)
	q_value: tensor([[-17.1073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8097796300327651, distance: 0.4990972814257656 entropy 0.7110485434532166
epoch: 5, step: 2
	action: tensor([[ 0.0339,  0.2590,  0.4645,  0.4433,  0.1153, -0.6152, -0.4375]],
       dtype=torch.float64)
	q_value: tensor([[-16.2263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6158244828893651, distance: 0.7092862413350702 entropy 0.7110485434532166
epoch: 5, step: 3
	action: tensor([[ 0.4150,  0.2205,  0.1747,  0.5071, -0.1703,  0.2139,  0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-15.4887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9221430842215987, distance: 0.3193046852148686 entropy 0.7110485434532166
epoch: 5, step: 4
	action: tensor([[ 0.5361, -0.6036, -0.5206, -0.1183, -0.3758,  0.2837, -0.0463]],
       dtype=torch.float64)
	q_value: tensor([[-13.3856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10469082257845874, distance: 1.0827874448946064 entropy 0.7110485434532166
epoch: 5, step: 5
	action: tensor([[ 1.0008,  0.7084, -0.2930, -0.4550, -0.3045, -0.4945, -0.1755]],
       dtype=torch.float64)
	q_value: tensor([[-14.0191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8672801315142475, distance: 0.416893059133941 entropy 0.7110485434532166
epoch: 5, step: 6
	action: tensor([[ 1.2836,  0.1410,  0.0167,  0.1104, -0.1436,  0.4724,  0.3011]],
       dtype=torch.float64)
	q_value: tensor([[-15.8173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.837723701952695, distance: 0.4609822880224209 entropy 0.7110485434532166
epoch: 5, step: 7
	action: tensor([[ 1.4523, -0.0893,  0.1980, -0.1881, -0.1650, -0.2973,  0.9023]],
       dtype=torch.float64)
	q_value: tensor([[-17.2567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.289096140514819, distance: 0.9648551722215553 entropy 0.7110485434532166
epoch: 5, step: 8
	action: tensor([[ 0.5745,  0.2342, -0.1328,  0.5594, -0.1036, -0.4896,  0.1356]],
       dtype=torch.float64)
	q_value: tensor([[-20.4526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388723793339332, distance: 0.2829276804545673 entropy 0.7110485434532166
epoch: 5, step: 9
	action: tensor([[ 1.0371, -1.3187, -0.3505,  0.0783, -0.8574, -0.4899, -0.9129]],
       dtype=torch.float64)
	q_value: tensor([[-14.5349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.625651062150959, distance: 1.4590506195179067 entropy 0.7110485434532166
epoch: 5, step: 10
	action: tensor([[ 1.3444, -0.1784,  0.3411, -0.5136, -0.1029, -0.6485, -0.5021]],
       dtype=torch.float64)
	q_value: tensor([[-23.1061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09349170503327298, distance: 1.0895385084835034 entropy 0.7110485434532166
epoch: 5, step: 11
	action: tensor([[ 0.4133,  0.5614, -1.1010,  0.6985,  1.0695, -0.1403,  0.4273]],
       dtype=torch.float64)
	q_value: tensor([[-20.7497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6453865104687588, distance: 0.6814505075891159 entropy 0.7110485434532166
epoch: 5, step: 12
	action: tensor([[ 0.7262,  0.7858, -0.1043,  0.0636, -0.6671, -0.5592,  1.2410]],
       dtype=torch.float64)
	q_value: tensor([[-19.0529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9873406617262708, distance: 0.12875443449827026 entropy 0.7110485434532166
epoch: 5, step: 13
	action: tensor([[ 0.0428,  1.0948, -1.0312, -0.2136, -0.4256,  0.2822, -0.2548]],
       dtype=torch.float64)
	q_value: tensor([[-18.2775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 14
	action: tensor([[-0.0051,  0.2682,  0.8400, -0.5991,  0.2944,  0.2388,  0.4333]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13714381830393985, distance: 1.0629819760735368 entropy 0.7110485434532166
epoch: 5, step: 15
	action: tensor([[ 0.0635,  1.4239,  0.0876, -0.4675,  0.0822, -0.5737,  0.0725]],
       dtype=torch.float64)
	q_value: tensor([[-15.4174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6631929478504328, distance: 0.6641211043578845 entropy 0.7110485434532166
epoch: 5, step: 16
	action: tensor([[-0.5112,  1.0181, -0.3783, -0.4469,  0.4924, -0.2708,  0.1438]],
       dtype=torch.float64)
	q_value: tensor([[-15.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28129603947558357, distance: 0.970133970694222 entropy 0.7110485434532166
epoch: 5, step: 17
	action: tensor([[ 0.2111, -0.7889, -1.3585,  1.2417,  0.1170, -0.2348,  0.6921]],
       dtype=torch.float64)
	q_value: tensor([[-12.5688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35450145084490314, distance: 1.331822754210625 entropy 0.7110485434532166
epoch: 5, step: 18
	action: tensor([[ 0.9110,  0.2999,  0.5830,  0.1842, -0.0444, -0.2888, -1.2111]],
       dtype=torch.float64)
	q_value: tensor([[-22.1926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9231772832496997, distance: 0.3171768809170011 entropy 0.7110485434532166
epoch: 5, step: 19
	action: tensor([[ 0.4783, -0.0762,  0.0270,  0.4702,  0.2119, -0.6174, -0.1748]],
       dtype=torch.float64)
	q_value: tensor([[-19.9040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6944042111506412, distance: 0.6326017309637131 entropy 0.7110485434532166
epoch: 5, step: 20
	action: tensor([[ 0.8138,  0.9354, -0.6729, -0.3632, -0.1610, -0.2712, -0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-15.5936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9752710142701095, distance: 0.17995331245187512 entropy 0.7110485434532166
epoch: 5, step: 21
	action: tensor([[-0.3176,  0.0757, -0.4718, -0.6991, -0.2998,  0.6212,  0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-14.8544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04150636029147159, distance: 1.167851590374265 entropy 0.7110485434532166
epoch: 5, step: 22
	action: tensor([[ 0.9538,  0.0567,  0.0775, -1.3718, -0.2789,  0.2244, -1.5276]],
       dtype=torch.float64)
	q_value: tensor([[-14.0867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15311797994701648, distance: 1.0530964519231882 entropy 0.7110485434532166
epoch: 5, step: 23
	action: tensor([[ 0.5144, -0.2191, -0.4402,  0.4967, -0.7630, -0.0808,  0.3675]],
       dtype=torch.float64)
	q_value: tensor([[-21.5623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6638674145002026, distance: 0.6634558094873153 entropy 0.7110485434532166
epoch: 5, step: 24
	action: tensor([[ 0.6298,  0.3578, -0.0193,  0.2203, -0.1160, -0.5896,  0.6412]],
       dtype=torch.float64)
	q_value: tensor([[-14.7289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9099105840522834, distance: 0.3434737715732514 entropy 0.7110485434532166
epoch: 5, step: 25
	action: tensor([[ 0.8022, -0.2754, -0.0684, -0.2981,  0.4785,  0.7220,  0.8169]],
       dtype=torch.float64)
	q_value: tensor([[-15.0606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5733157276486274, distance: 0.7474979582840169 entropy 0.7110485434532166
epoch: 5, step: 26
	action: tensor([[ 0.9788,  0.4802,  0.6721, -1.0845,  0.1347, -0.1574, -0.2592]],
       dtype=torch.float64)
	q_value: tensor([[-17.5545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6614731296582772, distance: 0.6658145267708583 entropy 0.7110485434532166
epoch: 5, step: 27
	action: tensor([[ 0.5833, -0.4923, -1.1822, -0.1722, -0.0285, -0.1629, -0.1503]],
       dtype=torch.float64)
	q_value: tensor([[-19.2274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18867916467367984, distance: 1.0307492033977759 entropy 0.7110485434532166
epoch: 5, step: 28
	action: tensor([[-0.2059,  0.0022, -0.4876, -0.0176, -0.9574, -0.5413,  0.4079]],
       dtype=torch.float64)
	q_value: tensor([[-14.8638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22313893766297532, distance: 1.0086218460897836 entropy 0.7110485434532166
epoch: 5, step: 29
	action: tensor([[-0.2833,  0.1508, -0.0334, -0.1297,  0.4309, -0.6481,  0.4639]],
       dtype=torch.float64)
	q_value: tensor([[-14.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11342753970493424, distance: 1.207501482925655 entropy 0.7110485434532166
epoch: 5, step: 30
	action: tensor([[ 0.9081, -0.2336, -0.3751,  0.0775, -0.0098,  0.2781, -0.5976]],
       dtype=torch.float64)
	q_value: tensor([[-13.5912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6480439612796656, distance: 0.6788923326158255 entropy 0.7110485434532166
epoch: 5, step: 31
	action: tensor([[ 0.5589,  0.4161,  0.3076, -0.3276,  0.0783,  0.7859,  1.1901]],
       dtype=torch.float64)
	q_value: tensor([[-15.5086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9404831867304114, distance: 0.2791750020867613 entropy 0.7110485434532166
epoch: 5, step: 32
	action: tensor([[ 1.6988,  1.0122, -0.3389,  0.6084, -0.3392,  0.1262, -0.0855]],
       dtype=torch.float64)
	q_value: tensor([[-18.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 33
	action: tensor([[-0.1920,  0.0553, -0.6880, -0.1973, -0.6154, -0.2849, -0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24223784463600706, distance: 0.9961463525584988 entropy 0.7110485434532166
epoch: 5, step: 34
	action: tensor([[ 0.1997,  0.1826, -0.6408,  0.7317,  0.5978, -0.2308, -0.2864]],
       dtype=torch.float64)
	q_value: tensor([[-12.5835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5561166109309696, distance: 0.7624144868981373 entropy 0.7110485434532166
epoch: 5, step: 35
	action: tensor([[ 0.3893, -0.4658, -0.9281,  0.2569, -0.3087,  0.3584,  0.3375]],
       dtype=torch.float64)
	q_value: tensor([[-15.4313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23809980849222145, distance: 0.9988625593212953 entropy 0.7110485434532166
epoch: 5, step: 36
	action: tensor([[ 0.3102,  0.4397, -0.4746, -0.1104,  0.1465, -0.0348,  0.0865]],
       dtype=torch.float64)
	q_value: tensor([[-13.9716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7808881189468855, distance: 0.5356605425946631 entropy 0.7110485434532166
epoch: 5, step: 37
	action: tensor([[ 0.6674,  0.1628,  0.1264,  0.0516, -0.3926, -0.1330, -0.4714]],
       dtype=torch.float64)
	q_value: tensor([[-10.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8218481001420415, distance: 0.4830053289880871 entropy 0.7110485434532166
epoch: 5, step: 38
	action: tensor([[-0.0868,  0.4799, -1.1436,  0.0214, -0.2918, -0.2205,  0.2119]],
       dtype=torch.float64)
	q_value: tensor([[-14.4854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.546762260531769, distance: 0.7704061228004141 entropy 0.7110485434532166
epoch: 5, step: 39
	action: tensor([[ 0.0718, -0.3781, -0.5184,  0.5268,  0.2135,  0.2593,  0.3825]],
       dtype=torch.float64)
	q_value: tensor([[-12.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2607548381050755, distance: 0.983899950497067 entropy 0.7110485434532166
epoch: 5, step: 40
	action: tensor([[ 0.8691,  0.3336,  0.0556,  0.4154, -0.0709,  0.1237, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[-13.7489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09845431288094716 entropy 0.7110485434532166
epoch: 5, step: 41
	action: tensor([[-5.2608e-01,  3.5919e-01, -3.0761e-04, -5.1077e-01,  3.2716e-01,
          4.6689e-01,  2.6496e-01]], dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13984285030767563, distance: 1.2217410971387523 entropy 0.7110485434532166
epoch: 5, step: 42
	action: tensor([[ 0.3408,  0.2627,  0.1713, -0.4101, -0.4575,  0.7498, -0.6740]],
       dtype=torch.float64)
	q_value: tensor([[-12.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.646159388915275, distance: 0.6807074934178005 entropy 0.7110485434532166
epoch: 5, step: 43
	action: tensor([[-0.5381, -0.6773, -0.8321, -0.0062,  0.2734,  0.2323, -0.6240]],
       dtype=torch.float64)
	q_value: tensor([[-16.4303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8202782301354421, distance: 1.5439228003377243 entropy 0.7110485434532166
epoch: 5, step: 44
	action: tensor([[ 0.2653, -0.0680,  0.0007,  0.1346, -0.0503, -0.3079,  0.5316]],
       dtype=torch.float64)
	q_value: tensor([[-14.9388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.444241475484471, distance: 0.8530996419864079 entropy 0.7110485434532166
epoch: 5, step: 45
	action: tensor([[-0.0095, -1.2513, -0.8065, -0.1575, -0.1016,  0.8680, -0.8292]],
       dtype=torch.float64)
	q_value: tensor([[-13.2106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7081840544615292, distance: 1.49562950816462 entropy 0.7110485434532166
epoch: 5, step: 46
	action: tensor([[ 0.3552, -0.2562, -0.9302,  0.4325, -0.3763, -0.4294, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-18.6411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4225560111789127, distance: 0.8695841708339748 entropy 0.7110485434532166
epoch: 5, step: 47
	action: tensor([[ 1.4234,  0.2170,  0.5714, -0.7021,  1.2274, -0.8833, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[-14.4439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7267405173502163, distance: 0.5981971149731288 entropy 0.7110485434532166
epoch: 5, step: 48
	action: tensor([[ 0.3617, -0.1734, -0.2787,  0.0947, -0.8753, -0.0438,  0.0373]],
       dtype=torch.float64)
	q_value: tensor([[-23.7771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4631091387847325, distance: 0.8384934976193404 entropy 0.7110485434532166
epoch: 5, step: 49
	action: tensor([[ 0.4761, -0.1551,  0.0223,  0.0294,  0.6659, -0.3145, -0.9880]],
       dtype=torch.float64)
	q_value: tensor([[-14.3154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4580934238988379, distance: 0.8424010585373672 entropy 0.7110485434532166
epoch: 5, step: 50
	action: tensor([[-0.4103, -0.0059, -0.1605,  0.4573,  0.4757,  0.4064,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-16.1772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14506117720901135, distance: 1.0580939049830236 entropy 0.7110485434532166
epoch: 5, step: 51
	action: tensor([[ 0.6653,  0.6959, -0.9328,  0.4933,  0.9198, -0.2875,  0.2092]],
       dtype=torch.float64)
	q_value: tensor([[-13.2717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8561955029122056, distance: 0.43395323182168266 entropy 0.7110485434532166
epoch: 5, step: 52
	action: tensor([[ 0.4140,  0.6539,  0.6888,  0.5353, -0.1581,  0.7177, -0.1481]],
       dtype=torch.float64)
	q_value: tensor([[-17.7514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 53
	action: tensor([[ 0.6973, -0.1537, -0.4133,  0.0563, -0.0575,  0.1799,  0.3355]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6670270155142227, distance: 0.6603302495764066 entropy 0.7110485434532166
epoch: 5, step: 54
	action: tensor([[ 0.8418,  0.1326,  0.0420, -0.3941,  0.0359, -0.5262, -0.5879]],
       dtype=torch.float64)
	q_value: tensor([[-13.2833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5412717227894905, distance: 0.7750584389199903 entropy 0.7110485434532166
epoch: 5, step: 55
	action: tensor([[ 0.8609,  0.6057, -0.7217, -0.5479,  0.1547,  0.1051, -0.1986]],
       dtype=torch.float64)
	q_value: tensor([[-16.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.917726056662989, distance: 0.3282372382717347 entropy 0.7110485434532166
epoch: 5, step: 56
	action: tensor([[ 0.8104, -0.1307,  0.4644,  0.4421, -0.3799,  0.1997,  0.6710]],
       dtype=torch.float64)
	q_value: tensor([[-14.5158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8740523978398929, distance: 0.4061174448513526 entropy 0.7110485434532166
epoch: 5, step: 57
	action: tensor([[ 1.4032,  0.4987, -0.9555, -0.7620,  0.0813,  0.6015,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[-17.3931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5785874398973061, distance: 0.7428659132924077 entropy 0.7110485434532166
epoch: 5, step: 58
	action: tensor([[ 1.0754,  0.5761, -0.0051,  0.8029, -0.7983, -0.2908, -0.5225]],
       dtype=torch.float64)
	q_value: tensor([[-19.2285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 59
	action: tensor([[ 0.9781,  0.1785, -0.1447, -0.2119, -0.2386,  0.0083,  0.2528]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7669998618866656, distance: 0.5523759767224955 entropy 0.7110485434532166
epoch: 5, step: 60
	action: tensor([[ 0.2598, -0.5575, -0.4009,  0.7179, -0.5425, -0.9864,  0.2188]],
       dtype=torch.float64)
	q_value: tensor([[-14.4773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37278817125798513, distance: 0.9062829578038947 entropy 0.7110485434532166
epoch: 5, step: 61
	action: tensor([[ 0.4479, -0.2885,  0.6843,  0.2942,  0.4949, -0.0331,  0.2507]],
       dtype=torch.float64)
	q_value: tensor([[-19.5432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5612545818563694, distance: 0.757989152143503 entropy 0.7110485434532166
epoch: 5, step: 62
	action: tensor([[-0.4397,  0.1800, -0.1083, -0.4885,  0.8358, -0.3433,  0.2881]],
       dtype=torch.float64)
	q_value: tensor([[-17.1528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35442201336633317, distance: 1.3317836999177555 entropy 0.7110485434532166
epoch: 5, step: 63
	action: tensor([[-0.2123,  0.7814,  0.0326, -0.6884, -0.0063,  0.4417,  0.4845]],
       dtype=torch.float64)
	q_value: tensor([[-13.4967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38384444014511065, distance: 0.8982596243136997 entropy 0.7110485434532166
epoch: 5, step: 64
	action: tensor([[ 0.3046,  1.4509,  0.0773,  0.0225, -1.0450,  0.5683, -0.4225]],
       dtype=torch.float64)
	q_value: tensor([[-14.3536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 65
	action: tensor([[ 0.5299, -0.3454,  0.2128, -0.2197,  0.5116, -0.4262,  0.3802]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09461816943568324, distance: 1.088861345422625 entropy 0.7110485434532166
epoch: 5, step: 66
	action: tensor([[-0.0303,  0.0007,  0.1852, -0.4402, -0.3948, -0.0652,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[-16.0734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022923108449677154, distance: 1.1311522515680899 entropy 0.7110485434532166
epoch: 5, step: 67
	action: tensor([[-0.0241,  1.0240,  1.0790,  0.4145, -0.1633,  0.3949, -0.1006]],
       dtype=torch.float64)
	q_value: tensor([[-12.7658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 68
	action: tensor([[-0.3866,  0.2037, -1.1169, -1.0787, -0.2604, -0.6212,  0.2145]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.607503783968312, distance: 0.7169261643859846 entropy 0.7110485434532166
epoch: 5, step: 69
	action: tensor([[ 0.8022,  0.1314, -0.5664, -0.7192,  0.4236, -0.3746,  0.4537]],
       dtype=torch.float64)
	q_value: tensor([[-15.1170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4369514018625099, distance: 0.8586766119223624 entropy 0.7110485434532166
epoch: 5, step: 70
	action: tensor([[ 1.3070,  0.9491, -0.7783,  0.2374,  0.0550,  0.0272,  0.2206]],
       dtype=torch.float64)
	q_value: tensor([[-15.6576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 71
	action: tensor([[ 0.5126, -0.0767, -0.0274,  0.1538, -0.3246, -0.3824,  0.7785]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6211344893778614, distance: 0.7043673703564509 entropy 0.7110485434532166
epoch: 5, step: 72
	action: tensor([[ 0.1345,  0.7497, -0.2754, -0.6627, -0.2486, -0.6962, -0.3525]],
       dtype=torch.float64)
	q_value: tensor([[-14.9755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7174576874726335, distance: 0.6082728620900091 entropy 0.7110485434532166
epoch: 5, step: 73
	action: tensor([[ 0.2319,  0.5366, -0.4134,  1.5282,  0.5869,  0.1127,  0.3262]],
       dtype=torch.float64)
	q_value: tensor([[-13.9981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 74
	action: tensor([[-0.0035, -0.0800, -1.0510,  0.0235,  0.2528, -0.6965, -0.2508]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30389525934588035, distance: 0.954759501368669 entropy 0.7110485434532166
epoch: 5, step: 75
	action: tensor([[ 1.1298, -0.2649,  0.1735,  0.2364,  0.2176,  0.3474, -0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-14.6660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6776602647916237, distance: 0.6497011203675629 entropy 0.7110485434532166
epoch: 5, step: 76
	action: tensor([[ 0.0365,  0.0168,  0.0208,  0.7503,  0.0397,  0.0031, -0.2739]],
       dtype=torch.float64)
	q_value: tensor([[-17.6624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6720312547894367, distance: 0.6553494245176827 entropy 0.7110485434532166
epoch: 5, step: 77
	action: tensor([[ 0.0924, -0.2534,  0.3023, -0.7758,  0.0654, -0.1779,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-13.9098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.346566894785427, distance: 1.327916171622177 entropy 0.7110485434532166
epoch: 5, step: 78
	action: tensor([[ 0.0798,  0.0728,  0.0473,  0.0877, -0.3667,  0.1361, -0.6373]],
       dtype=torch.float64)
	q_value: tensor([[-13.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45785716867527215, distance: 0.8425846694880673 entropy 0.7110485434532166
epoch: 5, step: 79
	action: tensor([[ 0.0982,  0.5215,  0.1107,  0.0971, -0.1487,  0.3363, -0.1153]],
       dtype=torch.float64)
	q_value: tensor([[-12.6686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6904661426591937, distance: 0.6366647032169074 entropy 0.7110485434532166
epoch: 5, step: 80
	action: tensor([[ 0.2062,  0.5891,  0.0534, -0.3378, -0.2003,  0.1627,  0.8197]],
       dtype=torch.float64)
	q_value: tensor([[-12.1074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7015245740414144, distance: 0.6251885040129517 entropy 0.7110485434532166
epoch: 5, step: 81
	action: tensor([[ 0.4680,  0.6838,  0.5363,  0.3401,  0.2767, -1.0828, -0.5107]],
       dtype=torch.float64)
	q_value: tensor([[-14.0615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8696309848858694, distance: 0.4131843709292777 entropy 0.7110485434532166
epoch: 5, step: 82
	action: tensor([[ 0.9397,  0.1875, -0.4646, -0.3367,  0.3092, -0.3580, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-18.8661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6479740276404217, distance: 0.6789597771941909 entropy 0.7110485434532166
epoch: 5, step: 83
	action: tensor([[-0.0790,  0.7591, -0.8417, -1.1630, -0.5003, -0.2740,  0.6842]],
       dtype=torch.float64)
	q_value: tensor([[-15.2409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7972970699214618, distance: 0.5152128739885597 entropy 0.7110485434532166
epoch: 5, step: 84
	action: tensor([[ 0.0487, -0.1997, -0.3911, -0.4497, -0.8783,  0.7490, -0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-17.3490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008012732573852377, distance: 1.139750370862576 entropy 0.7110485434532166
epoch: 5, step: 85
	action: tensor([[-0.0305,  0.8776, -0.0632, -0.4248,  0.5424, -0.7596, -0.0948]],
       dtype=torch.float64)
	q_value: tensor([[-15.7071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5049062471692647, distance: 0.8051937894281119 entropy 0.7110485434532166
epoch: 5, step: 86
	action: tensor([[0.1938, 0.2973, 0.2010, 0.3965, 0.3354, 1.2265, 0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-13.5856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9258506352474642, distance: 0.3116092999208183 entropy 0.7110485434532166
epoch: 5, step: 87
	action: tensor([[ 0.1161,  0.6068,  0.1796, -0.1783,  0.2278, -0.0041, -0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-16.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6625186540774043, distance: 0.6647855631543161 entropy 0.7110485434532166
epoch: 5, step: 88
	action: tensor([[ 0.6296,  0.3904, -1.2087, -0.4700, -0.2220,  0.4665, -0.1872]],
       dtype=torch.float64)
	q_value: tensor([[-11.6446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9203247952932478, distance: 0.32301172513639975 entropy 0.7110485434532166
epoch: 5, step: 89
	action: tensor([[ 0.2885, -0.3287, -0.8101,  0.6369, -0.4154, -0.1333, -0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-15.0711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3769293753985482, distance: 0.9032861087941073 entropy 0.7110485434532166
epoch: 5, step: 90
	action: tensor([[ 0.6969,  0.5314,  0.2768,  0.4265,  0.2824, -1.3222,  0.5003]],
       dtype=torch.float64)
	q_value: tensor([[-14.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9454579751345145, distance: 0.26725283556209045 entropy 0.7110485434532166
epoch: 5, step: 91
	action: tensor([[ 0.6726,  0.8319,  0.4048,  0.3756,  0.2273,  0.0170, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-20.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 92
	action: tensor([[-0.1888, -0.2737,  0.4351,  0.2481, -0.2750, -0.3793,  0.2667]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04440434354375844, distance: 1.1694752306476826 entropy 0.7110485434532166
epoch: 5, step: 93
	action: tensor([[ 0.2475, -0.2423, -0.0566, -0.8205,  1.1650, -0.6865,  0.2209]],
       dtype=torch.float64)
	q_value: tensor([[-14.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2258074554184888, distance: 1.266974349524101 entropy 0.7110485434532166
epoch: 5, step: 94
	action: tensor([[ 0.8248, -0.6671, -0.3392, -0.4202,  0.2378, -0.2915,  0.1700]],
       dtype=torch.float64)
	q_value: tensor([[-17.4915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.313123894997406, distance: 1.3113225978520966 entropy 0.7110485434532166
epoch: 5, step: 95
	action: tensor([[ 1.0283, -0.7874,  0.5488, -1.0289,  0.1721, -1.0576,  0.7100]],
       dtype=torch.float64)
	q_value: tensor([[-16.2172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30375629639916335, distance: 1.306636851841313 entropy 0.7110485434532166
epoch: 5, step: 96
	action: tensor([[ 1.1857, -0.3877, -0.1906, -0.7032, -0.1011,  0.2719,  0.6841]],
       dtype=torch.float64)
	q_value: tensor([[-23.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09592719088624668, distance: 1.1979744189577644 entropy 0.7110485434532166
epoch: 5, step: 97
	action: tensor([[ 0.8822,  0.5367, -0.5222,  0.0445, -1.5405, -0.0345,  0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-18.4635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9828418464345663, distance: 0.1498966643117069 entropy 0.7110485434532166
epoch: 5, step: 98
	action: tensor([[ 0.5550, -0.4943, -0.5200, -0.9363,  1.0748, -0.4582,  0.0707]],
       dtype=torch.float64)
	q_value: tensor([[-19.4515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2957995144273009, distance: 1.3026435688444038 entropy 0.7110485434532166
epoch: 5, step: 99
	action: tensor([[ 1.2309,  0.6689, -0.3047,  0.3326, -0.8691, -0.4181, -0.7095]],
       dtype=torch.float64)
	q_value: tensor([[-17.6708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8694642912719656, distance: 0.413448441320335 entropy 0.7110485434532166
epoch: 5, step: 100
	action: tensor([[ 0.8302,  0.0250, -0.0266,  0.0927,  0.4363, -0.8273, -1.1108]],
       dtype=torch.float64)
	q_value: tensor([[-19.5193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6607541024765813, distance: 0.6665212429067843 entropy 0.7110485434532166
epoch: 5, step: 101
	action: tensor([[ 0.5355,  0.0996,  0.3513, -0.2773, -0.6671,  0.2835, -0.1797]],
       dtype=torch.float64)
	q_value: tensor([[-19.3781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6778447475240995, distance: 0.6495151740207676 entropy 0.7110485434532166
epoch: 5, step: 102
	action: tensor([[ 5.2913e-01,  3.5473e-01, -2.9060e-04,  1.3933e-01, -9.8068e-02,
          7.2430e-01,  4.7723e-01]], dtype=torch.float64)
	q_value: tensor([[-15.9717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9437092032799227, distance: 0.27150347406760833 entropy 0.7110485434532166
epoch: 5, step: 103
	action: tensor([[ 0.8785, -0.0269, -0.0749, -0.5996, -0.0829, -0.6628,  0.6752]],
       dtype=torch.float64)
	q_value: tensor([[-14.5905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25191082752554317, distance: 0.9897679297082048 entropy 0.7110485434532166
epoch: 5, step: 104
	action: tensor([[ 0.4415, -0.8664, -0.1300, -0.3165,  1.0662, -0.2496,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-17.1366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5302119765429909, distance: 1.4155737207811505 entropy 0.7110485434532166
epoch: 5, step: 105
	action: tensor([[-0.3290,  0.1250, -0.0170, -0.0766,  0.3988,  0.0270,  1.0187]],
       dtype=torch.float64)
	q_value: tensor([[-18.3325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024181660986067444, distance: 1.1580976779021548 entropy 0.7110485434532166
epoch: 5, step: 106
	action: tensor([[ 0.2508, -0.5338,  0.9457, -0.4241, -0.0643,  0.0076, -0.5737]],
       dtype=torch.float64)
	q_value: tensor([[-15.4229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27955199984159074, distance: 1.2944511427131034 entropy 0.7110485434532166
epoch: 5, step: 107
	action: tensor([[ 0.2154,  0.0142, -0.3106, -0.0276, -0.0770, -0.4625,  0.2023]],
       dtype=torch.float64)
	q_value: tensor([[-17.9600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4193351612242373, distance: 0.8720059688704075 entropy 0.7110485434532166
epoch: 5, step: 108
	action: tensor([[ 0.2679,  0.0930,  0.3263, -0.0472, -0.0628,  0.0214,  0.3232]],
       dtype=torch.float64)
	q_value: tensor([[-12.1176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5757434569604745, distance: 0.7453683843535005 entropy 0.7110485434532166
epoch: 5, step: 109
	action: tensor([[ 0.0511, -0.3076,  0.0310, -1.0810, -0.0813, -0.5383,  0.5500]],
       dtype=torch.float64)
	q_value: tensor([[-12.6722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4212537032635604, distance: 1.3642453994745385 entropy 0.7110485434532166
epoch: 5, step: 110
	action: tensor([[ 1.3175,  0.5470, -0.0629, -0.3892, -0.2979, -0.8212, -1.2336]],
       dtype=torch.float64)
	q_value: tensor([[-15.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6317936873520369, distance: 0.6943881618294042 entropy 0.7110485434532166
epoch: 5, step: 111
	action: tensor([[ 0.6951,  0.5526, -0.8060, -0.0375, -0.1931,  0.4579, -0.1990]],
       dtype=torch.float64)
	q_value: tensor([[-21.8064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.95347607203616, distance: 0.24682825936220693 entropy 0.7110485434532166
epoch: 5, step: 112
	action: tensor([[ 0.4513,  0.3935, -0.4975, -0.6789, -0.7013,  0.3912,  0.8863]],
       dtype=torch.float64)
	q_value: tensor([[-13.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7522998869111185, distance: 0.5695341836391019 entropy 0.7110485434532166
epoch: 5, step: 113
	action: tensor([[0.6568, 0.8733, 0.2677, 0.5776, 0.6053, 0.4811, 0.9841]],
       dtype=torch.float64)
	q_value: tensor([[-17.3754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 114
	action: tensor([[ 0.1474, -0.1041, -0.1501, -0.2467,  0.1416, -0.5789, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14356834607987512, distance: 1.0590172848508685 entropy 0.7110485434532166
epoch: 5, step: 115
	action: tensor([[ 0.9759, -0.5753, -0.3217,  0.3196, -0.2684, -0.2781, -0.3141]],
       dtype=torch.float64)
	q_value: tensor([[-12.6045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29432441441551205, distance: 0.9613006576534551 entropy 0.7110485434532166
epoch: 5, step: 116
	action: tensor([[ 0.8164,  0.3756, -1.0306, -0.2921, -0.2538,  0.3369,  0.8431]],
       dtype=torch.float64)
	q_value: tensor([[-17.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9242599617505809, distance: 0.3149339304588588 entropy 0.7110485434532166
epoch: 5, step: 117
	action: tensor([[ 0.4795,  0.1254, -0.7539,  0.2896, -0.1069,  0.3197,  0.3561]],
       dtype=torch.float64)
	q_value: tensor([[-16.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.771549491707249, distance: 0.5469564619398978 entropy 0.7110485434532166
epoch: 5, step: 118
	action: tensor([[-0.4499,  0.5244,  0.0726, -0.5943,  0.1845,  0.0415, -0.3264]],
       dtype=torch.float64)
	q_value: tensor([[-12.7091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07180937758633399, distance: 1.1847193163954202 entropy 0.7110485434532166
epoch: 5, step: 119
	action: tensor([[ 0.1502,  1.0487, -0.0232,  1.2109,  0.4852,  0.3906, -0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-11.6123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 120
	action: tensor([[ 0.4559, -0.1725,  0.5872, -0.1868, -0.2075,  0.9206, -0.2576]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6801830223274316, distance: 0.6471537180945378 entropy 0.7110485434532166
epoch: 5, step: 121
	action: tensor([[ 0.7978,  0.0399,  0.0067, -0.2662, -0.1083,  0.3043, -0.3454]],
       dtype=torch.float64)
	q_value: tensor([[-17.4401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7128397192785669, distance: 0.6132236246933774 entropy 0.7110485434532166
epoch: 5, step: 122
	action: tensor([[ 0.4645,  0.1599, -0.2586, -0.4580, -0.2992,  0.2749,  0.4995]],
       dtype=torch.float64)
	q_value: tensor([[-14.2127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6435511797790964, distance: 0.6832116827605036 entropy 0.7110485434532166
epoch: 5, step: 123
	action: tensor([[-0.3994,  1.4171,  0.0719, -0.6881,  0.0743, -0.6118, -0.4065]],
       dtype=torch.float64)
	q_value: tensor([[-13.7158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3596668039677199, distance: 0.9157136790444363 entropy 0.7110485434532166
epoch: 5, step: 124
	action: tensor([[ 0.5013,  0.4882,  0.6919, -0.6747,  0.1130,  0.4601, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-15.5953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7559972875054939, distance: 0.5652675051351912 entropy 0.7110485434532166
epoch: 5, step: 125
	action: tensor([[-0.3062,  0.2991, -0.4367, -0.4391, -0.2373, -0.7957,  0.4014]],
       dtype=torch.float64)
	q_value: tensor([[-16.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19152574492207963, distance: 1.0289393839298426 entropy 0.7110485434532166
epoch: 5, step: 126
	action: tensor([[ 0.3988,  0.1182, -0.1490,  0.4281,  0.1242,  0.0797, -0.7135]],
       dtype=torch.float64)
	q_value: tensor([[-12.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.810306783064167, distance: 0.49840523362086075 entropy 0.7110485434532166
epoch: 5, step: 127
	action: tensor([[ 0.0641,  0.7524, -0.1585,  0.4379, -0.6484,  0.1252, -0.3507]],
       dtype=torch.float64)
	q_value: tensor([[-13.4477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
LOSS epoch 5 actor 195.65527928525444 critic 1056.4390175569122 
epoch: 6, step: 0
	action: tensor([[ 0.9935,  0.2451, -0.2578, -0.2335, -0.4279,  0.3333,  0.0635]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8533380435546691, distance: 0.43824344645840096 entropy 0.6056879162788391
epoch: 6, step: 1
	action: tensor([[ 0.7983,  0.3757, -0.4644,  0.1664, -0.5122,  0.1243, -0.5617]],
       dtype=torch.float64)
	q_value: tensor([[-13.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9682695772775276, distance: 0.20384244611951563 entropy 0.6056879162788391
epoch: 6, step: 2
	action: tensor([[ 0.8931,  0.7285, -0.6045, -0.3162, -0.2320, -0.4550,  0.7404]],
       dtype=torch.float64)
	q_value: tensor([[-12.7639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.93755931672631, distance: 0.28595027376410365 entropy 0.6056879162788391
epoch: 6, step: 3
	action: tensor([[ 0.6792,  0.0995, -0.1274,  0.1206,  0.2116,  0.0784, -0.3322]],
       dtype=torch.float64)
	q_value: tensor([[-14.2162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8653205342746759, distance: 0.41995947652381826 entropy 0.6056879162788391
epoch: 6, step: 4
	action: tensor([[ 0.0581,  0.0088, -0.0443, -0.4444, -0.5011, -1.0384,  0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-11.6861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22852245262619286, distance: 1.0051209815689488 entropy 0.6056879162788391
epoch: 6, step: 5
	action: tensor([[ 0.0144,  0.3109, -0.6241,  0.2966, -0.5868,  0.5382, -0.0732]],
       dtype=torch.float64)
	q_value: tensor([[-13.6151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36767467120510944, distance: 0.9099698069064796 entropy 0.6056879162788391
epoch: 6, step: 6
	action: tensor([[-0.6019,  0.4228, -0.4684,  0.1724, -0.3020, -0.2362, -0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-11.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05572790727983867, distance: 1.1757979379939336 entropy 0.6056879162788391
epoch: 6, step: 7
	action: tensor([[ 1.1903,  0.7587, -0.4821,  0.0529,  0.3477, -0.1016,  0.7534]],
       dtype=torch.float64)
	q_value: tensor([[-9.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9256815687511711, distance: 0.3119643447463669 entropy 0.6056879162788391
epoch: 6, step: 8
	action: tensor([[ 1.7629, -0.0645, -1.1191,  0.1473, -0.3616,  0.2313,  0.5497]],
       dtype=torch.float64)
	q_value: tensor([[-15.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 9
	action: tensor([[-0.0590, -0.3540, -0.7756, -0.0079,  0.1756, -0.9775,  0.0708]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02054590820534241, distance: 1.13252744622703 entropy 0.6056879162788391
epoch: 6, step: 10
	action: tensor([[-0.1486, -0.0531,  0.0760,  0.2813, -0.8644,  0.6031, -0.1354]],
       dtype=torch.float64)
	q_value: tensor([[-14.5573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2461794541662038, distance: 0.9935521749446256 entropy 0.6056879162788391
epoch: 6, step: 11
	action: tensor([[ 0.0210, -0.0445, -0.3076,  0.8418,  0.3501,  0.0296,  0.7618]],
       dtype=torch.float64)
	q_value: tensor([[-13.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49217693507934657, distance: 0.8154792304603873 entropy 0.6056879162788391
epoch: 6, step: 12
	action: tensor([[ 0.3279,  0.1419,  0.0428,  0.2078, -0.0130, -0.6724, -0.1942]],
       dtype=torch.float64)
	q_value: tensor([[-14.0387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6228529405228094, distance: 0.7027681266236476 entropy 0.6056879162788391
epoch: 6, step: 13
	action: tensor([[ 0.7712,  0.0792,  0.1563,  0.3293, -0.4294,  0.2182,  0.1731]],
       dtype=torch.float64)
	q_value: tensor([[-12.3370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9416100193006262, distance: 0.27651956124795785 entropy 0.6056879162788391
epoch: 6, step: 14
	action: tensor([[-0.4164,  0.5143, -0.4505, -0.0467, -0.4025,  0.2658,  0.2716]],
       dtype=torch.float64)
	q_value: tensor([[-13.2619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14890069322227795, distance: 1.055715290346394 entropy 0.6056879162788391
epoch: 6, step: 15
	action: tensor([[ 0.3498, -0.1042, -0.0518, -0.3335,  0.2866,  0.7693, -0.0913]],
       dtype=torch.float64)
	q_value: tensor([[-10.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5708175931869138, distance: 0.7496829757312072 entropy 0.6056879162788391
epoch: 6, step: 16
	action: tensor([[ 0.7316,  0.0985,  0.2274,  0.0125,  0.8184, -0.2821, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-12.1392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7859565147110099, distance: 0.5294289679980619 entropy 0.6056879162788391
epoch: 6, step: 17
	action: tensor([[ 0.9418,  0.1310, -0.0190,  0.1893, -0.4307,  0.0618, -0.0280]],
       dtype=torch.float64)
	q_value: tensor([[-14.4990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.901054311482475, distance: 0.3599607356604935 entropy 0.6056879162788391
epoch: 6, step: 18
	action: tensor([[ 0.1698,  0.1872, -0.5530,  0.4252,  0.0695, -0.3547, -0.1901]],
       dtype=torch.float64)
	q_value: tensor([[-13.2100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.611147648128644, distance: 0.7135904977243845 entropy 0.6056879162788391
epoch: 6, step: 19
	action: tensor([[ 0.0317,  0.4806,  0.3996,  0.9009,  0.7732,  0.1671, -0.1794]],
       dtype=torch.float64)
	q_value: tensor([[-10.9711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8629387124662303, distance: 0.42365671740240807 entropy 0.6056879162788391
epoch: 6, step: 20
	action: tensor([[ 0.8396,  0.1808, -0.6572, -0.6959,  0.0200, -0.2801, -0.3350]],
       dtype=torch.float64)
	q_value: tensor([[-14.5801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5183411404323176, distance: 0.794193758749011 entropy 0.6056879162788391
epoch: 6, step: 21
	action: tensor([[ 0.5827,  0.4372, -0.8028,  0.3229, -0.5381, -0.2881,  0.3055]],
       dtype=torch.float64)
	q_value: tensor([[-13.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9219429072078756, distance: 0.31971490199578934 entropy 0.6056879162788391
epoch: 6, step: 22
	action: tensor([[ 0.4205,  0.0322, -0.2345, -0.1478,  0.5311, -0.6484,  0.2239]],
       dtype=torch.float64)
	q_value: tensor([[-12.2075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4395168839102801, distance: 0.8567181357571101 entropy 0.6056879162788391
epoch: 6, step: 23
	action: tensor([[ 0.4384,  0.4259, -1.1880,  0.3777, -0.0486, -0.3115,  0.3461]],
       dtype=torch.float64)
	q_value: tensor([[-13.0678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8110913690120909, distance: 0.4973734441553408 entropy 0.6056879162788391
epoch: 6, step: 24
	action: tensor([[ 0.8257,  0.0075, -0.2505,  0.0475, -0.3326, -0.0577,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-12.4361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7623989783852283, distance: 0.5578029992532437 entropy 0.6056879162788391
epoch: 6, step: 25
	action: tensor([[ 1.3685,  0.2569, -0.0546, -0.0440,  0.8647, -0.1559, -0.5056]],
       dtype=torch.float64)
	q_value: tensor([[-12.7227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7294568302168654, distance: 0.5952165253855308 entropy 0.6056879162788391
epoch: 6, step: 26
	action: tensor([[ 0.3441,  0.6611, -0.4620, -0.2666, -0.3113, -0.0929, -0.0561]],
       dtype=torch.float64)
	q_value: tensor([[-17.7829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8954189303253807, distance: 0.3700694486239566 entropy 0.6056879162788391
epoch: 6, step: 27
	action: tensor([[ 0.4356,  0.5222, -0.7070, -0.4474,  0.1888,  0.0377,  0.0911]],
       dtype=torch.float64)
	q_value: tensor([[-10.5754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9045662768553089, distance: 0.3535148215793235 entropy 0.6056879162788391
epoch: 6, step: 28
	action: tensor([[ 0.5364,  0.4496, -0.5000,  0.1039, -0.6605,  0.5441,  0.6375]],
       dtype=torch.float64)
	q_value: tensor([[-10.9941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8694965287591812, distance: 0.4133973849389555 entropy 0.6056879162788391
epoch: 6, step: 29
	action: tensor([[ 1.4569,  0.0642,  0.5436,  0.3832,  0.4046, -0.9938, -0.3487]],
       dtype=torch.float64)
	q_value: tensor([[-14.1197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.687338852242217, distance: 0.6398728041292246 entropy 0.6056879162788391
epoch: 6, step: 30
	action: tensor([[ 0.5064,  0.0736, -0.3532, -0.2421, -0.7412,  0.0974,  0.0952]],
       dtype=torch.float64)
	q_value: tensor([[-20.8291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6472353418038197, distance: 0.6796717632139841 entropy 0.6056879162788391
epoch: 6, step: 31
	action: tensor([[ 0.3562, -0.2223, -0.2794,  0.2472, -0.1795, -0.2647,  0.7399]],
       dtype=torch.float64)
	q_value: tensor([[-12.5346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4833537468116954, distance: 0.8225330085112761 entropy 0.6056879162788391
epoch: 6, step: 32
	action: tensor([[ 0.1768,  0.8229, -0.9773,  0.4348, -0.3887, -0.3418,  0.4504]],
       dtype=torch.float64)
	q_value: tensor([[-12.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 33
	action: tensor([[-0.0326,  0.2435, -0.8057, -0.0355,  0.3676, -0.6671,  0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45070727345408146, distance: 0.8481225636742388 entropy 0.6056879162788391
epoch: 6, step: 34
	action: tensor([[-0.2767,  0.1669, -0.5435,  0.3150, -0.1699, -0.4628,  0.2949]],
       dtype=torch.float64)
	q_value: tensor([[-11.6985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19022977799779894, distance: 1.0297637376100202 entropy 0.6056879162788391
epoch: 6, step: 35
	action: tensor([[ 1.0573,  0.1399,  0.0726,  0.1371, -0.1794,  0.2431,  0.5660]],
       dtype=torch.float64)
	q_value: tensor([[-10.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9052819546839884, distance: 0.35218678559360095 entropy 0.6056879162788391
epoch: 6, step: 36
	action: tensor([[-0.3830, -0.4292,  0.4676, -0.8759, -0.2146,  0.5118,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-14.4846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8197218899466876, distance: 1.543686844117153 entropy 0.6056879162788391
epoch: 6, step: 37
	action: tensor([[ 1.0998, -0.3588, -0.0718, -0.4221,  0.3889, -0.6546, -0.6237]],
       dtype=torch.float64)
	q_value: tensor([[-15.4817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020110990356614122, distance: 1.155793922637805 entropy 0.6056879162788391
epoch: 6, step: 38
	action: tensor([[ 0.7961,  0.8777, -0.3989,  0.1951,  0.3081, -0.4411, -0.3563]],
       dtype=torch.float64)
	q_value: tensor([[-16.9880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9557568910457048, distance: 0.24070189635297354 entropy 0.6056879162788391
epoch: 6, step: 39
	action: tensor([[ 0.7417, -0.2071,  0.1855, -0.1943,  0.0131, -0.7774, -0.2247]],
       dtype=torch.float64)
	q_value: tensor([[-13.6405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2561286123627484, distance: 0.9869737915318657 entropy 0.6056879162788391
epoch: 6, step: 40
	action: tensor([[ 0.6924,  1.3293, -0.2849, -0.4325, -0.7288, -0.6733,  0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-14.8720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9407945205566711, distance: 0.27844385923150344 entropy 0.6056879162788391
epoch: 6, step: 41
	action: tensor([[ 0.2123,  0.6286,  0.1487, -0.0480, -0.9881, -0.5743,  0.4215]],
       dtype=torch.float64)
	q_value: tensor([[-16.4395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8543677542224408, distance: 0.436702287338131 entropy 0.6056879162788391
epoch: 6, step: 42
	action: tensor([[ 0.0772, -0.0571, -0.6097,  0.0503,  0.5475, -0.5249, -0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-14.7932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2596296581141957, distance: 0.984648446164207 entropy 0.6056879162788391
epoch: 6, step: 43
	action: tensor([[ 0.7216, -0.1383, -0.2906,  0.0716, -0.2395, -0.0937, -0.6091]],
       dtype=torch.float64)
	q_value: tensor([[-11.9430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6235852600497358, distance: 0.7020855004692436 entropy 0.6056879162788391
epoch: 6, step: 44
	action: tensor([[ 0.7766,  0.5558,  0.1975, -0.2138, -1.1584, -0.6060,  0.1408]],
       dtype=torch.float64)
	q_value: tensor([[-12.7261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9303453805173696, distance: 0.3020171849723663 entropy 0.6056879162788391
epoch: 6, step: 45
	action: tensor([[ 0.5014,  0.4146, -0.2911,  0.4421,  0.6409, -0.2223,  0.4418]],
       dtype=torch.float64)
	q_value: tensor([[-16.7361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8897599904608819, distance: 0.3799498823263547 entropy 0.6056879162788391
epoch: 6, step: 46
	action: tensor([[ 0.7778,  1.0845,  0.4532,  0.4821,  0.0344, -0.0520, -0.3645]],
       dtype=torch.float64)
	q_value: tensor([[-13.4616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 47
	action: tensor([[-0.2299,  0.3183,  0.2331, -0.3797, -0.5970, -0.5437,  0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10051403083419175, distance: 1.0853102132108556 entropy 0.6056879162788391
epoch: 6, step: 48
	action: tensor([[ 0.4626, -0.0184, -0.4906,  0.6291, -0.5132,  0.2805, -0.2916]],
       dtype=torch.float64)
	q_value: tensor([[-12.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7280337151540885, distance: 0.5967799553407787 entropy 0.6056879162788391
epoch: 6, step: 49
	action: tensor([[ 1.3598, -0.0056, -1.2297,  0.3303, -0.1553, -0.4786, -0.7249]],
       dtype=torch.float64)
	q_value: tensor([[-11.9425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.652217046327629, distance: 0.6748555735956233 entropy 0.6056879162788391
epoch: 6, step: 50
	action: tensor([[ 0.4269, -0.5672, -0.3266,  0.6031,  0.0020, -0.7875,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[-17.9250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27757283688681844, distance: 0.9726435849742917 entropy 0.6056879162788391
epoch: 6, step: 51
	action: tensor([[ 1.2552,  1.0371, -0.3178, -0.2316,  0.1694, -0.3796,  0.2614]],
       dtype=torch.float64)
	q_value: tensor([[-16.1193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8742549516500673, distance: 0.40579074654694514 entropy 0.6056879162788391
epoch: 6, step: 52
	action: tensor([[ 0.5833,  0.1605, -0.6789,  0.6607, -0.6303, -0.0631,  0.3009]],
       dtype=torch.float64)
	q_value: tensor([[-15.6681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8571121349873506, distance: 0.4325679783758988 entropy 0.6056879162788391
epoch: 6, step: 53
	action: tensor([[ 0.4498,  0.8988, -1.1110,  0.2973,  0.4848, -0.1585, -0.2084]],
       dtype=torch.float64)
	q_value: tensor([[-12.4455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 54
	action: tensor([[ 0.0148, -1.0427, -0.3325, -0.4903,  0.8831,  0.5491,  0.6406]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5355882412692841, distance: 1.418058287034783 entropy 0.6056879162788391
epoch: 6, step: 55
	action: tensor([[ 0.7324,  0.4702, -0.0755, -0.0893, -0.7731, -1.0992, -0.0382]],
       dtype=torch.float64)
	q_value: tensor([[-17.7118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8840928587063925, distance: 0.38959357904033776 entropy 0.6056879162788391
epoch: 6, step: 56
	action: tensor([[ 0.9673,  0.0561, -1.3223, -0.5574, -1.4765,  0.0791, -0.4181]],
       dtype=torch.float64)
	q_value: tensor([[-15.7034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5946366230861488, distance: 0.7285828561187824 entropy 0.6056879162788391
epoch: 6, step: 57
	action: tensor([[ 0.3297,  0.7759, -0.8009,  0.3263, -0.5380,  0.5216,  0.4531]],
       dtype=torch.float64)
	q_value: tensor([[-18.4591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6053309023388106, distance: 0.7189078976670386 entropy 0.6056879162788391
epoch: 6, step: 58
	action: tensor([[ 0.3095,  0.0998,  0.8468,  0.4982, -0.0179, -0.2908,  0.3886]],
       dtype=torch.float64)
	q_value: tensor([[-13.6955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8232821491727736, distance: 0.48105740461551616 entropy 0.6056879162788391
epoch: 6, step: 59
	action: tensor([[-0.0468,  0.1960, -0.3119, -0.0179,  0.1914,  0.2143, -0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-15.6042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42500587127762435, distance: 0.8677375640874955 entropy 0.6056879162788391
epoch: 6, step: 60
	action: tensor([[-0.6446, -0.4124,  0.0967,  0.0955, -0.0535, -0.2695,  0.8869]],
       dtype=torch.float64)
	q_value: tensor([[-9.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7830836443137439, distance: 1.5280675440215377 entropy 0.6056879162788391
epoch: 6, step: 61
	action: tensor([[ 0.6822,  0.1341,  0.3966, -0.0370, -0.1352,  0.0509,  0.4136]],
       dtype=torch.float64)
	q_value: tensor([[-15.8830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8213824137061154, distance: 0.48363620138372215 entropy 0.6056879162788391
epoch: 6, step: 62
	action: tensor([[ 1.1601,  0.6404,  0.2989, -0.0689, -0.3242, -0.1071, -0.4760]],
       dtype=torch.float64)
	q_value: tensor([[-13.1005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9378943825632707, distance: 0.28518201624084527 entropy 0.6056879162788391
epoch: 6, step: 63
	action: tensor([[ 0.1451, -0.3083, -0.3731,  0.6296, -0.2394,  0.2906,  0.3495]],
       dtype=torch.float64)
	q_value: tensor([[-16.1807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.446478396468878, distance: 0.8513810533203832 entropy 0.6056879162788391
epoch: 6, step: 64
	action: tensor([[ 0.9517, -0.1835, -0.6023,  0.4516, -0.5681, -0.4829, -0.6338]],
       dtype=torch.float64)
	q_value: tensor([[-12.1788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7653901133309238, distance: 0.5542808166085803 entropy 0.6056879162788391
epoch: 6, step: 65
	action: tensor([[ 0.6079, -0.7242,  0.3278,  0.4565, -0.0569, -1.1518,  0.1794]],
       dtype=torch.float64)
	q_value: tensor([[-15.8557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005578226957832388, distance: 1.1411480845437214 entropy 0.6056879162788391
epoch: 6, step: 66
	action: tensor([[ 0.3804,  0.5750,  0.1044, -0.1297,  0.3180, -0.3345, -0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-19.7608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.813155805354324, distance: 0.49464827350927554 entropy 0.6056879162788391
epoch: 6, step: 67
	action: tensor([[ 0.7144,  0.9340, -0.2398,  0.4845, -0.1042,  0.0911,  0.2947]],
       dtype=torch.float64)
	q_value: tensor([[-11.3157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 68
	action: tensor([[ 0.7579,  0.6165,  0.1455,  0.2222,  0.1163,  0.0774, -0.5243]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9725455233335206, distance: 0.1896108905978685 entropy 0.6056879162788391
epoch: 6, step: 69
	action: tensor([[-0.0335,  0.9649, -0.4826,  0.6120,  0.6118, -0.7151,  0.4830]],
       dtype=torch.float64)
	q_value: tensor([[-13.1472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 70
	action: tensor([[-0.1261,  0.3165, -0.3414, -0.4830, -0.2127, -0.2770,  0.2754]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3302751830354972, distance: 0.9364937630798964 entropy 0.6056879162788391
epoch: 6, step: 71
	action: tensor([[ 0.1446,  0.2420,  0.0185, -0.3029, -0.5201,  0.0880,  0.5865]],
       dtype=torch.float64)
	q_value: tensor([[-10.1696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4943956617321159, distance: 0.8136958277007705 entropy 0.6056879162788391
epoch: 6, step: 72
	action: tensor([[ 0.1807,  0.0010, -0.1766, -0.7772,  0.0960, -0.3768, -0.1512]],
       dtype=torch.float64)
	q_value: tensor([[-12.1829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09120046283105243, distance: 1.0909145693402862 entropy 0.6056879162788391
epoch: 6, step: 73
	action: tensor([[ 0.5292,  0.3598, -0.7086,  0.1550, -0.3541, -0.1531, -0.2624]],
       dtype=torch.float64)
	q_value: tensor([[-11.1097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8824609134726533, distance: 0.39232668518352554 entropy 0.6056879162788391
epoch: 6, step: 74
	action: tensor([[ 1.3493e+00,  6.6039e-01, -7.3728e-04,  4.9031e-01, -5.2194e-01,
          1.1268e-01,  3.0893e-01]], dtype=torch.float64)
	q_value: tensor([[-11.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 75
	action: tensor([[ 0.5951, -0.1209, -1.1100,  0.1806,  0.0758, -0.3589,  0.7842]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6009491238108854, distance: 0.7228876873156168 entropy 0.6056879162788391
epoch: 6, step: 76
	action: tensor([[ 9.4589e-01,  1.7711e-04, -2.2299e+00, -1.0184e+00,  1.7089e-01,
         -6.1094e-01, -5.2514e-02]], dtype=torch.float64)
	q_value: tensor([[-14.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6947463435998955, distance: 0.6322475143825639 entropy 0.6056879162788391
epoch: 6, step: 77
	action: tensor([[ 0.7057,  0.3814, -1.3159,  0.9031, -0.3106,  0.0879, -0.0207]],
       dtype=torch.float64)
	q_value: tensor([[-18.6095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7087500285716575, distance: 0.6175749032154161 entropy 0.6056879162788391
epoch: 6, step: 78
	action: tensor([[ 1.2999, -0.8382, -0.1989, -0.2646, -0.2129,  0.3867, -0.3606]],
       dtype=torch.float64)
	q_value: tensor([[-14.5262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28111383636487863, distance: 1.2952409130837228 entropy 0.6056879162788391
epoch: 6, step: 79
	action: tensor([[ 0.9320, -0.0856,  0.1619,  0.3643, -0.6279, -0.1369,  0.2250]],
       dtype=torch.float64)
	q_value: tensor([[-16.7663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8100113183963854, distance: 0.4987932386338164 entropy 0.6056879162788391
epoch: 6, step: 80
	action: tensor([[ 0.8057, -0.8306, -0.5712,  0.1269, -0.4189, -1.5014, -0.2517]],
       dtype=torch.float64)
	q_value: tensor([[-14.6023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04148816135384936, distance: 1.1678413870035027 entropy 0.6056879162788391
epoch: 6, step: 81
	action: tensor([[ 0.1558,  0.7329, -0.5308,  0.0493, -0.5518,  0.0502, -0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-21.0059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7203301226072375, distance: 0.6051729939974584 entropy 0.6056879162788391
epoch: 6, step: 82
	action: tensor([[ 0.3984, -0.4529,  0.7092, -0.3804, -0.1784,  0.5891,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-10.9403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16667005257192413, distance: 1.0446364802148116 entropy 0.6056879162788391
epoch: 6, step: 83
	action: tensor([[-0.1014,  0.4112, -0.0010,  0.2656, -0.3306,  0.3453,  0.9146]],
       dtype=torch.float64)
	q_value: tensor([[-15.6843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5401317451021387, distance: 0.7760208834959564 entropy 0.6056879162788391
epoch: 6, step: 84
	action: tensor([[ 0.4943,  0.4110, -0.0567,  0.6489, -0.3774,  0.3989, -0.3772]],
       dtype=torch.float64)
	q_value: tensor([[-12.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8656566256360945, distance: 0.4194351467091561 entropy 0.6056879162788391
epoch: 6, step: 85
	action: tensor([[ 0.1868,  0.1869, -0.2209, -0.1692,  0.4877,  0.8767, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-12.7521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7617570071492572, distance: 0.558556051413054 entropy 0.6056879162788391
epoch: 6, step: 86
	action: tensor([[ 0.7884,  0.2080, -0.1976, -0.8172, -0.8483, -0.2523,  0.2539]],
       dtype=torch.float64)
	q_value: tensor([[-11.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48300319235087363, distance: 0.8228120134577033 entropy 0.6056879162788391
epoch: 6, step: 87
	action: tensor([[ 0.7064,  0.7372, -1.0692,  0.0630, -0.7292,  0.4823,  1.8718]],
       dtype=torch.float64)
	q_value: tensor([[-15.4599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.938264163728752, distance: 0.284331751938968 entropy 0.6056879162788391
epoch: 6, step: 88
	action: tensor([[ 0.6851,  0.2670, -0.2504, -0.1190, -0.5940, -0.5661, -0.2169]],
       dtype=torch.float64)
	q_value: tensor([[-20.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8223756404257339, distance: 0.48228966533493667 entropy 0.6056879162788391
epoch: 6, step: 89
	action: tensor([[ 0.0963,  0.4943, -0.5893,  0.4557,  0.4767, -0.2337, -0.8293]],
       dtype=torch.float64)
	q_value: tensor([[-12.6993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5898148313444396, distance: 0.7329032877112628 entropy 0.6056879162788391
epoch: 6, step: 90
	action: tensor([[ 1.0451, -0.0425,  0.5530, -0.7387, -0.1336,  0.1983,  0.0900]],
       dtype=torch.float64)
	q_value: tensor([[-12.7971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43107786953700067, distance: 0.863143704180452 entropy 0.6056879162788391
epoch: 6, step: 91
	action: tensor([[ 0.7339,  0.4189, -0.3923,  0.0876, -0.4668,  0.5094,  0.3498]],
       dtype=torch.float64)
	q_value: tensor([[-16.2410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9639945332228913, distance: 0.21714054125022267 entropy 0.6056879162788391
epoch: 6, step: 92
	action: tensor([[ 0.8577, -0.2186, -0.1410,  1.1670,  0.1249,  0.4430,  1.2578]],
       dtype=torch.float64)
	q_value: tensor([[-13.3347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.992191756853815, distance: 0.10111913772799119 entropy 0.6056879162788391
epoch: 6, step: 93
	action: tensor([[ 0.3883,  0.6054, -1.3830,  0.2919, -0.7630,  0.2968,  0.2379]],
       dtype=torch.float64)
	q_value: tensor([[-19.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7051336519891138, distance: 0.6213972096683861 entropy 0.6056879162788391
epoch: 6, step: 94
	action: tensor([[ 1.0283,  0.6992,  0.0864,  0.9861, -0.0516,  0.3503,  0.1101]],
       dtype=torch.float64)
	q_value: tensor([[-14.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 95
	action: tensor([[ 0.5276, -0.2494,  0.5355,  0.3050, -0.2733, -0.2821, -1.5053]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5973568466525322, distance: 0.726134134169291 entropy 0.6056879162788391
epoch: 6, step: 96
	action: tensor([[ 0.9407,  0.5520, -0.4655,  0.0119, -0.6050, -1.2426,  0.7420]],
       dtype=torch.float64)
	q_value: tensor([[-18.4242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.884765592394687, distance: 0.38846131863113453 entropy 0.6056879162788391
epoch: 6, step: 97
	action: tensor([[ 0.2061,  0.7236, -0.7398,  0.0793, -0.2762, -0.4754, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-16.9293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8524968545396856, distance: 0.43949843614661477 entropy 0.6056879162788391
epoch: 6, step: 98
	action: tensor([[ 0.6981, -0.4973, -0.4809,  0.3659,  0.2777, -0.4897, -1.1306]],
       dtype=torch.float64)
	q_value: tensor([[-11.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3439203603210613, distance: 0.926904463396064 entropy 0.6056879162788391
epoch: 6, step: 99
	action: tensor([[ 1.3542, -0.0630, -0.6263, -0.5343, -0.0518, -0.4027,  0.3840]],
       dtype=torch.float64)
	q_value: tensor([[-16.7555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06288560822882905, distance: 1.1077786659760558 entropy 0.6056879162788391
epoch: 6, step: 100
	action: tensor([[ 1.5571,  1.0421, -0.6051,  0.7719, -0.4810, -0.5306,  0.0940]],
       dtype=torch.float64)
	q_value: tensor([[-16.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 101
	action: tensor([[-0.0407,  0.2923, -0.6677, -0.4117, -0.5585, -0.5260,  0.2683]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5884970979834809, distance: 0.7340795817204471 entropy 0.6056879162788391
epoch: 6, step: 102
	action: tensor([[ 0.4517, -0.0428, -0.1083,  0.0532,  0.3963,  0.1390,  0.2209]],
       dtype=torch.float64)
	q_value: tensor([[-11.8262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.670005068422719, distance: 0.6573706773417065 entropy 0.6056879162788391
epoch: 6, step: 103
	action: tensor([[ 1.1975,  0.2741, -0.0169,  0.1179,  0.3203, -0.1535, -0.3873]],
       dtype=torch.float64)
	q_value: tensor([[-11.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8610301299792815, distance: 0.4265962357276756 entropy 0.6056879162788391
epoch: 6, step: 104
	action: tensor([[ 1.0437,  0.3006,  0.2295,  0.3456, -0.0621, -0.0231,  0.2359]],
       dtype=torch.float64)
	q_value: tensor([[-15.2446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9542921676777398, distance: 0.24465382273158495 entropy 0.6056879162788391
epoch: 6, step: 105
	action: tensor([[ 0.0694,  0.4317, -0.7211, -0.2817, -0.0207, -0.5708, -0.0971]],
       dtype=torch.float64)
	q_value: tensor([[-14.3074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6918520906315113, distance: 0.6352377606111236 entropy 0.6056879162788391
epoch: 6, step: 106
	action: tensor([[-0.1116,  0.0032,  0.0351,  0.0539, -0.3196,  0.3071,  1.1441]],
       dtype=torch.float64)
	q_value: tensor([[-10.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24311800654539106, distance: 0.9955676585608587 entropy 0.6056879162788391
epoch: 6, step: 107
	action: tensor([[ 0.7232,  1.2078, -0.4278, -0.3494,  0.2480, -0.3629,  0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-14.5808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9517655503180508, distance: 0.25132480671749563 entropy 0.6056879162788391
epoch: 6, step: 108
	action: tensor([[ 0.3843, -0.2407, -0.4360,  0.0415,  1.0727,  0.1331, -0.8648]],
       dtype=torch.float64)
	q_value: tensor([[-13.7098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.472666232080326, distance: 0.8309970545623707 entropy 0.6056879162788391
epoch: 6, step: 109
	action: tensor([[ 0.7968,  0.4291, -0.1866,  0.2593, -0.1932,  0.7270,  0.1645]],
       dtype=torch.float64)
	q_value: tensor([[-14.4109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9674095149245884, distance: 0.20658657975839986 entropy 0.6056879162788391
epoch: 6, step: 110
	action: tensor([[ 0.5838,  0.5114, -0.3383,  0.8678,  0.8800, -1.1961,  0.4555]],
       dtype=torch.float64)
	q_value: tensor([[-13.7099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.820327535919645, distance: 0.48506222572620344 entropy 0.6056879162788391
epoch: 6, step: 111
	action: tensor([[ 0.7349,  0.2771, -0.6789,  0.0685,  0.0136, -0.2013, -0.3909]],
       dtype=torch.float64)
	q_value: tensor([[-18.9940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8886109810027518, distance: 0.38192482019253154 entropy 0.6056879162788391
epoch: 6, step: 112
	action: tensor([[ 0.7504,  0.4829, -0.0053,  0.5024,  0.1331, -0.0444,  0.6596]],
       dtype=torch.float64)
	q_value: tensor([[-12.0033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9678227991679084, distance: 0.20527252469629875 entropy 0.6056879162788391
epoch: 6, step: 113
	action: tensor([[ 1.0280,  0.3111, -0.1924,  0.8047,  0.5530, -0.2615,  0.6712]],
       dtype=torch.float64)
	q_value: tensor([[-13.3739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9867431326194943, distance: 0.13175804708841718 entropy 0.6056879162788391
epoch: 6, step: 114
	action: tensor([[ 0.6311,  0.2282, -0.8538,  0.0700, -0.0917, -0.7443,  0.6851]],
       dtype=torch.float64)
	q_value: tensor([[-16.7727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8330720424969648, distance: 0.4675426488624808 entropy 0.6056879162788391
epoch: 6, step: 115
	action: tensor([[ 0.0224, -0.5910, -0.4988,  0.2374, -0.0715, -0.5058,  0.7649]],
       dtype=torch.float64)
	q_value: tensor([[-14.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15651592629732147, distance: 1.2306441800102026 entropy 0.6056879162788391
epoch: 6, step: 116
	action: tensor([[ 0.3826, -0.4008, -0.4754, -0.7473,  0.2848,  0.3045,  0.3476]],
       dtype=torch.float64)
	q_value: tensor([[-14.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02093001944545314, distance: 1.1323053535410095 entropy 0.6056879162788391
epoch: 6, step: 117
	action: tensor([[ 1.0122,  0.4319, -0.5240,  0.3591, -0.6505,  0.7627,  0.7417]],
       dtype=torch.float64)
	q_value: tensor([[-12.7843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9801127111063552, distance: 0.16137805786461096 entropy 0.6056879162788391
epoch: 6, step: 118
	action: tensor([[ 0.4201,  0.9982, -0.2549,  0.1342, -0.1940, -0.8698,  0.5743]],
       dtype=torch.float64)
	q_value: tensor([[-16.7501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9156601924982191, distance: 0.33233263881395714 entropy 0.6056879162788391
epoch: 6, step: 119
	action: tensor([[ 0.2916,  0.7327,  0.2624,  0.2881,  0.9463, -0.0997, -0.4865]],
       dtype=torch.float64)
	q_value: tensor([[-13.4837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 120
	action: tensor([[-0.4758, -0.2298, -0.1052,  0.3021, -0.0243, -1.0027, -0.4153]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3527022912753657, distance: 1.3309379423578886 entropy 0.6056879162788391
epoch: 6, step: 121
	action: tensor([[ 0.9639, -0.0812, -0.6737, -0.1920, -0.4492, -0.1926, -0.5417]],
       dtype=torch.float64)
	q_value: tensor([[-14.7860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5254917600246057, distance: 0.7882764870891831 entropy 0.6056879162788391
epoch: 6, step: 122
	action: tensor([[ 0.1423, -0.1765, -0.7047, -0.4316, -0.2537,  0.3422, -0.6851]],
       dtype=torch.float64)
	q_value: tensor([[-13.5789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23711683952013374, distance: 0.9995066951499845 entropy 0.6056879162788391
epoch: 6, step: 123
	action: tensor([[ 0.9770, -0.3629, -0.8094,  0.0441, -0.5334, -0.0972,  0.8350]],
       dtype=torch.float64)
	q_value: tensor([[-11.2882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4141292466059815, distance: 0.8759062042773021 entropy 0.6056879162788391
epoch: 6, step: 124
	action: tensor([[ 0.9499,  0.4057, -0.5140,  0.0314,  0.0371, -0.3968, -0.0714]],
       dtype=torch.float64)
	q_value: tensor([[-15.9345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9151656653243277, distance: 0.3333055323738145 entropy 0.6056879162788391
epoch: 6, step: 125
	action: tensor([[ 0.6780,  0.2831, -0.9667,  0.6244,  0.0620, -0.1172, -0.5159]],
       dtype=torch.float64)
	q_value: tensor([[-13.0852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8660033776553312, distance: 0.41889349754187966 entropy 0.6056879162788391
epoch: 6, step: 126
	action: tensor([[ 1.4301,  0.8267, -0.4396,  0.4160, -0.6340, -0.2991, -0.5269]],
       dtype=torch.float64)
	q_value: tensor([[-13.6594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 127
	action: tensor([[ 0.2158,  0.1188, -0.1275, -0.2033,  0.0077, -0.1838,  0.0930]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4682458983228962, distance: 0.834472669909191 entropy 0.6056879162788391
LOSS epoch 6 actor 160.1066199523196 critic 512.8372324388538 
epoch: 7, step: 0
	action: tensor([[-0.0726,  0.2797, -0.9643,  0.2717, -0.1670, -0.2056,  0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-10.5129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38412191895415015, distance: 0.8980573409173932 entropy 0.6056879162788391
epoch: 7, step: 1
	action: tensor([[ 0.0015,  0.0867, -0.9882, -0.9914,  0.3123,  0.0027, -0.0815]],
       dtype=torch.float64)
	q_value: tensor([[-10.8408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.536601130588116, distance: 0.7789941176007269 entropy 0.6056879162788391
epoch: 7, step: 2
	action: tensor([[-0.0417,  0.3927, -0.4329,  0.1932, -0.3000,  0.5088,  0.3640]],
       dtype=torch.float64)
	q_value: tensor([[-12.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4757280528779503, distance: 0.8285810627646972 entropy 0.6056879162788391
epoch: 7, step: 3
	action: tensor([[ 0.1589,  0.0584, -0.1497, -0.2894,  0.6795,  0.2620,  0.3967]],
       dtype=torch.float64)
	q_value: tensor([[-11.8071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4651882872866606, distance: 0.836868359517686 entropy 0.6056879162788391
epoch: 7, step: 4
	action: tensor([[-0.5228,  0.1416,  0.0059, -0.6701,  0.1847,  0.3526,  0.1024]],
       dtype=torch.float64)
	q_value: tensor([[-12.3073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3761133469198912, distance: 1.3424057286431987 entropy 0.6056879162788391
epoch: 7, step: 5
	action: tensor([[ 1.3092, -0.0404, -0.1677,  0.2897, -0.3025, -0.3162,  0.3877]],
       dtype=torch.float64)
	q_value: tensor([[-12.3347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6474999951264082, distance: 0.6794167615633478 entropy 0.6056879162788391
epoch: 7, step: 6
	action: tensor([[ 0.1590, -0.2535,  0.3237, -0.0599,  0.1736, -0.4394, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-16.8441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05541517021185216, distance: 1.112185366276528 entropy 0.6056879162788391
epoch: 7, step: 7
	action: tensor([[ 0.8401, -0.7619,  0.1414,  0.2787,  0.0023, -0.4006, -0.3185]],
       dtype=torch.float64)
	q_value: tensor([[-12.8887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02022869076577205, distance: 1.1327108281619902 entropy 0.6056879162788391
epoch: 7, step: 8
	action: tensor([[-0.0638, -0.5543, -0.6134, -0.2120, -0.4683, -0.3229,  0.2702]],
       dtype=torch.float64)
	q_value: tensor([[-17.0400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22771532948303919, distance: 1.2679599397429269 entropy 0.6056879162788391
epoch: 7, step: 9
	action: tensor([[ 0.6007,  0.2135, -0.2254, -0.0394,  0.3303, -0.2682,  0.3652]],
       dtype=torch.float64)
	q_value: tensor([[-13.3357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7840211040342651, distance: 0.5318171663688755 entropy 0.6056879162788391
epoch: 7, step: 10
	action: tensor([[ 0.4786, -0.2813, -0.6666, -0.3049,  0.0265,  0.3049,  0.2961]],
       dtype=torch.float64)
	q_value: tensor([[-12.7391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36223031627765656, distance: 0.9138788550351107 entropy 0.6056879162788391
epoch: 7, step: 11
	action: tensor([[ 0.0938, -0.0373, -0.4415,  0.6338, -0.7926,  0.4201,  0.5540]],
       dtype=torch.float64)
	q_value: tensor([[-12.5794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42057969987327004, distance: 0.8710709826414588 entropy 0.6056879162788391
epoch: 7, step: 12
	action: tensor([[-0.2386, -0.0201,  0.2978,  0.8773, -0.2246, -0.6178, -0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-14.2158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44022435144159255, distance: 0.8561772705375943 entropy 0.6056879162788391
epoch: 7, step: 13
	action: tensor([[ 0.3820,  0.6251,  0.2019,  0.0894,  0.5703, -0.9731, -0.4932]],
       dtype=torch.float64)
	q_value: tensor([[-15.3195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.76288080021233, distance: 0.5572371387834972 entropy 0.6056879162788391
epoch: 7, step: 14
	action: tensor([[ 0.8373,  0.3550,  0.4368, -0.7485,  0.0993, -0.4378,  0.3143]],
       dtype=torch.float64)
	q_value: tensor([[-16.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6088155311905057, distance: 0.7157271554239956 entropy 0.6056879162788391
epoch: 7, step: 15
	action: tensor([[ 0.2040,  0.1064, -1.1203, -0.4494, -0.0573, -0.2585, -0.7404]],
       dtype=torch.float64)
	q_value: tensor([[-16.2783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6406312077563175, distance: 0.6860043558439394 entropy 0.6056879162788391
epoch: 7, step: 16
	action: tensor([[ 0.5827,  0.1599,  0.1732,  0.1313, -0.4883,  0.3623,  0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-12.3222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8869668677158864, distance: 0.3847331201125717 entropy 0.6056879162788391
epoch: 7, step: 17
	action: tensor([[-0.3629, -0.4532, -0.9752, -0.4951, -0.4846,  0.7775, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-13.7716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4590337407630669, distance: 1.3822587916838724 entropy 0.6056879162788391
epoch: 7, step: 18
	action: tensor([[ 0.0552,  0.3086,  0.0323, -0.5337, -0.3944,  0.3437, -0.4015]],
       dtype=torch.float64)
	q_value: tensor([[-14.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3508873242112335, distance: 0.9219698907410417 entropy 0.6056879162788391
epoch: 7, step: 19
	action: tensor([[-0.1064,  0.7697, -0.2513, -0.4736, -0.2401, -0.0414, -0.4372]],
       dtype=torch.float64)
	q_value: tensor([[-13.1160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5394474831383762, distance: 0.7765980098670331 entropy 0.6056879162788391
epoch: 7, step: 20
	action: tensor([[ 0.5634,  0.3791, -0.8201, -0.5596, -0.0531, -0.2108, -0.5637]],
       dtype=torch.float64)
	q_value: tensor([[-11.8764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8210666290770486, distance: 0.4840635317469857 entropy 0.6056879162788391
epoch: 7, step: 21
	action: tensor([[-0.3225,  0.0228,  0.4134,  0.3275, -0.4632,  0.2876, -0.6044]],
       dtype=torch.float64)
	q_value: tensor([[-12.5974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2757199989655319, distance: 0.9738900752624011 entropy 0.6056879162788391
epoch: 7, step: 22
	action: tensor([[-0.3885, -0.4846, -0.4822, -0.0345,  0.7939, -0.1244,  0.3864]],
       dtype=torch.float64)
	q_value: tensor([[-13.8612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5681602562629626, distance: 1.4330188879959689 entropy 0.6056879162788391
epoch: 7, step: 23
	action: tensor([[ 0.9314,  0.3016,  0.0715, -0.3083,  0.5636, -0.0409, -0.4479]],
       dtype=torch.float64)
	q_value: tensor([[-14.3476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.817692709026713, distance: 0.4886059058965305 entropy 0.6056879162788391
epoch: 7, step: 24
	action: tensor([[ 0.6347,  0.3668, -0.5631,  0.5024,  0.6800,  0.3463,  0.1799]],
       dtype=torch.float64)
	q_value: tensor([[-14.6626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9285984351047097, distance: 0.30578104350058694 entropy 0.6056879162788391
epoch: 7, step: 25
	action: tensor([[ 0.1758, -0.4066,  0.1372, -0.3279,  0.3878, -0.3584, -0.3147]],
       dtype=torch.float64)
	q_value: tensor([[-14.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22841879487606054, distance: 1.2683231502450747 entropy 0.6056879162788391
epoch: 7, step: 26
	action: tensor([[ 0.3792, -0.1971, -0.2070, -0.4431, -0.8053, -0.3047, -0.1010]],
       dtype=torch.float64)
	q_value: tensor([[-12.8169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1923108988129162, distance: 1.0284396327045453 entropy 0.6056879162788391
epoch: 7, step: 27
	action: tensor([[ 0.1955,  0.0593, -0.2475, -0.0683, -0.1786, -0.1382,  0.4640]],
       dtype=torch.float64)
	q_value: tensor([[-14.0331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47929907115866477, distance: 0.8257543486077666 entropy 0.6056879162788391
epoch: 7, step: 28
	action: tensor([[ 0.9086,  0.1932, -0.0781,  0.3006,  0.0144, -0.8929, -0.2086]],
       dtype=torch.float64)
	q_value: tensor([[-11.2520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8376304422893972, distance: 0.4611147315109462 entropy 0.6056879162788391
epoch: 7, step: 29
	action: tensor([[ 0.3136,  0.2869, -0.3336, -0.3295,  0.4159,  0.0835, -0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-16.2962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6980566783618514, distance: 0.6288099533086187 entropy 0.6056879162788391
epoch: 7, step: 30
	action: tensor([[ 0.0286,  0.5864, -0.4585,  0.0078,  0.6609, -0.0718,  0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-10.5404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6343228958576524, distance: 0.6919991767532845 entropy 0.6056879162788391
epoch: 7, step: 31
	action: tensor([[ 0.6667, -0.2154, -0.5825, -0.4360, -0.8831,  0.1228, -0.0614]],
       dtype=torch.float64)
	q_value: tensor([[-11.8655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3601018710292586, distance: 0.9154025405960727 entropy 0.6056879162788391
epoch: 7, step: 32
	action: tensor([[ 0.6796,  0.3418, -0.1962, -0.2446,  0.1758,  0.9317, -0.4976]],
       dtype=torch.float64)
	q_value: tensor([[-15.0047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9665067665620349, distance: 0.2094282344309554 entropy 0.6056879162788391
epoch: 7, step: 33
	action: tensor([[ 0.5229,  0.4228,  0.6629, -0.1393, -0.4676, -0.6117,  0.6637]],
       dtype=torch.float64)
	q_value: tensor([[-14.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7714255430337975, distance: 0.5471048208452943 entropy 0.6056879162788391
epoch: 7, step: 34
	action: tensor([[ 0.2345, -0.0392, -0.1884, -0.1594, -0.1059,  0.4062, -0.1328]],
       dtype=torch.float64)
	q_value: tensor([[-16.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4861577347764825, distance: 0.8202979100273667 entropy 0.6056879162788391
epoch: 7, step: 35
	action: tensor([[-0.3885,  0.2710, -0.5296, -0.2169, -0.0373,  0.2133, -0.1492]],
       dtype=torch.float64)
	q_value: tensor([[-10.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0734568635830205, distance: 1.1015127155911812 entropy 0.6056879162788391
epoch: 7, step: 36
	action: tensor([[-0.4401,  0.6061,  0.3878,  1.0575,  0.1762, -0.7479, -0.3898]],
       dtype=torch.float64)
	q_value: tensor([[-9.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5447621268464999, distance: 0.7721041488398636 entropy 0.6056879162788391
epoch: 7, step: 37
	action: tensor([[ 1.0232, -0.0520,  0.2653, -0.2436, -0.6795,  0.0799,  0.1110]],
       dtype=torch.float64)
	q_value: tensor([[-16.4160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.584299132942008, distance: 0.7378144531745666 entropy 0.6056879162788391
epoch: 7, step: 38
	action: tensor([[ 0.1417,  0.8060, -0.0696, -0.2087, -0.4260, -0.2954, -0.4790]],
       dtype=torch.float64)
	q_value: tensor([[-16.3818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7716031123053769, distance: 0.546892268924979 entropy 0.6056879162788391
epoch: 7, step: 39
	action: tensor([[-0.0085, -0.4309, -0.2845,  0.4129, -0.3869,  0.4040,  0.3195]],
       dtype=torch.float64)
	q_value: tensor([[-12.8676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16681612764532694, distance: 1.044544918619837 entropy 0.6056879162788391
epoch: 7, step: 40
	action: tensor([[-0.4941,  0.2773,  0.3753, -0.6914, -0.2855, -0.2340, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-12.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49092436713985954, distance: 1.397283400216644 entropy 0.6056879162788391
epoch: 7, step: 41
	action: tensor([[-0.1459, -0.0385,  0.1478,  0.0079,  0.0274,  0.2440, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-13.3675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16945671530831763, distance: 1.0428883807477558 entropy 0.6056879162788391
epoch: 7, step: 42
	action: tensor([[ 0.0497,  0.0377,  0.5896,  0.5491, -0.5542, -0.2210, -0.6656]],
       dtype=torch.float64)
	q_value: tensor([[-11.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6554907200925097, distance: 0.6716718630382679 entropy 0.6056879162788391
epoch: 7, step: 43
	action: tensor([[ 0.0752, -0.8201, -0.4220,  0.3745, -0.1149, -0.2909, -0.4538]],
       dtype=torch.float64)
	q_value: tensor([[-15.3872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2482809639058019, distance: 1.2785357233760901 entropy 0.6056879162788391
epoch: 7, step: 44
	action: tensor([[ 7.7833e-01, -4.7506e-01, -9.1410e-01,  5.8674e-04, -1.7480e-02,
         -1.0860e-01, -4.4171e-02]], dtype=torch.float64)
	q_value: tensor([[-14.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26805785511882374, distance: 0.9790279047114896 entropy 0.6056879162788391
epoch: 7, step: 45
	action: tensor([[ 0.6185,  0.4709, -0.4328, -0.3466,  0.2944,  0.4195,  0.2796]],
       dtype=torch.float64)
	q_value: tensor([[-13.7867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.944136042999356, distance: 0.270472142135734 entropy 0.6056879162788391
epoch: 7, step: 46
	action: tensor([[ 0.6582,  0.1515,  0.1556,  1.1008,  0.0976,  0.1291, -0.6790]],
       dtype=torch.float64)
	q_value: tensor([[-13.0285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9589805031150881, distance: 0.2317671380339348 entropy 0.6056879162788391
epoch: 7, step: 47
	action: tensor([[-0.4055, -0.1976, -0.2266,  0.4817, -0.2382, -0.1638,  0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-16.8019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12565252064804966, distance: 1.2141123236425821 entropy 0.6056879162788391
epoch: 7, step: 48
	action: tensor([[ 0.7139, -0.0699, -0.0639,  0.4916, -0.1258, -0.7701, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[-12.2357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7555087259894356, distance: 0.5658331337779375 entropy 0.6056879162788391
epoch: 7, step: 49
	action: tensor([[-2.2083e-01,  4.1881e-04, -7.0793e-01, -5.4631e-02, -2.2477e-01,
          2.4645e-01,  5.6630e-01]], dtype=torch.float64)
	q_value: tensor([[-15.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06269865362746918, distance: 1.107889161552902 entropy 0.6056879162788391
epoch: 7, step: 50
	action: tensor([[ 0.0478,  0.0380,  0.3968,  0.5988, -0.9632, -0.2863,  0.1271]],
       dtype=torch.float64)
	q_value: tensor([[-12.0194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6877950908676955, distance: 0.6394057789374306 entropy 0.6056879162788391
epoch: 7, step: 51
	action: tensor([[-0.0947,  0.3882, -0.0428,  0.6149,  0.4026,  0.6057, -0.7379]],
       dtype=torch.float64)
	q_value: tensor([[-15.3540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6526643440399812, distance: 0.6744214544524382 entropy 0.6056879162788391
epoch: 7, step: 52
	action: tensor([[-0.0851,  0.6069,  0.0071, -0.2878,  0.8580,  0.3989,  0.4593]],
       dtype=torch.float64)
	q_value: tensor([[-13.5482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5986961980195902, distance: 0.7249254226139502 entropy 0.6056879162788391
epoch: 7, step: 53
	action: tensor([[-0.0733, -0.3962, -0.6999,  0.4891, -0.0251, -0.1235,  0.3732]],
       dtype=torch.float64)
	q_value: tensor([[-13.4532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08668893603925976, distance: 1.192914495739902 entropy 0.6056879162788391
epoch: 7, step: 54
	action: tensor([[ 0.2596, -0.5155, -0.0528,  0.0037, -0.5793, -0.1894, -0.3974]],
       dtype=torch.float64)
	q_value: tensor([[-12.8723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015321993933025024, distance: 1.135543595102472 entropy 0.6056879162788391
epoch: 7, step: 55
	action: tensor([[ 0.4814, -0.0939, -0.1056,  0.1535,  0.0092,  0.1845, -0.2488]],
       dtype=torch.float64)
	q_value: tensor([[-13.4039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6853281890969787, distance: 0.6419269558522162 entropy 0.6056879162788391
epoch: 7, step: 56
	action: tensor([[ 0.4939,  0.3249, -0.8146,  0.2109, -0.2536,  0.0205, -0.0868]],
       dtype=torch.float64)
	q_value: tensor([[-11.5813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8361587919703466, distance: 0.463199694184683 entropy 0.6056879162788391
epoch: 7, step: 57
	action: tensor([[-0.5800, -0.7959, -0.9444,  0.2923, -0.6640,  0.6062, -0.4286]],
       dtype=torch.float64)
	q_value: tensor([[-11.5641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0936965703032024, distance: 1.6558216780381132 entropy 0.6056879162788391
epoch: 7, step: 58
	action: tensor([[ 0.5784, -0.0533,  0.1956, -0.6819,  0.2997,  0.6535,  0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-15.3407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48756298669511056, distance: 0.819175469849194 entropy 0.6056879162788391
epoch: 7, step: 59
	action: tensor([[ 0.2055, -0.0645, -0.7329,  0.0048, -0.1873,  0.5478,  0.8014]],
       dtype=torch.float64)
	q_value: tensor([[-15.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4496113912023234, distance: 0.8489681777740458 entropy 0.6056879162788391
epoch: 7, step: 60
	action: tensor([[-0.0383,  0.3722, -0.3212,  0.0473,  0.2249, -0.0910,  0.7285]],
       dtype=torch.float64)
	q_value: tensor([[-13.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48332804334652435, distance: 0.8225534690153988 entropy 0.6056879162788391
epoch: 7, step: 61
	action: tensor([[-0.9071, -0.4404, -0.0801,  0.2918,  0.1818, -0.8982, -0.2139]],
       dtype=torch.float64)
	q_value: tensor([[-12.1887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0522664944381557, distance: 1.6393571179745674 entropy 0.6056879162788391
epoch: 7, step: 62
	action: tensor([[-0.1650, -0.1792,  0.2791, -0.6579,  0.5667,  0.3437, -0.0975]],
       dtype=torch.float64)
	q_value: tensor([[-16.3793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2959513060844874, distance: 1.3027198632995165 entropy 0.6056879162788391
epoch: 7, step: 63
	action: tensor([[ 0.4107,  0.2154, -0.4871,  0.4751, -0.7344, -0.3519,  0.1366]],
       dtype=torch.float64)
	q_value: tensor([[-12.6416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8305721608084269, distance: 0.47103055421366247 entropy 0.6056879162788391
epoch: 7, step: 64
	action: tensor([[ 0.6328,  0.3403, -0.1246,  0.2209,  0.2847,  0.3448,  0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-12.7691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9623163416887147, distance: 0.22214330141115485 entropy 0.6056879162788391
epoch: 7, step: 65
	action: tensor([[ 0.5617, -0.4635, -0.1060,  0.6686, -0.2278,  0.2268, -0.5481]],
       dtype=torch.float64)
	q_value: tensor([[-12.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7337057171463683, distance: 0.5905240838905155 entropy 0.6056879162788391
epoch: 7, step: 66
	action: tensor([[-0.3881, -0.6790, -0.9070,  0.3738,  0.3017, -0.7588, -0.4203]],
       dtype=torch.float64)
	q_value: tensor([[-14.9422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6599940432293641, distance: 1.4743817892349067 entropy 0.6056879162788391
epoch: 7, step: 67
	action: tensor([[ 0.4066,  0.0618, -0.2014, -0.6541,  0.7983, -0.0092,  0.6457]],
       dtype=torch.float64)
	q_value: tensor([[-15.9585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3839130117536943, distance: 0.8982096395166818 entropy 0.6056879162788391
epoch: 7, step: 68
	action: tensor([[ 0.6736, -0.2618, -1.0689,  0.2008, -0.4368, -0.4230, -0.3199]],
       dtype=torch.float64)
	q_value: tensor([[-14.6325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5433000325599211, distance: 0.773343044114588 entropy 0.6056879162788391
epoch: 7, step: 69
	action: tensor([[ 0.3773, -0.5234,  0.0393, -0.2648,  0.0710, -0.6663,  0.6845]],
       dtype=torch.float64)
	q_value: tensor([[-14.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22591617492753424, distance: 1.2670305336232264 entropy 0.6056879162788391
epoch: 7, step: 70
	action: tensor([[ 1.3336,  0.8219, -0.6922, -0.0247,  0.3898,  0.3935, -0.0536]],
       dtype=torch.float64)
	q_value: tensor([[-15.5100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 7, step: 71
	action: tensor([[ 0.4392,  0.1671,  0.5271, -0.0871, -0.0943,  0.6493, -0.7899]],
       dtype=torch.float64)
	q_value: tensor([[-16.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8396108125160837, distance: 0.4582940690516 entropy 0.6056879162788391
epoch: 7, step: 72
	action: tensor([[ 0.7107,  0.1547, -0.4091,  0.5724, -0.3224, -0.2268,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-15.8521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9423685732600925, distance: 0.2747175336791465 entropy 0.6056879162788391
epoch: 7, step: 73
	action: tensor([[-0.0195,  0.1414, -0.1017,  0.1762,  0.3416, -0.4547,  0.1084]],
       dtype=torch.float64)
	q_value: tensor([[-13.1248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34758405439194195, distance: 0.9243128203079448 entropy 0.6056879162788391
epoch: 7, step: 74
	action: tensor([[-0.5137, -0.2160, -0.0469, -0.0057, -0.5581, -0.2847, -0.8682]],
       dtype=torch.float64)
	q_value: tensor([[-11.7735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44627975119351615, distance: 1.376204096296219 entropy 0.6056879162788391
epoch: 7, step: 75
	action: tensor([[ 0.3797,  0.5929, -0.3251,  0.0775,  0.2215, -0.6138, -0.5998]],
       dtype=torch.float64)
	q_value: tensor([[-13.2589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8471266941088178, distance: 0.447427343953389 entropy 0.6056879162788391
epoch: 7, step: 76
	action: tensor([[ 0.1472, -0.6125,  0.0095,  0.2016,  0.4394,  0.0829,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-13.1994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04293970545084558, distance: 1.1195057867767968 entropy 0.6056879162788391
epoch: 7, step: 77
	action: tensor([[ 0.0626, -0.1875, -0.0265,  0.2702, -0.0183, -0.4640, -0.2992]],
       dtype=torch.float64)
	q_value: tensor([[-13.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2654527439435075, distance: 0.9807686236904557 entropy 0.6056879162788391
epoch: 7, step: 78
	action: tensor([[ 0.5738,  0.2896,  0.2229, -0.1473,  0.0833,  0.6658, -0.1944]],
       dtype=torch.float64)
	q_value: tensor([[-12.4053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9235059791292458, distance: 0.3164976125033555 entropy 0.6056879162788391
epoch: 7, step: 79
	action: tensor([[ 0.2779, -0.6308,  0.0938,  0.6138,  0.3373, -0.0929, -0.1565]],
       dtype=torch.float64)
	q_value: tensor([[-13.8766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35628789512807846, distance: 0.9181265182080784 entropy 0.6056879162788391
epoch: 7, step: 80
	action: tensor([[ 0.3800, -0.1850, -0.6289,  0.1092,  0.5276,  0.1358,  0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-15.1402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.511577348307962, distance: 0.7997506317654971 entropy 0.6056879162788391
epoch: 7, step: 81
	action: tensor([[ 0.0858,  0.2041,  0.0335, -0.0156, -0.2285, -0.2887,  0.0990]],
       dtype=torch.float64)
	q_value: tensor([[-13.0153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4701836732591955, distance: 0.8329508235677286 entropy 0.6056879162788391
epoch: 7, step: 82
	action: tensor([[ 0.5402, -0.5867,  0.7085, -0.3884,  0.5038,  0.4577, -0.1823]],
       dtype=torch.float64)
	q_value: tensor([[-10.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021002771864571113, distance: 1.1562990100706145 entropy 0.6056879162788391
epoch: 7, step: 83
	action: tensor([[ 1.0929,  0.1765, -0.3777,  0.2133, -0.7964,  0.0231,  0.5911]],
       dtype=torch.float64)
	q_value: tensor([[-16.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.89795426212569, distance: 0.3655561783683618 entropy 0.6056879162788391
epoch: 7, step: 84
	action: tensor([[ 0.8377,  0.1024, -0.1268, -0.1431, -0.2991, -0.0722,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-16.4250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7455593448989555, distance: 0.5772313975627779 entropy 0.6056879162788391
epoch: 7, step: 85
	action: tensor([[-0.5456, -0.0292, -0.4025, -0.0511, -0.5008, -0.5636, -0.9165]],
       dtype=torch.float64)
	q_value: tensor([[-13.0861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2554779924107897, distance: 1.2822161580098101 entropy 0.6056879162788391
epoch: 7, step: 86
	action: tensor([[ 0.0181,  0.5125,  0.4531, -0.5122,  0.0376, -0.5433,  0.4407]],
       dtype=torch.float64)
	q_value: tensor([[-13.3914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2448352487617519, distance: 0.9944376266354382 entropy 0.6056879162788391
epoch: 7, step: 87
	action: tensor([[ 0.9553, -0.8597, -0.5140, -0.4329, -0.3064,  0.0671, -0.4706]],
       dtype=torch.float64)
	q_value: tensor([[-13.7352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4760768090044616, distance: 1.3903084802271535 entropy 0.6056879162788391
epoch: 7, step: 88
	action: tensor([[ 0.3373,  0.7504, -0.5692, -0.7742, -0.2436, -0.8332, -0.8407]],
       dtype=torch.float64)
	q_value: tensor([[-16.0633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8496867435496892, distance: 0.44366516937633405 entropy 0.6056879162788391
epoch: 7, step: 89
	action: tensor([[ 0.6078, -0.1960, -0.0975,  0.9700, -0.2231,  0.3942, -0.1664]],
       dtype=torch.float64)
	q_value: tensor([[-15.1747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9310766762536715, distance: 0.3004275800718527 entropy 0.6056879162788391
epoch: 7, step: 90
	action: tensor([[-0.3280, -0.4799,  0.1655, -0.0550, -0.2330, -0.0110,  0.2215]],
       dtype=torch.float64)
	q_value: tensor([[-15.2282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4684886764049856, distance: 1.386730265830379 entropy 0.6056879162788391
epoch: 7, step: 91
	action: tensor([[-0.7014,  0.3207, -0.9860,  0.2673,  0.1589,  0.3449, -0.2776]],
       dtype=torch.float64)
	q_value: tensor([[-13.0820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38892674630549395, distance: 1.3486410164996963 entropy 0.6056879162788391
epoch: 7, step: 92
	action: tensor([[ 0.0587,  0.0570,  0.1130, -0.1003,  0.4608, -0.4180, -0.3728]],
       dtype=torch.float64)
	q_value: tensor([[-11.6217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22566490216878776, distance: 1.0069807435312876 entropy 0.6056879162788391
epoch: 7, step: 93
	action: tensor([[ 0.5815,  0.2316,  0.1510,  0.7644, -0.5223, -0.3081, -0.2550]],
       dtype=torch.float64)
	q_value: tensor([[-11.7684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9810422387977827, distance: 0.15756154020512278 entropy 0.6056879162788391
epoch: 7, step: 94
	action: tensor([[-0.2228,  0.3576, -0.0275,  0.1324,  0.3603,  0.1492,  0.5686]],
       dtype=torch.float64)
	q_value: tensor([[-14.8767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37605677224220113, distance: 0.90391840832946 entropy 0.6056879162788391
epoch: 7, step: 95
	action: tensor([[ 0.2384, -0.6701,  0.6870, -0.0363,  1.5097, -0.2603,  0.5555]],
       dtype=torch.float64)
	q_value: tensor([[-12.1241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12403653135944692, distance: 1.21324051944889 entropy 0.6056879162788391
epoch: 7, step: 96
	action: tensor([[ 1.0901,  0.7463, -0.2927,  0.1161,  0.5905, -0.0163,  0.5029]],
       dtype=torch.float64)
	q_value: tensor([[-21.4929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9407098906546801, distance: 0.27864279572991674 entropy 0.6056879162788391
epoch: 7, step: 97
	action: tensor([[ 0.9339,  0.7053,  0.0747,  1.5428, -0.5940, -0.2925,  0.0584]],
       dtype=torch.float64)
	q_value: tensor([[-16.1421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 7, step: 98
	action: tensor([[ 0.1392,  0.4439, -0.8375,  0.1834, -0.7051,  0.0508, -0.8875]],
       dtype=torch.float64)
	q_value: tensor([[-16.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6059103669867296, distance: 0.7183799430640816 entropy 0.6056879162788391
epoch: 7, step: 99
	action: tensor([[ 0.4757, -0.0502, -0.5999,  0.5665,  0.0232, -0.2123,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[-12.9294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7308308538157218, distance: 0.5937031206979595 entropy 0.6056879162788391
epoch: 7, step: 100
	action: tensor([[ 0.5746,  0.2291, -0.5627,  0.5275, -0.8082, -0.0351, -0.7906]],
       dtype=torch.float64)
	q_value: tensor([[-12.6958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.876535818235323, distance: 0.40209362214507127 entropy 0.6056879162788391
epoch: 7, step: 101
	action: tensor([[ 1.0983,  0.1480, -0.3023,  0.2781,  0.8112, -0.1141, -0.1730]],
       dtype=torch.float64)
	q_value: tensor([[-14.6352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8878162269722941, distance: 0.38328490392214337 entropy 0.6056879162788391
epoch: 7, step: 102
	action: tensor([[ 0.6972,  0.9367, -0.7558, -0.4533, -0.6032,  0.1791, -0.2848]],
       dtype=torch.float64)
	q_value: tensor([[-16.4859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9831875467245572, distance: 0.1483789312003652 entropy 0.6056879162788391
epoch: 7, step: 103
	action: tensor([[ 0.5182,  0.9829,  0.1466, -0.5967,  0.0060, -0.0416,  0.1106]],
       dtype=torch.float64)
	q_value: tensor([[-15.3054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.925223882138029, distance: 0.3129234792931198 entropy 0.6056879162788391
epoch: 7, step: 104
	action: tensor([[-0.5777,  0.0051, -0.2869, -0.3178, -0.1338,  0.8083,  0.9372]],
       dtype=torch.float64)
	q_value: tensor([[-14.6350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34570078906630086, distance: 1.3274890481008612 entropy 0.6056879162788391
epoch: 7, step: 105
	action: tensor([[ 0.3930, -0.4513,  0.2984,  0.1845,  1.0471, -0.2289, -0.4424]],
       dtype=torch.float64)
	q_value: tensor([[-16.6334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29776589623350014, distance: 0.9589537278485154 entropy 0.6056879162788391
epoch: 7, step: 106
	action: tensor([[ 0.0184,  0.1282, -0.2906, -0.0992,  0.2310,  0.2755, -0.5114]],
       dtype=torch.float64)
	q_value: tensor([[-16.1831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.424983161063459, distance: 0.8677547001853746 entropy 0.6056879162788391
epoch: 7, step: 107
	action: tensor([[0.2683, 0.2392, 1.2836, 0.3057, 0.0648, 0.2138, 0.1273]],
       dtype=torch.float64)
	q_value: tensor([[-10.0161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8673566286515422, distance: 0.41677289731255796 entropy 0.6056879162788391
epoch: 7, step: 108
	action: tensor([[ 0.5861,  0.0390, -0.4307,  0.6488,  0.2591,  0.0920, -0.5669]],
       dtype=torch.float64)
	q_value: tensor([[-18.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.887783698583454, distance: 0.38334046782598724 entropy 0.6056879162788391
epoch: 7, step: 109
	action: tensor([[ 0.5717,  0.5465, -0.3041,  0.6100, -0.6980, -0.5407, -0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-13.6939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9463799647101834, distance: 0.26498435963168465 entropy 0.6056879162788391
epoch: 7, step: 110
	action: tensor([[ 0.2025,  0.7882,  0.0883,  0.0711,  0.2053, -0.3031,  0.3493]],
       dtype=torch.float64)
	q_value: tensor([[-14.0178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7765922312305025, distance: 0.5408861099878669 entropy 0.6056879162788391
epoch: 7, step: 111
	action: tensor([[ 0.0998,  0.3076, -0.5230,  0.6022,  0.5352,  0.3031, -0.5693]],
       dtype=torch.float64)
	q_value: tensor([[-12.1853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.604673509432115, distance: 0.7195063842431709 entropy 0.6056879162788391
epoch: 7, step: 112
	action: tensor([[-0.1165,  0.2051, -0.2047,  0.1113,  0.1617,  0.4360, -0.0892]],
       dtype=torch.float64)
	q_value: tensor([[-12.9636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4534346152589598, distance: 0.8460143994557217 entropy 0.6056879162788391
epoch: 7, step: 113
	action: tensor([[ 0.2391, -0.3216, -0.8224,  0.6252,  0.1780, -0.2852, -0.3287]],
       dtype=torch.float64)
	q_value: tensor([[-10.1893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29264089870838095, distance: 0.9624466521545084 entropy 0.6056879162788391
epoch: 7, step: 114
	action: tensor([[ 0.5575,  0.4428, -0.3092, -0.9138,  0.5385, -0.3772, -0.5838]],
       dtype=torch.float64)
	q_value: tensor([[-13.7352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6034917219408285, distance: 0.7205810265192771 entropy 0.6056879162788391
epoch: 7, step: 115
	action: tensor([[-0.0492, -0.8214, -0.3294, -0.1417,  0.4839, -0.2425, -0.2032]],
       dtype=torch.float64)
	q_value: tensor([[-14.3125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5789700324128202, distance: 1.4379495096100543 entropy 0.6056879162788391
epoch: 7, step: 116
	action: tensor([[-0.0975,  0.1090,  0.2013,  0.4066,  0.1557,  0.0720, -0.2356]],
       dtype=torch.float64)
	q_value: tensor([[-13.3842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.523210944557085, distance: 0.7901687175351589 entropy 0.6056879162788391
epoch: 7, step: 117
	action: tensor([[ 0.2402, -0.4880,  0.1231,  0.7405, -0.1810, -0.4644, -0.2337]],
       dtype=torch.float64)
	q_value: tensor([[-11.7939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4524679822066513, distance: 0.8467621821071712 entropy 0.6056879162788391
epoch: 7, step: 118
	action: tensor([[ 0.8081,  0.5362, -0.5674,  0.5270,  0.3134, -0.4078,  0.8301]],
       dtype=torch.float64)
	q_value: tensor([[-15.6079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9663023448068484, distance: 0.21006637171315412 entropy 0.6056879162788391
epoch: 7, step: 119
	action: tensor([[ 0.4552,  0.0963,  0.4099,  0.3054, -0.7434, -0.1064, -0.1410]],
       dtype=torch.float64)
	q_value: tensor([[-15.8794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8312905976660602, distance: 0.4700308211801865 entropy 0.6056879162788391
epoch: 7, step: 120
	action: tensor([[-0.3661,  0.3750, -0.4650, -0.2389,  0.0811,  0.3832,  0.6031]],
       dtype=torch.float64)
	q_value: tensor([[-14.6089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1750527096747686, distance: 1.0393690812457899 entropy 0.6056879162788391
epoch: 7, step: 121
	action: tensor([[-0.0156, -0.4812, -0.8940, -0.6260,  0.3672,  0.5692,  0.3774]],
       dtype=torch.float64)
	q_value: tensor([[-12.5495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012525852098639545, distance: 1.1514888938875256 entropy 0.6056879162788391
epoch: 7, step: 122
	action: tensor([[-0.5041, -0.0315, -0.3262,  0.1770, -0.7148,  0.8050,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-14.3426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30094785400357127, distance: 1.3052287694160758 entropy 0.6056879162788391
epoch: 7, step: 123
	action: tensor([[-0.1461, -0.1307, -0.4919, -0.3893, -0.2748, -0.8254,  0.6968]],
       dtype=torch.float64)
	q_value: tensor([[-13.9898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06432259717859268, distance: 1.1069289957365638 entropy 0.6056879162788391
epoch: 7, step: 124
	action: tensor([[ 0.8239, -0.3761,  0.0798,  0.1701,  0.3129, -0.7537,  0.2623]],
       dtype=torch.float64)
	q_value: tensor([[-14.5292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30713454398414464, distance: 0.9525354508976347 entropy 0.6056879162788391
epoch: 7, step: 125
	action: tensor([[ 0.0411,  0.0383,  0.0164, -0.2065,  0.2217,  0.1794, -0.7064]],
       dtype=torch.float64)
	q_value: tensor([[-16.9964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34848312167644035, distance: 0.9236757222122123 entropy 0.6056879162788391
epoch: 7, step: 126
	action: tensor([[ 0.5309,  0.4755,  0.0642, -0.6572, -0.0866,  1.1542,  0.1245]],
       dtype=torch.float64)
	q_value: tensor([[-11.1893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9064882414454126, distance: 0.34993695254606355 entropy 0.6056879162788391
epoch: 7, step: 127
	action: tensor([[ 0.0912,  0.3422, -0.0775,  0.1568, -0.4995,  0.1228, -0.2381]],
       dtype=torch.float64)
	q_value: tensor([[-17.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6482418795647118, distance: 0.6787014222656689 entropy 0.6056879162788391
LOSS epoch 7 actor 108.74330916208437 critic 156.12538350077378 
epoch: 8, step: 0
	action: tensor([[ 0.1132, -0.2246,  0.2395, -0.2554,  0.0439,  0.1443, -0.1268]],
       dtype=torch.float64)
	q_value: tensor([[-12.0847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11220226350258389, distance: 1.0782357072420703 entropy 0.5003275275230408
epoch: 8, step: 1
	action: tensor([[-0.2058,  0.0258, -0.2826,  0.9717,  0.0108, -0.2700, -0.9787]],
       dtype=torch.float64)
	q_value: tensor([[-12.3026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3233229200726121, distance: 0.9413419792531799 entropy 0.5003275275230408
epoch: 8, step: 2
	action: tensor([[-0.0291,  0.4207, -0.4470, -0.2273, -0.3325,  0.0627,  0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-16.1604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.535029159921204, distance: 0.780314275284781 entropy 0.5003275275230408
epoch: 8, step: 3
	action: tensor([[-0.3502,  0.2026, -0.0970, -0.4670,  0.3608, -0.1534,  0.2211]],
       dtype=torch.float64)
	q_value: tensor([[-11.6452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1358667698787459, distance: 1.2196083537751272 entropy 0.5003275275230408
epoch: 8, step: 4
	action: tensor([[ 0.6540, -0.7747,  0.1245,  0.2024, -0.0041, -0.0331, -0.2449]],
       dtype=torch.float64)
	q_value: tensor([[-11.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05276240050645542, distance: 1.1137460006436022 entropy 0.5003275275230408
epoch: 8, step: 5
	action: tensor([[ 0.8754,  0.7601, -0.7190, -0.3700, -0.2934, -0.0683, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-16.1141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9641764122855964, distance: 0.21659141187558575 entropy 0.5003275275230408
epoch: 8, step: 6
	action: tensor([[ 0.3882,  0.7688, -0.5381,  0.4276,  0.0984,  0.5466, -0.2883]],
       dtype=torch.float64)
	q_value: tensor([[-15.1809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 7
	action: tensor([[-0.7461,  0.1536,  0.0538,  0.0431, -0.2543, -0.2674,  0.0634]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49407015818121947, distance: 1.3987567296246803 entropy 0.5003275275230408
epoch: 8, step: 8
	action: tensor([[ 0.3856, -0.1958, -0.2234, -0.2307,  0.0740,  0.9282, -0.2659]],
       dtype=torch.float64)
	q_value: tensor([[-12.4506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5752682715353201, distance: 0.7457856897285806 entropy 0.5003275275230408
epoch: 8, step: 9
	action: tensor([[ 0.4910, -0.3398,  0.2815, -0.3128,  0.2379,  0.1188, -0.9430]],
       dtype=torch.float64)
	q_value: tensor([[-14.4674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1911277314005424, distance: 1.0291926272422576 entropy 0.5003275275230408
epoch: 8, step: 10
	action: tensor([[-0.0264,  0.0562, -0.1382,  0.1194, -0.1616,  0.1383, -0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-15.3020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4008177586125201, distance: 0.8858009870479445 entropy 0.5003275275230408
epoch: 8, step: 11
	action: tensor([[-0.0325,  0.1627,  0.9927, -0.3543,  0.1334, -0.6261,  0.3631]],
       dtype=torch.float64)
	q_value: tensor([[-10.6757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05155555327026429, distance: 1.173472195764116 entropy 0.5003275275230408
epoch: 8, step: 12
	action: tensor([[-0.7740,  0.3072, -0.2060, -0.2962,  0.3257,  0.0206,  0.2502]],
       dtype=torch.float64)
	q_value: tensor([[-17.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4991778877685349, distance: 1.4011456319684068 entropy 0.5003275275230408
epoch: 8, step: 13
	action: tensor([[ 0.6618,  0.4252,  0.1217,  0.3279, -0.6081, -0.0115,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-12.4453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9732979738513665, distance: 0.18699448653154896 entropy 0.5003275275230408
epoch: 8, step: 14
	action: tensor([[-0.1293, -0.1772,  0.3271,  0.1651,  0.4218,  0.3471,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[-14.8670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2808641994546278, distance: 0.9704253839775291 entropy 0.5003275275230408
epoch: 8, step: 15
	action: tensor([[ 0.4037,  0.0684,  0.0662,  0.0558, -0.3013,  0.0589,  0.8707]],
       dtype=torch.float64)
	q_value: tensor([[-13.5640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7027194357475611, distance: 0.6239358666897616 entropy 0.5003275275230408
epoch: 8, step: 16
	action: tensor([[-0.2493,  0.6426, -0.0726, -0.6407,  0.5251,  0.5439, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-14.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41046137363108703, distance: 0.8786437534880305 entropy 0.5003275275230408
epoch: 8, step: 17
	action: tensor([[ 0.0822, -0.9207, -0.0096,  0.2617, -0.9895, -0.0362,  0.6620]],
       dtype=torch.float64)
	q_value: tensor([[-12.8758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.310814863169669, distance: 1.3101691583797963 entropy 0.5003275275230408
epoch: 8, step: 18
	action: tensor([[ 0.2711,  0.4146, -0.3857, -0.4461, -0.1977, -0.3618,  0.6459]],
       dtype=torch.float64)
	q_value: tensor([[-18.6208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6776837467740232, distance: 0.6496774550442 entropy 0.5003275275230408
epoch: 8, step: 19
	action: tensor([[ 0.2479,  1.2660,  0.4156,  0.0439, -0.5151,  0.7271,  0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-13.8930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 20
	action: tensor([[ 0.6418,  0.3040, -0.4044, -0.1913, -0.4221, -0.7812,  0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7939304426871011, distance: 0.5194737567166163 entropy 0.5003275275230408
epoch: 8, step: 21
	action: tensor([[ 0.2978, -0.3705, -0.4632,  0.4664,  0.2722, -0.1216,  0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-14.8844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38883239235312983, distance: 0.8946164042112517 entropy 0.5003275275230408
epoch: 8, step: 22
	action: tensor([[ 0.5665, -0.0017, -0.7964, -0.0097, -0.1854, -0.2186, -0.1140]],
       dtype=torch.float64)
	q_value: tensor([[-13.8403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6796457230157524, distance: 0.6476971059567321 entropy 0.5003275275230408
epoch: 8, step: 23
	action: tensor([[ 0.2560,  0.1204, -0.1547, -0.4497,  0.0411,  0.0927,  0.2201]],
       dtype=torch.float64)
	q_value: tensor([[-12.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.466214583845723, distance: 0.8360650043034662 entropy 0.5003275275230408
epoch: 8, step: 24
	action: tensor([[-0.4422,  0.7078, -0.6933,  0.4815, -0.2406, -0.1928,  0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-12.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 25
	action: tensor([[ 0.0081,  0.4487, -0.3836, -0.3811, -0.9898, -0.7517,  0.1349]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7219583278556626, distance: 0.603408799393818 entropy 0.5003275275230408
epoch: 8, step: 26
	action: tensor([[ 0.6794,  0.8341,  0.0236,  0.8282, -0.4518, -0.5619, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-15.9228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 27
	action: tensor([[-0.2766,  0.0297, -0.7443,  0.3745,  0.6063,  0.0601, -0.1477]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007301817906200991, distance: 1.1401587027858415 entropy 0.5003275275230408
epoch: 8, step: 28
	action: tensor([[ 0.2607, -0.3889,  0.3267, -0.6240,  0.0428,  0.9434,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[-13.1634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1301221106369158, distance: 1.0672983531163327 entropy 0.5003275275230408
epoch: 8, step: 29
	action: tensor([[ 0.0440,  0.6872,  0.3239, -0.3731,  0.0274,  0.2279,  0.2069]],
       dtype=torch.float64)
	q_value: tensor([[-17.1394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5966078923528044, distance: 0.7268091594532696 entropy 0.5003275275230408
epoch: 8, step: 30
	action: tensor([[-0.0418,  0.2643, -0.1569,  0.6100,  0.4512,  0.6955, -0.0431]],
       dtype=torch.float64)
	q_value: tensor([[-13.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6996584470871569, distance: 0.6271398593276413 entropy 0.5003275275230408
epoch: 8, step: 31
	action: tensor([[ 0.4941, -0.1346, -0.4233,  0.3838, -0.4354,  0.1527,  0.3368]],
       dtype=torch.float64)
	q_value: tensor([[-13.4579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7164282128479817, distance: 0.6093800097217469 entropy 0.5003275275230408
epoch: 8, step: 32
	action: tensor([[ 0.6476,  0.3748, -0.5975,  0.6238, -0.0657, -0.3087, -0.1468]],
       dtype=torch.float64)
	q_value: tensor([[-13.1697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9243611141408516, distance: 0.31472355980232125 entropy 0.5003275275230408
epoch: 8, step: 33
	action: tensor([[-0.1248,  1.0737, -0.6934,  0.3557, -0.1973, -0.1536, -0.0582]],
       dtype=torch.float64)
	q_value: tensor([[-14.1792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 34
	action: tensor([[ 0.3736, -0.2173, -0.1703, -0.2294, -0.0481, -0.1697, -0.1510]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2739364034328128, distance: 0.9750884777514974 entropy 0.5003275275230408
epoch: 8, step: 35
	action: tensor([[-0.9015,  0.5072, -0.5081,  0.1977, -0.0239, -1.0609, -0.2638]],
       dtype=torch.float64)
	q_value: tensor([[-12.0598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.279088649101886, distance: 1.2942167484865117 entropy 0.5003275275230408
epoch: 8, step: 36
	action: tensor([[ 1.1540, -0.8453, -0.1826, -0.0982,  0.2140, -0.1169, -0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-14.5329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32597703458441196, distance: 1.3177247237064562 entropy 0.5003275275230408
epoch: 8, step: 37
	action: tensor([[ 0.4985,  0.2414, -0.3339,  0.0825,  0.6217, -0.6347, -0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-18.1759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7331149525855376, distance: 0.5911787495255529 entropy 0.5003275275230408
epoch: 8, step: 38
	action: tensor([[ 0.2221,  0.9225,  0.2020,  0.1795,  0.3568, -0.5730,  0.8829]],
       dtype=torch.float64)
	q_value: tensor([[-14.7940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7450590151812784, distance: 0.577798650042768 entropy 0.5003275275230408
epoch: 8, step: 39
	action: tensor([[ 0.2873,  0.1415, -0.6575, -0.0785, -0.4945, -0.0504,  0.8094]],
       dtype=torch.float64)
	q_value: tensor([[-16.1824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.623795056684501, distance: 0.7018898177696471 entropy 0.5003275275230408
epoch: 8, step: 40
	action: tensor([[-0.0444, -0.0734,  0.0730,  0.0055,  0.1855,  0.5873, -0.1600]],
       dtype=torch.float64)
	q_value: tensor([[-15.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3964605076353549, distance: 0.8890159235856305 entropy 0.5003275275230408
epoch: 8, step: 41
	action: tensor([[ 0.2948,  0.0857, -0.1673, -0.9956,  0.5120, -0.0357,  0.0793]],
       dtype=torch.float64)
	q_value: tensor([[-12.1180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17842599119847635, distance: 1.0372418689514569 entropy 0.5003275275230408
epoch: 8, step: 42
	action: tensor([[ 0.0351,  0.4221, -0.2128, -0.0342,  0.2245, -0.0953, -0.5629]],
       dtype=torch.float64)
	q_value: tensor([[-13.7514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5653757937303927, distance: 0.7544207909715466 entropy 0.5003275275230408
epoch: 8, step: 43
	action: tensor([[-0.0141, -0.2299, -0.2203, -0.0773, -0.0700, -0.1792,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-10.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06190533731517278, distance: 1.1083579118929916 entropy 0.5003275275230408
epoch: 8, step: 44
	action: tensor([[ 0.1455, -0.3088, -0.2318, -0.2487, -0.2191, -0.0467,  0.0473]],
       dtype=torch.float64)
	q_value: tensor([[-11.3525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07329715952822369, distance: 1.10160764288618 entropy 0.5003275275230408
epoch: 8, step: 45
	action: tensor([[ 0.2442, -0.1269, -0.2839,  0.8462,  0.1589, -0.1258, -0.7366]],
       dtype=torch.float64)
	q_value: tensor([[-11.9421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6486574479553718, distance: 0.6783003935566607 entropy 0.5003275275230408
epoch: 8, step: 46
	action: tensor([[ 0.3425, -0.1984, -1.0306, -0.2216,  0.6518,  0.0189, -0.3019]],
       dtype=torch.float64)
	q_value: tensor([[-15.3846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44649987624089404, distance: 0.8513645339576109 entropy 0.5003275275230408
epoch: 8, step: 47
	action: tensor([[-0.1080,  0.1629, -0.2428,  0.4468, -0.2555,  0.1958,  0.3284]],
       dtype=torch.float64)
	q_value: tensor([[-13.4652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42797455322875566, distance: 0.8654946103488962 entropy 0.5003275275230408
epoch: 8, step: 48
	action: tensor([[ 0.1389, -0.0325, -0.4460, -0.3441, -0.2649, -0.7361, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-12.1604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34895290026881776, distance: 0.9233426523074278 entropy 0.5003275275230408
epoch: 8, step: 49
	action: tensor([[-0.2033,  0.1924,  0.1047,  0.0710,  0.6016, -0.0781,  0.3231]],
       dtype=torch.float64)
	q_value: tensor([[-13.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2142179932200523, distance: 1.0143964793034594 entropy 0.5003275275230408
epoch: 8, step: 50
	action: tensor([[ 0.2839,  0.6057,  0.4442, -0.0967, -0.0490,  0.8361, -0.4107]],
       dtype=torch.float64)
	q_value: tensor([[-12.9520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.880057857563696, distance: 0.39631690191626634 entropy 0.5003275275230408
epoch: 8, step: 51
	action: tensor([[-0.0378, -0.0642, -0.7030, -0.0642,  0.4114,  0.3560, -0.0487]],
       dtype=torch.float64)
	q_value: tensor([[-16.1314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29930182952218276, distance: 0.9579044373012092 entropy 0.5003275275230408
epoch: 8, step: 52
	action: tensor([[ 0.2820,  0.9120, -0.3176,  0.2686, -0.6144,  0.3392,  0.6376]],
       dtype=torch.float64)
	q_value: tensor([[-11.5536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 53
	action: tensor([[ 0.2358, -0.0305,  0.0954,  0.0807,  0.6724,  0.3849, -0.2600]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6417823861337141, distance: 0.6849047238749585 entropy 0.5003275275230408
epoch: 8, step: 54
	action: tensor([[ 0.8932,  0.9094,  0.1371, -0.2192,  0.0123,  0.4504,  0.3230]],
       dtype=torch.float64)
	q_value: tensor([[-12.8468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 55
	action: tensor([[ 0.2729, -0.0141,  0.2363,  0.9045,  0.4509,  0.3080,  0.5831]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9345429298004712, distance: 0.29277566403252303 entropy 0.5003275275230408
epoch: 8, step: 56
	action: tensor([[-0.0687,  0.0768, -0.4669,  0.4978,  0.6356, -0.7199, -0.0716]],
       dtype=torch.float64)
	q_value: tensor([[-16.7786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17164133259741332, distance: 1.0415158983071837 entropy 0.5003275275230408
epoch: 8, step: 57
	action: tensor([[-0.0488, -1.3668, -0.2614,  0.4272, -0.2179, -0.1832,  0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-14.9704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6989165087724882, distance: 1.4915668118820467 entropy 0.5003275275230408
epoch: 8, step: 58
	action: tensor([[ 0.5638,  0.3594, -0.6630, -0.0054, -0.4551, -0.0296, -0.4004]],
       dtype=torch.float64)
	q_value: tensor([[-18.3407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.887190690785259, distance: 0.384352015841682 entropy 0.5003275275230408
epoch: 8, step: 59
	action: tensor([[ 0.9620,  0.0480, -0.1073,  0.1619,  0.6929,  0.4993, -0.1901]],
       dtype=torch.float64)
	q_value: tensor([[-12.7450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9069132496380767, distance: 0.3491408203372684 entropy 0.5003275275230408
epoch: 8, step: 60
	action: tensor([[-0.3549,  0.7430,  0.1205,  0.9454,  0.0507, -0.0382, -0.3237]],
       dtype=torch.float64)
	q_value: tensor([[-16.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 61
	action: tensor([[ 0.5342,  0.1422, -1.0175, -0.0883,  0.4550, -0.2438, -0.0480]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.750440840349133, distance: 0.5716674312911365 entropy 0.5003275275230408
epoch: 8, step: 62
	action: tensor([[-0.1166,  0.2282, -0.4290, -0.1368, -0.1273, -0.2851,  0.0645]],
       dtype=torch.float64)
	q_value: tensor([[-13.9526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3483437932129265, distance: 0.9237744820784415 entropy 0.5003275275230408
epoch: 8, step: 63
	action: tensor([[ 0.1677, -0.1587,  0.6024,  0.1941,  1.1182,  0.2513,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-10.6200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.522751592704686, distance: 0.7905492611731615 entropy 0.5003275275230408
epoch: 8, step: 64
	action: tensor([[-0.1743, -0.2593,  0.2033,  0.0665, -0.3442, -0.4837, -0.1424]],
       dtype=torch.float64)
	q_value: tensor([[-16.6657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16228101342037093, distance: 1.2337076705712438 entropy 0.5003275275230408
epoch: 8, step: 65
	action: tensor([[-0.0223, -0.1718, -0.4822,  0.0052,  0.0809, -0.3209, -0.0785]],
       dtype=torch.float64)
	q_value: tensor([[-13.4078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07738330177386088, distance: 1.0991762817947757 entropy 0.5003275275230408
epoch: 8, step: 66
	action: tensor([[ 0.2485,  0.0016,  0.0509, -0.1763, -0.4568,  0.4599,  0.3028]],
       dtype=torch.float64)
	q_value: tensor([[-11.5228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4840257091132065, distance: 0.8219979315564736 entropy 0.5003275275230408
epoch: 8, step: 67
	action: tensor([[ 0.0822, -0.1438,  0.0804,  0.6221,  0.2675,  0.3610,  0.0656]],
       dtype=torch.float64)
	q_value: tensor([[-14.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6732823936140961, distance: 0.6540982129490314 entropy 0.5003275275230408
epoch: 8, step: 68
	action: tensor([[-0.0875, -0.0802,  0.4180, -0.1514,  0.5674,  0.0602, -0.2358]],
       dtype=torch.float64)
	q_value: tensor([[-13.7937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0542458554208306, distance: 1.1128735482861718 entropy 0.5003275275230408
epoch: 8, step: 69
	action: tensor([[ 0.4406, -0.4857,  0.2794, -0.5017, -0.3667,  0.0468,  0.2295]],
       dtype=torch.float64)
	q_value: tensor([[-12.8691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17484679366468403, distance: 1.240358740190008 entropy 0.5003275275230408
epoch: 8, step: 70
	action: tensor([[-0.1743, -0.4599, -0.1718,  0.0617, -0.3515, -0.5597, -0.1690]],
       dtype=torch.float64)
	q_value: tensor([[-15.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2764795414230603, distance: 1.2928960915459324 entropy 0.5003275275230408
epoch: 8, step: 71
	action: tensor([[ 1.1512, -0.2249,  0.2793, -0.3466,  0.6388, -0.3592,  0.3427]],
       dtype=torch.float64)
	q_value: tensor([[-13.9201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926491278739174, distance: 0.9624410537562964 entropy 0.5003275275230408
epoch: 8, step: 72
	action: tensor([[ 0.2707,  0.1207, -0.3157, -0.0546, -0.2118,  0.3172,  0.2325]],
       dtype=torch.float64)
	q_value: tensor([[-19.4114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6490652499245285, distance: 0.6779066289146163 entropy 0.5003275275230408
epoch: 8, step: 73
	action: tensor([[ 0.3583,  0.2562, -0.6613, -0.1681,  0.2926,  0.1996, -0.4908]],
       dtype=torch.float64)
	q_value: tensor([[-11.8689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7816834366112047, distance: 0.534687506404778 entropy 0.5003275275230408
epoch: 8, step: 74
	action: tensor([[ 0.0470,  0.1310,  0.3198, -0.3461, -0.2047, -0.0279,  0.2156]],
       dtype=torch.float64)
	q_value: tensor([[-11.5499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24009762982130667, distance: 0.9975521128702581 entropy 0.5003275275230408
epoch: 8, step: 75
	action: tensor([[-0.0327, -0.0955, -0.2307, -0.2853, -0.3654,  0.7552,  0.3938]],
       dtype=torch.float64)
	q_value: tensor([[-12.8575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18985193971219427, distance: 1.0300039531542595 entropy 0.5003275275230408
epoch: 8, step: 76
	action: tensor([[ 0.1082,  0.0248, -0.8672, -0.7119,  0.2992,  0.2194, -0.7155]],
       dtype=torch.float64)
	q_value: tensor([[-14.8325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48562230410731166, distance: 0.8207251795726889 entropy 0.5003275275230408
epoch: 8, step: 77
	action: tensor([[ 0.1763, -0.1713, -0.0432, -0.1902, -0.0086,  0.8577,  0.3155]],
       dtype=torch.float64)
	q_value: tensor([[-12.3981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48270998769338347, distance: 0.8230453012801089 entropy 0.5003275275230408
epoch: 8, step: 78
	action: tensor([[ 0.1599,  0.4157, -0.3776, -0.3134, -0.2876,  0.1318,  0.0682]],
       dtype=torch.float64)
	q_value: tensor([[-15.0364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6729018145336294, distance: 0.6544790673016099 entropy 0.5003275275230408
epoch: 8, step: 79
	action: tensor([[ 0.1529,  0.3056, -0.1982, -0.1411,  0.2505, -0.3616,  0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-11.8938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5492648225676642, distance: 0.7682762716447132 entropy 0.5003275275230408
epoch: 8, step: 80
	action: tensor([[-0.3734, -0.4438, -0.2263,  0.1724, -0.3846,  0.1626, -0.1900]],
       dtype=torch.float64)
	q_value: tensor([[-11.8726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37281177020514544, distance: 1.3407944089614 entropy 0.5003275275230408
epoch: 8, step: 81
	action: tensor([[ 0.3014, -0.2180,  0.1112,  0.2900, -1.1675,  0.0675, -0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-12.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5376798883774346, distance: 0.7780868693709951 entropy 0.5003275275230408
epoch: 8, step: 82
	action: tensor([[ 0.3918,  0.3496, -0.4074,  0.4966, -0.9776,  0.1706,  0.1943]],
       dtype=torch.float64)
	q_value: tensor([[-16.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7729210125142408, distance: 0.5453121419496588 entropy 0.5003275275230408
epoch: 8, step: 83
	action: tensor([[-0.1980, -0.0841,  0.2790,  0.2002,  0.4676, -0.8535, -0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-15.1870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09836714792999524, distance: 1.1993072543046748 entropy 0.5003275275230408
epoch: 8, step: 84
	action: tensor([[ 1.1353,  0.1910, -0.5049,  0.2796, -0.0426,  0.3231,  0.2717]],
       dtype=torch.float64)
	q_value: tensor([[-15.5512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9564971428334937, distance: 0.23867975530989644 entropy 0.5003275275230408
epoch: 8, step: 85
	action: tensor([[-0.1099, -0.0938,  0.4293, -0.1877, -0.0258,  0.4932, -0.1703]],
       dtype=torch.float64)
	q_value: tensor([[-16.0680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1815150666017823, distance: 1.0352900448825069 entropy 0.5003275275230408
epoch: 8, step: 86
	action: tensor([[ 0.2576,  0.3708, -0.6752,  0.1966,  0.3997,  0.1344, -0.7887]],
       dtype=torch.float64)
	q_value: tensor([[-13.6970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7305201387059082, distance: 0.594045692139091 entropy 0.5003275275230408
epoch: 8, step: 87
	action: tensor([[ 0.3232, -0.9316,  0.1634,  0.1136,  0.1121,  0.8337, -0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-12.6493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1164333811158359, distance: 1.0756632796914436 entropy 0.5003275275230408
epoch: 8, step: 88
	action: tensor([[ 0.0702, -0.4854, -0.7616,  0.1618,  0.1905, -0.2051,  0.2141]],
       dtype=torch.float64)
	q_value: tensor([[-17.3024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04101295967017782, distance: 1.1675749300568092 entropy 0.5003275275230408
epoch: 8, step: 89
	action: tensor([[ 0.4496,  0.7878, -0.0784, -0.0860, -0.3376, -0.2524,  0.0710]],
       dtype=torch.float64)
	q_value: tensor([[-13.3438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9129157342293095, distance: 0.33769648527119994 entropy 0.5003275275230408
epoch: 8, step: 90
	action: tensor([[-0.1215, -0.1765, -0.4865,  0.0142, -0.1910, -0.0581,  0.4841]],
       dtype=torch.float64)
	q_value: tensor([[-13.5415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059301242742521465, distance: 1.1098952133432856 entropy 0.5003275275230408
epoch: 8, step: 91
	action: tensor([[ 0.8750,  1.0585,  0.3027, -0.7346, -0.2750,  0.6800,  0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-12.3462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9632864645735348, distance: 0.219265244356558 entropy 0.5003275275230408
epoch: 8, step: 92
	action: tensor([[ 0.5244, -0.2035, -0.1421, -0.4690, -0.1716,  0.0980,  0.6552]],
       dtype=torch.float64)
	q_value: tensor([[-20.4456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2627920419666785, distance: 0.9825433047472455 entropy 0.5003275275230408
epoch: 8, step: 93
	action: tensor([[-0.1631,  0.4424, -0.1504,  0.8523, -0.2606,  0.6752, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-15.1261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.477950037224696, distance: 0.8268233404119341 entropy 0.5003275275230408
epoch: 8, step: 94
	action: tensor([[ 0.0093,  0.2925, -0.0687, -0.3236, -0.7018, -0.2262,  0.5328]],
       dtype=torch.float64)
	q_value: tensor([[-13.8622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4249803768290672, distance: 0.8677568010191169 entropy 0.5003275275230408
epoch: 8, step: 95
	action: tensor([[-0.0425, -0.2689,  0.6791,  0.3305,  0.0098,  0.0131, -0.1026]],
       dtype=torch.float64)
	q_value: tensor([[-14.4255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309304720392088, distance: 0.9510425267157386 entropy 0.5003275275230408
epoch: 8, step: 96
	action: tensor([[ 0.6416,  0.1476,  0.1119, -0.1530, -0.3161, -0.0367, -0.9432]],
       dtype=torch.float64)
	q_value: tensor([[-15.1810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.735138973686009, distance: 0.5889327721779303 entropy 0.5003275275230408
epoch: 8, step: 97
	action: tensor([[-0.0691, -0.0972,  0.2529,  0.3641, -0.2838, -0.4581,  0.5799]],
       dtype=torch.float64)
	q_value: tensor([[-15.5152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991123439177572, distance: 0.9580339487224497 entropy 0.5003275275230408
epoch: 8, step: 98
	action: tensor([[ 0.8790,  0.2733,  0.2298,  0.1293,  0.5069, -0.5544,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[-15.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8992345100749716, distance: 0.36325583879694856 entropy 0.5003275275230408
epoch: 8, step: 99
	action: tensor([[-0.0474,  0.0484, -0.0548,  0.9081,  0.0408,  0.4006,  0.4448]],
       dtype=torch.float64)
	q_value: tensor([[-16.7546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6559721251700505, distance: 0.6712024137355725 entropy 0.5003275275230408
epoch: 8, step: 100
	action: tensor([[ 0.3551, -0.4105, -0.1520, -0.4096, -0.1951,  0.3026,  0.5227]],
       dtype=torch.float64)
	q_value: tensor([[-14.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04656798414067265, distance: 1.1173817116062839 entropy 0.5003275275230408
epoch: 8, step: 101
	action: tensor([[-0.0071, -1.0140, -0.5894,  0.1519,  0.7166, -0.1590,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-14.8269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6661028811653154, distance: 1.4770921870086136 entropy 0.5003275275230408
epoch: 8, step: 102
	action: tensor([[ 1.3601,  0.8879, -0.0056,  0.3381, -0.0713,  0.3843,  0.7860]],
       dtype=torch.float64)
	q_value: tensor([[-16.2595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 103
	action: tensor([[ 0.5659, -0.2140, -0.1603,  0.1884, -0.6702, -0.5736,  0.4089]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5621124207246937, distance: 0.7572477760792037 entropy 0.5003275275230408
epoch: 8, step: 104
	action: tensor([[ 0.4283, -0.2643, -0.4014,  0.2547, -0.3360, -0.1485, -0.2701]],
       dtype=torch.float64)
	q_value: tensor([[-15.8267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5262296418050161, distance: 0.7876633458309069 entropy 0.5003275275230408
epoch: 8, step: 105
	action: tensor([[-0.5353,  0.4113,  0.1081,  0.3230, -0.3769,  0.0083,  0.3290]],
       dtype=torch.float64)
	q_value: tensor([[-12.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14412799028901624, distance: 1.0586712153531719 entropy 0.5003275275230408
epoch: 8, step: 106
	action: tensor([[-0.0293,  0.1391, -0.3054, -0.0140,  0.1722,  0.5279, -0.4786]],
       dtype=torch.float64)
	q_value: tensor([[-12.9819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4692078882345737, distance: 0.8337175110119327 entropy 0.5003275275230408
epoch: 8, step: 107
	action: tensor([[ 0.6811, -0.2002, -0.1021,  0.3500,  0.6822,  0.5509,  0.4702]],
       dtype=torch.float64)
	q_value: tensor([[-10.9633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8771196245168498, distance: 0.4011418362201055 entropy 0.5003275275230408
epoch: 8, step: 108
	action: tensor([[-0.1603, -0.2965,  0.2715, -0.0326, -0.2836,  0.1552,  0.0874]],
       dtype=torch.float64)
	q_value: tensor([[-16.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08850353736047412, distance: 1.1939100710324064 entropy 0.5003275275230408
epoch: 8, step: 109
	action: tensor([[-1.0882,  0.4306, -0.0299,  0.3690, -0.4218,  0.0041, -0.0980]],
       dtype=torch.float64)
	q_value: tensor([[-13.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4365522089921843, distance: 1.3715681780028275 entropy 0.5003275275230408
epoch: 8, step: 110
	action: tensor([[ 0.2094, -0.6363,  0.0170,  0.1734,  0.0795,  0.6557, -0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-13.8842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25421475143898675, distance: 0.9882426379923653 entropy 0.5003275275230408
epoch: 8, step: 111
	action: tensor([[ 0.1630,  0.3659, -0.5183, -0.3332, -0.4544, -0.3116, -0.1508]],
       dtype=torch.float64)
	q_value: tensor([[-14.8103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6623810468106468, distance: 0.6649210817573569 entropy 0.5003275275230408
epoch: 8, step: 112
	action: tensor([[-0.4190, -0.4139, -0.3086, -0.3456,  0.2637,  1.0842, -0.3926]],
       dtype=torch.float64)
	q_value: tensor([[-12.0805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19265618791263184, distance: 1.2497246214636033 entropy 0.5003275275230408
epoch: 8, step: 113
	action: tensor([[ 0.2127,  0.3990, -0.0472, -0.4544,  0.6552,  0.5844,  0.0552]],
       dtype=torch.float64)
	q_value: tensor([[-15.4361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7391035475803301, distance: 0.5845084314071566 entropy 0.5003275275230408
epoch: 8, step: 114
	action: tensor([[-0.2036, -0.1819,  0.4347,  0.1055,  0.0051,  0.0487,  0.3748]],
       dtype=torch.float64)
	q_value: tensor([[-13.2526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026787459991114648, distance: 1.1289131747047623 entropy 0.5003275275230408
epoch: 8, step: 115
	action: tensor([[ 0.2313,  0.5532,  0.2605, -0.1660,  0.3683, -0.3292,  0.4071]],
       dtype=torch.float64)
	q_value: tensor([[-14.2953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6597522275292911, distance: 0.6675047163172066 entropy 0.5003275275230408
epoch: 8, step: 116
	action: tensor([[ 0.3830, -0.3044,  0.1055, -0.0552,  0.0806, -0.1111, -1.1014]],
       dtype=torch.float64)
	q_value: tensor([[-13.7124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25024785018319884, distance: 0.9908674295991379 entropy 0.5003275275230408
epoch: 8, step: 117
	action: tensor([[-0.7717,  0.6544,  0.0594, -0.0582, -0.7419, -0.0068,  0.3653]],
       dtype=torch.float64)
	q_value: tensor([[-15.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.157186097809777, distance: 1.2310006918436869 entropy 0.5003275275230408
epoch: 8, step: 118
	action: tensor([[-0.5569, -0.0392, -0.4313,  0.0689, -0.1427,  0.3379,  0.4820]],
       dtype=torch.float64)
	q_value: tensor([[-14.9138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34705041014325166, distance: 1.328154559420979 entropy 0.5003275275230408
epoch: 8, step: 119
	action: tensor([[ 0.0497,  0.2904,  0.0684,  0.0302, -0.3276,  0.0453,  0.4199]],
       dtype=torch.float64)
	q_value: tensor([[-12.8628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5267558484564174, distance: 0.7872258038496462 entropy 0.5003275275230408
epoch: 8, step: 120
	action: tensor([[-0.1156, -0.2157, -0.5815,  0.1525,  0.4201, -0.1804, -0.9772]],
       dtype=torch.float64)
	q_value: tensor([[-12.4867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008893018856020118, distance: 1.1392445532321023 entropy 0.5003275275230408
epoch: 8, step: 121
	action: tensor([[ 0.6917,  0.1497,  0.2081,  0.3419,  0.1157, -0.4450, -0.0855]],
       dtype=torch.float64)
	q_value: tensor([[-13.7008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8793902870886319, distance: 0.3974182760654879 entropy 0.5003275275230408
epoch: 8, step: 122
	action: tensor([[ 0.6340,  0.3650, -0.2321, -0.2716, -0.2823, -0.0805, -0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-15.0564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8365269607972468, distance: 0.46267897170929756 entropy 0.5003275275230408
epoch: 8, step: 123
	action: tensor([[-0.1920,  0.3747, -0.2310,  0.1113,  0.5010, -0.3882, -0.6231]],
       dtype=torch.float64)
	q_value: tensor([[-12.8318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29092265505599013, distance: 0.9636148811385884 entropy 0.5003275275230408
epoch: 8, step: 124
	action: tensor([[-0.1696, -0.1336, -0.3710,  0.0461, -0.5874, -0.6166,  0.4731]],
       dtype=torch.float64)
	q_value: tensor([[-12.2171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13015717822396178, distance: 1.0672768397777517 entropy 0.5003275275230408
epoch: 8, step: 125
	action: tensor([[-0.0942, -0.5390,  0.3158,  0.3794,  0.1224, -0.2715, -0.4906]],
       dtype=torch.float64)
	q_value: tensor([[-14.5723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047093038340261906, distance: 1.170979600526507 entropy 0.5003275275230408
epoch: 8, step: 126
	action: tensor([[-0.5715, -0.0885, -0.5605, -0.0852,  0.2439,  0.1456, -0.5864]],
       dtype=torch.float64)
	q_value: tensor([[-14.9768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40631130393332815, distance: 1.35705492993424 entropy 0.5003275275230408
epoch: 8, step: 127
	action: tensor([[-0.0519, -0.6254, -0.6023, -1.0223, -0.0209, -0.2543, -0.0444]],
       dtype=torch.float64)
	q_value: tensor([[-11.6597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22669574005836335, distance: 1.2674333245594964 entropy 0.5003275275230408
LOSS epoch 8 actor 145.5308034346828 critic 388.69373051236323 
epoch: 9, step: 0
	action: tensor([[-0.1194,  0.6033,  0.0016, -0.3055,  0.5120, -0.2411,  0.6063]],
       dtype=torch.float64)
	q_value: tensor([[-15.3190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3728022559940838, distance: 0.9062727819531142 entropy 0.5003275275230408
epoch: 9, step: 1
	action: tensor([[ 0.2275, -0.2529,  0.8201, -0.6473,  0.2090,  0.4962,  0.1989]],
       dtype=torch.float64)
	q_value: tensor([[-14.8144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02650405552555013, distance: 1.129077535383039 entropy 0.5003275275230408
epoch: 9, step: 2
	action: tensor([[-0.2813,  0.0218, -0.0342,  0.1808,  0.7150,  0.4498,  0.1521]],
       dtype=torch.float64)
	q_value: tensor([[-18.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2713346580400767, distance: 0.9768339607545053 entropy 0.5003275275230408
epoch: 9, step: 3
	action: tensor([[ 0.1165, -0.3472,  0.0075, -0.0837,  0.2800, -0.1818,  0.4207]],
       dtype=torch.float64)
	q_value: tensor([[-14.2587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004464374779801661, distance: 1.1468958002201328 entropy 0.5003275275230408
epoch: 9, step: 4
	action: tensor([[ 0.8559,  0.3446, -0.7093, -0.3560,  0.2026,  0.2405, -1.0188]],
       dtype=torch.float64)
	q_value: tensor([[-14.5309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.874774883683435, distance: 0.4049509434805444 entropy 0.5003275275230408
epoch: 9, step: 5
	action: tensor([[ 0.3180,  0.4677,  0.3698,  0.5157, -0.0130, -0.6873,  0.3442]],
       dtype=torch.float64)
	q_value: tensor([[-16.7094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8764587433461206, distance: 0.4022191099007506 entropy 0.5003275275230408
epoch: 9, step: 6
	action: tensor([[ 0.3618, -0.1760, -0.1623, -0.2958,  0.6433, -0.2610,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[-17.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21208064412717775, distance: 1.015775135878367 entropy 0.5003275275230408
epoch: 9, step: 7
	action: tensor([[ 0.8159, -0.2841, -0.0703, -0.2849, -0.5374,  0.2938,  0.1626]],
       dtype=torch.float64)
	q_value: tensor([[-14.4395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3793469303698459, distance: 0.9015320008941358 entropy 0.5003275275230408
epoch: 9, step: 8
	action: tensor([[ 0.1566, -0.6141, -0.1792,  0.0070,  0.8324,  0.5493,  0.7959]],
       dtype=torch.float64)
	q_value: tensor([[-17.1316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14064489132511837, distance: 1.0608232383906568 entropy 0.5003275275230408
epoch: 9, step: 9
	action: tensor([[-0.3514,  0.0469, -0.3168,  0.3426,  0.0271, -0.1977,  0.2780]],
       dtype=torch.float64)
	q_value: tensor([[-19.4176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01736823358167039, distance: 1.1343631062917316 entropy 0.5003275275230408
epoch: 9, step: 10
	action: tensor([[-0.2559, -0.6110, -0.5324,  0.1300,  0.3771,  0.0393,  0.3708]],
       dtype=torch.float64)
	q_value: tensor([[-12.6997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4513634975037002, distance: 1.378620687972142 entropy 0.5003275275230408
epoch: 9, step: 11
	action: tensor([[ 5.4620e-01,  2.9026e-01,  8.2383e-02,  2.2555e-02,  1.3940e-01,
          2.0077e-01, -3.2816e-04]], dtype=torch.float64)
	q_value: tensor([[-15.1623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8910528364225409, distance: 0.3777153697692484 entropy 0.5003275275230408
epoch: 9, step: 12
	action: tensor([[-0.0370, -0.2926, -0.9125,  0.3851,  0.5631, -0.1982, -0.7279]],
       dtype=torch.float64)
	q_value: tensor([[-13.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024091000046321698, distance: 1.1580464191502018 entropy 0.5003275275230408
epoch: 9, step: 13
	action: tensor([[-0.0947,  0.3111, -0.8600, -0.4804,  0.4066, -0.2486,  0.3221]],
       dtype=torch.float64)
	q_value: tensor([[-15.7988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4795873434628575, distance: 0.8255257384295082 entropy 0.5003275275230408
epoch: 9, step: 14
	action: tensor([[-1.0600,  1.0796, -1.0163,  0.0621, -0.8615, -0.3292,  0.4064]],
       dtype=torch.float64)
	q_value: tensor([[-13.3891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 9, step: 15
	action: tensor([[-0.0333,  0.1478,  0.2222, -0.0238,  0.5477,  0.6391,  0.1492]],
       dtype=torch.float64)
	q_value: tensor([[-19.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5785824403133305, distance: 0.7428703199117781 entropy 0.5003275275230408
epoch: 9, step: 16
	action: tensor([[-0.0029,  0.3032,  0.3360,  0.0259,  0.2977,  0.0707,  0.5966]],
       dtype=torch.float64)
	q_value: tensor([[-14.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.527424694867711, distance: 0.7866693053447181 entropy 0.5003275275230408
epoch: 9, step: 17
	action: tensor([[-0.4949,  0.7878, -0.1881,  0.3644,  0.5782, -0.1350,  0.0976]],
       dtype=torch.float64)
	q_value: tensor([[-15.0397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 9, step: 18
	action: tensor([[ 0.0728, -0.1661, -0.1335,  0.8559, -0.3746, -0.5164,  0.3440]],
       dtype=torch.float64)
	q_value: tensor([[-19.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5451406027511272, distance: 0.7717831259243877 entropy 0.5003275275230408
epoch: 9, step: 19
	action: tensor([[ 0.6029,  0.2183, -0.6804, -0.1134, -0.3407,  0.2425, -0.2941]],
       dtype=torch.float64)
	q_value: tensor([[-16.7623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8235175453208959, distance: 0.48073690276875186 entropy 0.5003275275230408
epoch: 9, step: 20
	action: tensor([[-0.2758, -0.0287,  0.2730,  0.6486,  0.0497, -0.2222, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-13.8086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.339348813582238, distance: 0.9301281811617699 entropy 0.5003275275230408
epoch: 9, step: 21
	action: tensor([[-0.5015,  0.1392,  0.5077,  0.4895, -0.1542, -0.1653,  0.2294]],
       dtype=torch.float64)
	q_value: tensor([[-14.8659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14778830894498518, distance: 1.0564049734479244 entropy 0.5003275275230408
epoch: 9, step: 22
	action: tensor([[-0.2426,  0.3138, -0.5346,  0.0148, -0.0263, -0.1615, -0.0881]],
       dtype=torch.float64)
	q_value: tensor([[-15.7398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25556217489236366, distance: 0.9873494966340015 entropy 0.5003275275230408
epoch: 9, step: 23
	action: tensor([[ 0.4287,  0.4656,  0.0639,  0.1397, -0.4631,  0.2046, -0.6495]],
       dtype=torch.float64)
	q_value: tensor([[-10.8360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8783716746338851, distance: 0.3990929507952589 entropy 0.5003275275230408
epoch: 9, step: 24
	action: tensor([[ 0.2469,  0.6497, -0.2304, -0.0162,  0.2520, -0.1124,  0.3084]],
       dtype=torch.float64)
	q_value: tensor([[-15.2762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7950888777786517, distance: 0.5180115690088016 entropy 0.5003275275230408
epoch: 9, step: 25
	action: tensor([[-0.3494, -0.6109, -0.6814,  0.3103,  0.9470,  0.5550,  0.1096]],
       dtype=torch.float64)
	q_value: tensor([[-12.9869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3218441472981246, distance: 1.3156695380376713 entropy 0.5003275275230408
epoch: 9, step: 26
	action: tensor([[-0.1397,  0.7492,  0.0246, -0.0562, -0.2599, -0.5063,  0.8068]],
       dtype=torch.float64)
	q_value: tensor([[-17.8671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5086358198954201, distance: 0.802155268255814 entropy 0.5003275275230408
epoch: 9, step: 27
	action: tensor([[-0.1224,  0.2207, -0.7473, -0.3335, -0.1556,  0.0525,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-15.7136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3368367734648301, distance: 0.9318948490255853 entropy 0.5003275275230408
epoch: 9, step: 28
	action: tensor([[ 0.0578,  0.2858, -0.6016,  0.5081, -0.4666,  0.6217, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-12.0816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3793487097511108, distance: 0.9015307085696631 entropy 0.5003275275230408
epoch: 9, step: 29
	action: tensor([[-0.1172, -0.3304, -0.0869, -0.2068,  0.5641,  0.1903, -0.4084]],
       dtype=torch.float64)
	q_value: tensor([[-14.1993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13081410770725954, distance: 1.2168927460439283 entropy 0.5003275275230408
epoch: 9, step: 30
	action: tensor([[ 0.5641,  0.5433,  0.6685, -0.9032,  0.0501, -0.4538, -0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-12.7607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5190568509143174, distance: 0.7936034819392335 entropy 0.5003275275230408
epoch: 9, step: 31
	action: tensor([[-1.0198, -0.5775, -0.2455,  0.6360, -0.0876,  0.0808, -0.4975]],
       dtype=torch.float64)
	q_value: tensor([[-18.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0700159644638343, distance: 1.646431022905886 entropy 0.5003275275230408
epoch: 9, step: 32
	action: tensor([[ 0.6428, -0.8231,  0.3333,  0.0551, -0.4699,  0.1108, -0.3071]],
       dtype=torch.float64)
	q_value: tensor([[-17.0817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08458244151131455, distance: 1.1917577310372953 entropy 0.5003275275230408
epoch: 9, step: 33
	action: tensor([[ 0.2320, -0.1775,  0.2810,  0.4822, -0.1323, -0.1291, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-18.1749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6204722396997319, distance: 0.7049827119292702 entropy 0.5003275275230408
epoch: 9, step: 34
	action: tensor([[-0.4366, -0.0582, -0.0727, -0.1460,  0.0195,  0.0990,  1.5067]],
       dtype=torch.float64)
	q_value: tensor([[-14.8240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30162586396740343, distance: 1.3055688456287804 entropy 0.5003275275230408
epoch: 9, step: 35
	action: tensor([[ 1.0082, -0.1384, -0.3649, -0.2438,  0.1050,  0.0592, -0.1783]],
       dtype=torch.float64)
	q_value: tensor([[-21.5531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4805412823168742, distance: 0.824768779236664 entropy 0.5003275275230408
epoch: 9, step: 36
	action: tensor([[-0.4625,  0.3355, -0.1113, -0.1845,  0.0524, -0.0516,  0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-16.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05060201758024019, distance: 1.1729400311091103 entropy 0.5003275275230408
epoch: 9, step: 37
	action: tensor([[ 0.2734, -0.4877,  0.0714, -0.6663, -0.3278, -0.1093,  0.1267]],
       dtype=torch.float64)
	q_value: tensor([[-11.6699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3598521906505734, distance: 1.3344507370467755 entropy 0.5003275275230408
epoch: 9, step: 38
	action: tensor([[ 0.4211, -0.5929, -0.7950, -0.1322, -0.4394, -0.5685, -0.1822]],
       dtype=torch.float64)
	q_value: tensor([[-15.9034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04259134813101839, distance: 1.119709510908111 entropy 0.5003275275230408
epoch: 9, step: 39
	action: tensor([[ 0.6164,  0.0094, -0.3000, -0.3782,  0.1541, -0.2795,  0.1558]],
       dtype=torch.float64)
	q_value: tensor([[-16.3286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4845755441126329, distance: 0.8215598441263301 entropy 0.5003275275230408
epoch: 9, step: 40
	action: tensor([[ 0.1323, -0.0921,  0.1847,  0.1280, -0.2597,  0.1708,  0.4467]],
       dtype=torch.float64)
	q_value: tensor([[-14.6680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47567744135628776, distance: 0.8286210560702717 entropy 0.5003275275230408
epoch: 9, step: 41
	action: tensor([[-0.4161, -0.2615, -0.2841, -0.2883, -0.0196,  0.0462,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-14.4834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41524050474411056, distance: 1.3613563396783723 entropy 0.5003275275230408
epoch: 9, step: 42
	action: tensor([[ 0.4778, -0.2885, -0.0109, -0.1925,  0.2906,  0.2395,  0.2627]],
       dtype=torch.float64)
	q_value: tensor([[-12.2855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3719402253860138, distance: 0.9068953660054756 entropy 0.5003275275230408
epoch: 9, step: 43
	action: tensor([[ 0.8808,  0.2826, -0.5662,  0.0358, -0.1225, -0.6264, -0.7125]],
       dtype=torch.float64)
	q_value: tensor([[-14.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8367640688495712, distance: 0.4623433055976925 entropy 0.5003275275230408
epoch: 9, step: 44
	action: tensor([[ 0.5512,  0.5331, -0.2642, -0.4403,  0.2719, -0.3906,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-16.9415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8294669527524299, distance: 0.4725643651660494 entropy 0.5003275275230408
epoch: 9, step: 45
	action: tensor([[ 0.1308,  0.7369, -0.6712,  0.4173, -0.4402, -0.5936, -0.1291]],
       dtype=torch.float64)
	q_value: tensor([[-14.5200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.741330368939263, distance: 0.5820086175318073 entropy 0.5003275275230408
epoch: 9, step: 46
	action: tensor([[-0.5290,  0.0404, -0.1256,  0.3250,  0.3510, -0.2673,  0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-14.2967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23195318887140837, distance: 1.2701464428966136 entropy 0.5003275275230408
epoch: 9, step: 47
	action: tensor([[ 0.5693,  0.1294, -0.2745,  0.5348,  0.4600, -0.7785, -0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-13.4785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7899651942922478, distance: 0.5244478724819871 entropy 0.5003275275230408
epoch: 9, step: 48
	action: tensor([[ 0.5556,  1.5256, -0.3186, -0.0676, -0.5845,  0.4494, -0.3773]],
       dtype=torch.float64)
	q_value: tensor([[-17.6437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 9, step: 49
	action: tensor([[-0.3639, -0.8497, -0.3680, -0.1563, -0.8508,  0.0121, -0.1357]],
       dtype=torch.float64)
	q_value: tensor([[-19.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.773196174346322, distance: 1.5238249709622402 entropy 0.5003275275230408
epoch: 9, step: 50
	action: tensor([[-0.3141,  0.2363, -0.4946, -0.3225,  0.1161, -0.2345,  0.1680]],
       dtype=torch.float64)
	q_value: tensor([[-16.7117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07580069339296613, distance: 1.100118612585117 entropy 0.5003275275230408
epoch: 9, step: 51
	action: tensor([[-0.1912, -0.5334, -0.1976, -0.1697,  0.4096,  0.0734, -0.2896]],
       dtype=torch.float64)
	q_value: tensor([[-11.5283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39264413340873694, distance: 1.3504445927788018 entropy 0.5003275275230408
epoch: 9, step: 52
	action: tensor([[-0.2269,  0.0559,  0.3490, -0.2144, -0.3950,  1.5868,  0.1222]],
       dtype=torch.float64)
	q_value: tensor([[-13.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4095556643392336, distance: 0.8793184254815958 entropy 0.5003275275230408
epoch: 9, step: 53
	action: tensor([[-0.5331, -0.1274, -0.6799, -0.0834, -0.6620,  0.1567, -0.1508]],
       dtype=torch.float64)
	q_value: tensor([[-22.8277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4523178146231248, distance: 1.37907385670905 entropy 0.5003275275230408
epoch: 9, step: 54
	action: tensor([[-0.1322, -0.3489,  0.2157,  0.6039, -0.0670, -0.2186,  0.4300]],
       dtype=torch.float64)
	q_value: tensor([[-13.3561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21942603759505153, distance: 1.0110292573958073 entropy 0.5003275275230408
epoch: 9, step: 55
	action: tensor([[-0.3621,  0.0312, -0.3188,  0.2516, -0.1351, -0.3001,  0.0724]],
       dtype=torch.float64)
	q_value: tensor([[-16.3075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047683670220112884, distance: 1.1713098101425534 entropy 0.5003275275230408
epoch: 9, step: 56
	action: tensor([[-0.4544, -0.2367, -0.3264, -0.5817,  0.5744, -0.3550,  0.0514]],
       dtype=torch.float64)
	q_value: tensor([[-12.2270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5277764894338597, distance: 1.4144467578177713 entropy 0.5003275275230408
epoch: 9, step: 57
	action: tensor([[-0.0809,  0.2979, -0.5257, -0.2830, -0.3115, -0.5885, -0.6220]],
       dtype=torch.float64)
	q_value: tensor([[-13.9708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45072350911314196, distance: 0.848110029437145 entropy 0.5003275275230408
epoch: 9, step: 58
	action: tensor([[ 0.0417, -0.3920,  0.0069, -0.0548, -0.1840,  0.3979,  0.8971]],
       dtype=torch.float64)
	q_value: tensor([[-13.5386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07619465139602632, distance: 1.0998841140711864 entropy 0.5003275275230408
epoch: 9, step: 59
	action: tensor([[ 0.5292, -0.6474, -0.6363, -0.3180,  0.1070, -0.3461, -0.0992]],
       dtype=torch.float64)
	q_value: tensor([[-17.4494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19433869684275829, distance: 1.250605819166208 entropy 0.5003275275230408
epoch: 9, step: 60
	action: tensor([[ 0.0075, -0.2301, -0.4897, -0.4613, -0.4091,  0.3396,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-15.6232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006144465854705183, distance: 1.140823144734498 entropy 0.5003275275230408
epoch: 9, step: 61
	action: tensor([[ 0.8752, -0.4971, -0.1841,  0.6772, -0.5037, -0.2442, -0.6049]],
       dtype=torch.float64)
	q_value: tensor([[-13.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6424482336455366, distance: 0.6842678844145756 entropy 0.5003275275230408
epoch: 9, step: 62
	action: tensor([[ 0.5634, -0.4168, -0.5180,  0.0605, -0.1075, -0.5731, -0.5813]],
       dtype=torch.float64)
	q_value: tensor([[-19.3556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26484142703462443, distance: 0.9811766544511098 entropy 0.5003275275230408
epoch: 9, step: 63
	action: tensor([[ 0.2730,  0.2297, -0.1519, -0.1183,  0.5862,  0.0965,  0.6638]],
       dtype=torch.float64)
	q_value: tensor([[-16.1425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.66674293570637, distance: 0.6606118737708793 entropy 0.5003275275230408
epoch: 9, step: 64
	action: tensor([[ 0.2142, -0.1963, -0.0801, -0.1627, -0.3621, -0.0009,  0.0770]],
       dtype=torch.float64)
	q_value: tensor([[-15.3186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24206418164386623, distance: 0.9962604938127717 entropy 0.5003275275230408
epoch: 9, step: 65
	action: tensor([[ 1.0950e-01,  1.9128e-01, -2.9933e-05,  5.6711e-01, -2.3832e-01,
          1.1220e-03,  3.3041e-01]], dtype=torch.float64)
	q_value: tensor([[-13.2271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7040467720695325, distance: 0.6225413940255627 entropy 0.5003275275230408
epoch: 9, step: 66
	action: tensor([[ 0.5661,  0.5600,  0.0457, -0.5042,  0.2035, -0.6379, -0.9009]],
       dtype=torch.float64)
	q_value: tensor([[-14.1184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7567925337344138, distance: 0.5643456020211973 entropy 0.5003275275230408
epoch: 9, step: 67
	action: tensor([[ 0.2544,  0.0460, -0.2459, -0.0286,  0.0722,  0.5830,  0.1283]],
       dtype=torch.float64)
	q_value: tensor([[-17.2874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6649622087680283, distance: 0.6623744802026049 entropy 0.5003275275230408
epoch: 9, step: 68
	action: tensor([[-0.0025, -0.1898, -0.2348, -0.3951,  0.1892, -0.3609,  0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-13.0765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06085851016674915, distance: 1.1786515335051992 entropy 0.5003275275230408
epoch: 9, step: 69
	action: tensor([[-0.3117,  0.3210,  0.4628, -0.3646, -0.6672, -0.0633, -0.1511]],
       dtype=torch.float64)
	q_value: tensor([[-12.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05039561407189286, distance: 1.1728248063020774 entropy 0.5003275275230408
epoch: 9, step: 70
	action: tensor([[-0.3768,  0.3112, -0.4037, -0.2644, -0.1806, -0.7648,  0.3534]],
       dtype=torch.float64)
	q_value: tensor([[-16.1664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13376511487317644, distance: 1.0650611127616971 entropy 0.5003275275230408
epoch: 9, step: 71
	action: tensor([[0.2645, 0.2478, 0.3831, 0.6349, 0.7964, 0.0779, 0.1156]],
       dtype=torch.float64)
	q_value: tensor([[-13.8252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.915858816116452, distance: 0.33194107981893145 entropy 0.5003275275230408
epoch: 9, step: 72
	action: tensor([[-0.1279,  0.3881, -0.5068, -0.3720,  0.3455,  0.2139,  0.8781]],
       dtype=torch.float64)
	q_value: tensor([[-16.5113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42746020175068056, distance: 0.8658836388513856 entropy 0.5003275275230408
epoch: 9, step: 73
	action: tensor([[ 0.3750,  0.7125, -0.0211, -0.0010, -0.1063, -0.1735,  0.8139]],
       dtype=torch.float64)
	q_value: tensor([[-16.1413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8646658928016553, distance: 0.4209788955434658 entropy 0.5003275275230408
epoch: 9, step: 74
	action: tensor([[-0.2811,  0.0636, -0.1729,  0.0976, -0.7694,  0.0932, -0.4758]],
       dtype=torch.float64)
	q_value: tensor([[-15.9028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07146808209458377, distance: 1.1026942546530336 entropy 0.5003275275230408
epoch: 9, step: 75
	action: tensor([[-0.3350,  0.2169, -0.6400, -0.6047, -0.5201, -0.4689, -0.3445]],
       dtype=torch.float64)
	q_value: tensor([[-13.6220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26755682067827957, distance: 0.9793629330917543 entropy 0.5003275275230408
epoch: 9, step: 76
	action: tensor([[ 0.4735,  0.1724,  0.6460,  0.2590,  0.3362, -0.1719, -0.4188]],
       dtype=torch.float64)
	q_value: tensor([[-13.8476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.847833503436729, distance: 0.44639180604067047 entropy 0.5003275275230408
epoch: 9, step: 77
	action: tensor([[ 0.3230, -0.6498, -0.2760,  0.5366,  0.2600, -0.0030, -0.1226]],
       dtype=torch.float64)
	q_value: tensor([[-16.5967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26561265494430697, distance: 0.9806618611727353 entropy 0.5003275275230408
epoch: 9, step: 78
	action: tensor([[-0.1401,  0.3738,  0.3634, -0.7525,  0.6004, -0.4141,  0.2892]],
       dtype=torch.float64)
	q_value: tensor([[-15.8832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12694608626701864, distance: 1.2148097338056836 entropy 0.5003275275230408
epoch: 9, step: 79
	action: tensor([[-0.1186, -0.2615, -0.2786, -0.5017, -0.1506,  0.3557, -0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-15.5442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1789002593922533, distance: 1.2424966455059814 entropy 0.5003275275230408
epoch: 9, step: 80
	action: tensor([[-0.4663, -0.1739, -0.6946, -0.4448, -0.4404,  0.2379, -0.5540]],
       dtype=torch.float64)
	q_value: tensor([[-13.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3625176558709804, distance: 1.335757934535051 entropy 0.5003275275230408
epoch: 9, step: 81
	action: tensor([[ 0.1455, -0.4457, -0.2407,  0.7952, -0.0754,  0.0613,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-13.0547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.413573887330855, distance: 0.8763212509485676 entropy 0.5003275275230408
epoch: 9, step: 82
	action: tensor([[-0.1244,  0.1168, -0.7970,  0.0572, -0.0049,  0.3966,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[-15.4738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24187878368678617, distance: 0.9963823335266848 entropy 0.5003275275230408
epoch: 9, step: 83
	action: tensor([[-0.0786,  0.6083,  0.2074,  0.2408, -0.0400, -0.3628,  0.5030]],
       dtype=torch.float64)
	q_value: tensor([[-11.9766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5649872935762341, distance: 0.7547578949817613 entropy 0.5003275275230408
epoch: 9, step: 84
	action: tensor([[-0.2810, -0.4298,  0.2307, -0.4018,  0.2219,  0.5384,  0.2715]],
       dtype=torch.float64)
	q_value: tensor([[-14.3168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39724944765791603, distance: 1.3526756324598965 entropy 0.5003275275230408
epoch: 9, step: 85
	action: tensor([[-0.4560,  0.0669, -0.3547, -0.5238,  0.3700, -0.8825, -0.2444]],
       dtype=torch.float64)
	q_value: tensor([[-15.9030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3182086021888926, distance: 1.3138590109204138 entropy 0.5003275275230408
epoch: 9, step: 86
	action: tensor([[ 0.2104,  0.1491, -0.1159,  0.3176, -0.4103,  0.3677, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-14.6826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6664091911527417, distance: 0.6609425801033789 entropy 0.5003275275230408
epoch: 9, step: 87
	action: tensor([[-0.8521,  0.0414, -0.4935,  0.1253,  0.1253,  0.2146,  0.4396]],
       dtype=torch.float64)
	q_value: tensor([[-13.0458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6702558415821391, distance: 1.4789319556052924 entropy 0.5003275275230408
epoch: 9, step: 88
	action: tensor([[ 0.9211, -0.0113, -0.5054,  0.5133, -0.0870,  0.7270, -0.3492]],
       dtype=torch.float64)
	q_value: tensor([[-14.6593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9576630780228643, distance: 0.23545956287246678 entropy 0.5003275275230408
epoch: 9, step: 89
	action: tensor([[-0.1557,  0.3102, -0.0675, -0.0557, -0.4848, -0.6389, -0.2564]],
       dtype=torch.float64)
	q_value: tensor([[-17.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37420460669302746, distance: 0.9052590478600931 entropy 0.5003275275230408
epoch: 9, step: 90
	action: tensor([[ 0.4218,  0.3379, -0.3332, -0.3639, -1.0083, -0.0239,  0.2776]],
       dtype=torch.float64)
	q_value: tensor([[-13.6661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7602734107196036, distance: 0.5602924837639066 entropy 0.5003275275230408
epoch: 9, step: 91
	action: tensor([[ 0.1985, -0.1386, -0.0930, -0.4063,  0.5079, -0.7006, -0.2560]],
       dtype=torch.float64)
	q_value: tensor([[-17.4146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0021902981088633977, distance: 1.143090339484256 entropy 0.5003275275230408
epoch: 9, step: 92
	action: tensor([[-0.0684,  0.3716,  0.4367,  0.4117,  0.2961,  0.3020, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-15.0799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.745751405337178, distance: 0.5770134995152447 entropy 0.5003275275230408
epoch: 9, step: 93
	action: tensor([[ 0.1296, -0.7535, -0.6330, -0.6655,  0.3868, -0.3369,  0.6142]],
       dtype=torch.float64)
	q_value: tensor([[-14.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43147980052610424, distance: 1.36914456046251 entropy 0.5003275275230408
epoch: 9, step: 94
	action: tensor([[ 0.3813,  0.1483, -0.2801, -0.8123, -0.5697,  0.1688,  0.4592]],
       dtype=torch.float64)
	q_value: tensor([[-17.5130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42902250880344495, distance: 0.8647014501007185 entropy 0.5003275275230408
epoch: 9, step: 95
	action: tensor([[ 0.0896, -0.1622, -0.2391,  0.6875,  0.5712,  0.5292,  1.0793]],
       dtype=torch.float64)
	q_value: tensor([[-17.4762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6382918170238994, distance: 0.68823358245015 entropy 0.5003275275230408
epoch: 9, step: 96
	action: tensor([[ 0.5584,  0.0023,  0.1481,  0.3089, -0.0777,  0.0108, -0.2949]],
       dtype=torch.float64)
	q_value: tensor([[-19.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8249058163953578, distance: 0.4788423495370608 entropy 0.5003275275230408
epoch: 9, step: 97
	action: tensor([[ 0.8299,  0.3697, -0.3098, -0.0918,  0.2447,  0.3734, -0.4074]],
       dtype=torch.float64)
	q_value: tensor([[-14.4852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.962817778678518, distance: 0.22066037864489563 entropy 0.5003275275230408
epoch: 9, step: 98
	action: tensor([[ 0.0492,  0.2683,  0.0845, -0.3597, -0.1944, -0.1703,  0.0285]],
       dtype=torch.float64)
	q_value: tensor([[-15.0733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36470937454675023, distance: 0.9121009680652516 entropy 0.5003275275230408
epoch: 9, step: 99
	action: tensor([[ 0.3858, -0.0387, -0.1123,  0.4214, -0.2097,  0.3212, -0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-12.8760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7814227940253577, distance: 0.5350065860972496 entropy 0.5003275275230408
epoch: 9, step: 100
	action: tensor([[-0.4149,  0.1647,  0.0777,  0.2055,  0.3895, -0.3081, -0.4099]],
       dtype=torch.float64)
	q_value: tensor([[-13.2712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03868360546927696, distance: 1.1662679248776673 entropy 0.5003275275230408
epoch: 9, step: 101
	action: tensor([[-0.3923, -0.0710, -0.1700,  0.2303,  0.1001, -0.1413, -0.3590]],
       dtype=torch.float64)
	q_value: tensor([[-13.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12659764627728087, distance: 1.2146219160296245 entropy 0.5003275275230408
epoch: 9, step: 102
	action: tensor([[-0.3698, -0.4584, -0.2478,  0.1926,  0.1119, -0.2996,  0.2449]],
       dtype=torch.float64)
	q_value: tensor([[-12.3633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4501237003698697, distance: 1.3780317331201017 entropy 0.5003275275230408
epoch: 9, step: 103
	action: tensor([[ 0.4618, -0.0410, -0.4212, -0.6477,  0.3338, -0.4368,  0.1514]],
       dtype=torch.float64)
	q_value: tensor([[-14.4885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2512030193689291, distance: 0.9902360558400854 entropy 0.5003275275230408
epoch: 9, step: 104
	action: tensor([[ 0.0646, -0.5489,  0.3111, -0.2415, -0.5315,  0.5119, -0.2937]],
       dtype=torch.float64)
	q_value: tensor([[-14.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13045861941932535, distance: 1.216701456812942 entropy 0.5003275275230408
epoch: 9, step: 105
	action: tensor([[ 0.5277,  0.0010, -0.4100,  0.3466,  0.2444,  0.2146, -0.1622]],
       dtype=torch.float64)
	q_value: tensor([[-16.6872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7985989322639531, distance: 0.5135557280749836 entropy 0.5003275275230408
epoch: 9, step: 106
	action: tensor([[-0.3059, -0.4197,  0.5070, -0.0957, -0.0937,  0.2098,  0.5850]],
       dtype=torch.float64)
	q_value: tensor([[-13.3818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3743358702200694, distance: 1.3415384796203516 entropy 0.5003275275230408
epoch: 9, step: 107
	action: tensor([[ 0.2902,  0.3512,  0.0940, -0.2275, -0.2883,  0.2072,  0.2844]],
       dtype=torch.float64)
	q_value: tensor([[-17.6643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6857966193959517, distance: 0.6414489816328502 entropy 0.5003275275230408
epoch: 9, step: 108
	action: tensor([[-0.2306, -0.5379, -0.2283, -0.0662, -0.5858, -0.2480,  0.3987]],
       dtype=torch.float64)
	q_value: tensor([[-14.2264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38122053322257776, distance: 1.3448944650654773 entropy 0.5003275275230408
epoch: 9, step: 109
	action: tensor([[ 0.6415, -0.2620, -0.2747, -0.0585,  0.5194, -0.6744, -0.2691]],
       dtype=torch.float64)
	q_value: tensor([[-15.7720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2780982007286965, distance: 0.9722898574766328 entropy 0.5003275275230408
epoch: 9, step: 110
	action: tensor([[ 0.3525,  0.2620,  0.4704, -0.0478, -0.2578, -0.4463, -0.7976]],
       dtype=torch.float64)
	q_value: tensor([[-17.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.651989050650129, distance: 0.675076744505838 entropy 0.5003275275230408
epoch: 9, step: 111
	action: tensor([[ 0.2444, -0.1869, -0.6264,  0.2127,  0.7914, -0.2742,  0.2465]],
       dtype=torch.float64)
	q_value: tensor([[-16.6513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33053115513032516, distance: 0.9363147797319475 entropy 0.5003275275230408
epoch: 9, step: 112
	action: tensor([[-0.0940,  0.0570,  0.2174, -0.4297,  0.5349, -0.1543, -0.2273]],
       dtype=torch.float64)
	q_value: tensor([[-16.1598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04775532427267204, distance: 1.1713498640548632 entropy 0.5003275275230408
epoch: 9, step: 113
	action: tensor([[ 0.7814,  0.6617, -0.3481, -0.0683, -0.4188, -0.4402,  0.4508]],
       dtype=torch.float64)
	q_value: tensor([[-12.7955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9835926178801127, distance: 0.14658054753870967 entropy 0.5003275275230408
epoch: 9, step: 114
	action: tensor([[ 0.1317, -0.5912, -0.1731, -0.0740,  0.3699, -0.8892,  0.3383]],
       dtype=torch.float64)
	q_value: tensor([[-16.5593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3303180768087117, distance: 1.3198799738816287 entropy 0.5003275275230408
epoch: 9, step: 115
	action: tensor([[-0.3434,  0.2646, -0.7385,  0.1557,  0.2396,  0.0232, -0.0700]],
       dtype=torch.float64)
	q_value: tensor([[-18.6118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09550435864003548, distance: 1.088328325334905 entropy 0.5003275275230408
epoch: 9, step: 116
	action: tensor([[-0.0369,  0.2953,  0.1876, -0.0860,  0.4193, -0.1494,  0.2945]],
       dtype=torch.float64)
	q_value: tensor([[-11.7930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36453913162171536, distance: 0.9122231706676058 entropy 0.5003275275230408
epoch: 9, step: 117
	action: tensor([[ 0.1506,  0.7971,  0.4323,  0.3434, -0.1940, -0.2454, -0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-13.5900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 9, step: 118
	action: tensor([[ 0.3881, -0.3230, -0.2892,  0.4126,  0.1884,  0.0365,  0.0410]],
       dtype=torch.float64)
	q_value: tensor([[-19.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5266700064288461, distance: 0.7872971982818394 entropy 0.5003275275230408
epoch: 9, step: 119
	action: tensor([[-0.1306, -0.2621, -0.2836, -0.0120,  0.4428,  0.1059, -0.0709]],
       dtype=torch.float64)
	q_value: tensor([[-14.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010474831002253993, distance: 1.1503220470217976 entropy 0.5003275275230408
epoch: 9, step: 120
	action: tensor([[ 0.2681, -0.1649, -0.8572, -0.0684,  0.0669,  0.8835,  0.1940]],
       dtype=torch.float64)
	q_value: tensor([[-12.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49346523907226447, distance: 0.8144441728003283 entropy 0.5003275275230408
epoch: 9, step: 121
	action: tensor([[-0.8429,  0.3833,  0.4143, -0.5686,  0.2194, -0.1902, -0.4893]],
       dtype=torch.float64)
	q_value: tensor([[-15.7551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8514440853166136, distance: 1.5570838261399498 entropy 0.5003275275230408
epoch: 9, step: 122
	action: tensor([[ 0.2241, -0.2609,  0.2966, -0.4410,  0.0476, -0.0215,  0.5214]],
       dtype=torch.float64)
	q_value: tensor([[-14.4559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02890413601383801, distance: 1.1607645863276121 entropy 0.5003275275230408
epoch: 9, step: 123
	action: tensor([[ 0.6933,  0.0659,  0.3477,  0.0938,  0.2172,  0.2451, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-15.9221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8690122309538952, distance: 0.4141637325538693 entropy 0.5003275275230408
epoch: 9, step: 124
	action: tensor([[-0.2058, -1.0959,  0.2160,  0.4002,  0.8162, -0.4929,  0.6463]],
       dtype=torch.float64)
	q_value: tensor([[-15.5372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7091434226440714, distance: 1.4960494447489225 entropy 0.5003275275230408
epoch: 9, step: 125
	action: tensor([[ 0.5744,  0.2888, -0.2319, -0.3941,  0.1090,  0.3859, -0.3420]],
       dtype=torch.float64)
	q_value: tensor([[-23.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8124523497490146, distance: 0.4955785571664387 entropy 0.5003275275230408
epoch: 9, step: 126
	action: tensor([[ 0.3507,  0.3113,  0.0191, -0.0544,  0.2264, -0.5996,  0.0781]],
       dtype=torch.float64)
	q_value: tensor([[-13.8131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.657364731557918, distance: 0.6698425404787134 entropy 0.5003275275230408
epoch: 9, step: 127
	action: tensor([[ 0.1311, -0.1767,  0.0734, -0.4036,  0.1110,  0.3849, -0.3527]],
       dtype=torch.float64)
	q_value: tensor([[-14.4343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18729787089246974, distance: 1.0316262682518773 entropy 0.5003275275230408
LOSS epoch 9 actor 136.89501750391187 critic 218.48385171803426 
epoch: 10, step: 0
	action: tensor([[-0.0520, -0.3609, -0.5390,  0.3860,  0.7647,  0.1816,  0.4673]],
       dtype=torch.float64)
	q_value: tensor([[-13.8343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09104628696117201, distance: 1.091007101061 entropy 0.3949669301509857
epoch: 10, step: 1
	action: tensor([[-0.1077,  0.0968, -0.4525, -0.4733, -0.2079,  0.2341, -0.4963]],
       dtype=torch.float64)
	q_value: tensor([[-17.7368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21335069136439622, distance: 1.014956141722608 entropy 0.3949669301509857
epoch: 10, step: 2
	action: tensor([[-0.2749, -0.8105,  0.0200,  0.1983,  0.2761, -0.4499,  0.4014]],
       dtype=torch.float64)
	q_value: tensor([[-13.0323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6729943017049604, distance: 1.480143847042942 entropy 0.3949669301509857
epoch: 10, step: 3
	action: tensor([[ 0.3856,  0.5248,  0.3310,  0.3060,  0.1073, -0.1304,  0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-19.3935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9110756239182343, distance: 0.3412456363960667 entropy 0.3949669301509857
epoch: 10, step: 4
	action: tensor([[ 0.4044,  0.5545,  0.2425, -0.4608, -0.4958, -0.1808, -0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-15.6074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7531803326414774, distance: 0.5685210829159234 entropy 0.3949669301509857
epoch: 10, step: 5
	action: tensor([[ 0.5475, -0.1319,  0.4738,  0.5233, -0.1006,  0.3313, -0.2135]],
       dtype=torch.float64)
	q_value: tensor([[-17.3335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081550900395565, distance: 0.3468041131524131 entropy 0.3949669301509857
epoch: 10, step: 6
	action: tensor([[ 0.6066, -0.3217, -0.5629, -0.0264, -0.4645,  0.6046,  0.2029]],
       dtype=torch.float64)
	q_value: tensor([[-17.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5031268176499921, distance: 0.8066394757195365 entropy 0.3949669301509857
epoch: 10, step: 7
	action: tensor([[-0.3873, -0.0121, -0.1949, -0.2883, -0.1461, -0.1470, -0.0287]],
       dtype=torch.float64)
	q_value: tensor([[-17.0336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22139088467777435, distance: 1.2646898426008737 entropy 0.3949669301509857
epoch: 10, step: 8
	action: tensor([[-0.6514,  0.6837,  0.0090,  0.5478, -0.1011, -0.2416, -0.2457]],
       dtype=torch.float64)
	q_value: tensor([[-12.5411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 10, step: 9
	action: tensor([[ 0.0537,  0.0444, -0.6911, -0.0751, -0.3715, -0.2055,  0.0607]],
       dtype=torch.float64)
	q_value: tensor([[-20.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43426026143377694, distance: 0.8607262260595852 entropy 0.3949669301509857
epoch: 10, step: 10
	action: tensor([[ 0.6272,  0.3667, -0.5902, -0.2702, -0.0939,  0.1270,  0.1217]],
       dtype=torch.float64)
	q_value: tensor([[-13.5860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8875295406142923, distance: 0.3837743349618647 entropy 0.3949669301509857
epoch: 10, step: 11
	action: tensor([[ 0.4847, -0.1816, -0.2203,  0.0440,  0.2131, -0.1708, -0.1815]],
       dtype=torch.float64)
	q_value: tensor([[-15.3229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5003614717899894, distance: 0.8088810357089796 entropy 0.3949669301509857
epoch: 10, step: 12
	action: tensor([[-0.5038, -0.6006, -0.0155, -0.3216,  0.1500, -0.3970,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[-14.3917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8473142218538177, distance: 1.555346227451744 entropy 0.3949669301509857
epoch: 10, step: 13
	action: tensor([[ 0.3239, -0.1420, -0.0405,  0.9825,  0.0963, -0.0286,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-16.5485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8224158387815825, distance: 0.4822350885059275 entropy 0.3949669301509857
epoch: 10, step: 14
	action: tensor([[ 0.3120, -0.3930, -0.1539,  0.1544,  0.1621, -0.3257, -0.2361]],
       dtype=torch.float64)
	q_value: tensor([[-17.7394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19856925324114127, distance: 1.0244474677708995 entropy 0.3949669301509857
epoch: 10, step: 15
	action: tensor([[ 0.5619,  0.0915,  0.1526, -0.0417, -0.4041,  0.2872, -0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-15.1627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7764030459345066, distance: 0.5411150770586256 entropy 0.3949669301509857
epoch: 10, step: 16
	action: tensor([[ 0.6800, -0.2458, -0.4734, -0.3482, -0.2917, -0.1704,  0.2675]],
       dtype=torch.float64)
	q_value: tensor([[-16.1138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29567354007765445, distance: 0.960381300463222 entropy 0.3949669301509857
epoch: 10, step: 17
	action: tensor([[-0.5690,  0.0015,  0.0600,  0.4237, -0.4892, -0.5220,  0.0638]],
       dtype=torch.float64)
	q_value: tensor([[-16.7249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13070826885939835, distance: 1.2168357970108756 entropy 0.3949669301509857
epoch: 10, step: 18
	action: tensor([[ 0.5176, -0.1718, -0.3451,  0.0539, -0.3487,  0.0044, -0.6754]],
       dtype=torch.float64)
	q_value: tensor([[-16.0347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5638023606597032, distance: 0.7557851400095945 entropy 0.3949669301509857
epoch: 10, step: 19
	action: tensor([[ 0.0247,  0.3049, -0.5843, -0.2947,  1.0036,  0.0897, -0.5017]],
       dtype=torch.float64)
	q_value: tensor([[-15.1066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5629354506684202, distance: 0.7565358002770255 entropy 0.3949669301509857
epoch: 10, step: 20
	action: tensor([[-0.1534, -0.0642, -0.4401, -0.3629,  0.4212, -0.2116,  0.3584]],
       dtype=torch.float64)
	q_value: tensor([[-15.3812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0035395300039586797, distance: 1.1463676854934084 entropy 0.3949669301509857
epoch: 10, step: 21
	action: tensor([[-0.0788, -0.2565,  0.2927, -0.1651, -1.0656,  0.3705, -0.3258]],
       dtype=torch.float64)
	q_value: tensor([[-14.1317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06800612751634505, distance: 1.1826154963741187 entropy 0.3949669301509857
epoch: 10, step: 22
	action: tensor([[-0.2369, -0.3988,  0.0546, -0.1694, -0.3199,  0.2352, -0.1879]],
       dtype=torch.float64)
	q_value: tensor([[-18.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30611248124575363, distance: 1.3078170142098697 entropy 0.3949669301509857
epoch: 10, step: 23
	action: tensor([[ 0.1721, -0.2262,  0.2958, -0.5871,  0.2223, -0.0840, -0.3395]],
       dtype=torch.float64)
	q_value: tensor([[-14.2731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13938865699478087, distance: 1.221497659227244 entropy 0.3949669301509857
epoch: 10, step: 24
	action: tensor([[-0.1987, -0.6674, -0.2461, -0.0513, -0.0504,  0.0703,  0.4488]],
       dtype=torch.float64)
	q_value: tensor([[-14.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4673820874386745, distance: 1.3862076776634695 entropy 0.3949669301509857
epoch: 10, step: 25
	action: tensor([[-0.0104,  0.1148,  0.1503, -0.5651,  0.0443, -0.4413, -0.2541]],
       dtype=torch.float64)
	q_value: tensor([[-16.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004627843108911511, distance: 1.1416932605127224 entropy 0.3949669301509857
epoch: 10, step: 26
	action: tensor([[-0.1164, -0.4059, -0.2676,  0.1490, -0.2052,  0.5071, -0.7077]],
       dtype=torch.float64)
	q_value: tensor([[-14.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0352244959218595, distance: 1.1643243046579983 entropy 0.3949669301509857
epoch: 10, step: 27
	action: tensor([[-0.3917, -0.0352, -0.2357, -0.2526,  0.4188, -0.8894,  0.3053]],
       dtype=torch.float64)
	q_value: tensor([[-14.5735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33339660393447, distance: 1.3214062773827955 entropy 0.3949669301509857
epoch: 10, step: 28
	action: tensor([[-0.1350, -0.0093, -0.1776, -0.0297,  0.5425,  0.1005,  0.1466]],
       dtype=torch.float64)
	q_value: tensor([[-17.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17840655254240578, distance: 1.0372541395859187 entropy 0.3949669301509857
epoch: 10, step: 29
	action: tensor([[ 0.3517,  0.1655, -0.3537, -0.7147, -0.2733,  0.2828, -0.3040]],
       dtype=torch.float64)
	q_value: tensor([[-13.6121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4994474770085935, distance: 0.8096205455786342 entropy 0.3949669301509857
epoch: 10, step: 30
	action: tensor([[-0.3268, -0.3024, -1.0235, -0.1126,  0.6371, -0.4644,  0.6975]],
       dtype=torch.float64)
	q_value: tensor([[-15.3767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25452874627048017, distance: 1.2817313351383524 entropy 0.3949669301509857
epoch: 10, step: 31
	action: tensor([[ 0.5605, -0.3691, -0.3199, -0.0595, -0.4830,  0.6114,  0.0251]],
       dtype=torch.float64)
	q_value: tensor([[-18.9994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4431042857019123, distance: 0.8539719996642103 entropy 0.3949669301509857
epoch: 10, step: 32
	action: tensor([[-0.2504, -0.0179, -0.1442,  0.2274, -0.2524,  0.4666,  0.1522]],
       dtype=torch.float64)
	q_value: tensor([[-16.8899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18138492905531867, distance: 1.035372346184455 entropy 0.3949669301509857
epoch: 10, step: 33
	action: tensor([[ 0.0632, -0.3636, -0.2428, -0.3980, -0.2035,  0.1097,  0.0588]],
       dtype=torch.float64)
	q_value: tensor([[-14.0321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08594152262219623, distance: 1.1925041880469351 entropy 0.3949669301509857
epoch: 10, step: 34
	action: tensor([[ 0.0821,  0.6401, -0.5419, -0.3043,  0.2725,  0.3526, -0.2409]],
       dtype=torch.float64)
	q_value: tensor([[-14.2557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.714696864066225, distance: 0.6112374650127611 entropy 0.3949669301509857
epoch: 10, step: 35
	action: tensor([[ 0.0963,  1.2259, -0.3825,  0.0384,  0.1830, -0.2707, -0.2715]],
       dtype=torch.float64)
	q_value: tensor([[-13.5030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 10, step: 36
	action: tensor([[ 0.3737, -0.5204, -0.3783, -0.0808,  0.4769,  0.0201, -0.4351]],
       dtype=torch.float64)
	q_value: tensor([[-20.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09008791951843986, distance: 1.0915821084617376 entropy 0.3949669301509857
epoch: 10, step: 37
	action: tensor([[-0.1801, -0.0913, -0.4786,  0.3890,  0.3404,  0.4104, -0.1063]],
       dtype=torch.float64)
	q_value: tensor([[-15.2388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21894553507836434, distance: 1.0113403921421626 entropy 0.3949669301509857
epoch: 10, step: 38
	action: tensor([[ 0.0949, -0.0960, -0.3523, -0.0888, -0.0316, -0.0222, -0.2012]],
       dtype=torch.float64)
	q_value: tensor([[-13.8788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30892973532264945, distance: 0.9513006567244159 entropy 0.3949669301509857
epoch: 10, step: 39
	action: tensor([[ 0.1976, -0.1661,  0.0314, -0.2832, -0.0836,  0.4235,  0.4939]],
       dtype=torch.float64)
	q_value: tensor([[-12.1639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30593050932481447, distance: 0.9533627311915748 entropy 0.3949669301509857
epoch: 10, step: 40
	action: tensor([[ 0.6542,  0.0340, -0.9200,  0.2979,  0.4862, -0.8122,  0.3977]],
       dtype=torch.float64)
	q_value: tensor([[-16.4081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.669602152075703, distance: 0.6577718722802786 entropy 0.3949669301509857
epoch: 10, step: 41
	action: tensor([[ 0.3261,  0.0188,  0.0455, -0.1203,  0.0678,  0.3349,  0.0529]],
       dtype=torch.float64)
	q_value: tensor([[-21.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6204843891629248, distance: 0.704971427866608 entropy 0.3949669301509857
epoch: 10, step: 42
	action: tensor([[-0.5599, -0.2381,  0.1983, -0.3664,  0.3378,  0.0813,  0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-13.9487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7089387251124499, distance: 1.4959598540234629 entropy 0.3949669301509857
epoch: 10, step: 43
	action: tensor([[ 0.3638,  0.6711, -0.7246, -0.0403, -0.3055, -0.0384,  0.5023]],
       dtype=torch.float64)
	q_value: tensor([[-15.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8672995433378208, distance: 0.41686257029101936 entropy 0.3949669301509857
epoch: 10, step: 44
	action: tensor([[-0.2871,  0.2364, -0.0822,  0.4494, -0.3012,  0.1815, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[-16.4445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3352136995185494, distance: 0.9330345457464261 entropy 0.3949669301509857
epoch: 10, step: 45
	action: tensor([[ 0.2969, -0.0740, -0.2284, -0.0408, -0.2324, -0.4300, -0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-13.5646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4490757249636418, distance: 0.8493812068477938 entropy 0.3949669301509857
epoch: 10, step: 46
	action: tensor([[-0.2638,  0.0506, -0.1778,  0.0497,  0.2258, -0.4462,  0.7941]],
       dtype=torch.float64)
	q_value: tensor([[-14.1662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024379403415544898, distance: 1.1582094715440856 entropy 0.3949669301509857
epoch: 10, step: 47
	action: tensor([[ 0.2480, -0.1002, -0.5693,  0.6080, -0.2252, -0.6040,  0.1424]],
       dtype=torch.float64)
	q_value: tensor([[-16.6662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.528754060110662, distance: 0.785562066666075 entropy 0.3949669301509857
epoch: 10, step: 48
	action: tensor([[-0.5298, -0.2357, -0.7367, -0.2123,  0.1337, -0.0478,  0.5215]],
       dtype=torch.float64)
	q_value: tensor([[-17.1157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4347221145295628, distance: 1.3706942455218831 entropy 0.3949669301509857
epoch: 10, step: 49
	action: tensor([[-0.1377,  0.2258, -0.7451, -0.2678,  0.2861, -0.2736, -0.1536]],
       dtype=torch.float64)
	q_value: tensor([[-15.6331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3200423076857537, distance: 0.9436210901683454 entropy 0.3949669301509857
epoch: 10, step: 50
	action: tensor([[ 0.2115, -1.3903, -0.6770, -0.3570,  0.1575, -0.0106,  0.2244]],
       dtype=torch.float64)
	q_value: tensor([[-12.8875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7569179005761095, distance: 1.5168143444945212 entropy 0.3949669301509857
epoch: 10, step: 51
	action: tensor([[ 0.3759,  0.6852,  0.2456, -0.5865,  0.6206, -0.1206, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-18.8839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7281477023716386, distance: 0.5966548802494573 entropy 0.3949669301509857
epoch: 10, step: 52
	action: tensor([[ 0.1991,  0.3222,  0.0191,  0.6281,  0.5274,  0.1633, -0.0373]],
       dtype=torch.float64)
	q_value: tensor([[-16.1927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8276148792372122, distance: 0.47512357710372144 entropy 0.3949669301509857
epoch: 10, step: 53
	action: tensor([[ 0.2849, -0.0279,  0.2428, -0.3150,  0.2731, -0.7693, -0.1670]],
       dtype=torch.float64)
	q_value: tensor([[-15.5617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14011655959456404, distance: 1.0611492854778375 entropy 0.3949669301509857
epoch: 10, step: 54
	action: tensor([[ 0.6126,  0.0060,  0.2389,  0.4327, -0.0402,  0.4303, -0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-16.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9491781898754545, distance: 0.25797745730397903 entropy 0.3949669301509857
epoch: 10, step: 55
	action: tensor([[-0.0185, -1.2008,  0.0311, -0.2254, -0.2264, -0.2974,  0.1406]],
       dtype=torch.float64)
	q_value: tensor([[-16.8979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.917115477683067, distance: 1.58445835888757 entropy 0.3949669301509857
epoch: 10, step: 56
	action: tensor([[ 0.1159,  0.4533,  0.2554, -0.3280, -0.1296,  0.0800, -0.2990]],
       dtype=torch.float64)
	q_value: tensor([[-20.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5521328795824265, distance: 0.7658280751794005 entropy 0.3949669301509857
epoch: 10, step: 57
	action: tensor([[ 0.8041, -0.3998,  0.1487, -0.1026, -0.1416,  0.1664,  0.8843]],
       dtype=torch.float64)
	q_value: tensor([[-14.7071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3814430470016976, distance: 0.9000083522271315 entropy 0.3949669301509857
epoch: 10, step: 58
	action: tensor([[ 0.2065, -0.0463, -0.2251,  0.1486, -0.6934, -0.2755, -0.6745]],
       dtype=torch.float64)
	q_value: tensor([[-20.7155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5091208968147953, distance: 0.8017592248509212 entropy 0.3949669301509857
epoch: 10, step: 59
	action: tensor([[-0.1176, -0.1003,  0.0863, -0.7363,  0.2903,  0.2129,  0.1731]],
       dtype=torch.float64)
	q_value: tensor([[-15.5901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20203164352535197, distance: 1.2546270407923517 entropy 0.3949669301509857
epoch: 10, step: 60
	action: tensor([[-0.3544, -0.1215, -0.5400,  0.1700, -0.2850, -0.3681, -0.2761]],
       dtype=torch.float64)
	q_value: tensor([[-14.9476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13029974632551666, distance: 1.2166159570372712 entropy 0.3949669301509857
epoch: 10, step: 61
	action: tensor([[-0.1543,  0.3836, -0.0663, -0.5132, -0.5796,  0.0663, -0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-13.8212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23683836971452166, distance: 0.999689099910459 entropy 0.3949669301509857
epoch: 10, step: 62
	action: tensor([[ 0.0650, -0.5781, -0.4356, -0.5301, -0.1505,  0.6573,  0.3054]],
       dtype=torch.float64)
	q_value: tensor([[-15.6920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19871055366559442, distance: 1.252892639202654 entropy 0.3949669301509857
epoch: 10, step: 63
	action: tensor([[ 0.4271,  0.1064, -0.3279, -0.1688,  0.1797, -0.3613,  0.3407]],
       dtype=torch.float64)
	q_value: tensor([[-17.3345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.574278720190204, distance: 0.7466539600200653 entropy 0.3949669301509857
epoch: 10, step: 64
	action: tensor([[-0.1620,  0.5576,  0.0301, -0.6628, -0.1617,  0.2848,  0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-15.2119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29872096956090644, distance: 0.9583013935754475 entropy 0.3949669301509857
epoch: 10, step: 65
	action: tensor([[-0.3940,  0.2363, -0.9002, -0.2441,  0.5279,  0.0118, -0.2271]],
       dtype=torch.float64)
	q_value: tensor([[-15.8102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11197182728533983, distance: 1.0783756312510686 entropy 0.3949669301509857
epoch: 10, step: 66
	action: tensor([[-0.1127, -0.1735, -0.4079, -0.3863, -0.4769,  0.0709,  0.3183]],
       dtype=torch.float64)
	q_value: tensor([[-13.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022959845999969586, distance: 1.1574066855662681 entropy 0.3949669301509857
epoch: 10, step: 67
	action: tensor([[ 0.3352, -0.2414,  0.0849,  0.1864,  0.1301,  0.1112, -0.0311]],
       dtype=torch.float64)
	q_value: tensor([[-14.8500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5159050036620219, distance: 0.7961996643101569 entropy 0.3949669301509857
epoch: 10, step: 68
	action: tensor([[ 0.0352,  0.2261, -0.2701,  0.0444, -0.5190,  0.5908,  0.7308]],
       dtype=torch.float64)
	q_value: tensor([[-14.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4779770749849044, distance: 0.826801928916608 entropy 0.3949669301509857
epoch: 10, step: 69
	action: tensor([[-0.8194, -0.0462,  0.2861, -0.4095,  0.0179,  0.2899,  0.1243]],
       dtype=torch.float64)
	q_value: tensor([[-17.5112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8962355953001622, distance: 1.5758063296987845 entropy 0.3949669301509857
epoch: 10, step: 70
	action: tensor([[ 0.3259, -0.1477, -0.2161, -0.4674, -0.5373,  0.5049, -0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-16.5496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25909551884064363, distance: 0.9850035688309371 entropy 0.3949669301509857
epoch: 10, step: 71
	action: tensor([[ 0.3231,  0.1531, -0.0342, -0.5126, -0.2946,  0.3116,  0.4706]],
       dtype=torch.float64)
	q_value: tensor([[-16.4542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4928172898894708, distance: 0.8149649167087557 entropy 0.3949669301509857
epoch: 10, step: 72
	action: tensor([[-0.2415,  0.2106,  0.1087,  0.0137,  0.3478,  0.3758, -0.2630]],
       dtype=torch.float64)
	q_value: tensor([[-17.0121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3096803825546697, distance: 0.9507838603435806 entropy 0.3949669301509857
epoch: 10, step: 73
	action: tensor([[-0.2602,  0.2557,  0.3698, -0.5833, -0.2495, -0.0104,  0.2970]],
       dtype=torch.float64)
	q_value: tensor([[-13.1513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17013250569400418, distance: 1.2378676553661438 entropy 0.3949669301509857
epoch: 10, step: 74
	action: tensor([[-0.8115,  0.5803, -0.4886, -0.0464,  0.8221,  0.1260, -0.2300]],
       dtype=torch.float64)
	q_value: tensor([[-16.0930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19938309855088154, distance: 1.2532440619779477 entropy 0.3949669301509857
epoch: 10, step: 75
	action: tensor([[-0.2576, -0.1980,  0.4664,  0.2112, -0.1982,  0.3250,  0.2905]],
       dtype=torch.float64)
	q_value: tensor([[-14.8568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12095136294394626, distance: 1.072909635837381 entropy 0.3949669301509857
epoch: 10, step: 76
	action: tensor([[ 0.5498,  0.1157, -0.0214, -0.5304,  0.3011, -0.3204, -0.4117]],
       dtype=torch.float64)
	q_value: tensor([[-16.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42730926681453785, distance: 0.8659977649609554 entropy 0.3949669301509857
epoch: 10, step: 77
	action: tensor([[ 0.1616, -0.3487, -0.3903,  0.3929,  0.1862,  0.1501, -0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-15.6465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3175603986972285, distance: 0.9453416740153128 entropy 0.3949669301509857
epoch: 10, step: 78
	action: tensor([[ 0.2433, -0.1225, -0.0878, -0.4006,  0.3913, -0.3999,  0.1809]],
       dtype=torch.float64)
	q_value: tensor([[-14.5471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0949208022781608, distance: 1.088679348869197 entropy 0.3949669301509857
epoch: 10, step: 79
	action: tensor([[-0.3072, -0.1800,  0.2331,  0.5589, -0.3304,  0.4353,  0.5334]],
       dtype=torch.float64)
	q_value: tensor([[-15.1754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008416893505169, distance: 0.9568513092174322 entropy 0.3949669301509857
epoch: 10, step: 80
	action: tensor([[ 0.2998, -0.2317, -0.6871, -0.4198,  0.5323,  0.3138, -0.2708]],
       dtype=torch.float64)
	q_value: tensor([[-17.9917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.328690111513832, distance: 0.937601331656985 entropy 0.3949669301509857
epoch: 10, step: 81
	action: tensor([[-0.5601, -0.5678, -0.2340, -0.1345,  0.0112, -0.0316,  0.2638]],
       dtype=torch.float64)
	q_value: tensor([[-14.4811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7902559090466901, distance: 1.531137704537533 entropy 0.3949669301509857
epoch: 10, step: 82
	action: tensor([[-0.0310, -0.0367, -0.1505,  0.3256,  0.0202, -0.6161,  0.3280]],
       dtype=torch.float64)
	q_value: tensor([[-15.9063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25347458374390663, distance: 0.9887329157754552 entropy 0.3949669301509857
epoch: 10, step: 83
	action: tensor([[-0.4042,  0.2245, -0.3742,  0.3348, -0.0950,  0.0946,  0.1730]],
       dtype=torch.float64)
	q_value: tensor([[-16.0643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06804812874357591, distance: 1.1047231008066827 entropy 0.3949669301509857
epoch: 10, step: 84
	action: tensor([[-0.3139,  0.0509, -0.2001,  0.7107, -0.0626, -0.4885, -0.6314]],
       dtype=torch.float64)
	q_value: tensor([[-13.1123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18471547143522715, distance: 1.033263988932394 entropy 0.3949669301509857
epoch: 10, step: 85
	action: tensor([[ 0.2243,  0.1848, -0.7701, -0.7281, -0.1496,  0.2998, -0.2638]],
       dtype=torch.float64)
	q_value: tensor([[-16.3649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6137879506577293, distance: 0.7111637360807398 entropy 0.3949669301509857
epoch: 10, step: 86
	action: tensor([[-0.1821, -0.2985,  0.1795, -0.4539,  0.1355, -0.4537, -0.5246]],
       dtype=torch.float64)
	q_value: tensor([[-14.9950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47900761013613713, distance: 1.3916880483583534 entropy 0.3949669301509857
epoch: 10, step: 87
	action: tensor([[ 0.1150, -0.0120, -0.8264,  0.0662, -0.2046, -0.1708,  0.3289]],
       dtype=torch.float64)
	q_value: tensor([[-14.9631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38017564212475063, distance: 0.9009299256286695 entropy 0.3949669301509857
epoch: 10, step: 88
	action: tensor([[ 0.1282,  0.4996, -0.5898, -0.3074,  0.5508,  0.2110, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-14.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7106645746236231, distance: 0.6155417268671056 entropy 0.3949669301509857
epoch: 10, step: 89
	action: tensor([[ 0.1182,  0.1112,  0.4799,  0.0252,  0.0124, -0.4201,  0.2160]],
       dtype=torch.float64)
	q_value: tensor([[-13.7531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37653130342625696, distance: 0.9035746117786555 entropy 0.3949669301509857
epoch: 10, step: 90
	action: tensor([[ 0.9201,  0.0648, -0.4612,  0.2437,  0.6176,  0.2895,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-16.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.926214850368054, distance: 0.31084305917734645 entropy 0.3949669301509857
epoch: 10, step: 91
	action: tensor([[-0.2757, -0.4878,  0.4015,  0.3514,  0.3632, -0.1287, -0.0746]],
       dtype=torch.float64)
	q_value: tensor([[-18.1700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18140515736159646, distance: 1.2438159580621488 entropy 0.3949669301509857
epoch: 10, step: 92
	action: tensor([[ 0.2707, -0.3071, -0.6795, -0.4093,  0.1007, -0.2741,  0.7102]],
       dtype=torch.float64)
	q_value: tensor([[-17.1700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.126595763118441, distance: 1.0694594945426512 entropy 0.3949669301509857
epoch: 10, step: 93
	action: tensor([[ 0.2238, -0.0167, -0.2984, -0.5941, -0.0831,  0.7906, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[-17.1109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4433159670605371, distance: 0.8538096827978428 entropy 0.3949669301509857
epoch: 10, step: 94
	action: tensor([[-0.1857, -0.2056, -0.1880, -0.4833,  0.0663,  0.2534, -0.2674]],
       dtype=torch.float64)
	q_value: tensor([[-16.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15800746127936272, distance: 1.231437492651671 entropy 0.3949669301509857
epoch: 10, step: 95
	action: tensor([[ 0.1568,  0.2985,  0.0520,  0.0848, -0.3785, -0.1473,  0.5690]],
       dtype=torch.float64)
	q_value: tensor([[-12.8899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6414561126454795, distance: 0.685216566988233 entropy 0.3949669301509857
epoch: 10, step: 96
	action: tensor([[-5.2286e-02, -4.0145e-02, -1.7596e-01,  1.3473e-01, -4.0843e-01,
         -2.2138e-04,  3.4285e-02]], dtype=torch.float64)
	q_value: tensor([[-15.4095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28507161884503496, distance: 0.9675824048414902 entropy 0.3949669301509857
epoch: 10, step: 97
	action: tensor([[ 0.3108,  0.2931, -0.2659, -0.3491,  0.2986, -0.4670, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[-13.0195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5730062660912711, distance: 0.747768978297808 entropy 0.3949669301509857
epoch: 10, step: 98
	action: tensor([[ 0.0237, -0.1456,  0.0750,  0.1213, -0.3609,  0.3305, -0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-14.4818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3595499439465102, distance: 0.9157972335428997 entropy 0.3949669301509857
epoch: 10, step: 99
	action: tensor([[ 0.3144,  0.6946, -0.0793, -0.0258,  0.1054, -0.0476,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-13.9788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8354294124269356, distance: 0.46422957310305546 entropy 0.3949669301509857
epoch: 10, step: 100
	action: tensor([[-0.0438,  0.0306,  0.4546,  0.2478, -0.0394, -0.6763,  0.4196]],
       dtype=torch.float64)
	q_value: tensor([[-13.9908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24659258121474514, distance: 0.9932798823324743 entropy 0.3949669301509857
epoch: 10, step: 101
	action: tensor([[-0.0241,  0.0836, -0.1000, -0.4008,  0.1402,  0.0429, -0.4413]],
       dtype=torch.float64)
	q_value: tensor([[-17.6999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1511797129809227, distance: 1.0543008791237616 entropy 0.3949669301509857
epoch: 10, step: 102
	action: tensor([[ 0.1104,  0.0404, -0.2588,  0.2723,  0.1735,  0.1275,  0.1351]],
       dtype=torch.float64)
	q_value: tensor([[-12.4203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5090661845405366, distance: 0.8018039047406127 entropy 0.3949669301509857
epoch: 10, step: 103
	action: tensor([[ 0.1534, -0.3843, -0.0727, -0.1218, -0.0957,  0.1155, -0.1931]],
       dtype=torch.float64)
	q_value: tensor([[-13.1855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04147344248918872, distance: 1.1203630272507805 entropy 0.3949669301509857
epoch: 10, step: 104
	action: tensor([[ 0.5088,  0.1161, -0.1725, -0.8386,  0.0100,  0.6638,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-13.6028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5465846752398521, distance: 0.7705570363118033 entropy 0.3949669301509857
epoch: 10, step: 105
	action: tensor([[ 0.4035,  0.0930,  0.1983,  0.0634,  0.0016, -0.9610, -0.2859]],
       dtype=torch.float64)
	q_value: tensor([[-17.6507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5293004421375564, distance: 0.7851065280151779 entropy 0.3949669301509857
epoch: 10, step: 106
	action: tensor([[ 0.0970, -0.1066,  0.5360, -0.4120,  0.0413, -0.2034, -0.0637]],
       dtype=torch.float64)
	q_value: tensor([[-18.2763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049787056009343855, distance: 1.1724850127093993 entropy 0.3949669301509857
epoch: 10, step: 107
	action: tensor([[ 0.6391, -0.3461, -0.4336, -0.2882,  0.1234,  0.2646, -0.4735]],
       dtype=torch.float64)
	q_value: tensor([[-15.6883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045236141968094, distance: 0.9543284863396975 entropy 0.3949669301509857
epoch: 10, step: 108
	action: tensor([[-0.0539, -0.3265, -0.0058, -0.3647,  0.5204, -0.6264,  0.1465]],
       dtype=torch.float64)
	q_value: tensor([[-15.1340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3685298361515983, distance: 1.338701741490739 entropy 0.3949669301509857
epoch: 10, step: 109
	action: tensor([[ 0.7854, -0.1013, -0.6025,  0.1019,  0.2463, -0.0653, -0.5828]],
       dtype=torch.float64)
	q_value: tensor([[-16.6949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6728965355466817, distance: 0.6544843485484457 entropy 0.3949669301509857
epoch: 10, step: 110
	action: tensor([[-0.1335,  0.1929,  0.0468, -0.1165,  0.1355,  0.2635, -0.4479]],
       dtype=torch.float64)
	q_value: tensor([[-16.5846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3316253014519419, distance: 0.9355493338817846 entropy 0.3949669301509857
epoch: 10, step: 111
	action: tensor([[-0.3183,  0.7135,  0.0271, -0.2789, -0.4535,  0.5049,  0.3325]],
       dtype=torch.float64)
	q_value: tensor([[-12.7356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297371817789312, distance: 0.9592227620553677 entropy 0.3949669301509857
epoch: 10, step: 112
	action: tensor([[-0.0692, -0.0800, -0.5660, -0.0123, -0.2143,  0.3466,  0.6941]],
       dtype=torch.float64)
	q_value: tensor([[-17.3135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1687023129916282, distance: 1.0433619134632903 entropy 0.3949669301509857
epoch: 10, step: 113
	action: tensor([[ 0.4648, -0.4501,  0.4582, -0.6279,  0.0137, -0.4300,  0.9148]],
       dtype=torch.float64)
	q_value: tensor([[-15.8704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2953697243375424, distance: 1.3024275208672156 entropy 0.3949669301509857
epoch: 10, step: 114
	action: tensor([[-0.3797, -0.7750, -0.6244, -0.3326,  0.1091,  0.2198,  0.1820]],
       dtype=torch.float64)
	q_value: tensor([[-21.9508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7059694352136532, distance: 1.4946596700060077 entropy 0.3949669301509857
epoch: 10, step: 115
	action: tensor([[ 0.0199,  0.1362, -0.5492, -0.3380,  0.1358, -0.9019, -0.2843]],
       dtype=torch.float64)
	q_value: tensor([[-16.3514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31744784788857117, distance: 0.9454196256634039 entropy 0.3949669301509857
epoch: 10, step: 116
	action: tensor([[-0.6344,  0.2925, -0.2802,  0.3624, -0.5181, -0.1464, -0.6053]],
       dtype=torch.float64)
	q_value: tensor([[-15.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17741889297814506, distance: 1.2417157603734432 entropy 0.3949669301509857
epoch: 10, step: 117
	action: tensor([[ 0.0716, -0.3053,  0.5071, -0.4167,  0.0722,  0.1498,  0.1829]],
       dtype=torch.float64)
	q_value: tensor([[-14.2964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15197197718977185, distance: 1.2282242012024605 entropy 0.3949669301509857
epoch: 10, step: 118
	action: tensor([[ 0.0098,  0.3113,  0.0979, -0.4194, -0.3395,  0.4361,  0.1394]],
       dtype=torch.float64)
	q_value: tensor([[-16.5821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3859779622186962, distance: 0.8967031029273074 entropy 0.3949669301509857
epoch: 10, step: 119
	action: tensor([[ 0.2371, -0.2811,  0.3890, -0.1057, -0.2051,  0.2931, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[-15.9136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27636946823792863, distance: 0.9734533287443405 entropy 0.3949669301509857
epoch: 10, step: 120
	action: tensor([[-0.2232, -0.5084,  0.0618, -0.2719,  0.6241, -0.0396, -0.6318]],
       dtype=torch.float64)
	q_value: tensor([[-15.7849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5297859947949559, distance: 1.4153766727415158 entropy 0.3949669301509857
epoch: 10, step: 121
	action: tensor([[ 0.4388, -0.0325, -0.0826,  0.2280, -0.0548, -0.3496,  0.1087]],
       dtype=torch.float64)
	q_value: tensor([[-15.2930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6330906746312135, distance: 0.6931641101609292 entropy 0.3949669301509857
epoch: 10, step: 122
	action: tensor([[ 0.1960,  0.4476,  0.5105,  0.5171, -0.6606,  0.3950,  0.3832]],
       dtype=torch.float64)
	q_value: tensor([[-14.9239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8589549619133355, distance: 0.4297695082827241 entropy 0.3949669301509857
epoch: 10, step: 123
	action: tensor([[ 0.3819,  0.2632, -0.5525, -0.4832, -0.1534,  0.0588, -0.1159]],
       dtype=torch.float64)
	q_value: tensor([[-19.0123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7053208455993296, distance: 0.6211999337581866 entropy 0.3949669301509857
epoch: 10, step: 124
	action: tensor([[-0.3811, -0.8878, -0.5530, -0.3009, -0.4107, -0.3090,  0.7250]],
       dtype=torch.float64)
	q_value: tensor([[-14.2621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6451847971720295, distance: 1.4677903803592125 entropy 0.3949669301509857
epoch: 10, step: 125
	action: tensor([[ 0.0548,  0.2013, -0.1008,  0.4382, -0.4057, -0.1993, -0.1950]],
       dtype=torch.float64)
	q_value: tensor([[-20.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5805704623215315, distance: 0.7411160166448296 entropy 0.3949669301509857
epoch: 10, step: 126
	action: tensor([[ 0.0670, -0.0488, -0.0906,  0.5553, -0.1630,  0.0236, -0.4149]],
       dtype=torch.float64)
	q_value: tensor([[-13.9481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5439851721679541, distance: 0.7727627431992246 entropy 0.3949669301509857
epoch: 10, step: 127
	action: tensor([[ 0.5854, -0.1000,  0.0089, -0.0841,  0.0738,  0.0243, -0.8776]],
       dtype=torch.float64)
	q_value: tensor([[-14.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6013301651916021, distance: 0.7225424733027707 entropy 0.3949669301509857
LOSS epoch 10 actor 137.4917998836706 critic 138.82580437012652 
epoch: 11, step: 0
	action: tensor([[ 0.3178,  0.5654,  0.1356, -0.2367,  0.1051, -0.1312, -0.5108]],
       dtype=torch.float64)
	q_value: tensor([[-16.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7791746008979935, distance: 0.5377509735100497 entropy 0.3949669301509857
epoch: 11, step: 1
	action: tensor([[-0.0602, -0.9613,  0.0985,  0.5654,  0.6233,  0.4061, -0.2868]],
       dtype=torch.float64)
	q_value: tensor([[-14.6893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0004298745352622202, distance: 1.1445901897996396 entropy 0.3949669301509857
epoch: 11, step: 2
	action: tensor([[ 0.0354, -0.0413, -0.5753, -0.2339,  0.3851,  0.3478,  0.1231]],
       dtype=torch.float64)
	q_value: tensor([[-19.9184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35868938690708807, distance: 0.916412294147576 entropy 0.3949669301509857
epoch: 11, step: 3
	action: tensor([[-0.2819, -0.0382,  0.4174,  0.1574,  0.5755,  0.0447,  0.9199]],
       dtype=torch.float64)
	q_value: tensor([[-13.5570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05967918990509724, distance: 1.1096722280964375 entropy 0.3949669301509857
epoch: 11, step: 4
	action: tensor([[ 3.2418e-01, -2.3918e-05, -5.5992e-01, -1.9010e-01,  9.1844e-02,
         -1.8253e-02,  2.6713e-01]], dtype=torch.float64)
	q_value: tensor([[-19.9335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5037924695994731, distance: 0.8060989745295485 entropy 0.3949669301509857
epoch: 11, step: 5
	action: tensor([[ 0.5525, -0.1473, -0.1057, -0.6009, -0.0423, -0.1086,  0.1212]],
       dtype=torch.float64)
	q_value: tensor([[-13.9245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18763174859701215, distance: 1.0314143379633463 entropy 0.3949669301509857
epoch: 11, step: 6
	action: tensor([[ 0.0621, -0.5943,  0.2601, -0.1526, -0.3864,  0.0609, -0.1068]],
       dtype=torch.float64)
	q_value: tensor([[-15.9061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2641165506721299, distance: 1.2866198635407264 entropy 0.3949669301509857
epoch: 11, step: 7
	action: tensor([[ 0.4333, -0.0623, -0.7201,  0.7690, -0.2643,  0.0254, -0.1047]],
       dtype=torch.float64)
	q_value: tensor([[-15.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6268347378959925, distance: 0.6990484829280322 entropy 0.3949669301509857
epoch: 11, step: 8
	action: tensor([[ 0.1530,  0.0587,  0.0062, -1.0272,  0.4167,  0.5913,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-15.7970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19324111577154068, distance: 1.0278472342217677 entropy 0.3949669301509857
epoch: 11, step: 9
	action: tensor([[ 0.1697,  0.3611, -0.1296, -0.3971,  0.0016,  0.3165, -0.3766]],
       dtype=torch.float64)
	q_value: tensor([[-17.1305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6012739729857683, distance: 0.7225933924108404 entropy 0.3949669301509857
epoch: 11, step: 10
	action: tensor([[ 0.5429, -0.1469, -0.1702,  0.1640, -0.1470, -0.0732,  0.2983]],
       dtype=torch.float64)
	q_value: tensor([[-13.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6234762179590357, distance: 0.7021871852853994 entropy 0.3949669301509857
epoch: 11, step: 11
	action: tensor([[ 0.0165, -0.3266,  0.6230, -0.3525, -0.1113, -0.6100, -0.2763]],
       dtype=torch.float64)
	q_value: tensor([[-14.9945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36969627937023075, distance: 1.3392721298501777 entropy 0.3949669301509857
epoch: 11, step: 12
	action: tensor([[ 0.0621,  0.1748, -0.2094, -0.0602,  0.5451,  0.0654,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-17.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4713916413966862, distance: 0.8320007280589976 entropy 0.3949669301509857
epoch: 11, step: 13
	action: tensor([[ 0.1273,  0.2451,  0.0628, -0.3144,  0.2304, -0.4606,  0.1178]],
       dtype=torch.float64)
	q_value: tensor([[-13.2067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3224807323925978, distance: 0.9419275910023416 entropy 0.3949669301509857
epoch: 11, step: 14
	action: tensor([[ 0.0945,  0.4035,  0.0457, -0.7776, -0.1251, -0.0040, -0.1472]],
       dtype=torch.float64)
	q_value: tensor([[-14.3103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.317315474941556, distance: 0.9455112977169401 entropy 0.3949669301509857
epoch: 11, step: 15
	action: tensor([[ 0.1725,  0.3037, -0.3985,  0.1773, -0.0162,  0.2605,  0.1394]],
       dtype=torch.float64)
	q_value: tensor([[-15.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6418442096015989, distance: 0.684845618723951 entropy 0.3949669301509857
epoch: 11, step: 16
	action: tensor([[ 0.3444, -0.0504, -0.1058,  0.2357,  0.4345, -0.1664, -0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-12.6536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5892789557942208, distance: 0.7333818724986609 entropy 0.3949669301509857
epoch: 11, step: 17
	action: tensor([[-0.6880,  0.3150, -0.3381, -0.3005,  0.0017,  0.0643,  0.1519]],
       dtype=torch.float64)
	q_value: tensor([[-14.6375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3294930085733967, distance: 1.319470613157557 entropy 0.3949669301509857
epoch: 11, step: 18
	action: tensor([[ 0.0030, -0.6301, -0.4683,  0.3213, -0.5259,  0.3322,  0.1221]],
       dtype=torch.float64)
	q_value: tensor([[-12.9517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12529109458822907, distance: 1.2139173935666945 entropy 0.3949669301509857
epoch: 11, step: 19
	action: tensor([[ 0.8207,  0.5144, -0.0345,  0.4179,  0.4145,  0.4146,  0.3949]],
       dtype=torch.float64)
	q_value: tensor([[-15.3767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9454391265658645, distance: 0.26729901003312123 entropy 0.3949669301509857
epoch: 11, step: 20
	action: tensor([[-0.4325, -0.2832, -0.4255, -0.1168, -0.2195, -0.4121, -0.0828]],
       dtype=torch.float64)
	q_value: tensor([[-18.3144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36763927595532864, distance: 1.3382660957086319 entropy 0.3949669301509857
epoch: 11, step: 21
	action: tensor([[-1.3738e-04, -7.0615e-01, -8.5175e-01,  1.4138e-02,  3.3694e-01,
         -2.1205e-01, -2.8030e-01]], dtype=torch.float64)
	q_value: tensor([[-14.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3340946741259796, distance: 1.321752128584149 entropy 0.3949669301509857
epoch: 11, step: 22
	action: tensor([[-0.9604,  0.3500, -0.5671,  0.7158, -0.0939,  0.1375,  0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-15.6731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4374187854742517, distance: 1.371981803601355 entropy 0.3949669301509857
epoch: 11, step: 23
	action: tensor([[ 0.4925,  0.1799, -0.6297, -0.0409, -0.0039,  0.1299,  0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-15.2956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7711740107962245, distance: 0.5474057658050406 entropy 0.3949669301509857
epoch: 11, step: 24
	action: tensor([[ 0.2100, -0.2791, -0.3572, -0.1972, -0.1562, -0.1017,  0.0897]],
       dtype=torch.float64)
	q_value: tensor([[-14.8100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17016610835943702, distance: 1.0424429037768845 entropy 0.3949669301509857
epoch: 11, step: 25
	action: tensor([[ 0.5600, -0.6634, -0.0429, -0.1656, -0.0578,  0.2201, -0.0716]],
       dtype=torch.float64)
	q_value: tensor([[-13.5703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011236105509567773, distance: 1.1507552819468208 entropy 0.3949669301509857
epoch: 11, step: 26
	action: tensor([[ 0.0115, -0.2683, -0.1191,  0.8048, -0.2436,  0.1305, -0.0742]],
       dtype=torch.float64)
	q_value: tensor([[-16.1548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5044004047134436, distance: 0.805605021865068 entropy 0.3949669301509857
epoch: 11, step: 27
	action: tensor([[-0.4173,  0.1838, -0.2180, -0.3526, -0.6346,  0.1139,  0.1965]],
       dtype=torch.float64)
	q_value: tensor([[-15.6026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13610253138619588, distance: 1.2197349187001758 entropy 0.3949669301509857
epoch: 11, step: 28
	action: tensor([[ 0.2257,  0.0683, -0.0584, -0.0473,  0.0951, -0.1865, -0.4512]],
       dtype=torch.float64)
	q_value: tensor([[-14.6979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4793897016652364, distance: 0.8256824822181004 entropy 0.3949669301509857
epoch: 11, step: 29
	action: tensor([[ 0.2180,  0.5709, -0.2936, -0.6160,  0.4457,  0.3930,  0.4444]],
       dtype=torch.float64)
	q_value: tensor([[-13.1587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7736282611058414, distance: 0.5444622790557202 entropy 0.3949669301509857
epoch: 11, step: 30
	action: tensor([[ 1.1739e-01, -5.1475e-01, -5.5373e-01, -1.2917e-01,  5.4483e-01,
          1.0634e-01, -3.2848e-04]], dtype=torch.float64)
	q_value: tensor([[-16.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07749040671034346, distance: 1.1878549159412655 entropy 0.3949669301509857
epoch: 11, step: 31
	action: tensor([[-0.8036, -0.5150, -0.3242, -0.0690, -0.4544, -0.2536, -0.1300]],
       dtype=torch.float64)
	q_value: tensor([[-14.7465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9675699140474039, distance: 1.6051727554545772 entropy 0.3949669301509857
epoch: 11, step: 32
	action: tensor([[-0.0601, -0.0388, -0.5225, -0.0962, -0.2442, -0.1309, -0.1870]],
       dtype=torch.float64)
	q_value: tensor([[-15.6950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17380484736579394, distance: 1.0401548886618193 entropy 0.3949669301509857
epoch: 11, step: 33
	action: tensor([[-0.5212, -0.4192, -0.3838,  0.0021, -0.2681, -0.6806, -0.5831]],
       dtype=torch.float64)
	q_value: tensor([[-12.3425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5116790938585389, distance: 1.406975375951357 entropy 0.3949669301509857
epoch: 11, step: 34
	action: tensor([[-0.8707,  0.0429, -0.2567,  0.0592,  0.0161,  0.2386, -0.1598]],
       dtype=torch.float64)
	q_value: tensor([[-15.6719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6967022876596354, distance: 1.4905945052645855 entropy 0.3949669301509857
epoch: 11, step: 35
	action: tensor([[ 0.2531, -0.1728,  0.0123,  0.0553, -0.0813,  0.2939,  0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-13.8606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4762536819505355, distance: 0.8281655956429533 entropy 0.3949669301509857
epoch: 11, step: 36
	action: tensor([[ 0.6515, -0.1573, -0.7552, -0.0538,  0.2571,  0.1237,  0.2286]],
       dtype=torch.float64)
	q_value: tensor([[-15.2050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5901579263506518, distance: 0.7325967090230506 entropy 0.3949669301509857
epoch: 11, step: 37
	action: tensor([[-0.4329, -0.1928, -0.4411, -0.5046,  0.0434,  0.0198, -0.2303]],
       dtype=torch.float64)
	q_value: tensor([[-16.2089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3534789744495199, distance: 1.3313199808789231 entropy 0.3949669301509857
epoch: 11, step: 38
	action: tensor([[-0.2362, -0.3562, -0.2236,  0.1776, -0.2225,  0.2154,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[-12.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13035569985607487, distance: 1.2166460698971124 entropy 0.3949669301509857
epoch: 11, step: 39
	action: tensor([[ 0.6028, -0.1544,  0.1889, -0.2207,  0.2847, -0.2526, -0.7364]],
       dtype=torch.float64)
	q_value: tensor([[-13.3559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36508551714882864, distance: 0.9118309099096238 entropy 0.3949669301509857
epoch: 11, step: 40
	action: tensor([[ 0.8762, -0.0951, -0.1948,  0.2527, -0.7506, -0.0513, -0.2786]],
       dtype=torch.float64)
	q_value: tensor([[-16.7484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7728732280152836, distance: 0.5453695142699364 entropy 0.3949669301509857
epoch: 11, step: 41
	action: tensor([[-0.4800,  0.2556,  0.1975, -0.3096, -0.3948, -0.2254, -0.3723]],
       dtype=torch.float64)
	q_value: tensor([[-17.5347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25267233890450314, distance: 1.2807826536535802 entropy 0.3949669301509857
epoch: 11, step: 42
	action: tensor([[ 0.4315,  0.7275, -0.0808, -0.1910,  0.0040, -0.2548, -0.1992]],
       dtype=torch.float64)
	q_value: tensor([[-14.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8813789796796518, distance: 0.3941282100134344 entropy 0.3949669301509857
epoch: 11, step: 43
	action: tensor([[ 0.3145,  0.2305,  0.0402, -0.0091,  0.0671,  0.3466, -0.1043]],
       dtype=torch.float64)
	q_value: tensor([[-14.8855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7597123626093256, distance: 0.5609477455162674 entropy 0.3949669301509857
epoch: 11, step: 44
	action: tensor([[-0.3093, -0.3153, -0.7064, -0.2566, -0.0263,  0.3413, -0.0614]],
       dtype=torch.float64)
	q_value: tensor([[-13.5881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25813393149103314, distance: 1.283571693059295 entropy 0.3949669301509857
epoch: 11, step: 45
	action: tensor([[-0.2100, -0.2302, -0.1767, -0.2308, -0.2466, -0.0707,  0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-13.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18285447012983913, distance: 1.244578662460774 entropy 0.3949669301509857
epoch: 11, step: 46
	action: tensor([[ 0.1477, -0.0676, -0.5879,  0.4306, -0.2575,  0.2311, -0.5571]],
       dtype=torch.float64)
	q_value: tensor([[-13.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4113307161445213, distance: 0.8779956838484319 entropy 0.3949669301509857
epoch: 11, step: 47
	action: tensor([[-0.1303, -0.7979, -0.3301, -0.7501,  0.2743,  0.1674,  0.7988]],
       dtype=torch.float64)
	q_value: tensor([[-13.5644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6240495253376992, distance: 1.4583317385929138 entropy 0.3949669301509857
epoch: 11, step: 48
	action: tensor([[-0.2339, -0.1288, -0.1922,  0.3904,  0.0335,  0.4261, -0.2918]],
       dtype=torch.float64)
	q_value: tensor([[-20.2118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2122982134826793, distance: 1.0156348824384187 entropy 0.3949669301509857
epoch: 11, step: 49
	action: tensor([[ 0.3015, -0.1099, -0.2922,  0.5961,  0.0682, -0.1920,  0.5692]],
       dtype=torch.float64)
	q_value: tensor([[-13.2266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.628420860664263, distance: 0.6975612632695121 entropy 0.3949669301509857
epoch: 11, step: 50
	action: tensor([[-0.2155, -0.1686, -0.5861, -0.1152,  0.2468,  0.0496,  0.0302]],
       dtype=torch.float64)
	q_value: tensor([[-16.9537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06376135249964987, distance: 1.1802630124148092 entropy 0.3949669301509857
epoch: 11, step: 51
	action: tensor([[-0.3324,  0.1007,  0.2308, -0.0844, -0.0450, -0.2772,  0.3184]],
       dtype=torch.float64)
	q_value: tensor([[-12.6782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11889848099073186, distance: 1.2104644393515984 entropy 0.3949669301509857
epoch: 11, step: 52
	action: tensor([[ 0.5372,  0.5193,  0.0121, -0.3564,  0.0327,  0.6713, -0.5572]],
       dtype=torch.float64)
	q_value: tensor([[-14.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9208272361087134, distance: 0.321991640239133 entropy 0.3949669301509857
epoch: 11, step: 53
	action: tensor([[ 0.7943, -0.7683,  0.1444, -0.2830,  0.3366,  0.6505, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-16.8835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008260727173263538, distance: 1.149061090746452 entropy 0.3949669301509857
epoch: 11, step: 54
	action: tensor([[-0.2950, -0.2944,  0.1811,  0.5300,  0.0170, -0.3697,  0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-19.0988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013949036523826397, distance: 1.1522978636544285 entropy 0.3949669301509857
epoch: 11, step: 55
	action: tensor([[ 0.0423,  0.1501,  0.5948,  0.1717,  0.3675, -0.3363, -0.5155]],
       dtype=torch.float64)
	q_value: tensor([[-16.4707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44574418135631966, distance: 0.8519455206152177 entropy 0.3949669301509857
epoch: 11, step: 56
	action: tensor([[ 0.0528,  0.2419, -0.1083, -0.2699,  0.1029,  0.3189,  0.0604]],
       dtype=torch.float64)
	q_value: tensor([[-16.2112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4686370843114481, distance: 0.8341656726779301 entropy 0.3949669301509857
epoch: 11, step: 57
	action: tensor([[-0.1062,  0.2756, -0.3069, -0.0294,  0.3186, -0.2078,  0.4967]],
       dtype=torch.float64)
	q_value: tensor([[-12.9623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31100628641305617, distance: 0.9498703313970518 entropy 0.3949669301509857
epoch: 11, step: 58
	action: tensor([[ 0.3607, -0.1996, -0.0804,  0.3141,  0.3091, -0.3972, -0.2328]],
       dtype=torch.float64)
	q_value: tensor([[-14.3965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4643963091861548, distance: 0.8374877702456743 entropy 0.3949669301509857
epoch: 11, step: 59
	action: tensor([[ 0.7174, -0.2956, -0.4232,  0.3298,  0.1382, -0.4163, -0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-15.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5245972077579639, distance: 0.7890191743904741 entropy 0.3949669301509857
epoch: 11, step: 60
	action: tensor([[ 0.0824,  0.9498, -0.0378, -0.6896,  0.1506, -0.4206,  0.2177]],
       dtype=torch.float64)
	q_value: tensor([[-17.3563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.66070369697472, distance: 0.6665707572710602 entropy 0.3949669301509857
epoch: 11, step: 61
	action: tensor([[-0.0499,  0.6322,  0.0108,  0.2480, -0.3637,  0.0917, -0.1103]],
       dtype=torch.float64)
	q_value: tensor([[-16.5286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6197840551826203, distance: 0.7056215827932111 entropy 0.3949669301509857
epoch: 11, step: 62
	action: tensor([[-0.3192,  0.0201, -0.0314, -0.4575, -0.1138, -0.1068, -0.0494]],
       dtype=torch.float64)
	q_value: tensor([[-14.0732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2269261105879632, distance: 1.267552329447877 entropy 0.3949669301509857
epoch: 11, step: 63
	action: tensor([[ 0.2340, -0.1038, -0.3378, -0.0886, -0.3798,  0.0152,  0.3623]],
       dtype=torch.float64)
	q_value: tensor([[-12.8296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3723444406175125, distance: 0.9066034830220375 entropy 0.3949669301509857
epoch: 11, step: 64
	action: tensor([[-0.5552,  0.2897, -0.4426,  0.1193, -0.0560,  0.4461, -0.1548]],
       dtype=torch.float64)
	q_value: tensor([[-14.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0855156749433037, distance: 1.1922703472161265 entropy 0.3949669301509857
epoch: 11, step: 65
	action: tensor([[ 0.1832,  0.3656, -0.4608, -0.2203, -0.1640, -0.4302,  0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-12.6255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6449329345601014, distance: 0.6818861801855967 entropy 0.3949669301509857
epoch: 11, step: 66
	action: tensor([[ 0.5387, -0.4066, -0.5557, -0.6070,  0.3583, -0.2017, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-14.0785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028407870274070657, distance: 1.1604846199278236 entropy 0.3949669301509857
epoch: 11, step: 67
	action: tensor([[-0.3505, -0.4260,  0.6229, -0.4447,  1.0396,  0.3606,  0.5552]],
       dtype=torch.float64)
	q_value: tensor([[-16.1357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6627992614425846, distance: 1.4756270397232625 entropy 0.3949669301509857
epoch: 11, step: 68
	action: tensor([[-0.2616, -0.4962, -0.4053, -0.1104,  0.6057, -0.0423, -0.3351]],
       dtype=torch.float64)
	q_value: tensor([[-21.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4183542522663084, distance: 1.362853113868758 entropy 0.3949669301509857
epoch: 11, step: 69
	action: tensor([[ 0.1953, -0.6899,  0.0775, -0.0517, -0.0450, -0.2867, -0.3783]],
       dtype=torch.float64)
	q_value: tensor([[-14.4605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3011556237432311, distance: 1.3053329919696814 entropy 0.3949669301509857
epoch: 11, step: 70
	action: tensor([[-0.0173, -0.1008,  0.1786, -0.1816,  0.4757,  0.1019, -0.2260]],
       dtype=torch.float64)
	q_value: tensor([[-15.9503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13193133562776083, distance: 1.0661878593526743 entropy 0.3949669301509857
epoch: 11, step: 71
	action: tensor([[ 0.0718,  0.3870, -0.7197, -0.1304,  0.1853, -0.9438, -0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-13.3910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5969833059135279, distance: 0.7264708812355773 entropy 0.3949669301509857
epoch: 11, step: 72
	action: tensor([[ 0.4975,  0.5477, -0.2406,  0.1436,  0.0801,  0.2812, -0.1580]],
       dtype=torch.float64)
	q_value: tensor([[-16.4449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8972160519142263, distance: 0.3668760326918038 entropy 0.3949669301509857
epoch: 11, step: 73
	action: tensor([[ 0.2876,  0.4812,  0.0849, -0.1204,  0.2220, -0.4810, -0.6322]],
       dtype=torch.float64)
	q_value: tensor([[-14.1550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7045190476459505, distance: 0.6220444768512906 entropy 0.3949669301509857
epoch: 11, step: 74
	action: tensor([[ 0.1129,  0.0752, -0.0239, -0.1293, -0.8132,  0.4027,  0.5164]],
       dtype=torch.float64)
	q_value: tensor([[-15.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4136875403801493, distance: 0.8762363285634132 entropy 0.3949669301509857
epoch: 11, step: 75
	action: tensor([[ 0.2508, -0.0357, -0.4283, -0.3783, -0.3131,  0.2010, -0.1658]],
       dtype=torch.float64)
	q_value: tensor([[-17.6736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41501433007736854, distance: 0.8752443320843522 entropy 0.3949669301509857
epoch: 11, step: 76
	action: tensor([[ 0.5799, -0.0068, -0.6254, -0.2460, -0.0167,  0.0742, -0.1335]],
       dtype=torch.float64)
	q_value: tensor([[-13.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6442637551434414, distance: 0.6825284382952316 entropy 0.3949669301509857
epoch: 11, step: 77
	action: tensor([[-0.3189,  0.0552,  0.1815, -0.2907, -0.7355,  0.6600,  0.5395]],
       dtype=torch.float64)
	q_value: tensor([[-14.3346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12195877687392342, distance: 1.2121186778290896 entropy 0.3949669301509857
epoch: 11, step: 78
	action: tensor([[-0.6292, -0.5934, -0.5547,  0.3120, -0.0530, -0.2449,  0.1808]],
       dtype=torch.float64)
	q_value: tensor([[-18.8718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8444898204850013, distance: 1.5541567702504113 entropy 0.3949669301509857
epoch: 11, step: 79
	action: tensor([[-0.0916,  0.6423,  0.0586, -0.3797, -0.3493, -0.2280,  0.1691]],
       dtype=torch.float64)
	q_value: tensor([[-16.7104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4085899722363371, distance: 0.8800372095293942 entropy 0.3949669301509857
epoch: 11, step: 80
	action: tensor([[ 0.2173,  0.4572,  0.2293,  0.1527, -0.4226,  0.1642,  0.1593]],
       dtype=torch.float64)
	q_value: tensor([[-15.2241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7638755351258482, distance: 0.556067082125766 entropy 0.3949669301509857
epoch: 11, step: 81
	action: tensor([[-0.3082, -0.0674, -0.1132, -0.3660,  0.1302, -0.3680,  0.4668]],
       dtype=torch.float64)
	q_value: tensor([[-15.3207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32015558549345347, distance: 1.3148289324079847 entropy 0.3949669301509857
epoch: 11, step: 82
	action: tensor([[ 0.3239, -0.0031, -0.4849,  0.3261,  0.0679, -0.2991, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-14.6885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5816028283460776, distance: 0.7402033789961373 entropy 0.3949669301509857
epoch: 11, step: 83
	action: tensor([[-0.2113, -0.1646, -0.2616, -0.2972, -0.1945, -0.0072, -0.1456]],
       dtype=torch.float64)
	q_value: tensor([[-14.3600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1274139731402133, distance: 1.2150618907413455 entropy 0.3949669301509857
epoch: 11, step: 84
	action: tensor([[ 0.3473,  0.4758, -0.0789,  0.1913,  0.6243,  0.2523, -0.1345]],
       dtype=torch.float64)
	q_value: tensor([[-12.4224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8633671316043916, distance: 0.4229940769733381 entropy 0.3949669301509857
epoch: 11, step: 85
	action: tensor([[ 0.3463, -0.1886, -0.0060, -0.0490, -0.8652,  0.4640, -0.1978]],
       dtype=torch.float64)
	q_value: tensor([[-14.5777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4564140317894936, distance: 0.8437053675981956 entropy 0.3949669301509857
epoch: 11, step: 86
	action: tensor([[ 0.4503,  0.2247,  0.2397,  0.0727,  0.1816,  0.4861, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-17.2061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8897537698572733, distance: 0.3799606020472523 entropy 0.3949669301509857
epoch: 11, step: 87
	action: tensor([[ 0.0861,  0.2131, -0.6026,  0.0242, -0.2066,  0.3827,  0.3482]],
       dtype=torch.float64)
	q_value: tensor([[-15.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.521597390588346, distance: 0.791504636404666 entropy 0.3949669301509857
epoch: 11, step: 88
	action: tensor([[-0.1922, -0.3163, -0.1905,  0.1185,  0.2883, -0.2060, -0.5087]],
       dtype=torch.float64)
	q_value: tensor([[-14.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15263873065381084, distance: 1.2285795936488688 entropy 0.3949669301509857
epoch: 11, step: 89
	action: tensor([[ 0.1832, -0.5914, -0.0980,  0.2532, -0.2851, -0.1657, -0.3322]],
       dtype=torch.float64)
	q_value: tensor([[-13.9443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05277565243564564, distance: 1.1137382099188857 entropy 0.3949669301509857
epoch: 11, step: 90
	action: tensor([[ 0.6233, -0.0277,  0.1916,  0.0147, -0.6872, -0.1144, -0.2423]],
       dtype=torch.float64)
	q_value: tensor([[-15.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6687172644213992, distance: 0.6586521216709659 entropy 0.3949669301509857
epoch: 11, step: 91
	action: tensor([[-0.2969,  0.6840,  0.0781, -0.2981, -0.2441, -0.2741,  0.1283]],
       dtype=torch.float64)
	q_value: tensor([[-16.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.275501312386204, distance: 0.9740370906535227 entropy 0.3949669301509857
epoch: 11, step: 92
	action: tensor([[-0.1105, -0.0414, -0.4020,  0.2638, -0.1488, -0.5830,  0.3123]],
       dtype=torch.float64)
	q_value: tensor([[-14.5470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19733985626213535, distance: 1.0252329192876255 entropy 0.3949669301509857
epoch: 11, step: 93
	action: tensor([[ 0.4321, -0.2212, -0.2445,  0.0187, -0.0574, -0.0330, -0.4418]],
       dtype=torch.float64)
	q_value: tensor([[-15.3912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4367774664659214, distance: 0.8588092316684968 entropy 0.3949669301509857
epoch: 11, step: 94
	action: tensor([[ 0.2224,  0.5568, -0.0590, -0.4501, -0.2546,  0.1981, -0.1235]],
       dtype=torch.float64)
	q_value: tensor([[-13.8982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6980148816560504, distance: 0.6288534735213052 entropy 0.3949669301509857
epoch: 11, step: 95
	action: tensor([[-0.4790,  0.6778, -0.2890,  0.0603, -0.3499, -0.1590,  0.6601]],
       dtype=torch.float64)
	q_value: tensor([[-15.0769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21533442296355376, distance: 1.0136756019233901 entropy 0.3949669301509857
epoch: 11, step: 96
	action: tensor([[-0.0700,  0.2084,  0.5084,  0.5239, -0.0739, -0.1749, -0.0791]],
       dtype=torch.float64)
	q_value: tensor([[-15.6797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6120234346031548, distance: 0.7127864583697295 entropy 0.3949669301509857
epoch: 11, step: 97
	action: tensor([[-0.1902, -0.0738, -0.0583, -0.2879, -0.1683, -0.0732, -0.4744]],
       dtype=torch.float64)
	q_value: tensor([[-16.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08867229486041683, distance: 1.1940026171120508 entropy 0.3949669301509857
epoch: 11, step: 98
	action: tensor([[ 0.4100, -0.7444, -0.0211, -0.8663,  0.1520, -0.5872, -0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-12.6366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6261808639413371, distance: 1.459288353395023 entropy 0.3949669301509857
epoch: 11, step: 99
	action: tensor([[ 0.1876, -0.0799, -0.6481, -0.3546,  0.1933,  0.0014,  0.4856]],
       dtype=torch.float64)
	q_value: tensor([[-18.8563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36438955353197455, distance: 0.9123305262716442 entropy 0.3949669301509857
epoch: 11, step: 100
	action: tensor([[ 0.3596, -0.4200,  0.6686,  0.0656, -0.1361, -0.0061,  0.3715]],
       dtype=torch.float64)
	q_value: tensor([[-15.1579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24898367899552465, distance: 0.9917024378623424 entropy 0.3949669301509857
epoch: 11, step: 101
	action: tensor([[ 0.7309, -0.1491,  0.1012,  0.1242,  0.7671,  0.1883,  0.2045]],
       dtype=torch.float64)
	q_value: tensor([[-18.5545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7280402340889067, distance: 0.5967728029966107 entropy 0.3949669301509857
epoch: 11, step: 102
	action: tensor([[ 0.5320,  0.1064, -0.8836,  0.4937, -0.1194,  0.0851,  0.1406]],
       dtype=torch.float64)
	q_value: tensor([[-18.5644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7626099293918249, distance: 0.5575553251634493 entropy 0.3949669301509857
epoch: 11, step: 103
	action: tensor([[ 0.4612, -0.3819, -0.6803,  0.0278,  0.4704,  0.3328,  1.0188]],
       dtype=torch.float64)
	q_value: tensor([[-15.5406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4151778024552346, distance: 0.8751220314224513 entropy 0.3949669301509857
epoch: 11, step: 104
	action: tensor([[ 0.0189,  0.5138, -0.2653, -0.3545, -0.1552, -0.1309, -0.3980]],
       dtype=torch.float64)
	q_value: tensor([[-20.4154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5602503195599315, distance: 0.7588561524540713 entropy 0.3949669301509857
epoch: 11, step: 105
	action: tensor([[ 0.4906, -0.0594, -0.3247, -0.5303,  0.3608,  0.1263,  0.0710]],
       dtype=torch.float64)
	q_value: tensor([[-13.1025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4145354237780291, distance: 0.8756025239965722 entropy 0.3949669301509857
epoch: 11, step: 106
	action: tensor([[ 0.4255, -0.5926, -0.3619, -0.1707,  0.4357, -0.2910, -0.1013]],
       dtype=torch.float64)
	q_value: tensor([[-14.9928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13217497049905869, distance: 1.2176247523560635 entropy 0.3949669301509857
epoch: 11, step: 107
	action: tensor([[-0.9615,  0.3082,  0.2282,  0.0298, -0.0565, -0.2799, -0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-16.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7097342345704256, distance: 1.496307997504425 entropy 0.3949669301509857
epoch: 11, step: 108
	action: tensor([[ 0.3651, -0.1497, -0.0659, -0.1721,  0.3719, -0.5350, -0.7903]],
       dtype=torch.float64)
	q_value: tensor([[-14.8814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21254994562106533, distance: 1.0154725822031792 entropy 0.3949669301509857
epoch: 11, step: 109
	action: tensor([[ 0.2215, -0.1034,  0.3986,  0.3492,  0.4570, -0.3695, -0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-16.4218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.520092690685656, distance: 0.7927484025476643 entropy 0.3949669301509857
epoch: 11, step: 110
	action: tensor([[-0.1445, -0.3923, -0.2782, -0.4394, -0.7659, -0.1738,  0.6020]],
       dtype=torch.float64)
	q_value: tensor([[-16.8296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3172032856708862, distance: 1.3133579156872635 entropy 0.3949669301509857
epoch: 11, step: 111
	action: tensor([[-0.0217,  0.0149, -0.2109, -0.8584,  0.4017, -0.6239,  0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-17.6185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09233294872538722, distance: 1.1960083456759132 entropy 0.3949669301509857
epoch: 11, step: 112
	action: tensor([[-0.1509, -0.0882,  0.0567, -0.0408, -0.0494,  0.0946, -0.1667]],
       dtype=torch.float64)
	q_value: tensor([[-15.3636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08711700945696244, distance: 1.0933626922767896 entropy 0.3949669301509857
epoch: 11, step: 113
	action: tensor([[ 0.2311, -0.0450, -0.5612,  0.2866,  0.1458,  0.2489, -0.3817]],
       dtype=torch.float64)
	q_value: tensor([[-12.5446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5475009736743952, distance: 0.7697780404945689 entropy 0.3949669301509857
epoch: 11, step: 114
	action: tensor([[-0.2783, -0.2785,  0.2423, -0.5853, -0.0981,  0.3793, -0.1035]],
       dtype=torch.float64)
	q_value: tensor([[-13.2803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4510682036994478, distance: 1.3784804340463048 entropy 0.3949669301509857
epoch: 11, step: 115
	action: tensor([[-0.0557, -0.2248, -0.5836,  0.1744,  0.2711,  0.1043, -0.1421]],
       dtype=torch.float64)
	q_value: tensor([[-15.4021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10058086225324836, distance: 1.0852698934218366 entropy 0.3949669301509857
epoch: 11, step: 116
	action: tensor([[ 0.3548, -0.2102,  0.2080,  0.3977,  0.1192,  0.3529, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[-13.0789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7416133935495997, distance: 0.5816901266362393 entropy 0.3949669301509857
epoch: 11, step: 117
	action: tensor([[ 0.4928,  0.3356, -0.7662,  0.3120,  0.1194, -0.1574, -1.1027]],
       dtype=torch.float64)
	q_value: tensor([[-15.8401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8431506367050835, distance: 0.4532085279480853 entropy 0.3949669301509857
epoch: 11, step: 118
	action: tensor([[ 0.3052,  0.2136,  0.7557, -0.0508,  0.4323,  0.6974,  0.5638]],
       dtype=torch.float64)
	q_value: tensor([[-17.3670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8447188197981924, distance: 0.45093724260269796 entropy 0.3949669301509857
epoch: 11, step: 119
	action: tensor([[ 0.1805,  0.4230, -0.1470, -0.1921,  0.3329, -0.3409,  0.2297]],
       dtype=torch.float64)
	q_value: tensor([[-19.5719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5876813219129188, distance: 0.7348068524827605 entropy 0.3949669301509857
epoch: 11, step: 120
	action: tensor([[-0.1786, -0.0950,  0.1227, -0.2190, -0.3235, -0.0264,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-14.2318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1170276338297398, distance: 1.2094520412584835 entropy 0.3949669301509857
epoch: 11, step: 121
	action: tensor([[ 0.3395, -0.3749, -0.3062,  0.2308,  0.2306,  0.0253, -0.1820]],
       dtype=torch.float64)
	q_value: tensor([[-15.2880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34399900014862905, distance: 0.926848910847904 entropy 0.3949669301509857
epoch: 11, step: 122
	action: tensor([[ 0.0287, -0.1914,  0.2484,  0.3181, -0.2220,  0.6437, -0.6791]],
       dtype=torch.float64)
	q_value: tensor([[-14.2716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5152037892618043, distance: 0.7967761055489324 entropy 0.3949669301509857
epoch: 11, step: 123
	action: tensor([[ 0.2185,  0.0687, -0.1168,  0.5475, -0.0555,  0.1256, -0.3585]],
       dtype=torch.float64)
	q_value: tensor([[-16.1410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.712932668455914, distance: 0.61312437133488 entropy 0.3949669301509857
epoch: 11, step: 124
	action: tensor([[ 0.0035, -0.2096, -0.1709, -0.2027, -0.6652,  0.7631, -0.3311]],
       dtype=torch.float64)
	q_value: tensor([[-14.0011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10297195321544994, distance: 1.0838263473050622 entropy 0.3949669301509857
epoch: 11, step: 125
	action: tensor([[ 0.1104, -0.0165,  0.1686, -0.1749, -0.0138, -0.2095,  0.0542]],
       dtype=torch.float64)
	q_value: tensor([[-16.6306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23874259883087534, distance: 0.9984411165337823 entropy 0.3949669301509857
epoch: 11, step: 126
	action: tensor([[ 0.3859, -0.1502, -0.2606, -0.1517, -0.2921,  0.0687,  0.2124]],
       dtype=torch.float64)
	q_value: tensor([[-13.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.414810941258691, distance: 0.8753964720849247 entropy 0.3949669301509857
epoch: 11, step: 127
	action: tensor([[-0.4223, -0.5239,  0.2795, -0.4166, -0.3856,  0.1306, -0.2355]],
       dtype=torch.float64)
	q_value: tensor([[-14.3997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7884366445007693, distance: 1.530359532948594 entropy 0.3949669301509857
LOSS epoch 11 actor 120.95871943293427 critic 38.70962351279056 
epoch: 12, step: 0
	action: tensor([[-0.2705,  0.0600,  0.2418, -0.5200, -0.3101, -0.3891, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[-14.0138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2803855845361234, distance: 1.294872719553719 entropy 0.2896064221858978
epoch: 12, step: 1
	action: tensor([[-0.5596, -0.4606, -0.3390, -0.2020,  0.0777,  0.3879, -0.2129]],
       dtype=torch.float64)
	q_value: tensor([[-12.9888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.653827789761146, distance: 1.471640854600332 entropy 0.2896064221858978
epoch: 12, step: 2
	action: tensor([[-0.2950, -0.0513, -0.0932, -0.0850, -0.5604,  0.1465,  0.7506]],
       dtype=torch.float64)
	q_value: tensor([[-12.3302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11119251526124896, distance: 1.206288942844293 entropy 0.2896064221858978
epoch: 12, step: 3
	action: tensor([[ 0.0867, -0.2935, -0.2101,  0.1228,  0.0298, -0.0053,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-14.5495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15990740296362482, distance: 1.048866638909392 entropy 0.2896064221858978
epoch: 12, step: 4
	action: tensor([[ 0.0990, -0.0930, -0.2688, -0.4586,  0.1936, -0.8117, -0.5286]],
       dtype=torch.float64)
	q_value: tensor([[-11.5854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03836344428428207, distance: 1.1221790986334075 entropy 0.2896064221858978
epoch: 12, step: 5
	action: tensor([[ 0.4261, -0.4094,  0.3897,  0.7559,  0.0208,  0.0921, -0.2038]],
       dtype=torch.float64)
	q_value: tensor([[-14.0955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7789705007599796, distance: 0.5379994270382963 entropy 0.2896064221858978
epoch: 12, step: 6
	action: tensor([[ 0.2002, -0.3854, -0.2086,  0.4872, -0.0542,  0.8599,  0.0789]],
       dtype=torch.float64)
	q_value: tensor([[-16.2987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5928196929612207, distance: 0.7302138669049266 entropy 0.2896064221858978
epoch: 12, step: 7
	action: tensor([[ 0.0988,  0.2942,  0.4213,  0.2416, -0.7051, -0.1768, -0.5251]],
       dtype=torch.float64)
	q_value: tensor([[-14.5919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6993418365085103, distance: 0.6274703277738248 entropy 0.2896064221858978
epoch: 12, step: 8
	action: tensor([[ 0.5481, -0.4119, -0.1256,  0.0472, -0.2970, -0.2965, -0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-14.8958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2886524152656781, distance: 0.9651562423382674 entropy 0.2896064221858978
epoch: 12, step: 9
	action: tensor([[ 0.5326,  0.5004, -0.6071, -0.6485, -0.4634, -0.2411,  0.1646]],
       dtype=torch.float64)
	q_value: tensor([[-13.9782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8383964325474187, distance: 0.46002577445690007 entropy 0.2896064221858978
epoch: 12, step: 10
	action: tensor([[-0.6452, -0.2149,  0.0384, -0.1969,  0.1548, -0.4742, -0.3878]],
       dtype=torch.float64)
	q_value: tensor([[-15.2263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7838390404036342, distance: 1.528391189528632 entropy 0.2896064221858978
epoch: 12, step: 11
	action: tensor([[ 0.3357, -0.1152,  0.1130, -0.2611, -0.4213, -0.3910, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[-12.5882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23752031772856042, distance: 0.9992423476536417 entropy 0.2896064221858978
epoch: 12, step: 12
	action: tensor([[-0.0305, -0.4767, -0.1631,  0.4054, -0.1492, -0.3015, -0.7947]],
       dtype=torch.float64)
	q_value: tensor([[-13.4267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003002687280962979, distance: 1.1460610202062136 entropy 0.2896064221858978
epoch: 12, step: 13
	action: tensor([[-0.0458, -0.5667, -0.1603,  0.1146, -0.0866, -0.0865,  0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-14.2015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1854682060145172, distance: 1.2459529671406073 entropy 0.2896064221858978
epoch: 12, step: 14
	action: tensor([[-0.3615, -0.1098, -0.1370,  0.1850, -0.1661,  0.2410,  0.1478]],
       dtype=torch.float64)
	q_value: tensor([[-12.5264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06405286088178652, distance: 1.1804247183240986 entropy 0.2896064221858978
epoch: 12, step: 15
	action: tensor([[ 0.2431, -0.8025,  0.1857, -0.3898, -0.6414, -0.1126, -0.2096]],
       dtype=torch.float64)
	q_value: tensor([[-11.7112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5452576563175491, distance: 1.422515952209957 entropy 0.2896064221858978
epoch: 12, step: 16
	action: tensor([[-0.3080,  0.3483,  0.1190, -0.6404,  0.5122,  0.0951,  0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-15.6112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06200765749979209, distance: 1.1792897325016953 entropy 0.2896064221858978
epoch: 12, step: 17
	action: tensor([[-0.2625, -0.7369, -0.0200, -0.3012, -0.3498,  0.3398,  0.1781]],
       dtype=torch.float64)
	q_value: tensor([[-12.4710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6842088476443338, distance: 1.4850964690669404 entropy 0.2896064221858978
epoch: 12, step: 18
	action: tensor([[-0.3532, -0.2797, -0.0688, -0.0906, -0.1995,  0.4580,  0.2824]],
       dtype=torch.float64)
	q_value: tensor([[-14.2828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2719674575627178, distance: 1.2906090122341056 entropy 0.2896064221858978
epoch: 12, step: 19
	action: tensor([[ 0.3240, -0.2179, -0.2957, -0.0513, -0.0041, -0.2999,  0.2812]],
       dtype=torch.float64)
	q_value: tensor([[-12.9197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28354776206971477, distance: 0.9686130480386544 entropy 0.2896064221858978
epoch: 12, step: 20
	action: tensor([[ 0.1945, -0.2613,  0.2149,  0.0501,  0.6910,  0.0325, -0.2274]],
       dtype=torch.float64)
	q_value: tensor([[-12.9123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2952135989829544, distance: 0.960694824632934 entropy 0.2896064221858978
epoch: 12, step: 21
	action: tensor([[-0.2681,  0.2398, -0.2570,  0.0861, -0.2582,  0.0529,  0.3363]],
       dtype=torch.float64)
	q_value: tensor([[-13.5764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20567715467494896, distance: 1.019894429991654 entropy 0.2896064221858978
epoch: 12, step: 22
	action: tensor([[-0.1383,  0.4508, -0.8113,  0.1400, -0.5569,  0.2360,  0.5875]],
       dtype=torch.float64)
	q_value: tensor([[-11.5657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3396179667447987, distance: 0.9299386920258296 entropy 0.2896064221858978
epoch: 12, step: 23
	action: tensor([[-0.1330, -0.0656, -0.1126,  0.0581,  0.2116, -0.1947,  0.1285]],
       dtype=torch.float64)
	q_value: tensor([[-14.4475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0891189700808086, distance: 1.0921631571500159 entropy 0.2896064221858978
epoch: 12, step: 24
	action: tensor([[-0.2363,  0.0131,  0.4390, -0.1095,  0.3177,  0.0716,  0.3838]],
       dtype=torch.float64)
	q_value: tensor([[-11.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017149351632877563, distance: 1.154114922914817 entropy 0.2896064221858978
epoch: 12, step: 25
	action: tensor([[ 0.5309,  0.0721,  0.0079, -0.3663,  0.3753, -0.1770,  0.5580]],
       dtype=torch.float64)
	q_value: tensor([[-13.9174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5169132221346275, distance: 0.7953701147206248 entropy 0.2896064221858978
epoch: 12, step: 26
	action: tensor([[-0.2578,  0.3677, -0.3072, -0.1006,  0.1047, -0.1995, -0.3383]],
       dtype=torch.float64)
	q_value: tensor([[-15.1705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24380233168559762, distance: 0.9951174919242972 entropy 0.2896064221858978
epoch: 12, step: 27
	action: tensor([[ 0.3137, -0.0859, -0.2631,  0.3074,  0.2842,  0.1291,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[-10.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6312608450943014, distance: 0.6948904150237432 entropy 0.2896064221858978
epoch: 12, step: 28
	action: tensor([[ 0.2431, -0.0710, -0.1252,  0.0452,  0.1789,  0.2546,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-12.3381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5339999108560594, distance: 0.7811774410023512 entropy 0.2896064221858978
epoch: 12, step: 29
	action: tensor([[ 0.3251,  0.8306, -0.2748,  0.1816,  0.4517,  0.4460,  0.1518]],
       dtype=torch.float64)
	q_value: tensor([[-11.7527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 12, step: 30
	action: tensor([[ 0.3384,  0.3478, -0.4359, -0.1869,  0.9791, -0.1932, -0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-17.5000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.718403707895988, distance: 0.6072536854744564 entropy 0.2896064221858978
epoch: 12, step: 31
	action: tensor([[ 0.2461, -0.1462,  0.1057, -0.2328, -0.1770, -0.1230,  0.0933]],
       dtype=torch.float64)
	q_value: tensor([[-15.0026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21570070936114571, distance: 1.0134389795097996 entropy 0.2896064221858978
epoch: 12, step: 32
	action: tensor([[-0.0938,  0.1625, -0.3416,  0.3261,  0.0158,  0.2325,  0.2193]],
       dtype=torch.float64)
	q_value: tensor([[-12.4597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4132329551242555, distance: 0.8765759486166216 entropy 0.2896064221858978
epoch: 12, step: 33
	action: tensor([[ 0.4527, -0.1527,  0.2763, -0.3419,  0.3447,  0.3310, -0.4241]],
       dtype=torch.float64)
	q_value: tensor([[-11.4203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40864706743593715, distance: 0.879994728754189 entropy 0.2896064221858978
epoch: 12, step: 34
	action: tensor([[-0.6335, -0.1147, -0.2013,  0.0329, -0.2758,  0.2433, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-13.3389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.477788969041532, distance: 1.3911145835124337 entropy 0.2896064221858978
epoch: 12, step: 35
	action: tensor([[ 0.8048,  0.0053, -0.3220, -0.2630,  0.3877,  0.0016,  0.1972]],
       dtype=torch.float64)
	q_value: tensor([[-11.6529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6265866160435218, distance: 0.6992808469764225 entropy 0.2896064221858978
epoch: 12, step: 36
	action: tensor([[ 0.1729, -0.3794,  0.1195, -0.0119,  0.4757,  0.0684,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-15.0624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1495519709654164, distance: 1.0553112861068892 entropy 0.2896064221858978
epoch: 12, step: 37
	action: tensor([[-0.4295, -0.5925, -0.7500,  0.1423,  0.2778, -0.3456, -0.2603]],
       dtype=torch.float64)
	q_value: tensor([[-13.2673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6296846475206539, distance: 1.460859605246936 entropy 0.2896064221858978
epoch: 12, step: 38
	action: tensor([[ 0.5584, -0.0907, -0.6905, -0.3080,  0.3002,  1.1974,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-13.8077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7747811009807196, distance: 0.5430741220988853 entropy 0.2896064221858978
epoch: 12, step: 39
	action: tensor([[-0.0043,  0.3227, -0.0814, -0.5556,  0.0279,  0.1861, -0.1645]],
       dtype=torch.float64)
	q_value: tensor([[-17.0892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38882643074610634, distance: 0.8946207674484112 entropy 0.2896064221858978
epoch: 12, step: 40
	action: tensor([[ 0.2542, -0.0450, -0.3512, -0.5639,  0.2832, -0.0350,  0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-11.5588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2804670254742183, distance: 0.9706933267704928 entropy 0.2896064221858978
epoch: 12, step: 41
	action: tensor([[-0.4444,  0.0902, -0.7136, -0.1736, -0.1943,  0.1178, -0.3153]],
       dtype=torch.float64)
	q_value: tensor([[-12.2081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10775999398555047, distance: 1.2044243626816657 entropy 0.2896064221858978
epoch: 12, step: 42
	action: tensor([[ 0.3792, -0.7676, -0.0151,  0.0703,  0.2267,  0.3588, -0.2509]],
       dtype=torch.float64)
	q_value: tensor([[-10.4937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035909601331707286, distance: 1.1236099390304497 entropy 0.2896064221858978
epoch: 12, step: 43
	action: tensor([[-0.0609,  0.0021, -0.2213, -0.7725, -0.0200,  0.3720, -0.2981]],
       dtype=torch.float64)
	q_value: tensor([[-14.3752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09357164359601067, distance: 1.089490468072245 entropy 0.2896064221858978
epoch: 12, step: 44
	action: tensor([[ 0.3723,  0.1301,  0.3514,  0.1487, -0.1544, -0.2248,  0.4589]],
       dtype=torch.float64)
	q_value: tensor([[-12.2611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.719317954611016, distance: 0.6062671116890773 entropy 0.2896064221858978
epoch: 12, step: 45
	action: tensor([[ 0.0357, -0.0702,  0.0896,  0.0421,  0.2935,  0.1311,  0.5025]],
       dtype=torch.float64)
	q_value: tensor([[-14.3771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35429532355129956, distance: 0.9195464225820362 entropy 0.2896064221858978
epoch: 12, step: 46
	action: tensor([[ 0.3865,  0.0079, -0.4114, -0.0792,  0.4092,  0.4244, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-13.3959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6829121003820893, distance: 0.6443866403459083 entropy 0.2896064221858978
epoch: 12, step: 47
	action: tensor([[-0.1837,  0.1419,  0.0085, -0.4115,  0.4507, -0.2105,  0.0770]],
       dtype=torch.float64)
	q_value: tensor([[-12.4871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025543425708998813, distance: 1.1588673327169308 entropy 0.2896064221858978
epoch: 12, step: 48
	action: tensor([[-0.2030,  0.1919, -0.6112, -0.3754,  0.1183,  0.3335,  0.2554]],
       dtype=torch.float64)
	q_value: tensor([[-11.6342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24883308690436667, distance: 0.9918018598416262 entropy 0.2896064221858978
epoch: 12, step: 49
	action: tensor([[ 0.3618,  0.4843, -0.6229, -0.0814,  0.0390, -0.1415, -0.3142]],
       dtype=torch.float64)
	q_value: tensor([[-11.9857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8376357994244246, distance: 0.46110712456081787 entropy 0.2896064221858978
epoch: 12, step: 50
	action: tensor([[ 0.0095, -0.0068,  0.2695,  0.3081, -0.1389, -0.0710, -0.6735]],
       dtype=torch.float64)
	q_value: tensor([[-12.1980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5031938048623086, distance: 0.8065850993182077 entropy 0.2896064221858978
epoch: 12, step: 51
	action: tensor([[-0.4474,  0.4576, -0.8694, -0.2954,  0.2726,  0.5056, -0.4918]],
       dtype=torch.float64)
	q_value: tensor([[-13.1202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21459791768085557, distance: 1.0141512200408753 entropy 0.2896064221858978
epoch: 12, step: 52
	action: tensor([[ 0.5798, -0.4918, -0.0533, -0.2402, -0.2870,  0.0572,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[-12.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0913243628934326, distance: 1.0908402025603496 entropy 0.2896064221858978
epoch: 12, step: 53
	action: tensor([[-0.0853, -0.3103,  0.1876, -0.9919,  0.1216,  0.6062,  0.2453]],
       dtype=torch.float64)
	q_value: tensor([[-14.3005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3580694721995106, distance: 1.333575741161646 entropy 0.2896064221858978
epoch: 12, step: 54
	action: tensor([[-0.1753, -0.2619, -0.5253,  0.1372,  0.2972, -0.0940,  0.1880]],
       dtype=torch.float64)
	q_value: tensor([[-16.1330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06130502126173165, distance: 1.1788995522636136 entropy 0.2896064221858978
epoch: 12, step: 55
	action: tensor([[-0.1953,  0.0096, -0.0742,  0.6288, -0.6121,  0.0163, -0.3235]],
       dtype=torch.float64)
	q_value: tensor([[-12.1376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3641790427946653, distance: 0.9124815932290318 entropy 0.2896064221858978
epoch: 12, step: 56
	action: tensor([[ 0.7207, -0.0210,  0.1156, -0.6160, -0.2197, -0.4756,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[-12.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2727032955492813, distance: 0.9759161457472879 entropy 0.2896064221858978
epoch: 12, step: 57
	action: tensor([[ 3.1718e-01, -1.3653e-01,  5.1604e-04, -1.1462e-01, -1.9239e-02,
          3.1247e-01, -6.0316e-01]], dtype=torch.float64)
	q_value: tensor([[-15.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5080256037679007, distance: 0.8026532046378407 entropy 0.2896064221858978
epoch: 12, step: 58
	action: tensor([[ 0.2584,  0.5745,  0.1950, -0.0707,  0.4194,  0.1602, -0.3586]],
       dtype=torch.float64)
	q_value: tensor([[-12.4003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8259067998686003, distance: 0.4774716578572179 entropy 0.2896064221858978
epoch: 12, step: 59
	action: tensor([[ 0.8002, -0.0523, -0.3035,  0.1629,  0.3779, -0.1268, -0.3481]],
       dtype=torch.float64)
	q_value: tensor([[-12.6027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7451486810616808, distance: 0.5776970316604987 entropy 0.2896064221858978
epoch: 12, step: 60
	action: tensor([[ 0.3835, -0.1752,  0.1604,  0.3254, -0.1083,  0.5984,  0.0716]],
       dtype=torch.float64)
	q_value: tensor([[-14.8478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7968197276117797, distance: 0.5158191510842736 entropy 0.2896064221858978
epoch: 12, step: 61
	action: tensor([[ 0.2048,  0.2762, -0.1629,  0.1345,  0.0012,  0.5807,  0.1048]],
       dtype=torch.float64)
	q_value: tensor([[-14.1574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7660335230069422, distance: 0.5535202465986322 entropy 0.2896064221858978
epoch: 12, step: 62
	action: tensor([[ 0.0512, -0.3628, -0.6556, -0.2674, -0.3689, -0.0931, -0.1965]],
       dtype=torch.float64)
	q_value: tensor([[-12.3024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04743855401046926, distance: 1.1168714597001737 entropy 0.2896064221858978
epoch: 12, step: 63
	action: tensor([[-0.2956,  0.5751, -0.0083, -0.2099,  0.0574,  0.4235, -0.1514]],
       dtype=torch.float64)
	q_value: tensor([[-12.0938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34808762189278597, distance: 0.9239560358440411 entropy 0.2896064221858978
epoch: 12, step: 64
	action: tensor([[-0.1754, -0.2039, -0.4557,  0.3507,  0.5860,  0.2839, -0.0617]],
       dtype=torch.float64)
	q_value: tensor([[-12.0330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12902061929482678, distance: 1.0679738777992571 entropy 0.2896064221858978
epoch: 12, step: 65
	action: tensor([[-0.2071,  0.1308, -0.0560,  0.1678, -0.1277,  0.1257,  0.3809]],
       dtype=torch.float64)
	q_value: tensor([[-13.0781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25961603405873546, distance: 0.984657505713575 entropy 0.2896064221858978
epoch: 12, step: 66
	action: tensor([[ 0.5847,  0.1355, -0.0086,  0.2431, -0.5720,  0.3764,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-12.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8873063029689874, distance: 0.38415501451715856 entropy 0.2896064221858978
epoch: 12, step: 67
	action: tensor([[ 0.0576,  0.1136,  0.2496, -0.5070,  0.3384,  0.6689,  0.4684]],
       dtype=torch.float64)
	q_value: tensor([[-14.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4087333564536243, distance: 0.87993052289102 entropy 0.2896064221858978
epoch: 12, step: 68
	action: tensor([[-0.3017,  0.3535,  0.4054,  0.0418,  0.1067, -0.1187,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[-15.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1950751710274794, distance: 1.026678234862872 entropy 0.2896064221858978
epoch: 12, step: 69
	action: tensor([[ 0.4448,  0.5419,  0.0706,  0.1964,  0.1700,  0.1070, -0.0487]],
       dtype=torch.float64)
	q_value: tensor([[-12.6803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9043519481885874, distance: 0.3539115674293248 entropy 0.2896064221858978
epoch: 12, step: 70
	action: tensor([[ 0.2681, -0.3771, -0.4385,  0.2827,  0.0741, -0.2646, -0.0479]],
       dtype=torch.float64)
	q_value: tensor([[-12.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2714893186526339, distance: 0.9767302878147329 entropy 0.2896064221858978
epoch: 12, step: 71
	action: tensor([[-0.3637, -0.3503, -0.5086, -0.3682,  0.1702,  0.0772,  0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-13.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3541053274579704, distance: 1.3316279944644729 entropy 0.2896064221858978
epoch: 12, step: 72
	action: tensor([[-0.1486,  0.2214,  0.5749, -0.3290,  0.1605,  0.1744, -0.3386]],
       dtype=torch.float64)
	q_value: tensor([[-11.7378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.156781647566562, distance: 1.050816100568478 entropy 0.2896064221858978
epoch: 12, step: 73
	action: tensor([[ 0.0293,  0.2303, -0.0590, -0.2973, -0.2341,  0.2616,  0.4779]],
       dtype=torch.float64)
	q_value: tensor([[-13.3547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4162997525871588, distance: 0.8742821910073331 entropy 0.2896064221858978
epoch: 12, step: 74
	action: tensor([[-0.3059, -0.0888, -0.0403,  0.4161, -0.4758,  0.1053, -0.2049]],
       dtype=torch.float64)
	q_value: tensor([[-13.2449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14062298527304218, distance: 1.0608367591696555 entropy 0.2896064221858978
epoch: 12, step: 75
	action: tensor([[-0.1928,  0.1274, -0.0040,  0.7546, -0.2439,  0.6150,  0.5422]],
       dtype=torch.float64)
	q_value: tensor([[-11.8647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5421466718451486, distance: 0.7743189376179005 entropy 0.2896064221858978
epoch: 12, step: 76
	action: tensor([[-0.5568,  0.1283, -0.6620,  0.1201, -0.1099,  0.3164, -0.6665]],
       dtype=torch.float64)
	q_value: tensor([[-15.1869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25381233708392914, distance: 1.281365311148822 entropy 0.2896064221858978
epoch: 12, step: 77
	action: tensor([[ 0.0096, -0.1981, -0.0050,  0.0601,  0.5137,  0.6662, -0.3156]],
       dtype=torch.float64)
	q_value: tensor([[-11.5172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4280952734551543, distance: 0.8654032785567651 entropy 0.2896064221858978
epoch: 12, step: 78
	action: tensor([[-0.0568, -0.8269,  0.2864,  0.3614, -0.0490, -0.1888, -0.3099]],
       dtype=torch.float64)
	q_value: tensor([[-12.8644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2762822978642807, distance: 1.292796197552313 entropy 0.2896064221858978
epoch: 12, step: 79
	action: tensor([[-0.0183,  0.1324,  0.1445, -0.0814, -0.1330, -0.1063,  0.3044]],
       dtype=torch.float64)
	q_value: tensor([[-15.3411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31995038122829444, distance: 0.9436848741459112 entropy 0.2896064221858978
epoch: 12, step: 80
	action: tensor([[ 0.3579,  0.3954, -0.2750,  0.2092, -0.1961, -0.0165, -0.3208]],
       dtype=torch.float64)
	q_value: tensor([[-12.1639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8185901945688556, distance: 0.48740173637853385 entropy 0.2896064221858978
epoch: 12, step: 81
	action: tensor([[ 0.0105,  0.0177, -0.0629, -0.0283,  0.0427,  0.2434,  0.3894]],
       dtype=torch.float64)
	q_value: tensor([[-11.8814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38148846265377045, distance: 0.8999753114405981 entropy 0.2896064221858978
epoch: 12, step: 82
	action: tensor([[-0.0649, -0.5204,  0.3039, -0.1391,  0.0785, -0.6320, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[-12.1438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47239737788161973, distance: 1.3885745812414902 entropy 0.2896064221858978
epoch: 12, step: 83
	action: tensor([[ 0.0869,  0.6651, -0.1900, -0.1260,  0.4162,  0.6416,  0.2023]],
       dtype=torch.float64)
	q_value: tensor([[-14.9855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7711144385126533, distance: 0.5474770166482532 entropy 0.2896064221858978
epoch: 12, step: 84
	action: tensor([[0.2139, 0.4062, 0.0082, 0.2755, 0.2182, 0.2277, 0.2474]],
       dtype=torch.float64)
	q_value: tensor([[-13.4288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7984667098969753, distance: 0.5137242783549648 entropy 0.2896064221858978
epoch: 12, step: 85
	action: tensor([[ 0.0080, -0.0998,  0.0788,  0.3593,  0.1143,  0.3216,  0.2076]],
       dtype=torch.float64)
	q_value: tensor([[-12.6536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5087201309974345, distance: 0.8020864460881096 entropy 0.2896064221858978
epoch: 12, step: 86
	action: tensor([[ 0.0281, -0.4439,  0.1923,  0.2830, -0.1024,  0.5923,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-12.8779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3394164301647493, distance: 0.9300805813848309 entropy 0.2896064221858978
epoch: 12, step: 87
	action: tensor([[-0.3592,  0.1069, -0.6877, -0.6092,  0.4311,  0.2489,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-14.2858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07504737646843607, distance: 1.1005668758310285 entropy 0.2896064221858978
epoch: 12, step: 88
	action: tensor([[ 0.0935,  0.7758, -0.0961, -0.0376,  0.2862, -0.1569, -0.0593]],
       dtype=torch.float64)
	q_value: tensor([[-11.9866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.69273830819059, distance: 0.6343236473171693 entropy 0.2896064221858978
epoch: 12, step: 89
	action: tensor([[-0.0969,  0.3802,  0.4903, -0.4018,  0.3580, -0.3099, -0.3247]],
       dtype=torch.float64)
	q_value: tensor([[-12.1020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11666707295214507, distance: 1.075521020836721 entropy 0.2896064221858978
epoch: 12, step: 90
	action: tensor([[-0.2395, -0.0620, -0.0125, -0.1369, -0.2847,  0.3854, -0.4266]],
       dtype=torch.float64)
	q_value: tensor([[-13.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026335792051888074, distance: 1.1593149345196638 entropy 0.2896064221858978
epoch: 12, step: 91
	action: tensor([[-0.0755, -0.2546, -0.4408,  0.4398, -0.1237,  0.1351, -0.3722]],
       dtype=torch.float64)
	q_value: tensor([[-11.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1485254179042329, distance: 1.0559480130295569 entropy 0.2896064221858978
epoch: 12, step: 92
	action: tensor([[ 0.0066,  0.2213,  0.2292,  0.1954,  0.1297,  0.0717, -0.5162]],
       dtype=torch.float64)
	q_value: tensor([[-11.7744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5523431769746836, distance: 0.765648255596985 entropy 0.2896064221858978
epoch: 12, step: 93
	action: tensor([[ 0.6916, -0.0749,  0.1056, -0.0687,  0.1711, -0.1652, -0.5586]],
       dtype=torch.float64)
	q_value: tensor([[-12.3244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5813552114074105, distance: 0.7404223811749279 entropy 0.2896064221858978
epoch: 12, step: 94
	action: tensor([[ 0.2711, -0.2249, -0.1672, -0.3708, -0.0992, -0.0403,  0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-14.4881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15721326912982114, distance: 1.050547123415457 entropy 0.2896064221858978
epoch: 12, step: 95
	action: tensor([[ 0.1006,  0.0405, -0.5573, -0.3848, -0.0975, -0.4047, -0.1982]],
       dtype=torch.float64)
	q_value: tensor([[-12.4019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37064686093977217, distance: 0.9078286712645632 entropy 0.2896064221858978
epoch: 12, step: 96
	action: tensor([[ 0.0463,  0.3526,  0.0036,  0.2172, -0.6545,  0.1067,  0.2041]],
       dtype=torch.float64)
	q_value: tensor([[-11.6040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6172693385512991, distance: 0.7079511983966885 entropy 0.2896064221858978
epoch: 12, step: 97
	action: tensor([[ 0.2035, -0.7296, -0.2355, -0.2333,  0.0628, -0.1936,  0.3362]],
       dtype=torch.float64)
	q_value: tensor([[-13.3496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3824073472164664, distance: 1.3454721414066126 entropy 0.2896064221858978
epoch: 12, step: 98
	action: tensor([[-0.5427,  0.1750, -0.5122,  0.0161,  0.2036, -0.6138, -0.3735]],
       dtype=torch.float64)
	q_value: tensor([[-14.4664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19193153380452577, distance: 1.2493448994219125 entropy 0.2896064221858978
epoch: 12, step: 99
	action: tensor([[ 0.1857, -0.1068, -0.4012, -0.1036,  0.3541,  0.4768, -0.1289]],
       dtype=torch.float64)
	q_value: tensor([[-12.3094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.463260159683316, distance: 0.8383755602909724 entropy 0.2896064221858978
epoch: 12, step: 100
	action: tensor([[ 0.5521,  0.3707, -0.4486, -0.3217, -0.2686,  0.0990, -0.0480]],
       dtype=torch.float64)
	q_value: tensor([[-11.7587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8359594470665177, distance: 0.46348139507763164 entropy 0.2896064221858978
epoch: 12, step: 101
	action: tensor([[ 0.2748, -0.5253,  0.4795,  0.2655, -1.2072,  0.0952, -0.3042]],
       dtype=torch.float64)
	q_value: tensor([[-13.1711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30819422057010404, distance: 0.9518067627082568 entropy 0.2896064221858978
epoch: 12, step: 102
	action: tensor([[ 0.5568,  0.2412,  0.5950,  0.4942, -0.0210, -0.2055, -0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-17.6770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9598262103990078, distance: 0.22936549976027265 entropy 0.2896064221858978
epoch: 12, step: 103
	action: tensor([[-0.0048, -0.3160, -0.4528,  0.1656, -0.0303, -0.0144, -0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-15.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.112358575099061, distance: 1.0781407823751192 entropy 0.2896064221858978
epoch: 12, step: 104
	action: tensor([[-0.3966, -0.2007, -0.4484, -0.1051, -0.4239,  0.1517, -0.3524]],
       dtype=torch.float64)
	q_value: tensor([[-11.3341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3026876014937476, distance: 1.3061012139977821 entropy 0.2896064221858978
epoch: 12, step: 105
	action: tensor([[ 0.4283, -0.1964, -0.3648, -0.2818, -0.4976,  0.5359, -0.7589]],
       dtype=torch.float64)
	q_value: tensor([[-11.1042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43374316297034377, distance: 0.861119497489711 entropy 0.2896064221858978
epoch: 12, step: 106
	action: tensor([[ 0.1904,  0.5056,  0.0416, -0.6106, -0.0584, -0.2315,  0.3556]],
       dtype=torch.float64)
	q_value: tensor([[-14.2877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5477388765425237, distance: 0.7695756572075919 entropy 0.2896064221858978
epoch: 12, step: 107
	action: tensor([[ 0.0650,  0.0793, -0.5428, -0.7799,  0.3098, -0.5649,  0.2164]],
       dtype=torch.float64)
	q_value: tensor([[-13.7438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2125454838027946, distance: 1.0154754591143416 entropy 0.2896064221858978
epoch: 12, step: 108
	action: tensor([[-0.1811,  0.1331, -0.4263,  0.0454,  0.1143,  0.1123,  0.0935]],
       dtype=torch.float64)
	q_value: tensor([[-13.3815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25409642324139003, distance: 0.9883210334368686 entropy 0.2896064221858978
epoch: 12, step: 109
	action: tensor([[ 0.4358,  0.1006, -0.0680, -0.2405,  0.3071,  0.0100,  0.3048]],
       dtype=torch.float64)
	q_value: tensor([[-10.5520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6103736725680853, distance: 0.7143003134801023 entropy 0.2896064221858978
epoch: 12, step: 110
	action: tensor([[ 1.0213,  0.2047, -0.0658, -0.0820, -0.1062, -0.0846,  0.0241]],
       dtype=torch.float64)
	q_value: tensor([[-13.2062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8082833804252391, distance: 0.5010563548703741 entropy 0.2896064221858978
epoch: 12, step: 111
	action: tensor([[-0.5982, -0.0121,  0.1692,  0.5622, -0.2077,  0.2207,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-15.7362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04300549751835958, distance: 1.119467306512494 entropy 0.2896064221858978
epoch: 12, step: 112
	action: tensor([[ 0.4678,  0.4142, -0.5190, -0.1868, -0.0570,  0.1865,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[-13.3585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8584159066432715, distance: 0.4305899858797965 entropy 0.2896064221858978
epoch: 12, step: 113
	action: tensor([[-0.1350, -0.3720,  0.1202,  0.5407,  0.2403, -0.3868,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[-12.5245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09287591192823796, distance: 1.0899085086271814 entropy 0.2896064221858978
epoch: 12, step: 114
	action: tensor([[ 0.1490, -0.1076, -0.1272, -0.1231, -0.1812, -0.2707,  0.3515]],
       dtype=torch.float64)
	q_value: tensor([[-14.6998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23876511088403995, distance: 0.9984263533763809 entropy 0.2896064221858978
epoch: 12, step: 115
	action: tensor([[-0.4715, -0.3479, -0.1470,  0.2206, -0.0571, -0.2292,  0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-12.5189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.422931323320495, distance: 1.3650503263000295 entropy 0.2896064221858978
epoch: 12, step: 116
	action: tensor([[ 0.2283, -0.1223,  0.1641,  0.3606,  0.1039, -0.5714,  0.1512]],
       dtype=torch.float64)
	q_value: tensor([[-12.4984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4592319740763048, distance: 0.8415156474494417 entropy 0.2896064221858978
epoch: 12, step: 117
	action: tensor([[-0.3723, -0.0515, -0.3700,  0.1853,  0.1239, -0.2116, -0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-14.6185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12467564219587146, distance: 1.2135853858981813 entropy 0.2896064221858978
epoch: 12, step: 118
	action: tensor([[ 0.1503,  0.2618, -0.4665, -0.1583,  0.5758,  0.2522, -0.4600]],
       dtype=torch.float64)
	q_value: tensor([[-11.2776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6258368221593009, distance: 0.6999825539706668 entropy 0.2896064221858978
epoch: 12, step: 119
	action: tensor([[-0.1949,  0.3205, -0.2961,  0.1895,  0.2510, -0.2596,  0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-11.6374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3205012991460152, distance: 0.9433025503953087 entropy 0.2896064221858978
epoch: 12, step: 120
	action: tensor([[-0.0600, -0.3870,  0.0768, -0.4911, -0.3352,  0.3135,  0.0431]],
       dtype=torch.float64)
	q_value: tensor([[-11.6716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.280471851287726, distance: 1.2949163402345234 entropy 0.2896064221858978
epoch: 12, step: 121
	action: tensor([[ 0.4553, -0.3165, -0.2841,  0.0092, -0.1961,  0.2917,  0.0895]],
       dtype=torch.float64)
	q_value: tensor([[-13.6008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42225352729855126, distance: 0.8698118992040116 entropy 0.2896064221858978
epoch: 12, step: 122
	action: tensor([[ 0.7874, -0.0482, -0.4194, -0.3107,  0.1968, -0.1762,  0.3504]],
       dtype=torch.float64)
	q_value: tensor([[-12.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5082916872270377, distance: 0.8024361185168633 entropy 0.2896064221858978
epoch: 12, step: 123
	action: tensor([[-0.1527,  0.1942, -0.5421, -0.1101,  0.0328,  0.0196, -0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-15.2994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30703971726316603, distance: 0.9526006314621414 entropy 0.2896064221858978
epoch: 12, step: 124
	action: tensor([[-0.0502, -0.0596,  0.1036,  0.0117, -0.0411, -0.2798, -0.1779]],
       dtype=torch.float64)
	q_value: tensor([[-10.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17291938036907606, distance: 1.0407121275781823 entropy 0.2896064221858978
epoch: 12, step: 125
	action: tensor([[ 0.0724,  0.2581, -0.2042,  0.1777, -0.1529,  0.2257, -0.1867]],
       dtype=torch.float64)
	q_value: tensor([[-11.6332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5986375074913197, distance: 0.7249784307094632 entropy 0.2896064221858978
epoch: 12, step: 126
	action: tensor([[-0.1164,  0.0496, -0.2631, -0.1700,  0.1024,  0.2230,  0.0952]],
       dtype=torch.float64)
	q_value: tensor([[-10.9053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24029314425424453, distance: 0.9974237750797206 entropy 0.2896064221858978
epoch: 12, step: 127
	action: tensor([[ 0.5005,  0.6246,  0.0363, -0.2088, -0.1930, -0.2668, -0.0991]],
       dtype=torch.float64)
	q_value: tensor([[-10.6324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8865655292764691, distance: 0.38541553671129225 entropy 0.2896064221858978
LOSS epoch 12 actor 96.3706400404336 critic 113.82562961192905 
epoch: 13, step: 0
	action: tensor([[-0.2434,  0.4684, -0.1022,  0.0603, -0.0726, -0.0191, -0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-11.6580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37544865150855067, distance: 0.9043587990801112 entropy 0.2896064221858978
epoch: 13, step: 1
	action: tensor([[ 0.2671, -0.2165,  0.2057, -0.0492,  0.2536,  0.3420, -0.1811]],
       dtype=torch.float64)
	q_value: tensor([[-9.0611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4358154883219193, distance: 0.8595423373205211 entropy 0.2896064221858978
epoch: 13, step: 2
	action: tensor([[ 0.2656, -0.5120, -0.6775,  0.2559, -0.1005, -0.3961, -0.2826]],
       dtype=torch.float64)
	q_value: tensor([[-10.4595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11251169400070593, distance: 1.0780477882630433 entropy 0.2896064221858978
epoch: 13, step: 3
	action: tensor([[-0.1453, -0.0794, -0.1662,  0.1484, -0.5752,  0.5395,  0.2689]],
       dtype=torch.float64)
	q_value: tensor([[-11.4153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18430605469013794, distance: 1.0335233968250035 entropy 0.2896064221858978
epoch: 13, step: 4
	action: tensor([[-0.2878, -0.3983, -0.0787,  0.3689, -0.3960, -0.2854,  0.1065]],
       dtype=torch.float64)
	q_value: tensor([[-10.9425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12847738958774158, distance: 1.2156348001966497 entropy 0.2896064221858978
epoch: 13, step: 5
	action: tensor([[ 0.7280, -0.2743, -0.6549, -0.3819,  0.3037, -0.5530, -0.0611]],
       dtype=torch.float64)
	q_value: tensor([[-11.0657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1755573483775843, distance: 1.039051129960542 entropy 0.2896064221858978
epoch: 13, step: 6
	action: tensor([[-0.3860,  0.4156,  0.1613, -0.3344,  0.2789,  0.3510, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-13.2899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12331734827066809, distance: 1.071464779003003 entropy 0.2896064221858978
epoch: 13, step: 7
	action: tensor([[ 0.2100, -0.2260, -0.0474, -0.2329,  0.2765, -0.2216, -0.6612]],
       dtype=torch.float64)
	q_value: tensor([[-9.9347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09355744125451915, distance: 1.089499003360754 entropy 0.2896064221858978
epoch: 13, step: 8
	action: tensor([[-0.6297, -0.0099,  0.1055,  0.0227,  0.5457,  0.3305, -0.3412]],
       dtype=torch.float64)
	q_value: tensor([[-10.3452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27288881583250646, distance: 1.2910763583213813 entropy 0.2896064221858978
epoch: 13, step: 9
	action: tensor([[ 0.4229,  0.3784,  0.6844,  0.1835,  0.0628,  0.1066, -0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-10.4626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.915870188257156, distance: 0.33191864723569187 entropy 0.2896064221858978
epoch: 13, step: 10
	action: tensor([[ 0.5296, -0.6555, -0.2384,  0.4052,  0.5073,  0.0189,  0.2821]],
       dtype=torch.float64)
	q_value: tensor([[-12.9616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32124178457280195, distance: 0.9427884271210306 entropy 0.2896064221858978
epoch: 13, step: 11
	action: tensor([[-0.2454,  0.0184, -0.0494, -0.0376,  0.5912, -0.3771,  0.5202]],
       dtype=torch.float64)
	q_value: tensor([[-13.4394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08800657099361997, distance: 1.1936374945692234 entropy 0.2896064221858978
epoch: 13, step: 12
	action: tensor([[-0.1618,  0.0821, -0.1525,  0.1802, -0.3169,  0.6475,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-11.8065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33987873965198623, distance: 0.9297550659209892 entropy 0.2896064221858978
epoch: 13, step: 13
	action: tensor([[-0.1582, -0.7817, -0.2167,  0.0020, -0.0707, -0.1083, -0.3257]],
       dtype=torch.float64)
	q_value: tensor([[-10.3473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5148301302085909, distance: 1.4084410053457082 entropy 0.2896064221858978
epoch: 13, step: 14
	action: tensor([[-0.4267,  0.0238, -0.2568, -0.1946, -0.3324, -0.0075, -0.3787]],
       dtype=torch.float64)
	q_value: tensor([[-10.8684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20528426665273214, distance: 1.2563233655364918 entropy 0.2896064221858978
epoch: 13, step: 15
	action: tensor([[-0.2110,  0.1855, -0.1103, -0.3833, -0.0141,  0.1550, -0.1741]],
       dtype=torch.float64)
	q_value: tensor([[-8.7916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1228915305542918, distance: 1.071724960550021 entropy 0.2896064221858978
epoch: 13, step: 16
	action: tensor([[-0.3789, -0.2351, -0.3221,  0.2188,  0.3565,  0.4402, -0.5629]],
       dtype=torch.float64)
	q_value: tensor([[-8.7605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.062200094937608474, distance: 1.1793965722280606 entropy 0.2896064221858978
epoch: 13, step: 17
	action: tensor([[ 0.2285,  0.0165,  0.1313, -0.1102,  0.4902,  0.3224, -0.2258]],
       dtype=torch.float64)
	q_value: tensor([[-9.9877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5565643364950686, distance: 0.7620298829854254 entropy 0.2896064221858978
epoch: 13, step: 18
	action: tensor([[-0.2439,  0.2813, -0.2343,  0.3082,  0.4636, -0.2171, -0.3222]],
       dtype=torch.float64)
	q_value: tensor([[-10.0420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2717795032399055, distance: 0.9765357400113162 entropy 0.2896064221858978
epoch: 13, step: 19
	action: tensor([[ 0.4496,  0.8369, -0.4726, -0.2764,  0.5647,  0.0684,  0.1955]],
       dtype=torch.float64)
	q_value: tensor([[-9.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.933298146486569, distance: 0.29554638009931533 entropy 0.2896064221858978
epoch: 13, step: 20
	action: tensor([[ 0.0041, -0.0524,  0.2916,  0.3155,  0.2537, -0.3628, -0.1605]],
       dtype=torch.float64)
	q_value: tensor([[-12.2780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35062874361887664, distance: 0.922153510466468 entropy 0.2896064221858978
epoch: 13, step: 21
	action: tensor([[ 0.2675,  0.0312, -0.3458, -0.5341, -0.0453,  0.1898, -0.0711]],
       dtype=torch.float64)
	q_value: tensor([[-11.1460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38960666384772313, distance: 0.8940495421149651 entropy 0.2896064221858978
epoch: 13, step: 22
	action: tensor([[ 0.3118,  0.0878, -0.6604,  0.6796,  0.1361, -0.1714,  0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-9.9789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6313752168242446, distance: 0.6947826396742387 entropy 0.2896064221858978
epoch: 13, step: 23
	action: tensor([[-0.3405, -0.2905,  0.0284,  0.3117,  0.5003,  0.2336, -0.6614]],
       dtype=torch.float64)
	q_value: tensor([[-11.6094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0016157378801208155, distance: 1.1434194000890583 entropy 0.2896064221858978
epoch: 13, step: 24
	action: tensor([[-0.1330,  0.7699, -0.4439, -0.5950, -0.3610, -0.4501,  0.0425]],
       dtype=torch.float64)
	q_value: tensor([[-10.8857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.640263480707933, distance: 0.6863552458808785 entropy 0.2896064221858978
epoch: 13, step: 25
	action: tensor([[ 0.0232,  0.4762, -0.4987, -0.5614,  0.1632, -0.1311, -0.1755]],
       dtype=torch.float64)
	q_value: tensor([[-11.3049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5970766616124341, distance: 0.7263867356830371 entropy 0.2896064221858978
epoch: 13, step: 26
	action: tensor([[ 0.3264, -0.2351, -1.1784, -0.3222,  0.1978, -0.1818, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[-9.2971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4318632978645226, distance: 0.8625476896963579 entropy 0.2896064221858978
epoch: 13, step: 27
	action: tensor([[-0.3165, -0.3226, -0.9540,  0.5218, -0.4035,  0.0651, -0.2663]],
       dtype=torch.float64)
	q_value: tensor([[-11.2734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38786867800672864, distance: 1.3481272290833897 entropy 0.2896064221858978
epoch: 13, step: 28
	action: tensor([[ 0.3344, -0.3519, -0.1976, -0.6130, -0.1433, -0.4302, -0.2366]],
       dtype=torch.float64)
	q_value: tensor([[-10.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09138385522937376, distance: 1.1954886458816223 entropy 0.2896064221858978
epoch: 13, step: 29
	action: tensor([[ 0.2778, -0.0313, -0.1222, -0.0697,  0.6524, -0.2942, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[-11.1849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40379400535877963, distance: 0.8835982813420089 entropy 0.2896064221858978
epoch: 13, step: 30
	action: tensor([[-0.2491,  0.2581, -0.3503,  0.1821,  0.2189,  0.2414, -0.2221]],
       dtype=torch.float64)
	q_value: tensor([[-11.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054717956687034, distance: 0.9536777200197784 entropy 0.2896064221858978
epoch: 13, step: 31
	action: tensor([[-0.0399,  0.0588,  0.1422, -0.4359,  0.1826,  0.4323, -0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-8.7307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22456399595799714, distance: 1.0076963236492562 entropy 0.2896064221858978
epoch: 13, step: 32
	action: tensor([[-0.2370, -0.4093, -0.2206, -0.3536,  0.4820, -0.3433, -0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-9.7933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4555666182467015, distance: 1.380615474284173 entropy 0.2896064221858978
epoch: 13, step: 33
	action: tensor([[ 0.2444, -0.0385, -0.3744,  0.3524,  0.1092, -0.4380,  0.2799]],
       dtype=torch.float64)
	q_value: tensor([[-10.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5197069435057291, distance: 0.7930669422210459 entropy 0.2896064221858978
epoch: 13, step: 34
	action: tensor([[-0.3774,  0.1630,  0.2057,  0.2210, -0.4661,  0.0942, -0.1047]],
       dtype=torch.float64)
	q_value: tensor([[-11.6040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1691995812571625, distance: 1.0430498060261397 entropy 0.2896064221858978
epoch: 13, step: 35
	action: tensor([[ 0.0492, -0.2736, -0.5367, -0.4366,  0.6543, -0.2868, -0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-10.2996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03746117405517957, distance: 1.1655814298462688 entropy 0.2896064221858978
epoch: 13, step: 36
	action: tensor([[ 0.4111, -0.1562,  0.2562, -0.2652, -0.2376,  0.1938, -0.4027]],
       dtype=torch.float64)
	q_value: tensor([[-10.5252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3796562469488617, distance: 0.9013073234047271 entropy 0.2896064221858978
epoch: 13, step: 37
	action: tensor([[ 0.3921, -0.3244, -0.1548, -0.0979,  0.3730,  0.1151, -0.3712]],
       dtype=torch.float64)
	q_value: tensor([[-11.3215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3094913635713551, distance: 0.9509140202677566 entropy 0.2896064221858978
epoch: 13, step: 38
	action: tensor([[ 0.2227, -0.1156,  0.2538, -0.0758, -0.0020,  0.1286,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-10.3442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40901455070344783, distance: 0.8797212595820707 entropy 0.2896064221858978
epoch: 13, step: 39
	action: tensor([[-0.2880, -0.0816,  0.0696, -0.4440,  0.0429, -0.0153,  0.1237]],
       dtype=torch.float64)
	q_value: tensor([[-10.3906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2764391762738785, distance: 1.292875649245645 entropy 0.2896064221858978
epoch: 13, step: 40
	action: tensor([[-0.0343,  0.2232, -0.0259, -0.3941,  0.0690, -0.3240, -0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-9.5928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21042353254870427, distance: 1.0168427379260885 entropy 0.2896064221858978
epoch: 13, step: 41
	action: tensor([[-0.1763,  0.1218, -0.0830, -0.7002,  0.0914, -0.0383, -0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-9.4391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04707450507499322, distance: 1.1709692374685947 entropy 0.2896064221858978
epoch: 13, step: 42
	action: tensor([[ 0.4078,  0.0180, -0.6674, -0.0012,  0.4594,  0.4373,  0.5958]],
       dtype=torch.float64)
	q_value: tensor([[-9.3209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7188745477791167, distance: 0.6067457972682678 entropy 0.2896064221858978
epoch: 13, step: 43
	action: tensor([[ 0.1610, -0.2241,  0.0181,  0.0642,  0.4552,  0.4312, -0.2083]],
       dtype=torch.float64)
	q_value: tensor([[-12.6747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4503504006163931, distance: 0.8483980295107815 entropy 0.2896064221858978
epoch: 13, step: 44
	action: tensor([[ 0.0762,  0.2662,  0.4347, -0.4038,  0.0629,  0.2411, -0.3048]],
       dtype=torch.float64)
	q_value: tensor([[-10.3724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3928262852315363, distance: 0.8916885179355635 entropy 0.2896064221858978
epoch: 13, step: 45
	action: tensor([[ 0.7414, -0.0674,  0.3065,  0.1129, -0.0600,  0.4230, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-10.9445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8393260586997777, distance: 0.45870071464888407 entropy 0.2896064221858978
epoch: 13, step: 46
	action: tensor([[ 0.1591, -0.2270, -0.2798,  0.1617, -0.1329,  0.2950,  0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-12.6955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.406021087994175, distance: 0.8819464298573038 entropy 0.2896064221858978
epoch: 13, step: 47
	action: tensor([[ 0.4626, -0.3432, -0.6303, -0.6461,  0.4664,  0.4815,  0.1412]],
       dtype=torch.float64)
	q_value: tensor([[-9.5735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25534222671311624, distance: 0.987495344734685 entropy 0.2896064221858978
epoch: 13, step: 48
	action: tensor([[-0.1416, -0.0556, -0.5671, -0.1135, -0.4322,  0.3501,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-12.1375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10137129666833555, distance: 1.0847929060015775 entropy 0.2896064221858978
epoch: 13, step: 49
	action: tensor([[-0.2065,  0.2682, -0.4374, -0.1195,  0.1648, -0.2460, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-9.3547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2647782908671672, distance: 0.9812187857720669 entropy 0.2896064221858978
epoch: 13, step: 50
	action: tensor([[ 0.2222, -0.6571, -0.3591,  0.5048,  0.1977,  0.1985,  0.2319]],
       dtype=torch.float64)
	q_value: tensor([[-8.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21878121056531896, distance: 1.0114467735053403 entropy 0.2896064221858978
epoch: 13, step: 51
	action: tensor([[ 0.5222, -0.2196,  0.4600, -0.1695, -0.0396,  0.1864, -0.1198]],
       dtype=torch.float64)
	q_value: tensor([[-11.9125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.442992189159231, distance: 0.85405794258812 entropy 0.2896064221858978
epoch: 13, step: 52
	action: tensor([[-0.0261,  0.1997, -0.2734,  0.0576, -0.2548, -0.1494, -0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-12.0470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44220536989857384, distance: 0.8546609431954201 entropy 0.2896064221858978
epoch: 13, step: 53
	action: tensor([[ 0.3965, -0.0089, -0.6636,  0.0778,  0.1356, -0.5504,  0.0503]],
       dtype=torch.float64)
	q_value: tensor([[-8.9334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.581924853662897, distance: 0.7399184701504937 entropy 0.2896064221858978
epoch: 13, step: 54
	action: tensor([[-0.5083, -0.1596,  0.4661, -0.5005, -0.3642,  0.3588, -0.1191]],
       dtype=torch.float64)
	q_value: tensor([[-11.7003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6300300344812706, distance: 1.4610144005715975 entropy 0.2896064221858978
epoch: 13, step: 55
	action: tensor([[ 0.5540, -0.0338, -0.0756,  0.1837, -0.1376, -0.1376,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-12.0062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7128704361217468, distance: 0.6131908262896223 entropy 0.2896064221858978
epoch: 13, step: 56
	action: tensor([[ 0.2648, -0.5356, -0.2020,  0.1229, -0.4834,  0.8026,  0.4726]],
       dtype=torch.float64)
	q_value: tensor([[-10.8359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2775713724947042, distance: 0.9726445707697785 entropy 0.2896064221858978
epoch: 13, step: 57
	action: tensor([[ 0.0328, -0.1565, -0.2620,  0.0837,  0.3702,  0.1149,  0.3497]],
       dtype=torch.float64)
	q_value: tensor([[-13.1742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2775063127696401, distance: 0.9726883664897386 entropy 0.2896064221858978
epoch: 13, step: 58
	action: tensor([[ 0.6144, -0.1491, -0.4159,  0.1310, -0.2284, -0.1113, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-10.2747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6124904526296295, distance: 0.7123573288931605 entropy 0.2896064221858978
epoch: 13, step: 59
	action: tensor([[ 0.4540,  0.0819,  0.0300,  0.0329,  0.0328,  0.2984, -0.2560]],
       dtype=torch.float64)
	q_value: tensor([[-10.9311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7750472660881225, distance: 0.5427531230293786 entropy 0.2896064221858978
epoch: 13, step: 60
	action: tensor([[ 0.0407, -0.1067, -0.1010,  0.1502,  0.5517,  0.1342, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[-10.3344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39827060149094173, distance: 0.8876817850198233 entropy 0.2896064221858978
epoch: 13, step: 61
	action: tensor([[-0.1637, -0.0777, -0.3868, -0.3875,  0.2767, -0.0082, -0.3910]],
       dtype=torch.float64)
	q_value: tensor([[-10.0328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023207871287995507, distance: 1.130987405995971 entropy 0.2896064221858978
epoch: 13, step: 62
	action: tensor([[-0.6751,  0.2062,  0.1831, -0.3373,  0.1219,  0.3541,  0.1111]],
       dtype=torch.float64)
	q_value: tensor([[-8.5663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4091834484887815, distance: 1.3584399966247473 entropy 0.2896064221858978
epoch: 13, step: 63
	action: tensor([[-0.1816,  0.4613, -0.2950, -0.5019, -0.1968, -0.3252, -0.1344]],
       dtype=torch.float64)
	q_value: tensor([[-10.7109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3099165663816521, distance: 0.950621197298864 entropy 0.2896064221858978
epoch: 13, step: 64
	action: tensor([[ 0.1324, -0.3160, -0.1893, -0.0054,  0.0545, -0.2206, -0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-9.6230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1058052016138673, distance: 1.0821133697463867 entropy 0.2896064221858978
epoch: 13, step: 65
	action: tensor([[ 0.4152, -0.2933, -0.2124, -0.4555,  0.1753, -0.0790, -0.1153]],
       dtype=torch.float64)
	q_value: tensor([[-9.7739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09674092979462023, distance: 1.087584122909119 entropy 0.2896064221858978
epoch: 13, step: 66
	action: tensor([[-0.5138, -0.0642,  0.0118,  0.0961,  0.3657, -0.1982,  0.0721]],
       dtype=torch.float64)
	q_value: tensor([[-10.4651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3499457446721268, distance: 1.329581153259632 entropy 0.2896064221858978
epoch: 13, step: 67
	action: tensor([[ 0.1836, -0.1586, -0.2388, -0.2664, -0.0609,  0.2395,  0.4688]],
       dtype=torch.float64)
	q_value: tensor([[-10.0987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2656689725309296, distance: 0.9806242586945972 entropy 0.2896064221858978
epoch: 13, step: 68
	action: tensor([[ 0.1210,  0.1437, -0.0281,  0.1256, -0.2453,  0.7189,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[-10.7964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6430130422553871, distance: 0.6837272170538189 entropy 0.2896064221858978
epoch: 13, step: 69
	action: tensor([[-0.1658, -0.4898, -0.0352, -0.5625,  0.2008,  0.5114, -0.2892]],
       dtype=torch.float64)
	q_value: tensor([[-11.0624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3418564871132044, distance: 1.3255915539994592 entropy 0.2896064221858978
epoch: 13, step: 70
	action: tensor([[ 0.1219,  0.6438,  0.3119, -0.0542,  0.3938,  0.1583, -0.1764]],
       dtype=torch.float64)
	q_value: tensor([[-10.3807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.749976538931168, distance: 0.5721989739094682 entropy 0.2896064221858978
epoch: 13, step: 71
	action: tensor([[ 0.2253,  0.6558, -0.8272,  0.5148,  0.1588,  0.2684, -0.2923]],
       dtype=torch.float64)
	q_value: tensor([[-10.7964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 13, step: 72
	action: tensor([[ 0.2869,  0.1546, -0.4968, -0.1908,  1.0422,  0.5459, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-14.3757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.761247074721985, distance: 0.5591534952100333 entropy 0.2896064221858978
epoch: 13, step: 73
	action: tensor([[ 0.2503,  0.0945, -0.5327, -0.0218,  0.2051,  0.0322, -0.5134]],
       dtype=torch.float64)
	q_value: tensor([[-12.5747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6043515435606179, distance: 0.7197993185111188 entropy 0.2896064221858978
epoch: 13, step: 74
	action: tensor([[-0.2923,  0.3649,  0.1296,  0.0185, -0.0412,  0.7006, -0.2569]],
       dtype=torch.float64)
	q_value: tensor([[-9.2375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4106637554021815, distance: 0.878492926438987 entropy 0.2896064221858978
epoch: 13, step: 75
	action: tensor([[ 0.0692,  0.4654,  0.0855, -0.8181, -0.5002,  0.2110, -0.5760]],
       dtype=torch.float64)
	q_value: tensor([[-10.7175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3784048411177431, distance: 0.9022159591288776 entropy 0.2896064221858978
epoch: 13, step: 76
	action: tensor([[-0.2917, -0.3924, -0.7188, -0.1974,  0.2283,  0.5020, -0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-12.9056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23205674383651753, distance: 1.270199824473065 entropy 0.2896064221858978
epoch: 13, step: 77
	action: tensor([[ 0.1768, -0.1858, -0.2405,  0.1771, -0.1704, -0.0770,  0.3802]],
       dtype=torch.float64)
	q_value: tensor([[-9.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36450920865531833, distance: 0.9122446480734625 entropy 0.2896064221858978
epoch: 13, step: 78
	action: tensor([[ 0.1887,  0.2718, -0.2758, -0.5354,  0.1983,  0.3013,  0.2371]],
       dtype=torch.float64)
	q_value: tensor([[-10.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5568518357801053, distance: 0.7617828138089554 entropy 0.2896064221858978
epoch: 13, step: 79
	action: tensor([[ 0.4279,  0.1385, -0.0464,  0.4671,  0.6292, -0.0630, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-10.6770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8478245025094856, distance: 0.44640500829229496 entropy 0.2896064221858978
epoch: 13, step: 80
	action: tensor([[-0.2887, -0.1663,  0.2061, -0.6442,  0.3344,  0.5672, -0.1986]],
       dtype=torch.float64)
	q_value: tensor([[-12.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30416932616906234, distance: 1.3068438066164847 entropy 0.2896064221858978
epoch: 13, step: 81
	action: tensor([[ 0.2582,  0.1510,  0.0659, -0.3027, -0.2053, -0.1927, -0.1091]],
       dtype=torch.float64)
	q_value: tensor([[-10.7774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4355936609083685, distance: 0.8597112991683172 entropy 0.2896064221858978
epoch: 13, step: 82
	action: tensor([[ 0.2694,  0.2813, -0.4283,  0.9372, -0.2167, -0.1683,  0.7596]],
       dtype=torch.float64)
	q_value: tensor([[-10.2364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6943197336380301, distance: 0.6326891616972912 entropy 0.2896064221858978
epoch: 13, step: 83
	action: tensor([[-0.4336,  0.2221, -0.0817, -0.0573,  0.0463, -0.2690, -0.2648]],
       dtype=torch.float64)
	q_value: tensor([[-13.5204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0831043705625143, distance: 1.1909453893623771 entropy 0.2896064221858978
epoch: 13, step: 84
	action: tensor([[-0.2799,  0.3780,  0.0147, -0.2582, -0.0200,  0.1544,  0.7012]],
       dtype=torch.float64)
	q_value: tensor([[-8.8662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.175273482534807, distance: 1.039229993669043 entropy 0.2896064221858978
epoch: 13, step: 85
	action: tensor([[-0.6908, -0.0734,  0.1290, -0.6234,  0.2314,  0.0432, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-11.6401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8507586103127227, distance: 1.5567955536338576 entropy 0.2896064221858978
epoch: 13, step: 86
	action: tensor([[ 0.1304, -0.0738,  0.1402, -0.0659, -0.2771,  0.0874, -0.3695]],
       dtype=torch.float64)
	q_value: tensor([[-10.4131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31868103565029005, distance: 0.9445651803346533 entropy 0.2896064221858978
epoch: 13, step: 87
	action: tensor([[ 0.2721, -0.2785, -0.5072,  0.0072,  0.1224, -0.6220, -0.5204]],
       dtype=torch.float64)
	q_value: tensor([[-9.9859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21996632700022611, distance: 1.0106792950736965 entropy 0.2896064221858978
epoch: 13, step: 88
	action: tensor([[ 0.1619,  0.1476,  0.3656,  0.2111,  0.4054, -0.8612,  0.4440]],
       dtype=torch.float64)
	q_value: tensor([[-11.6999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44183670511649853, distance: 0.8549433332798073 entropy 0.2896064221858978
epoch: 13, step: 89
	action: tensor([[ 0.4800,  0.2549,  0.1012,  0.2685,  0.2469,  0.4647, -0.0986]],
       dtype=torch.float64)
	q_value: tensor([[-14.3343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9491816206401562, distance: 0.2579687496754102 entropy 0.2896064221858978
epoch: 13, step: 90
	action: tensor([[-0.2322,  0.5505,  0.0576,  0.5390,  0.5689,  0.1912, -0.2531]],
       dtype=torch.float64)
	q_value: tensor([[-11.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5845927527916237, distance: 0.7375538388079754 entropy 0.2896064221858978
epoch: 13, step: 91
	action: tensor([[ 0.5086,  0.1029, -0.2032, -0.1311,  0.4096, -0.1193, -0.1118]],
       dtype=torch.float64)
	q_value: tensor([[-11.0206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6600439273639437, distance: 0.6672185237521863 entropy 0.2896064221858978
epoch: 13, step: 92
	action: tensor([[ 0.0635,  0.2593,  0.1719,  0.4495,  0.9194, -0.1756, -0.2820]],
       dtype=torch.float64)
	q_value: tensor([[-10.6278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6575945939019421, distance: 0.6696178154972185 entropy 0.2896064221858978
epoch: 13, step: 93
	action: tensor([[ 0.2400,  0.1001, -0.4630,  0.2047, -0.4353,  0.4931, -0.0570]],
       dtype=torch.float64)
	q_value: tensor([[-12.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6119085875735929, distance: 0.712891948439999 entropy 0.2896064221858978
epoch: 13, step: 94
	action: tensor([[-0.3077,  0.1397, -0.2996, -0.1091, -0.4924,  0.0237, -0.0360]],
       dtype=torch.float64)
	q_value: tensor([[-10.1948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05830071640660328, distance: 1.110485298247301 entropy 0.2896064221858978
epoch: 13, step: 95
	action: tensor([[ 0.0046, -0.1532, -0.3651,  0.3037, -0.1493,  0.3584,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-9.2083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3309338015800074, distance: 0.9360331680272078 entropy 0.2896064221858978
epoch: 13, step: 96
	action: tensor([[ 0.7426, -0.2748,  0.1252, -0.3161,  0.2438, -0.1602, -0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-9.2916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2464550269354272, distance: 0.9933705528376985 entropy 0.2896064221858978
epoch: 13, step: 97
	action: tensor([[ 0.2229, -0.6318, -0.0014, -0.3021,  0.1526, -0.0439, -0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-12.5248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32095035580473263, distance: 1.3152246546171316 entropy 0.2896064221858978
epoch: 13, step: 98
	action: tensor([[ 0.1491,  0.0855, -0.7174, -0.1665, -0.1685,  0.0261,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[-10.6720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4707929303106624, distance: 0.8324717640197824 entropy 0.2896064221858978
epoch: 13, step: 99
	action: tensor([[ 0.3528,  0.4250, -0.2882, -0.2666,  0.2042, -0.2657, -0.3649]],
       dtype=torch.float64)
	q_value: tensor([[-9.3521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.73901196735947, distance: 0.58461100987091 entropy 0.2896064221858978
epoch: 13, step: 100
	action: tensor([[ 0.5502,  0.2153, -0.4329, -0.2413, -0.1260, -0.4529, -0.5853]],
       dtype=torch.float64)
	q_value: tensor([[-10.1377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7114033689563051, distance: 0.6147553568481706 entropy 0.2896064221858978
epoch: 13, step: 101
	action: tensor([[ 0.0768, -0.2031, -0.3846, -0.2767, -0.1292, -0.1027, -0.3794]],
       dtype=torch.float64)
	q_value: tensor([[-11.4084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1402002355790095, distance: 1.0610976535547407 entropy 0.2896064221858978
epoch: 13, step: 102
	action: tensor([[ 0.0247,  0.1479, -0.0746,  0.1553, -0.4016,  0.1592,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-9.1649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4976008204580108, distance: 0.8111126114628783 entropy 0.2896064221858978
epoch: 13, step: 103
	action: tensor([[-0.1887, -0.1435,  0.3233,  0.0779,  0.0832,  0.2658, -0.0658]],
       dtype=torch.float64)
	q_value: tensor([[-9.6832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14651515317582364, distance: 1.0571937834050371 entropy 0.2896064221858978
epoch: 13, step: 104
	action: tensor([[ 0.6516,  0.3058, -0.2496, -0.2863,  0.3558, -0.2095, -0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-10.3181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7783238373092587, distance: 0.5387858615948637 entropy 0.2896064221858978
epoch: 13, step: 105
	action: tensor([[-0.2113, -0.2461, -0.0917, -0.3272,  0.4778,  0.1922,  0.1457]],
       dtype=torch.float64)
	q_value: tensor([[-11.6477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1876434554415427, distance: 1.247095560556796 entropy 0.2896064221858978
epoch: 13, step: 106
	action: tensor([[-0.5368,  0.8255, -0.3471, -0.0601,  0.0114, -0.2225, -0.7595]],
       dtype=torch.float64)
	q_value: tensor([[-9.7454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1996070157390113, distance: 1.02378398090706 entropy 0.2896064221858978
epoch: 13, step: 107
	action: tensor([[-0.1157, -0.2403,  0.1800,  0.0218, -0.0361, -0.4005,  0.3074]],
       dtype=torch.float64)
	q_value: tensor([[-10.3325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.100948279605513, distance: 1.200715596431853 entropy 0.2896064221858978
epoch: 13, step: 108
	action: tensor([[ 0.0634,  0.3036, -0.6503, -0.1399, -0.1902, -0.0184,  0.6230]],
       dtype=torch.float64)
	q_value: tensor([[-11.1335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5502661104154818, distance: 0.767422451673983 entropy 0.2896064221858978
epoch: 13, step: 109
	action: tensor([[-0.4184, -0.0533,  0.4129, -0.0719,  0.1414, -0.2978, -0.5072]],
       dtype=torch.float64)
	q_value: tensor([[-11.3043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3574676627235802, distance: 1.3332802307092424 entropy 0.2896064221858978
epoch: 13, step: 110
	action: tensor([[ 0.6773,  0.0758, -0.3319,  0.0101, -0.0541, -0.0250,  0.2015]],
       dtype=torch.float64)
	q_value: tensor([[-10.6770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7739573087203793, distance: 0.5440664275998865 entropy 0.2896064221858978
epoch: 13, step: 111
	action: tensor([[-0.5264,  0.1021, -0.2735, -0.0217, -0.4528, -0.0479,  0.3904]],
       dtype=torch.float64)
	q_value: tensor([[-11.1783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2211959337317968, distance: 1.2645889075425885 entropy 0.2896064221858978
epoch: 13, step: 112
	action: tensor([[-0.1391, -0.2343, -0.5744, -0.5888, -0.0124,  0.1114, -0.5187]],
       dtype=torch.float64)
	q_value: tensor([[-10.1795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04198126552749404, distance: 1.1681178180455765 entropy 0.2896064221858978
epoch: 13, step: 113
	action: tensor([[ 0.0740, -0.4393, -0.5766,  0.0966,  0.4134, -0.2858,  0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-9.1218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.037040518456900484, distance: 1.1653451038627325 entropy 0.2896064221858978
epoch: 13, step: 114
	action: tensor([[ 0.4926,  0.5209, -0.4042,  0.6173, -0.3799,  0.2418,  0.0932]],
       dtype=torch.float64)
	q_value: tensor([[-11.3256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7851185806629437, distance: 0.5304642558274036 entropy 0.2896064221858978
epoch: 13, step: 115
	action: tensor([[-0.1321, -0.3030, -0.4899, -0.2482,  0.0017,  0.1334, -0.3456]],
       dtype=torch.float64)
	q_value: tensor([[-11.8234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08478563287540264, distance: 1.191869360881734 entropy 0.2896064221858978
epoch: 13, step: 116
	action: tensor([[-0.2757,  0.6930, -0.4233, -0.2028,  0.6019, -0.0596, -0.1871]],
       dtype=torch.float64)
	q_value: tensor([[-8.7693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4067021740005248, distance: 0.881440642824111 entropy 0.2896064221858978
epoch: 13, step: 117
	action: tensor([[ 0.4488, -0.4771, -0.1443, -0.4107,  0.3494, -0.3557,  1.0110]],
       dtype=torch.float64)
	q_value: tensor([[-9.7005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1522373697576307, distance: 1.2283656728742691 entropy 0.2896064221858978
epoch: 13, step: 118
	action: tensor([[-0.3413, -0.4574, -0.3459,  0.1589,  0.0030,  0.3244, -0.5767]],
       dtype=torch.float64)
	q_value: tensor([[-14.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30779783082608314, distance: 1.3086605167100527 entropy 0.2896064221858978
epoch: 13, step: 119
	action: tensor([[ 0.0174, -0.2469,  0.1270, -0.8672, -0.1141,  0.0792, -0.4393]],
       dtype=torch.float64)
	q_value: tensor([[-9.7679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31454987745558505, distance: 1.3120344178277226 entropy 0.2896064221858978
epoch: 13, step: 120
	action: tensor([[-0.3960, -0.1295, -0.2190, -0.1129,  0.4296,  0.3529, -0.0918]],
       dtype=torch.float64)
	q_value: tensor([[-11.0028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.146297627708734, distance: 1.2251954917013368 entropy 0.2896064221858978
epoch: 13, step: 121
	action: tensor([[ 0.0905,  0.3504, -0.1381, -0.4951, -0.0782, -0.2560, -0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-9.3153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4353107101797472, distance: 0.8599267693373749 entropy 0.2896064221858978
epoch: 13, step: 122
	action: tensor([[-0.0098,  0.2015,  0.3271,  0.0247,  0.2333, -0.6512,  0.2363]],
       dtype=torch.float64)
	q_value: tensor([[-9.9350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27201559966533084, distance: 0.9763774258133735 entropy 0.2896064221858978
epoch: 13, step: 123
	action: tensor([[-0.4386,  0.7747, -0.4481,  0.0291,  0.2068,  0.5080, -0.1421]],
       dtype=torch.float64)
	q_value: tensor([[-11.9055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 13, step: 124
	action: tensor([[-0.1289, -0.1494,  0.1380, -0.5551, -0.2734, -0.3515, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-14.3757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2699835118886398, distance: 1.2896021085738039 entropy 0.2896064221858978
epoch: 13, step: 125
	action: tensor([[ 0.0460,  0.0199, -0.3490, -0.4196,  0.1408, -0.1825,  0.3983]],
       dtype=torch.float64)
	q_value: tensor([[-10.5958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2145601779999684, distance: 1.0141755854484236 entropy 0.2896064221858978
epoch: 13, step: 126
	action: tensor([[-0.0111,  0.0049, -0.4907, -0.2820,  0.5067,  0.2341, -0.4496]],
       dtype=torch.float64)
	q_value: tensor([[-9.9735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3139384328504272, distance: 0.9478469976776901 entropy 0.2896064221858978
epoch: 13, step: 127
	action: tensor([[ 0.1326,  0.1730,  0.3145, -0.1213,  0.7115,  0.0480,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-9.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48734277153451044, distance: 0.8193514675585405 entropy 0.2896064221858978
LOSS epoch 13 actor 72.18886825691705 critic 184.98265433466128 
epoch: 14, step: 0
	action: tensor([[-0.4148,  0.4030, -0.2172, -0.2758, -0.2752,  0.1962, -0.0565]],
       dtype=torch.float64)
	q_value: tensor([[-9.5399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06391761992881373, distance: 1.1071685187621285 entropy 0.18424582481384277
epoch: 14, step: 1
	action: tensor([[-0.1389,  0.2890, -0.0144, -0.3594,  0.2711, -0.0841,  0.3457]],
       dtype=torch.float64)
	q_value: tensor([[-8.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18430533612805144, distance: 1.033523852051237 entropy 0.18424582481384277
epoch: 14, step: 2
	action: tensor([[-0.1408,  0.2921,  0.0256, -0.2737,  0.4464, -0.1789, -0.4906]],
       dtype=torch.float64)
	q_value: tensor([[-8.5428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1733773631966069, distance: 1.0404239487289024 entropy 0.18424582481384277
epoch: 14, step: 3
	action: tensor([[-0.4425,  0.2470, -0.0307, -0.6164,  0.0968,  0.0439, -0.3374]],
       dtype=torch.float64)
	q_value: tensor([[-8.1258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23775749578301664, distance: 1.273135053433727 entropy 0.18424582481384277
epoch: 14, step: 4
	action: tensor([[ 0.2452,  0.0289,  0.1724, -0.2397,  0.0343, -0.0364, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-7.7437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37807187890929417, distance: 0.9024575662234042 entropy 0.18424582481384277
epoch: 14, step: 5
	action: tensor([[ 0.2249,  0.2492, -0.3068, -0.1258,  0.7287, -0.2280, -0.3674]],
       dtype=torch.float64)
	q_value: tensor([[-8.6612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5605255578814184, distance: 0.7586186321153318 entropy 0.18424582481384277
epoch: 14, step: 6
	action: tensor([[-0.4496,  0.3404, -0.1893, -0.1687,  0.2787, -0.3927, -0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-9.1711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08090332899725161, distance: 1.1897346780653975 entropy 0.18424582481384277
epoch: 14, step: 7
	action: tensor([[-0.0696, -0.4187,  0.1359, -0.5712,  0.2923, -0.1072, -0.0752]],
       dtype=torch.float64)
	q_value: tensor([[-7.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49291544454445657, distance: 1.3982161005057576 entropy 0.18424582481384277
epoch: 14, step: 8
	action: tensor([[ 0.1215, -0.1344, -0.2369, -0.2307, -0.0963,  0.2928,  0.3209]],
       dtype=torch.float64)
	q_value: tensor([[-8.9725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2504039051425011, distance: 0.9907643036371013 entropy 0.18424582481384277
epoch: 14, step: 9
	action: tensor([[-0.2502, -0.4585, -0.0966, -0.0641,  0.0331, -0.4307,  0.1221]],
       dtype=torch.float64)
	q_value: tensor([[-8.6592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45343764115621843, distance: 1.3796054297238751 entropy 0.18424582481384277
epoch: 14, step: 10
	action: tensor([[ 0.3078,  0.4521, -0.0797, -0.3035,  0.4085, -0.1751,  0.1084]],
       dtype=torch.float64)
	q_value: tensor([[-9.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6924544824390643, distance: 0.6346165504573525 entropy 0.18424582481384277
epoch: 14, step: 11
	action: tensor([[-0.0436, -0.9072, -0.1593,  0.0985,  0.5526,  0.2833, -0.1040]],
       dtype=torch.float64)
	q_value: tensor([[-9.0888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4086958280353903, distance: 1.358204945453586 entropy 0.18424582481384277
epoch: 14, step: 12
	action: tensor([[ 0.2399,  0.4253,  0.3639, -0.2359,  0.1883,  0.3061, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-10.4641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7134201164673126, distance: 0.6126035993743553 entropy 0.18424582481384277
epoch: 14, step: 13
	action: tensor([[ 0.2227,  0.0116, -0.1836,  0.4363,  0.1049,  0.4324, -0.2192]],
       dtype=torch.float64)
	q_value: tensor([[-9.5844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7079077082140939, distance: 0.6184672987419106 entropy 0.18424582481384277
epoch: 14, step: 14
	action: tensor([[ 0.5702, -0.0486, -0.1495, -0.3235,  0.1658,  0.0095, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[-8.7098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4894866309736132, distance: 0.817636467197816 entropy 0.18424582481384277
epoch: 14, step: 15
	action: tensor([[ 0.1378, -0.1510, -0.0697,  0.2169, -0.2427,  0.3493, -0.5539]],
       dtype=torch.float64)
	q_value: tensor([[-9.2638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4782708160021495, distance: 0.8265692764768291 entropy 0.18424582481384277
epoch: 14, step: 16
	action: tensor([[ 0.0766, -0.2325,  0.1892,  0.2690, -0.3016,  0.6368, -0.9305]],
       dtype=torch.float64)
	q_value: tensor([[-8.5854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.518326121384511, distance: 0.7942061408954307 entropy 0.18424582481384277
epoch: 14, step: 17
	action: tensor([[ 0.0618,  0.3038, -0.7015, -0.0614,  0.1933, -0.0498,  0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-10.5863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5750128352158825, distance: 0.7460099161112971 entropy 0.18424582481384277
epoch: 14, step: 18
	action: tensor([[-0.2104, -0.0318,  0.3655, -0.3449,  0.3506,  0.2052,  0.1543]],
       dtype=torch.float64)
	q_value: tensor([[-8.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10873136077645884, distance: 1.2049523116392762 entropy 0.18424582481384277
epoch: 14, step: 19
	action: tensor([[ 0.1598, -0.1307, -0.2241, -0.1692,  0.0330,  0.1400,  0.1923]],
       dtype=torch.float64)
	q_value: tensor([[-9.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28210703507273505, distance: 0.9695864598279794 entropy 0.18424582481384277
epoch: 14, step: 20
	action: tensor([[ 0.3022, -0.4183,  0.1601,  0.4929,  0.3900,  0.2308,  0.2475]],
       dtype=torch.float64)
	q_value: tensor([[-8.1605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6026655132426877, distance: 0.7213313771839853 entropy 0.18424582481384277
epoch: 14, step: 21
	action: tensor([[ 0.2162,  0.1389,  0.1879,  0.0695, -0.1142,  0.2755,  0.0648]],
       dtype=torch.float64)
	q_value: tensor([[-10.9796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6689888753471025, distance: 0.6583820596111447 entropy 0.18424582481384277
epoch: 14, step: 22
	action: tensor([[ 0.3426, -0.2966, -0.3509, -0.0097,  0.7459,  0.2832, -0.4684]],
       dtype=torch.float64)
	q_value: tensor([[-8.9730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40433171344721464, distance: 0.8831997403003076 entropy 0.18424582481384277
epoch: 14, step: 23
	action: tensor([[-0.0169, -0.3747, -0.4287,  0.2713, -0.5647,  0.1909, -0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-9.6149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06736933490182873, distance: 1.1051253440799185 entropy 0.18424582481384277
epoch: 14, step: 24
	action: tensor([[ 0.1688,  0.2476, -0.2952, -0.2530, -0.1049,  0.1365,  0.2063]],
       dtype=torch.float64)
	q_value: tensor([[-8.5676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5702750460342352, distance: 0.7501566786311233 entropy 0.18424582481384277
epoch: 14, step: 25
	action: tensor([[ 0.2605, -0.0985, -0.2611,  0.3020,  0.0603,  1.0427, -0.1446]],
       dtype=torch.float64)
	q_value: tensor([[-8.3566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7294803661026202, distance: 0.5951906344106408 entropy 0.18424582481384277
epoch: 14, step: 26
	action: tensor([[-0.6075,  0.2088,  0.0535,  0.2887, -0.0571,  0.1037,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[-10.4689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07444103975729677, distance: 1.1861728722291256 entropy 0.18424582481384277
epoch: 14, step: 27
	action: tensor([[-0.1879, -0.4668, -0.3802, -0.0660, -0.1773,  0.0815, -0.2074]],
       dtype=torch.float64)
	q_value: tensor([[-8.3452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27278113621367406, distance: 1.2910217480723611 entropy 0.18424582481384277
epoch: 14, step: 28
	action: tensor([[ 0.1269,  0.7382,  0.2656, -0.2147,  0.0903, -0.2306, -0.0935]],
       dtype=torch.float64)
	q_value: tensor([[-7.8721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6544650420504237, distance: 0.6726709755708817 entropy 0.18424582481384277
epoch: 14, step: 29
	action: tensor([[-0.3165, -0.0080, -0.2667, -0.0939, -0.1345, -0.3306,  0.3166]],
       dtype=torch.float64)
	q_value: tensor([[-9.6775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09883568341832483, distance: 1.199563024016518 entropy 0.18424582481384277
epoch: 14, step: 30
	action: tensor([[ 0.1268, -0.1009,  0.0746, -0.6298, -0.2892, -0.0424,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-8.3797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018935089524355853, distance: 1.155127578024836 entropy 0.18424582481384277
epoch: 14, step: 31
	action: tensor([[ 0.0916,  0.0116, -0.3234, -0.3407,  0.2991, -0.2844,  0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-9.3280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22366180373000588, distance: 1.0082823626670212 entropy 0.18424582481384277
epoch: 14, step: 32
	action: tensor([[-0.0724, -0.1570, -0.2547,  0.1870,  0.1828, -0.1532,  0.2720]],
       dtype=torch.float64)
	q_value: tensor([[-8.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14313834466829944, distance: 1.0592831097885083 entropy 0.18424582481384277
epoch: 14, step: 33
	action: tensor([[ 0.0498,  0.2412, -0.2651,  0.3227,  0.3269, -0.1996,  0.3521]],
       dtype=torch.float64)
	q_value: tensor([[-8.6375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5382256575830406, distance: 0.7776274677112258 entropy 0.18424582481384277
epoch: 14, step: 34
	action: tensor([[-0.4659, -0.1122, -0.0742,  0.3250,  0.2927, -0.3027, -0.1161]],
       dtype=torch.float64)
	q_value: tensor([[-9.4282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2541322238962225, distance: 1.2815287589439641 entropy 0.18424582481384277
epoch: 14, step: 35
	action: tensor([[ 0.3909,  0.1816, -0.4216,  0.1261, -0.0100, -0.0047, -0.2580]],
       dtype=torch.float64)
	q_value: tensor([[-8.9074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7343333265767408, distance: 0.5898277919752003 entropy 0.18424582481384277
epoch: 14, step: 36
	action: tensor([[ 0.3699, -0.0621,  0.2124,  0.4161,  0.0073,  0.3781,  0.3171]],
       dtype=torch.float64)
	q_value: tensor([[-8.3576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8371730264736558, distance: 0.46176378431039805 entropy 0.18424582481384277
epoch: 14, step: 37
	action: tensor([[ 0.0893,  0.5469, -0.1809, -0.5553, -0.1222,  0.4587, -0.1730]],
       dtype=torch.float64)
	q_value: tensor([[-10.3573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6283410616681027, distance: 0.6976361621193272 entropy 0.18424582481384277
epoch: 14, step: 38
	action: tensor([[-0.5478, -0.0822,  0.0150, -0.1385, -0.0771, -0.1654, -0.3430]],
       dtype=torch.float64)
	q_value: tensor([[-9.4785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48342645706102094, distance: 1.3937654784916138 entropy 0.18424582481384277
epoch: 14, step: 39
	action: tensor([[-0.3283, -0.0862, -0.2440, -0.3164,  0.0499,  0.4277,  0.1719]],
       dtype=torch.float64)
	q_value: tensor([[-7.7364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14238925653785173, distance: 1.223105019173958 entropy 0.18424582481384277
epoch: 14, step: 40
	action: tensor([[ 0.0280, -0.0378, -0.4024,  0.1793,  0.3849,  0.4563,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-8.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4312926281982783, distance: 0.8629807775925478 entropy 0.18424582481384277
epoch: 14, step: 41
	action: tensor([[-0.0207,  0.0414, -0.4395, -0.0805, -0.1747,  0.0701,  0.2231]],
       dtype=torch.float64)
	q_value: tensor([[-8.3251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883735906030849, distance: 0.9653453784259372 entropy 0.18424582481384277
epoch: 14, step: 42
	action: tensor([[-0.0247, -0.3000,  0.0384,  0.1245, -0.3998, -0.1142,  0.5374]],
       dtype=torch.float64)
	q_value: tensor([[-7.7266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07835849346537305, distance: 1.0985952219804276 entropy 0.18424582481384277
epoch: 14, step: 43
	action: tensor([[ 0.3926,  0.2866, -0.5946, -0.3047,  0.2511, -0.2668, -0.1141]],
       dtype=torch.float64)
	q_value: tensor([[-9.9942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7023103909992121, distance: 0.6243649730434443 entropy 0.18424582481384277
epoch: 14, step: 44
	action: tensor([[-0.1796,  0.6826, -0.3587,  0.1247,  0.4237, -0.1914,  0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-9.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46713554594176787, distance: 0.8353434441374301 entropy 0.18424582481384277
epoch: 14, step: 45
	action: tensor([[ 0.1752,  0.3962,  0.3398, -0.6869, -0.1791,  0.2095, -0.3749]],
       dtype=torch.float64)
	q_value: tensor([[-8.5526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4208439260196378, distance: 0.8708723479248799 entropy 0.18424582481384277
epoch: 14, step: 46
	action: tensor([[-0.4372, -0.0708,  0.0129, -0.3198,  0.1538,  0.2438,  0.6224]],
       dtype=torch.float64)
	q_value: tensor([[-10.4323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3400957681533665, distance: 1.324721580069403 entropy 0.18424582481384277
epoch: 14, step: 47
	action: tensor([[-0.1595, -0.5747, -0.0549, -0.5268,  0.0058, -0.2484,  0.1899]],
       dtype=torch.float64)
	q_value: tensor([[-9.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6322571269094237, distance: 1.4620121439830591 entropy 0.18424582481384277
epoch: 14, step: 48
	action: tensor([[ 0.1998, -0.1231, -0.1598,  0.0091,  0.0756,  0.0387, -0.3613]],
       dtype=torch.float64)
	q_value: tensor([[-9.5491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36753108949842384, distance: 0.9100731141724253 entropy 0.18424582481384277
epoch: 14, step: 49
	action: tensor([[ 0.2039, -0.1385, -0.3390,  0.0349, -0.9823, -0.2431,  0.4009]],
       dtype=torch.float64)
	q_value: tensor([[-7.9986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4024302300869068, distance: 0.8846082855864935 entropy 0.18424582481384277
epoch: 14, step: 50
	action: tensor([[ 0.0598,  0.4044, -0.4707,  0.3087, -0.0869, -0.2341, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-11.0795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.587879583177441, distance: 0.7346301672384068 entropy 0.18424582481384277
epoch: 14, step: 51
	action: tensor([[ 0.0292,  0.4944, -0.1993, -0.5158, -0.2826, -0.0154, -0.2641]],
       dtype=torch.float64)
	q_value: tensor([[-8.3852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5380631507239645, distance: 0.7777642863580148 entropy 0.18424582481384277
epoch: 14, step: 52
	action: tensor([[-0.0997,  0.1654, -0.1158, -0.0304, -0.6908,  0.2363, -0.0929]],
       dtype=torch.float64)
	q_value: tensor([[-8.9316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30904596143500396, distance: 0.9512206571701903 entropy 0.18424582481384277
epoch: 14, step: 53
	action: tensor([[ 0.1909, -0.5286,  0.0055,  0.0183, -0.1033, -0.1903,  0.1879]],
       dtype=torch.float64)
	q_value: tensor([[-9.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043111381750344346, distance: 1.1687511073338595 entropy 0.18424582481384277
epoch: 14, step: 54
	action: tensor([[-0.0740,  0.2989,  0.0708, -0.1800,  0.4398,  0.0721, -0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-9.5538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38527684080084457, distance: 0.8972149073454809 entropy 0.18424582481384277
epoch: 14, step: 55
	action: tensor([[ 0.4222,  0.2071, -0.1035,  0.1820,  0.0738, -0.4968, -0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-8.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7374145038815285, distance: 0.5863974328567721 entropy 0.18424582481384277
epoch: 14, step: 56
	action: tensor([[ 0.5917,  0.2844,  0.2335, -0.4383,  0.2315, -0.0109, -0.3463]],
       dtype=torch.float64)
	q_value: tensor([[-9.9059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6920336114554205, distance: 0.6350506331563446 entropy 0.18424582481384277
epoch: 14, step: 57
	action: tensor([[ 0.4365, -0.1334, -0.4478, -0.2717, -0.1819,  0.0544, -0.1072]],
       dtype=torch.float64)
	q_value: tensor([[-10.2142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4192657855238451, distance: 0.8720580593483769 entropy 0.18424582481384277
epoch: 14, step: 58
	action: tensor([[ 0.3147, -0.2036, -0.0384,  0.3592, -0.1434,  0.4420,  0.2118]],
       dtype=torch.float64)
	q_value: tensor([[-8.6522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6737223224964467, distance: 0.6536576893557033 entropy 0.18424582481384277
epoch: 14, step: 59
	action: tensor([[0.5015, 0.2991, 0.0266, 0.1315, 0.2041, 0.0815, 0.1185]],
       dtype=torch.float64)
	q_value: tensor([[-9.5338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8842305841401551, distance: 0.3893620450172117 entropy 0.18424582481384277
epoch: 14, step: 60
	action: tensor([[-0.3923, -0.3063, -0.0709, -0.1812, -0.3733,  0.0195, -0.7889]],
       dtype=torch.float64)
	q_value: tensor([[-9.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44520730817009535, distance: 1.3756937613544555 entropy 0.18424582481384277
epoch: 14, step: 61
	action: tensor([[ 0.4187,  0.1431,  0.1462, -0.2780, -0.2539, -0.1850, -0.2258]],
       dtype=torch.float64)
	q_value: tensor([[-8.7262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5515564232507023, distance: 0.7663207709586789 entropy 0.18424582481384277
epoch: 14, step: 62
	action: tensor([[ 0.6300, -0.2431,  0.0860, -0.2465, -0.0173,  0.3330,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[-9.5473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45425093976412134, distance: 0.8453823796019914 entropy 0.18424582481384277
epoch: 14, step: 63
	action: tensor([[ 0.3987,  0.7326, -0.1868, -0.1087,  0.2378,  0.0145, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[-10.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8873746160577984, distance: 0.38403856260141983 entropy 0.18424582481384277
epoch: 14, step: 64
	action: tensor([[-0.1066, -0.2719, -0.3482, -0.3018,  0.1976,  0.3878,  0.2265]],
       dtype=torch.float64)
	q_value: tensor([[-9.3536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007911281537733794, distance: 1.1488619511937872 entropy 0.18424582481384277
epoch: 14, step: 65
	action: tensor([[ 0.0815, -0.0638, -0.1031, -0.6763, -0.1844, -0.3606, -0.0349]],
       dtype=torch.float64)
	q_value: tensor([[-8.2899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006411082179226479, distance: 1.1406701131997083 entropy 0.18424582481384277
epoch: 14, step: 66
	action: tensor([[ 0.5897,  0.2405, -0.7038,  0.3611, -0.0514, -0.0311,  0.1886]],
       dtype=torch.float64)
	q_value: tensor([[-9.0421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8561779226177695, distance: 0.4339797566960468 entropy 0.18424582481384277
epoch: 14, step: 67
	action: tensor([[ 0.1206, -0.0917, -0.2385, -0.0522,  0.4300, -0.4690, -0.1635]],
       dtype=torch.float64)
	q_value: tensor([[-9.9878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2133292897282284, distance: 1.014969948112631 entropy 0.18424582481384277
epoch: 14, step: 68
	action: tensor([[ 0.2977, -0.1506,  0.3131, -0.6572, -0.1689, -0.3825,  0.0787]],
       dtype=torch.float64)
	q_value: tensor([[-9.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07069659082504098, distance: 1.1841041499478526 entropy 0.18424582481384277
epoch: 14, step: 69
	action: tensor([[-0.0927,  0.0888,  0.4380, -0.0448,  0.2761,  0.2311, -0.3147]],
       dtype=torch.float64)
	q_value: tensor([[-10.5664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3337449446349131, distance: 0.9340646835156049 entropy 0.18424582481384277
epoch: 14, step: 70
	action: tensor([[ 0.1560,  0.5370,  0.0016, -0.7372,  0.0437, -0.0439, -0.1703]],
       dtype=torch.float64)
	q_value: tensor([[-9.1157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5174525629332605, distance: 0.7949259964259958 entropy 0.18424582481384277
epoch: 14, step: 71
	action: tensor([[ 0.3033, -0.1059, -0.2019, -0.1040,  0.2375,  0.1162,  0.1843]],
       dtype=torch.float64)
	q_value: tensor([[-9.4017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.452439671973321, distance: 0.846784072813386 entropy 0.18424582481384277
epoch: 14, step: 72
	action: tensor([[-0.2185, -0.1081,  0.2315, -0.3125,  0.1470, -0.0816, -0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-8.5728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22722181397660002, distance: 1.2677050676342352 entropy 0.18424582481384277
epoch: 14, step: 73
	action: tensor([[-0.1275, -0.0770, -0.0274,  0.5565,  0.4691,  0.1101, -0.1922]],
       dtype=torch.float64)
	q_value: tensor([[-8.2207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41632238087780615, distance: 0.8742652442068528 entropy 0.18424582481384277
epoch: 14, step: 74
	action: tensor([[-0.2250, -0.7801, -0.1856, -0.0809, -0.5086, -0.3668, -0.3455]],
       dtype=torch.float64)
	q_value: tensor([[-9.4577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5643154943316708, distance: 1.4312610964487902 entropy 0.18424582481384277
epoch: 14, step: 75
	action: tensor([[ 0.1766,  0.0086, -0.3382,  0.1078, -0.3336, -0.0581, -0.2197]],
       dtype=torch.float64)
	q_value: tensor([[-9.9696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4684743220929546, distance: 0.8342934198880329 entropy 0.18424582481384277
epoch: 14, step: 76
	action: tensor([[ 0.1594,  0.2476,  0.3450, -0.5255,  0.5208,  0.1824,  0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-7.9988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3845966911644041, distance: 0.8977111239592426 entropy 0.18424582481384277
epoch: 14, step: 77
	action: tensor([[-0.7733, -0.2115, -0.3278, -0.3118,  0.1752, -0.1913, -0.4127]],
       dtype=torch.float64)
	q_value: tensor([[-9.4700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8251653383912154, distance: 1.5459939837652361 entropy 0.18424582481384277
epoch: 14, step: 78
	action: tensor([[ 0.1839,  0.4163, -0.3281, -0.3858,  0.3570,  0.4478, -0.6284]],
       dtype=torch.float64)
	q_value: tensor([[-8.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7059309804039766, distance: 0.6205565015939897 entropy 0.18424582481384277
epoch: 14, step: 79
	action: tensor([[-0.0055,  0.0638, -0.5238, -0.1481,  0.1287,  0.2178, -0.4752]],
       dtype=torch.float64)
	q_value: tensor([[-8.5319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37384967733191643, distance: 0.9055157271788261 entropy 0.18424582481384277
epoch: 14, step: 80
	action: tensor([[ 0.4327,  0.4361, -0.0032, -0.2330,  0.4025,  0.2422, -0.1870]],
       dtype=torch.float64)
	q_value: tensor([[-7.1291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8476580350746765, distance: 0.4466491066727147 entropy 0.18424582481384277
epoch: 14, step: 81
	action: tensor([[-0.0046, -0.1410, -0.4538, -0.1346,  0.0993,  0.6328, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-9.0097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2953663983508883, distance: 0.9605906785264466 entropy 0.18424582481384277
epoch: 14, step: 82
	action: tensor([[-0.0186,  0.0166, -0.1345, -0.1967,  0.2801,  0.0123, -0.4247]],
       dtype=torch.float64)
	q_value: tensor([[-8.2807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24149832817679007, distance: 0.9966323143960774 entropy 0.18424582481384277
epoch: 14, step: 83
	action: tensor([[ 0.3084,  0.2590, -0.2738,  0.0334,  0.1220, -0.0334, -0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-7.4692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7234350743654292, distance: 0.6018042410341004 entropy 0.18424582481384277
epoch: 14, step: 84
	action: tensor([[ 0.2769, -0.3839, -0.3041, -0.1448,  0.1489,  0.1240, -0.2974]],
       dtype=torch.float64)
	q_value: tensor([[-8.0734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1856957824026133, distance: 1.0326425956298086 entropy 0.18424582481384277
epoch: 14, step: 85
	action: tensor([[ 0.3089, -0.0469, -0.3708, -0.1693, -0.4740, -0.0177,  0.1908]],
       dtype=torch.float64)
	q_value: tensor([[-8.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4878846518465413, distance: 0.8189183245234739 entropy 0.18424582481384277
epoch: 14, step: 86
	action: tensor([[ 0.6369, -0.0634, -0.1115,  0.3106,  0.3314,  0.2824,  0.4154]],
       dtype=torch.float64)
	q_value: tensor([[-9.1241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8647531483155914, distance: 0.42084316238153185 entropy 0.18424582481384277
epoch: 14, step: 87
	action: tensor([[ 0.2609,  0.1399,  0.3255,  0.3939,  0.2351,  0.1782, -0.4240]],
       dtype=torch.float64)
	q_value: tensor([[-10.9228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8369914496778972, distance: 0.46202118093339417 entropy 0.18424582481384277
epoch: 14, step: 88
	action: tensor([[ 0.0760,  0.2394, -0.2629, -0.1139,  0.4173,  0.1135, -0.2487]],
       dtype=torch.float64)
	q_value: tensor([[-9.8518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5495541142193052, distance: 0.7680296838518246 entropy 0.18424582481384277
epoch: 14, step: 89
	action: tensor([[ 0.1399,  0.2881, -0.0707, -0.0070,  0.3465, -0.3037, -0.3376]],
       dtype=torch.float64)
	q_value: tensor([[-7.5306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5618475073794057, distance: 0.7574768014269381 entropy 0.18424582481384277
epoch: 14, step: 90
	action: tensor([[-0.3256, -0.7778, -0.0277, -0.0117, -0.1652, -0.1739,  0.3892]],
       dtype=torch.float64)
	q_value: tensor([[-8.4926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6900543841590374, distance: 1.4876714662381683 entropy 0.18424582481384277
epoch: 14, step: 91
	action: tensor([[ 0.0308,  0.2243,  0.6781, -0.3414,  0.0668, -0.1671, -0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-10.5180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20614574604291913, distance: 1.0195935547032688 entropy 0.18424582481384277
epoch: 14, step: 92
	action: tensor([[ 0.4836,  0.3459, -0.2120, -0.4696, -0.3726,  0.0119, -0.2440]],
       dtype=torch.float64)
	q_value: tensor([[-10.2396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7164464948290716, distance: 0.6093603659245046 entropy 0.18424582481384277
epoch: 14, step: 93
	action: tensor([[-0.7492, -0.2817, -0.5456, -0.2448,  0.1272,  0.0167, -0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-9.6224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7743646478997237, distance: 1.5243269616531787 entropy 0.18424582481384277
epoch: 14, step: 94
	action: tensor([[ 0.3589,  0.0429,  0.2741, -0.5104,  0.2305,  0.1950, -0.4284]],
       dtype=torch.float64)
	q_value: tensor([[-8.2481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37705444733240656, distance: 0.903195443782709 entropy 0.18424582481384277
epoch: 14, step: 95
	action: tensor([[ 0.2070,  0.0675, -0.2653,  0.1583,  0.1957, -0.1800,  0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-9.3829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5365388148582018, distance: 0.7790464935972733 entropy 0.18424582481384277
epoch: 14, step: 96
	action: tensor([[-0.0203,  0.0443, -0.1950, -0.0015,  0.5108,  0.2100, -0.4572]],
       dtype=torch.float64)
	q_value: tensor([[-8.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36848376882113976, distance: 0.9093874404892714 entropy 0.18424582481384277
epoch: 14, step: 97
	action: tensor([[-0.1473, -0.0478,  0.0736, -0.0872,  0.3084, -0.1202,  0.4394]],
       dtype=torch.float64)
	q_value: tensor([[-7.8884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013461347719117223, distance: 1.1366159495740034 entropy 0.18424582481384277
epoch: 14, step: 98
	action: tensor([[ 0.2625,  0.1659, -0.2773,  0.3735, -0.1787,  0.0598,  0.0635]],
       dtype=torch.float64)
	q_value: tensor([[-9.2178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.693651475070272, distance: 0.6333803563494195 entropy 0.18424582481384277
epoch: 14, step: 99
	action: tensor([[-0.2661, -0.1968, -0.7957, -0.2871,  0.1050, -0.0362, -0.4338]],
       dtype=torch.float64)
	q_value: tensor([[-8.4018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07916766900016237, distance: 1.1887790863594727 entropy 0.18424582481384277
epoch: 14, step: 100
	action: tensor([[-0.2346,  0.5173, -0.0472,  0.0391,  0.0346, -0.0521,  0.2373]],
       dtype=torch.float64)
	q_value: tensor([[-7.7546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37390830543620057, distance: 0.9054733332748959 entropy 0.18424582481384277
epoch: 14, step: 101
	action: tensor([[ 0.1141,  0.3024, -0.3109, -0.1847, -0.5468, -0.1917,  0.1333]],
       dtype=torch.float64)
	q_value: tensor([[-8.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.566522881460338, distance: 0.753424575654118 entropy 0.18424582481384277
epoch: 14, step: 102
	action: tensor([[ 0.1079, -0.0270, -0.6778, -0.1372,  0.0463,  0.1940, -0.4779]],
       dtype=torch.float64)
	q_value: tensor([[-9.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40799850827779827, distance: 0.8804771582897941 entropy 0.18424582481384277
epoch: 14, step: 103
	action: tensor([[-0.0433,  0.1032, -0.2841,  0.0075, -0.1346, -0.3506, -0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-7.4160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3346829280878353, distance: 0.9334069430650702 entropy 0.18424582481384277
epoch: 14, step: 104
	action: tensor([[-0.3970, -0.0322,  0.1547, -0.5211,  0.4775,  0.0273, -0.3161]],
       dtype=torch.float64)
	q_value: tensor([[-7.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42597072639441014, distance: 1.3665074328040716 entropy 0.18424582481384277
epoch: 14, step: 105
	action: tensor([[ 0.0744,  0.0277, -0.4429, -0.1832,  0.3799,  0.0897,  0.2701]],
       dtype=torch.float64)
	q_value: tensor([[-8.2710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3568637373901097, distance: 0.9177157644423632 entropy 0.18424582481384277
epoch: 14, step: 106
	action: tensor([[ 0.1165,  0.0510, -0.0381, -0.1953, -0.0839, -0.1821,  0.2095]],
       dtype=torch.float64)
	q_value: tensor([[-8.4304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301999063973853, distance: 0.9560590036023594 entropy 0.18424582481384277
epoch: 14, step: 107
	action: tensor([[-0.2380, -0.5270, -0.1707, -0.0299,  0.5631,  0.3300, -0.5668]],
       dtype=torch.float64)
	q_value: tensor([[-8.3531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28037051838653015, distance: 1.2948651012222874 entropy 0.18424582481384277
epoch: 14, step: 108
	action: tensor([[-0.3015, -0.1018, -0.7207, -0.2729,  0.2752,  0.0134, -0.2433]],
       dtype=torch.float64)
	q_value: tensor([[-9.0603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07351969707574968, distance: 1.1856641862976884 entropy 0.18424582481384277
epoch: 14, step: 109
	action: tensor([[-0.0726, -0.0749,  0.0757, -0.3789,  0.0573,  0.3628, -0.4384]],
       dtype=torch.float64)
	q_value: tensor([[-7.5718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08870987621765303, distance: 1.092408385163046 entropy 0.18424582481384277
epoch: 14, step: 110
	action: tensor([[ 0.1804,  0.2464, -0.4797,  0.0081,  0.4301, -0.3841, -0.0693]],
       dtype=torch.float64)
	q_value: tensor([[-8.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5680868869309059, distance: 0.7520641520886823 entropy 0.18424582481384277
epoch: 14, step: 111
	action: tensor([[-0.4980,  0.3285, -0.3484,  0.0246, -0.2948, -0.3050, -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-8.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0011106281979383903, distance: 1.1437086069607665 entropy 0.18424582481384277
epoch: 14, step: 112
	action: tensor([[ 0.1653, -0.0105, -0.3895, -0.0523,  0.1696, -0.1828, -0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-7.7938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4039821531279313, distance: 0.8834588495307873 entropy 0.18424582481384277
epoch: 14, step: 113
	action: tensor([[-0.1686,  0.2613, -0.7241, -0.1770,  0.1228,  0.0952,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-7.9435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3432181090613061, distance: 0.9274003984158488 entropy 0.18424582481384277
epoch: 14, step: 114
	action: tensor([[-0.1602,  0.5853,  0.1524, -0.0028,  0.2511,  0.5613,  0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-7.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5923085851142802, distance: 0.7306720189631581 entropy 0.18424582481384277
epoch: 14, step: 115
	action: tensor([[ 0.1219, -0.1985, -0.2280, -0.1007,  0.4129, -0.0033,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-9.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18823449534847203, distance: 1.0310316315986812 entropy 0.18424582481384277
epoch: 14, step: 116
	action: tensor([[-0.2237,  0.2084, -0.0244, -0.0196,  0.0727,  0.3218,  0.4166]],
       dtype=torch.float64)
	q_value: tensor([[-8.2608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2730389254837534, distance: 0.9756909388492695 entropy 0.18424582481384277
epoch: 14, step: 117
	action: tensor([[ 0.1155, -0.3649, -0.0276, -0.2978,  0.2102,  0.0081,  0.4048]],
       dtype=torch.float64)
	q_value: tensor([[-8.7928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08555924613945787, distance: 1.1922942750713796 entropy 0.18424582481384277
epoch: 14, step: 118
	action: tensor([[ 0.0781, -0.0725, -0.2527, -0.2246, -0.3478,  0.1757,  0.1108]],
       dtype=torch.float64)
	q_value: tensor([[-9.4718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22820650603187542, distance: 1.0053267763100522 entropy 0.18424582481384277
epoch: 14, step: 119
	action: tensor([[-0.3902,  0.2782,  0.2877,  0.6229,  0.3636,  0.3645, -0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-8.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5147666258876804, distance: 0.7971352697028524 entropy 0.18424582481384277
epoch: 14, step: 120
	action: tensor([[ 0.1603, -0.0508,  0.0987, -0.5250,  0.1353,  0.0748,  0.0761]],
       dtype=torch.float64)
	q_value: tensor([[-9.9141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12219128552222991, distance: 1.0721526843604963 entropy 0.18424582481384277
epoch: 14, step: 121
	action: tensor([[ 0.1107,  0.1573,  0.2306, -0.5391,  0.2268, -0.1711, -0.1300]],
       dtype=torch.float64)
	q_value: tensor([[-8.6689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1616654233614082, distance: 1.0477686083840836 entropy 0.18424582481384277
epoch: 14, step: 122
	action: tensor([[-0.2496,  0.4011,  0.1253, -0.1895,  0.5443, -0.1156,  0.2989]],
       dtype=torch.float64)
	q_value: tensor([[-8.8626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16041038096651195, distance: 1.048552604577224 entropy 0.18424582481384277
epoch: 14, step: 123
	action: tensor([[ 0.1506, -0.2096,  0.4456,  0.0497,  0.0230, -0.0529, -0.3406]],
       dtype=torch.float64)
	q_value: tensor([[-9.1680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26458336690877016, distance: 0.9813488488347031 entropy 0.18424582481384277
epoch: 14, step: 124
	action: tensor([[-0.4946, -0.1263, -0.4433, -0.5528,  0.3516,  0.4451, -0.5547]],
       dtype=torch.float64)
	q_value: tensor([[-9.6390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2959457292109944, distance: 1.302717060296172 entropy 0.18424582481384277
epoch: 14, step: 125
	action: tensor([[ 0.1870, -0.1873, -0.0217,  0.1095,  0.0803, -0.2268,  0.5850]],
       dtype=torch.float64)
	q_value: tensor([[-8.2368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3106715134956425, distance: 0.9501010680773951 entropy 0.18424582481384277
epoch: 14, step: 126
	action: tensor([[-0.0749, -0.7680, -0.3722, -0.1887, -0.0387,  0.2310, -0.1579]],
       dtype=torch.float64)
	q_value: tensor([[-10.0830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4851174189515095, distance: 1.3945596307894217 entropy 0.18424582481384277
epoch: 14, step: 127
	action: tensor([[ 0.6113,  0.1762,  0.4711, -0.1524, -0.3069, -0.1095, -0.3601]],
       dtype=torch.float64)
	q_value: tensor([[-8.7319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7271264619670474, distance: 0.5977745266502152 entropy 0.18424582481384277
LOSS epoch 14 actor 42.57664954735427 critic 32.90710729035585 
epoch: 15, step: 0
	action: tensor([[ 0.2877, -0.4827, -0.0417,  0.0709,  0.0219,  0.0641,  0.0553]],
       dtype=torch.float64)
	q_value: tensor([[-9.9826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16885507973884373, distance: 1.043266040269106 entropy 0.18424582481384277
epoch: 15, step: 1
	action: tensor([[ 0.0427, -0.0062, -0.3700, -0.3576,  0.2543,  0.5544, -0.0694]],
       dtype=torch.float64)
	q_value: tensor([[-7.8201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38848079508550826, distance: 0.8948736981629709 entropy 0.18424582481384277
epoch: 15, step: 2
	action: tensor([[ 0.1257, -0.1830, -0.5286, -0.0594,  0.0512,  0.1045, -0.0261]],
       dtype=torch.float64)
	q_value: tensor([[-6.9526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2947391657372116, distance: 0.9610181203754022 entropy 0.18424582481384277
epoch: 15, step: 3
	action: tensor([[-0.1009, -0.0389,  0.0179, -0.2065,  0.2862, -0.1108,  0.1898]],
       dtype=torch.float64)
	q_value: tensor([[-6.4019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03445293559169671, distance: 1.1244584623283875 entropy 0.18424582481384277
epoch: 15, step: 4
	action: tensor([[-0.5835,  0.1243,  0.0047,  0.0170, -0.1243, -0.2606,  0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-7.1013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3190565787452435, distance: 1.3142815322582608 entropy 0.18424582481384277
epoch: 15, step: 5
	action: tensor([[ 0.0057, -0.2480, -0.0620, -0.1048,  0.2190,  0.3439, -0.1865]],
       dtype=torch.float64)
	q_value: tensor([[-6.9507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.134963008142287, distance: 1.0643244355953543 entropy 0.18424582481384277
epoch: 15, step: 6
	action: tensor([[-0.3109, -0.4640, -0.1418, -0.4128,  0.2948, -0.1135, -0.1323]],
       dtype=torch.float64)
	q_value: tensor([[-6.7600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5838889012355817, distance: 1.4401875460650577 entropy 0.18424582481384277
epoch: 15, step: 7
	action: tensor([[ 0.3889, -0.1499, -0.0839, -0.2485,  0.0581,  0.1415, -0.7940]],
       dtype=torch.float64)
	q_value: tensor([[-7.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36720747431596845, distance: 0.9103059128193075 entropy 0.18424582481384277
epoch: 15, step: 8
	action: tensor([[ 0.3554,  0.7489, -0.2765,  0.0102,  0.2137, -0.2444, -0.1024]],
       dtype=torch.float64)
	q_value: tensor([[-7.7910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8502064821699312, distance: 0.442897473959237 entropy 0.18424582481384277
epoch: 15, step: 9
	action: tensor([[ 0.1522, -0.0845, -0.2171, -0.8052, -0.1303,  0.1492, -0.1899]],
       dtype=torch.float64)
	q_value: tensor([[-8.1691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09822203004321872, distance: 1.0866920853766453 entropy 0.18424582481384277
epoch: 15, step: 10
	action: tensor([[ 0.2517, -0.0013, -0.0466, -0.0682,  0.2083, -0.0629,  0.2218]],
       dtype=torch.float64)
	q_value: tensor([[-7.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4760443315972468, distance: 0.8283310950766519 entropy 0.18424582481384277
epoch: 15, step: 11
	action: tensor([[ 0.4137,  0.3550, -0.4182,  0.2237,  0.4396,  0.5210, -0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-7.5821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.88707318440742, distance: 0.384552141427748 entropy 0.18424582481384277
epoch: 15, step: 12
	action: tensor([[-0.1904, -0.1202, -0.0338, -0.4772,  0.4433, -0.0639,  0.3831]],
       dtype=torch.float64)
	q_value: tensor([[-8.0773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2370395972723789, distance: 1.2727657911479147 entropy 0.18424582481384277
epoch: 15, step: 13
	action: tensor([[ 0.3734, -0.1558,  0.1465,  0.0277, -0.1997,  0.1762, -0.1609]],
       dtype=torch.float64)
	q_value: tensor([[-7.7174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5195299532537627, distance: 0.7932130532121509 entropy 0.18424582481384277
epoch: 15, step: 14
	action: tensor([[ 0.1252,  0.3352, -0.0607, -0.2768,  0.0278,  0.1831,  0.0517]],
       dtype=torch.float64)
	q_value: tensor([[-7.9369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5653809247413244, distance: 0.7544163377534625 entropy 0.18424582481384277
epoch: 15, step: 15
	action: tensor([[ 0.1508,  0.1152,  0.4180,  0.0695, -0.3456,  0.1838, -0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-7.0442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5909968532967029, distance: 0.7318465298769009 entropy 0.18424582481384277
epoch: 15, step: 16
	action: tensor([[-0.1354, -0.3629,  0.1236, -0.1920,  0.4538, -0.0180, -0.1055]],
       dtype=torch.float64)
	q_value: tensor([[-8.5070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2755234057186309, distance: 1.2924117846494834 entropy 0.18424582481384277
epoch: 15, step: 17
	action: tensor([[-0.3836, -0.0648,  0.2991,  0.2299, -0.5829, -0.2250,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[-7.3182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06201528247829469, distance: 1.1792939660129393 entropy 0.18424582481384277
epoch: 15, step: 18
	action: tensor([[-0.0464,  0.4936,  0.2432, -0.0300,  0.1554, -0.3146,  0.1558]],
       dtype=torch.float64)
	q_value: tensor([[-8.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.449422843118917, distance: 0.8491135819608034 entropy 0.18424582481384277
epoch: 15, step: 19
	action: tensor([[-0.2197, -0.1488,  0.0500,  0.3851, -0.3458,  0.3104,  0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-8.0170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21018462798785076, distance: 1.0169965608961615 entropy 0.18424582481384277
epoch: 15, step: 20
	action: tensor([[-0.3759, -0.3856, -0.2298,  0.3912,  0.2911, -0.1116, -0.1853]],
       dtype=torch.float64)
	q_value: tensor([[-7.8436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2915738201397775, distance: 1.300517827763385 entropy 0.18424582481384277
epoch: 15, step: 21
	action: tensor([[-0.3722, -0.2309, -0.4858, -0.3750,  0.7394, -0.0437, -0.4215]],
       dtype=torch.float64)
	q_value: tensor([[-7.7917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32692735347614676, distance: 1.3181968413885095 entropy 0.18424582481384277
epoch: 15, step: 22
	action: tensor([[ 0.1554,  0.4817, -0.5266, -0.4890, -0.2793, -0.2106, -0.0756]],
       dtype=torch.float64)
	q_value: tensor([[-7.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6844560722628152, distance: 0.6428158946196784 entropy 0.18424582481384277
epoch: 15, step: 23
	action: tensor([[-0.1647, -0.6857, -0.1341,  0.0133,  0.1345, -0.6119,  0.1064]],
       dtype=torch.float64)
	q_value: tensor([[-7.5447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5313868532585613, distance: 1.4161170459937282 entropy 0.18424582481384277
epoch: 15, step: 24
	action: tensor([[ 0.0055, -0.1048, -0.2499, -0.2310,  0.4759, -0.1761, -0.3769]],
       dtype=torch.float64)
	q_value: tensor([[-9.3164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09651459404922058, distance: 1.0877203760355763 entropy 0.18424582481384277
epoch: 15, step: 25
	action: tensor([[ 0.3674,  0.4479,  0.2569,  0.2819,  0.3081, -0.0825, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-6.8573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8802992953964933, distance: 0.395917817435741 entropy 0.18424582481384277
epoch: 15, step: 26
	action: tensor([[-0.2674,  0.3380, -0.5134, -0.5312,  0.3402,  0.7308, -0.6617]],
       dtype=torch.float64)
	q_value: tensor([[-8.5536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37791871973092817, distance: 0.902568681275164 entropy 0.18424582481384277
epoch: 15, step: 27
	action: tensor([[ 0.3088, -0.3252, -0.3006, -0.2518,  0.0245,  0.2596, -0.1500]],
       dtype=torch.float64)
	q_value: tensor([[-7.6600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24482121523251255, distance: 0.9944468666090021 entropy 0.18424582481384277
epoch: 15, step: 28
	action: tensor([[ 0.1661,  0.2038,  0.2434, -0.0578,  0.2050,  0.7371,  0.0630]],
       dtype=torch.float64)
	q_value: tensor([[-7.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7548398007453186, distance: 0.5666066614412488 entropy 0.18424582481384277
epoch: 15, step: 29
	action: tensor([[-0.1892, -0.1205, -0.1933, -0.1755, -0.2130, -0.0336, -0.2262]],
       dtype=torch.float64)
	q_value: tensor([[-8.2679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0385236068218584, distance: 1.1661780955609788 entropy 0.18424582481384277
epoch: 15, step: 30
	action: tensor([[-0.6143, -0.1144,  0.2129, -0.4833, -0.2742,  0.0669,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[-6.3390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7471725105255886, distance: 1.5126017108707304 entropy 0.18424582481384277
epoch: 15, step: 31
	action: tensor([[ 0.2642,  0.0151,  0.4060, -0.2530,  0.3967, -0.2343,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-7.8711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3132555791537337, distance: 0.9483185878991295 entropy 0.18424582481384277
epoch: 15, step: 32
	action: tensor([[-0.2153,  0.0991, -0.5695, -0.1908, -0.2296, -0.1651,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[-8.6240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11647931654252053, distance: 1.0756353181926241 entropy 0.18424582481384277
epoch: 15, step: 33
	action: tensor([[ 0.2299,  0.0560,  0.1032,  0.1894, -0.3504, -0.2860, -0.5629]],
       dtype=torch.float64)
	q_value: tensor([[-6.4452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5704052869686986, distance: 0.7500429913645984 entropy 0.18424582481384277
epoch: 15, step: 34
	action: tensor([[ 0.1499,  0.2468, -0.4881, -0.0989, -0.4900, -0.3914,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-8.3870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.619333126531445, distance: 0.7060398854821406 entropy 0.18424582481384277
epoch: 15, step: 35
	action: tensor([[ 0.0873,  0.1505, -0.0898, -0.5475,  0.2491,  0.3104, -0.4085]],
       dtype=torch.float64)
	q_value: tensor([[-7.7877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3829750512011364, distance: 0.8988931183189379 entropy 0.18424582481384277
epoch: 15, step: 36
	action: tensor([[ 0.9685,  0.3412, -0.3179,  0.1784, -0.3472,  0.1558, -0.4423]],
       dtype=torch.float64)
	q_value: tensor([[-6.7793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9718070886628373, distance: 0.19214392415165432 entropy 0.18424582481384277
epoch: 15, step: 37
	action: tensor([[ 0.2068,  0.2794,  0.2402, -0.4590,  0.6579,  0.2913, -0.5444]],
       dtype=torch.float64)
	q_value: tensor([[-9.9943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5474543451888472, distance: 0.7698177009772638 entropy 0.18424582481384277
epoch: 15, step: 38
	action: tensor([[-0.0174, -0.1035,  0.2597, -0.1499, -0.4124, -0.1819, -0.3479]],
       dtype=torch.float64)
	q_value: tensor([[-7.9052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09403549454820259, distance: 1.089211667295148 entropy 0.18424582481384277
epoch: 15, step: 39
	action: tensor([[-0.0276, -0.2316, -0.2390,  0.0410,  0.2531, -0.1611, -0.4622]],
       dtype=torch.float64)
	q_value: tensor([[-7.7759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08220080356440507, distance: 1.0963028164450925 entropy 0.18424582481384277
epoch: 15, step: 40
	action: tensor([[-0.0712,  0.0216, -0.3668,  0.4056, -0.2720, -0.2421,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[-6.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3461367181196535, distance: 0.9253375119597057 entropy 0.18424582481384277
epoch: 15, step: 41
	action: tensor([[ 0.1751, -0.4143, -0.0239,  0.4017,  0.3514, -0.4269,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-7.1977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20575017496511294, distance: 1.0198475506281404 entropy 0.18424582481384277
epoch: 15, step: 42
	action: tensor([[ 0.0404,  0.2196, -0.0682, -0.0227, -0.0864, -0.2879,  0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-9.1505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4488429359507945, distance: 0.8495606377959846 entropy 0.18424582481384277
epoch: 15, step: 43
	action: tensor([[-0.4962,  0.1107, -0.2052, -0.4218, -0.0323,  0.5155, -0.1012]],
       dtype=torch.float64)
	q_value: tensor([[-7.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20573295867960284, distance: 1.256557189971185 entropy 0.18424582481384277
epoch: 15, step: 44
	action: tensor([[-0.1521, -0.0426, -0.0776, -0.2441, -0.0175, -0.0509, -0.4761]],
       dtype=torch.float64)
	q_value: tensor([[-7.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015170467463577197, distance: 1.1356309628977297 entropy 0.18424582481384277
epoch: 15, step: 45
	action: tensor([[-0.3056,  0.2062, -0.3982, -0.3666, -0.3610,  0.5554, -0.0939]],
       dtype=torch.float64)
	q_value: tensor([[-6.4569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06401587818715238, distance: 1.1071104088630317 entropy 0.18424582481384277
epoch: 15, step: 46
	action: tensor([[ 0.5293,  0.0686,  0.2467,  0.1341,  0.4315, -0.1896,  0.3871]],
       dtype=torch.float64)
	q_value: tensor([[-7.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7591933032327918, distance: 0.5615532866899263 entropy 0.18424582481384277
epoch: 15, step: 47
	action: tensor([[-0.8116,  0.1290, -0.4823, -0.5068,  0.1384, -0.4820, -0.0467]],
       dtype=torch.float64)
	q_value: tensor([[-9.7096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5718346177852192, distance: 1.4346967613793102 entropy 0.18424582481384277
epoch: 15, step: 48
	action: tensor([[-0.1186, -0.1494, -0.1678, -0.0316,  0.5092, -0.3512, -0.8567]],
       dtype=torch.float64)
	q_value: tensor([[-7.0970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08030648064142953, distance: 1.1894061615282427 entropy 0.18424582481384277
epoch: 15, step: 49
	action: tensor([[ 0.1518,  0.1490, -0.5000, -0.1448, -0.3904,  0.0818, -0.1201]],
       dtype=torch.float64)
	q_value: tensor([[-8.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49509707530010083, distance: 0.8131312208226904 entropy 0.18424582481384277
epoch: 15, step: 50
	action: tensor([[ 0.2900,  0.0974, -0.2749,  0.1205, -0.1033, -0.0892,  0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-6.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.632034329618858, distance: 0.6941612150789971 entropy 0.18424582481384277
epoch: 15, step: 51
	action: tensor([[-0.4083,  0.0462,  0.0010, -0.1710, -0.1397, -0.1022,  0.2721]],
       dtype=torch.float64)
	q_value: tensor([[-7.0897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20667552132207034, distance: 1.257048240862248 entropy 0.18424582481384277
epoch: 15, step: 52
	action: tensor([[-0.2123, -0.1477, -0.2648, -0.2404,  0.0257,  0.1170,  0.2218]],
       dtype=torch.float64)
	q_value: tensor([[-7.0958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09728936001016919, distance: 1.198718691489373 entropy 0.18424582481384277
epoch: 15, step: 53
	action: tensor([[-0.0601, -0.2534,  0.0414, -0.1115,  0.0405,  0.0529, -0.0586]],
       dtype=torch.float64)
	q_value: tensor([[-6.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023236573861403542, distance: 1.157563223977992 entropy 0.18424582481384277
epoch: 15, step: 54
	action: tensor([[-0.5654,  0.0022, -0.1407, -0.2135, -0.2198,  0.1038,  0.3313]],
       dtype=torch.float64)
	q_value: tensor([[-6.8379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41175189249999344, distance: 1.359677411343849 entropy 0.18424582481384277
epoch: 15, step: 55
	action: tensor([[ 0.4715, -0.1417,  0.2538, -0.1043, -0.1210, -0.0554,  0.2080]],
       dtype=torch.float64)
	q_value: tensor([[-7.2647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4556126894263287, distance: 0.8443270244329569 entropy 0.18424582481384277
epoch: 15, step: 56
	action: tensor([[ 9.6751e-01,  2.3015e-01, -1.8954e-01, -8.7509e-02, -9.0348e-02,
         -4.9342e-01, -2.5223e-04]], dtype=torch.float64)
	q_value: tensor([[-8.5925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7575540787994185, distance: 0.5634613537367505 entropy 0.18424582481384277
epoch: 15, step: 57
	action: tensor([[ 0.3077,  0.3392, -0.1165, -0.1446, -0.0914,  0.5075,  0.5949]],
       dtype=torch.float64)
	q_value: tensor([[-10.3795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7880598346341331, distance: 0.5268213024515533 entropy 0.18424582481384277
epoch: 15, step: 58
	action: tensor([[-0.0745, -0.0602, -0.2981,  0.2091,  0.2383,  0.2937,  0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-9.0950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3306771773900611, distance: 0.936212661271022 entropy 0.18424582481384277
epoch: 15, step: 59
	action: tensor([[ 0.4617, -0.5295, -0.0102, -0.6886, -0.0637,  0.2157,  0.1216]],
       dtype=torch.float64)
	q_value: tensor([[-6.7964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24944715281269847, distance: 1.2791328109483597 entropy 0.18424582481384277
epoch: 15, step: 60
	action: tensor([[ 0.2538, -0.1016,  0.4299,  0.1194, -0.1280, -0.2766,  0.1953]],
       dtype=torch.float64)
	q_value: tensor([[-9.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4386482921426077, distance: 0.8573817152836745 entropy 0.18424582481384277
epoch: 15, step: 61
	action: tensor([[-0.0160, -0.0528, -0.2705, -0.3876,  0.1936, -0.2576, -0.1159]],
       dtype=torch.float64)
	q_value: tensor([[-8.8153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06208749804238223, distance: 1.108250295309344 entropy 0.18424582481384277
epoch: 15, step: 62
	action: tensor([[ 0.0233,  0.0396,  0.1668, -0.0717,  0.1881,  0.6619, -0.2349]],
       dtype=torch.float64)
	q_value: tensor([[-6.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5093178528030644, distance: 0.8015983633247641 entropy 0.18424582481384277
epoch: 15, step: 63
	action: tensor([[ 0.1773, -0.0324, -0.0577, -0.2206,  0.2595, -0.4252, -0.3110]],
       dtype=torch.float64)
	q_value: tensor([[-7.4811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21499008371760775, distance: 1.0138979960345271 entropy 0.18424582481384277
epoch: 15, step: 64
	action: tensor([[-0.4637,  0.5265, -0.4239, -0.1315, -0.2291, -0.1736, -0.0596]],
       dtype=torch.float64)
	q_value: tensor([[-7.5241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12245394714854807, distance: 1.0719922653348999 entropy 0.18424582481384277
epoch: 15, step: 65
	action: tensor([[ 0.3526,  0.0622, -0.1598, -0.2695,  0.2774, -0.0757, -0.0353]],
       dtype=torch.float64)
	q_value: tensor([[-6.5473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4909201539790986, distance: 0.8164876974203694 entropy 0.18424582481384277
epoch: 15, step: 66
	action: tensor([[ 0.3917,  0.5695, -0.5652, -0.1320, -0.2552,  0.0028,  0.6647]],
       dtype=torch.float64)
	q_value: tensor([[-7.2894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8691892143663963, distance: 0.4138838404396732 entropy 0.18424582481384277
epoch: 15, step: 67
	action: tensor([[-0.0823, -0.2069, -0.2469, -0.2305,  0.4709,  0.2501, -0.4035]],
       dtype=torch.float64)
	q_value: tensor([[-9.4600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07656846439436393, distance: 1.0996615603805922 entropy 0.18424582481384277
epoch: 15, step: 68
	action: tensor([[ 0.2797,  0.2549, -0.3295, -0.2160, -0.0322,  0.3202,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[-6.5571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.694911548037195, distance: 0.6320764038509128 entropy 0.18424582481384277
epoch: 15, step: 69
	action: tensor([[-0.1311,  0.1547, -0.1147, -0.0658, -0.0644, -0.2528, -0.2552]],
       dtype=torch.float64)
	q_value: tensor([[-7.1035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24080705927857693, distance: 0.9970863569186487 entropy 0.18424582481384277
epoch: 15, step: 70
	action: tensor([[ 0.6711, -0.0911, -0.1399, -0.2792, -0.3007,  0.1495, -0.6126]],
       dtype=torch.float64)
	q_value: tensor([[-6.5601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.539908266712581, distance: 0.7762094188332319 entropy 0.18424582481384277
epoch: 15, step: 71
	action: tensor([[-0.1858,  0.0746, -0.2279, -0.2195,  0.1026,  0.0070, -0.3962]],
       dtype=torch.float64)
	q_value: tensor([[-8.7361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12025161486517133, distance: 1.073336584402131 entropy 0.18424582481384277
epoch: 15, step: 72
	action: tensor([[-0.6646, -0.3908, -0.1617, -0.2276,  0.2238, -0.1036,  0.0305]],
       dtype=torch.float64)
	q_value: tensor([[-5.9632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8267531537881361, distance: 1.54666631173165 entropy 0.18424582481384277
epoch: 15, step: 73
	action: tensor([[ 0.0152, -0.3879, -0.5548,  0.0187, -0.0327, -0.1237, -0.1553]],
       dtype=torch.float64)
	q_value: tensor([[-7.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049231247928094746, distance: 1.1721745864559947 entropy 0.18424582481384277
epoch: 15, step: 74
	action: tensor([[-0.5308,  0.2285, -0.3346, -0.0026, -0.2885,  0.3584, -0.7567]],
       dtype=torch.float64)
	q_value: tensor([[-6.8996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18897445883713626, distance: 1.2477941808436048 entropy 0.18424582481384277
epoch: 15, step: 75
	action: tensor([[-0.0175,  0.0046,  0.2602, -0.4408, -0.1093,  0.4083, -0.2377]],
       dtype=torch.float64)
	q_value: tensor([[-6.9523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.129127658914586, distance: 1.0679082510848008 entropy 0.18424582481384277
epoch: 15, step: 76
	action: tensor([[ 0.4605, -0.5355, -0.1938, -0.1368,  0.0872,  0.2713, -0.0482]],
       dtype=torch.float64)
	q_value: tensor([[-7.8813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12676317214214394, distance: 1.0693569957555302 entropy 0.18424582481384277
epoch: 15, step: 77
	action: tensor([[ 0.3753, -0.5440, -0.1773, -0.5316,  0.2035,  0.1069,  0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-8.0003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20421790171836562, distance: 1.255767481885925 entropy 0.18424582481384277
epoch: 15, step: 78
	action: tensor([[ 0.3900,  0.0041, -0.3671, -0.4010,  0.1007,  0.0795, -0.4623]],
       dtype=torch.float64)
	q_value: tensor([[-8.2888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4632035742345668, distance: 0.838419751725512 entropy 0.18424582481384277
epoch: 15, step: 79
	action: tensor([[ 0.0881, -0.3802, -0.4519,  0.0367, -0.3814, -0.0314, -0.2508]],
       dtype=torch.float64)
	q_value: tensor([[-7.1226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09692360555771229, distance: 1.087474140451037 entropy 0.18424582481384277
epoch: 15, step: 80
	action: tensor([[ 0.2774, -0.3610,  0.1807,  0.0133,  0.4563, -0.0209,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-7.0559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21592254438097358, distance: 1.0132956463727711 entropy 0.18424582481384277
epoch: 15, step: 81
	action: tensor([[-0.2605, -0.0316,  0.1123, -0.0243,  0.3264,  0.0577,  0.1794]],
       dtype=torch.float64)
	q_value: tensor([[-8.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0024691352768707597, distance: 1.1429306104581778 entropy 0.18424582481384277
epoch: 15, step: 82
	action: tensor([[ 0.3669, -0.6867, -0.0692, -0.0123,  0.6506, -0.1652,  0.4182]],
       dtype=torch.float64)
	q_value: tensor([[-7.2427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13546406915251197, distance: 1.219392139725448 entropy 0.18424582481384277
epoch: 15, step: 83
	action: tensor([[ 0.4241,  0.2018,  0.0023, -0.3959, -0.0373, -0.3381, -0.0921]],
       dtype=torch.float64)
	q_value: tensor([[-9.9426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5173438052394228, distance: 0.7950155725388617 entropy 0.18424582481384277
epoch: 15, step: 84
	action: tensor([[-0.1239,  0.1704, -0.4626,  0.1485,  0.2401, -0.2751, -0.5509]],
       dtype=torch.float64)
	q_value: tensor([[-8.1468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27799834153231995, distance: 0.9723571025828963 entropy 0.18424582481384277
epoch: 15, step: 85
	action: tensor([[-0.1104,  0.3973,  0.1400,  0.1010, -0.0024, -0.1730, -0.1690]],
       dtype=torch.float64)
	q_value: tensor([[-7.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4497299715228934, distance: 0.8488767184487663 entropy 0.18424582481384277
epoch: 15, step: 86
	action: tensor([[-0.0698,  0.1289, -0.1155,  0.0847,  0.1017, -0.1189, -0.4688]],
       dtype=torch.float64)
	q_value: tensor([[-7.3664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34541373155277644, distance: 0.9258489504966223 entropy 0.18424582481384277
epoch: 15, step: 87
	action: tensor([[-0.2700,  0.5495,  0.2095,  0.1369,  0.3330,  0.3104,  0.1677]],
       dtype=torch.float64)
	q_value: tensor([[-6.7340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49501037858956043, distance: 0.8132010290696738 entropy 0.18424582481384277
epoch: 15, step: 88
	action: tensor([[-0.1972,  0.0512,  0.2246,  0.2503, -0.2891,  0.0978,  0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-7.8089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2912313256743817, distance: 0.9634051212440892 entropy 0.18424582481384277
epoch: 15, step: 89
	action: tensor([[-0.0297,  0.0676, -0.1923,  0.1598, -0.2038, -0.0485,  0.2334]],
       dtype=torch.float64)
	q_value: tensor([[-7.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3549413287629338, distance: 0.9190863202671269 entropy 0.18424582481384277
epoch: 15, step: 90
	action: tensor([[ 0.5467,  0.2145, -0.1027, -0.5215,  0.2881,  0.0591, -0.0438]],
       dtype=torch.float64)
	q_value: tensor([[-6.9832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6289330380440894, distance: 0.697080345125271 entropy 0.18424582481384277
epoch: 15, step: 91
	action: tensor([[-0.4554,  0.2143, -0.0426, -0.5141,  0.0561, -0.0950, -0.3664]],
       dtype=torch.float64)
	q_value: tensor([[-8.2215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24891092318234032, distance: 1.278858296527269 entropy 0.18424582481384277
epoch: 15, step: 92
	action: tensor([[ 0.0389,  0.3238, -0.0187, -0.1961,  0.1600,  0.0122,  0.1403]],
       dtype=torch.float64)
	q_value: tensor([[-6.4205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4682282793477892, distance: 0.8344864943737008 entropy 0.18424582481384277
epoch: 15, step: 93
	action: tensor([[ 0.2140,  0.4337,  0.0555, -0.0597,  0.1421,  0.0360,  0.2348]],
       dtype=torch.float64)
	q_value: tensor([[-6.9196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7087757730460928, distance: 0.617547607946358 entropy 0.18424582481384277
epoch: 15, step: 94
	action: tensor([[-0.3413,  0.0738, -0.4382, -0.0547,  0.3892,  0.4806, -0.1192]],
       dtype=torch.float64)
	q_value: tensor([[-7.6297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.095682599377924, distance: 1.0882210865641848 entropy 0.18424582481384277
epoch: 15, step: 95
	action: tensor([[ 0.2552,  0.0948, -0.4285,  0.1080,  0.2840,  0.6635,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-6.4994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7058878230667346, distance: 0.6206020361146193 entropy 0.18424582481384277
epoch: 15, step: 96
	action: tensor([[-0.5268,  0.0737,  0.0171, -0.5897, -0.0634,  0.1534, -0.4977]],
       dtype=torch.float64)
	q_value: tensor([[-7.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44799906689133406, distance: 1.3770218587371001 entropy 0.18424582481384277
epoch: 15, step: 97
	action: tensor([[ 0.0564,  0.0403, -0.4123, -0.1872,  0.4190,  0.4056, -0.2635]],
       dtype=torch.float64)
	q_value: tensor([[-6.9570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41853781395723166, distance: 0.8726044664944319 entropy 0.18424582481384277
epoch: 15, step: 98
	action: tensor([[-0.2924,  0.0446, -0.2489,  0.5036,  0.0789, -0.1609,  0.1905]],
       dtype=torch.float64)
	q_value: tensor([[-6.5577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14978566976546537, distance: 1.055166279052976 entropy 0.18424582481384277
epoch: 15, step: 99
	action: tensor([[ 0.4450,  0.6944, -0.3633, -0.4943,  0.4651, -0.1796,  0.0465]],
       dtype=torch.float64)
	q_value: tensor([[-7.4806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.881958580093973, distance: 0.3931641456284826 entropy 0.18424582481384277
epoch: 15, step: 100
	action: tensor([[-0.1980,  0.0658, -0.3654, -0.1908,  0.0866,  0.3120,  0.4081]],
       dtype=torch.float64)
	q_value: tensor([[-8.8136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16389729470936343, distance: 1.0463729584281498 entropy 0.18424582481384277
epoch: 15, step: 101
	action: tensor([[ 0.0640,  0.1847,  0.0667,  0.3414,  0.2222,  0.1232, -0.4387]],
       dtype=torch.float64)
	q_value: tensor([[-7.2472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6420660905681188, distance: 0.6846334516259441 entropy 0.18424582481384277
epoch: 15, step: 102
	action: tensor([[-0.1854, -0.3641,  0.2037,  0.2366,  0.0916,  0.2930, -0.2532]],
       dtype=torch.float64)
	q_value: tensor([[-7.5350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09561155559368473, distance: 1.0882638314155375 entropy 0.18424582481384277
epoch: 15, step: 103
	action: tensor([[ 0.8627,  0.1226, -0.3716, -0.3091,  0.2210, -0.2269, -0.3203]],
       dtype=torch.float64)
	q_value: tensor([[-7.5604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6366509236617957, distance: 0.689792905788836 entropy 0.18424582481384277
epoch: 15, step: 104
	action: tensor([[-0.3205,  0.0552, -0.0233,  0.1173,  0.0757,  0.0237, -0.2093]],
       dtype=torch.float64)
	q_value: tensor([[-9.5415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06698858476269887, distance: 1.1053509069458498 entropy 0.18424582481384277
epoch: 15, step: 105
	action: tensor([[ 0.2039,  0.2026, -0.3814, -0.0401,  0.1936,  0.4220,  0.2504]],
       dtype=torch.float64)
	q_value: tensor([[-6.5160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6623795969210152, distance: 0.6649225094928826 entropy 0.18424582481384277
epoch: 15, step: 106
	action: tensor([[ 0.1874,  0.2965, -0.6732, -0.5218,  0.0409,  0.2327, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-7.3418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6497728515068038, distance: 0.6772228410453414 entropy 0.18424582481384277
epoch: 15, step: 107
	action: tensor([[ 0.0104, -0.1479, -0.6317,  0.4013,  0.3130,  0.3090,  0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-7.2706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090477806096078, distance: 0.9512194049612845 entropy 0.18424582481384277
epoch: 15, step: 108
	action: tensor([[ 0.5242, -0.1385, -0.1388, -0.2719,  0.2097, -0.4670,  0.2746]],
       dtype=torch.float64)
	q_value: tensor([[-7.5464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29316384278306074, distance: 0.9620908224000361 entropy 0.18424582481384277
epoch: 15, step: 109
	action: tensor([[ 0.1321,  0.0764, -0.1503, -0.2159, -0.0192,  0.0722,  0.1460]],
       dtype=torch.float64)
	q_value: tensor([[-9.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4173637322686582, distance: 0.873484998627391 entropy 0.18424582481384277
epoch: 15, step: 110
	action: tensor([[ 0.6430,  0.2640, -0.3650, -0.0697,  0.5954,  0.3247, -0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-6.8194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9105722118215587, distance: 0.34221019047786394 entropy 0.18424582481384277
epoch: 15, step: 111
	action: tensor([[-0.2642, -0.5251, -0.0244, -0.3120,  0.1197, -0.2904,  0.7607]],
       dtype=torch.float64)
	q_value: tensor([[-8.7728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6224159170258254, distance: 1.4575980964825133 entropy 0.18424582481384277
epoch: 15, step: 112
	action: tensor([[ 0.0696,  0.3406, -0.7956, -0.4403,  0.1390,  0.0465, -0.1929]],
       dtype=torch.float64)
	q_value: tensor([[-9.6691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6040262097061416, distance: 0.7200951960298266 entropy 0.18424582481384277
epoch: 15, step: 113
	action: tensor([[ 0.1256,  0.5429, -0.1467, -0.1133,  0.4574,  0.8277,  0.2182]],
       dtype=torch.float64)
	q_value: tensor([[-6.9845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7940430890833188, distance: 0.5193317540682787 entropy 0.18424582481384277
epoch: 15, step: 114
	action: tensor([[ 0.1216, -0.2542, -0.4224,  0.0378,  0.0254, -0.2848, -0.4670]],
       dtype=torch.float64)
	q_value: tensor([[-8.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1947848152950996, distance: 1.0268633919246184 entropy 0.18424582481384277
epoch: 15, step: 115
	action: tensor([[ 0.2566, -0.1123,  0.1122,  0.1080,  0.8780,  0.1769, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-7.2671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5492048096886307, distance: 0.7683274157961575 entropy 0.18424582481384277
epoch: 15, step: 116
	action: tensor([[ 0.4973,  0.4623, -0.5553, -0.4837,  0.1446, -0.1694, -0.1700]],
       dtype=torch.float64)
	q_value: tensor([[-8.6941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8212181322210466, distance: 0.4838585597039883 entropy 0.18424582481384277
epoch: 15, step: 117
	action: tensor([[ 0.8400, -0.1746,  0.0583,  0.1682,  0.2351,  0.0019, -0.1336]],
       dtype=torch.float64)
	q_value: tensor([[-8.2680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6971101387787033, distance: 0.6297947867876802 entropy 0.18424582481384277
epoch: 15, step: 118
	action: tensor([[-0.3444, -0.4518,  0.0322,  0.2180,  0.0134,  0.2056, -0.1478]],
       dtype=torch.float64)
	q_value: tensor([[-9.5989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.226711432566292, distance: 1.2674414313553535 entropy 0.18424582481384277
epoch: 15, step: 119
	action: tensor([[-0.3159,  0.0765, -0.2816, -0.2399,  0.4281,  0.4412,  0.0980]],
       dtype=torch.float64)
	q_value: tensor([[-7.4133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08614559729665316, distance: 1.0939442692289023 entropy 0.18424582481384277
epoch: 15, step: 120
	action: tensor([[ 0.0109, -0.0531, -0.2831, -0.0744, -0.0605, -0.4381,  0.0471]],
       dtype=torch.float64)
	q_value: tensor([[-6.8410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18040056137610794, distance: 1.035994666067102 entropy 0.18424582481384277
epoch: 15, step: 121
	action: tensor([[ 0.3470, -0.1881, -0.1336, -0.0292,  0.0829,  0.1935,  0.1324]],
       dtype=torch.float64)
	q_value: tensor([[-7.2226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4563050929311967, distance: 0.8437899059236273 entropy 0.18424582481384277
epoch: 15, step: 122
	action: tensor([[-0.1395,  0.0946,  0.1289, -0.0990, -0.2327,  0.1819,  0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-7.4660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21891577500132509, distance: 1.0113596592244485 entropy 0.18424582481384277
epoch: 15, step: 123
	action: tensor([[ 0.4208,  0.0382,  0.0574,  0.0563, -0.2229,  0.0461, -0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-7.3538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6740695840079654, distance: 0.6533097486258107 entropy 0.18424582481384277
epoch: 15, step: 124
	action: tensor([[ 0.3461,  0.1289,  0.0664, -0.4021, -0.1582, -0.4205,  0.4534]],
       dtype=torch.float64)
	q_value: tensor([[-7.8739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39805690494872614, distance: 0.8878393954752928 entropy 0.18424582481384277
epoch: 15, step: 125
	action: tensor([[ 0.6335,  0.0424, -0.1767,  0.6105,  0.0810,  0.1896, -0.2517]],
       dtype=torch.float64)
	q_value: tensor([[-8.8643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9316016147342379, distance: 0.2992813249250063 entropy 0.18424582481384277
epoch: 15, step: 126
	action: tensor([[ 0.3545,  0.0410, -0.1768, -0.2260,  0.1975,  0.4434, -0.7146]],
       dtype=torch.float64)
	q_value: tensor([[-8.8608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6575138866020069, distance: 0.6696967276006945 entropy 0.18424582481384277
epoch: 15, step: 127
	action: tensor([[ 0.4563, -0.0293, -0.2472, -0.4684,  0.1220,  0.2367,  0.2049]],
       dtype=torch.float64)
	q_value: tensor([[-7.5646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4830906983130743, distance: 0.8227423766603101 entropy 0.18424582481384277
LOSS epoch 15 actor 32.23181745365945 critic 29.794201269060714 
epoch: 16, step: 0
	action: tensor([[-0.2106, -0.3324, -0.2511, -0.4420,  0.0531, -0.2964, -0.3645]],
       dtype=torch.float64)
	q_value: tensor([[-6.7560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3245189095572134, distance: 1.3169999992718613 entropy 0.07888543605804443
epoch: 16, step: 1
	action: tensor([[-0.2099,  0.1142, -0.5183, -0.2263, -0.0123, -0.1656, -0.3781]],
       dtype=torch.float64)
	q_value: tensor([[-5.6869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13377128445708242, distance: 1.0650573199130637 entropy 0.07888543605804443
epoch: 16, step: 2
	action: tensor([[ 0.1439,  0.1973, -0.2027, -0.4549,  0.7589,  0.0480, -0.3512]],
       dtype=torch.float64)
	q_value: tensor([[-4.9899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43014543169183717, distance: 0.863850741500053 entropy 0.07888543605804443
epoch: 16, step: 3
	action: tensor([[ 0.1540,  0.1565, -0.6403,  0.0970,  0.4269,  0.2027, -0.4910]],
       dtype=torch.float64)
	q_value: tensor([[-6.0107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5730685480562416, distance: 0.7477144409404387 entropy 0.07888543605804443
epoch: 16, step: 4
	action: tensor([[-0.0160,  0.2819, -0.4032,  0.5608,  0.0724,  0.0458, -0.2673]],
       dtype=torch.float64)
	q_value: tensor([[-5.8380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5006484484158732, distance: 0.8086487044546137 entropy 0.07888543605804443
epoch: 16, step: 5
	action: tensor([[ 0.1408,  0.0527, -0.1038, -0.6428,  0.3029,  0.0435,  0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-6.0791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20607358048424995, distance: 1.0196398968788052 entropy 0.07888543605804443
epoch: 16, step: 6
	action: tensor([[-0.1650, -0.2157, -0.0273, -0.1227,  0.0319,  0.0293, -0.2540]],
       dtype=torch.float64)
	q_value: tensor([[-6.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07487295115202763, distance: 1.1864112613469857 entropy 0.07888543605804443
epoch: 16, step: 7
	action: tensor([[ 0.4233, -0.4340, -0.4349, -0.4543,  0.0175, -0.0913, -0.1048]],
       dtype=torch.float64)
	q_value: tensor([[-5.3651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004468692077216119, distance: 1.1468982649590904 entropy 0.07888543605804443
epoch: 16, step: 8
	action: tensor([[ 0.3494, -0.2378,  0.2056, -0.3114, -0.3238, -0.3111,  0.0532]],
       dtype=torch.float64)
	q_value: tensor([[-6.5166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12036476736577673, distance: 1.0732675563561338 entropy 0.07888543605804443
epoch: 16, step: 9
	action: tensor([[-0.0683,  0.0184,  0.0830, -0.1769, -0.5321,  0.4671, -0.3270]],
       dtype=torch.float64)
	q_value: tensor([[-7.2303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23559858195732697, distance: 1.0005007886152046 entropy 0.07888543605804443
epoch: 16, step: 10
	action: tensor([[ 0.1854, -0.2126, -0.4196, -0.4173,  0.0990, -0.1265, -0.1170]],
       dtype=torch.float64)
	q_value: tensor([[-6.7947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1554965430743921, distance: 1.051616542326864 entropy 0.07888543605804443
epoch: 16, step: 11
	action: tensor([[ 0.2730, -0.0563, -0.1728, -0.4383, -0.1444, -0.3785, -0.1500]],
       dtype=torch.float64)
	q_value: tensor([[-5.7224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2569838633814099, distance: 0.9864062518207751 entropy 0.07888543605804443
epoch: 16, step: 12
	action: tensor([[ 0.0375,  0.2476, -0.0142, -0.2077,  0.4968,  0.2351, -0.6066]],
       dtype=torch.float64)
	q_value: tensor([[-6.3877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5120731675197625, distance: 0.7993445977580708 entropy 0.07888543605804443
epoch: 16, step: 13
	action: tensor([[ 0.0764, -0.0740, -0.1140, -0.5227,  0.3047,  0.0217, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-5.8040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11107378260341405, distance: 1.0789207630475641 entropy 0.07888543605804443
epoch: 16, step: 14
	action: tensor([[-0.0518, -0.0058,  0.0501, -0.0094, -0.2477,  0.0423, -0.1957]],
       dtype=torch.float64)
	q_value: tensor([[-5.4634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2724401210685251, distance: 0.9760926988605576 entropy 0.07888543605804443
epoch: 16, step: 15
	action: tensor([[-0.5302, -0.2187, -0.5343,  0.1960, -0.3865,  0.1636,  0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-5.7137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46483428191215403, distance: 1.3850037233562216 entropy 0.07888543605804443
epoch: 16, step: 16
	action: tensor([[-0.0875,  0.2602, -0.2785, -0.1456,  0.0583,  0.1497,  0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-5.7811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3512565417725342, distance: 0.9217076435940031 entropy 0.07888543605804443
epoch: 16, step: 17
	action: tensor([[ 0.2526,  0.2498, -0.4599,  0.0639,  0.4038,  0.2423, -0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-5.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.700963656595861, distance: 0.6257756788808952 entropy 0.07888543605804443
epoch: 16, step: 18
	action: tensor([[ 0.1394,  0.1718, -0.3678, -0.1107,  0.3510,  0.4667, -0.2492]],
       dtype=torch.float64)
	q_value: tensor([[-5.8878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6031023142920855, distance: 0.7209347781543565 entropy 0.07888543605804443
epoch: 16, step: 19
	action: tensor([[ 0.2395,  0.2156, -0.3615, -0.2029, -0.0630, -0.1547, -0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-5.5149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5985178084514919, distance: 0.7250865284447474 entropy 0.07888543605804443
epoch: 16, step: 20
	action: tensor([[ 0.0838, -0.0530,  0.0896, -0.6797, -0.0680, -0.2267,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-5.6982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04791738866653317, distance: 1.1714404514091394 entropy 0.07888543605804443
epoch: 16, step: 21
	action: tensor([[ 0.2541,  0.4373,  0.1656, -0.6413,  0.4383,  0.4080, -0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-6.5003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6445170220744093, distance: 0.6822854314838103 entropy 0.07888543605804443
epoch: 16, step: 22
	action: tensor([[ 0.1601,  0.3509, -0.4741,  0.0061,  0.2855,  0.1115, -0.1203]],
       dtype=torch.float64)
	q_value: tensor([[-6.8709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6891819517666892, distance: 0.6379840302259614 entropy 0.07888543605804443
epoch: 16, step: 23
	action: tensor([[ 0.5259,  0.5730, -0.2633,  0.0775,  0.3821,  0.2387,  0.0678]],
       dtype=torch.float64)
	q_value: tensor([[-5.5306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9336808223002182, distance: 0.29469736971170063 entropy 0.07888543605804443
epoch: 16, step: 24
	action: tensor([[ 0.3348,  0.2081,  0.0436, -0.4976,  0.0360, -0.1544, -0.3945]],
       dtype=torch.float64)
	q_value: tensor([[-7.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4817277749187119, distance: 0.8238263160014546 entropy 0.07888543605804443
epoch: 16, step: 25
	action: tensor([[-0.2096, -0.1218, -0.0205, -0.2783, -0.0667, -0.0982, -0.4375]],
       dtype=torch.float64)
	q_value: tensor([[-6.5965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14278603498184972, distance: 1.223317407189576 entropy 0.07888543605804443
epoch: 16, step: 26
	action: tensor([[-0.0548,  0.2145, -0.5032, -0.2026,  0.2207, -0.1087, -0.2587]],
       dtype=torch.float64)
	q_value: tensor([[-5.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36511370028402323, distance: 0.9118106721140716 entropy 0.07888543605804443
epoch: 16, step: 27
	action: tensor([[ 0.2646, -0.0472, -0.3629, -0.5453,  0.0516, -0.1911, -0.3339]],
       dtype=torch.float64)
	q_value: tensor([[-5.0601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28615285632982734, distance: 0.9668504558460387 entropy 0.07888543605804443
epoch: 16, step: 28
	action: tensor([[ 0.0482,  0.3836, -0.4876, -0.1589, -0.0292, -0.0076, -0.5631]],
       dtype=torch.float64)
	q_value: tensor([[-5.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5770072081967592, distance: 0.7442574259679002 entropy 0.07888543605804443
epoch: 16, step: 29
	action: tensor([[ 0.2332,  0.3641, -0.5162, -0.5016,  0.6502, -0.0916, -0.5790]],
       dtype=torch.float64)
	q_value: tensor([[-5.4135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6732828209021221, distance: 0.6540977852273228 entropy 0.07888543605804443
epoch: 16, step: 30
	action: tensor([[ 0.1025,  0.6317, -0.3698, -0.7007,  0.0599, -0.1616,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-6.4458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.682283205020447, distance: 0.6450253449921529 entropy 0.07888543605804443
epoch: 16, step: 31
	action: tensor([[ 0.3907, -0.3819,  0.1711, -0.1133, -0.0529, -0.6120, -0.1383]],
       dtype=torch.float64)
	q_value: tensor([[-6.6169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035088236993476074, distance: 1.1240884712575434 entropy 0.07888543605804443
epoch: 16, step: 32
	action: tensor([[ 0.0293,  0.7911, -0.0021, -0.2515,  0.3829,  0.1652,  0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-7.8638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6887262754193441, distance: 0.6384515187799253 entropy 0.07888543605804443
epoch: 16, step: 33
	action: tensor([[-0.2060,  0.0179, -0.4250, -0.1750,  0.5963, -0.0457,  0.2700]],
       dtype=torch.float64)
	q_value: tensor([[-6.4786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04627585467176998, distance: 1.1175528801430918 entropy 0.07888543605804443
epoch: 16, step: 34
	action: tensor([[ 0.1256,  0.4300,  0.0816, -0.5159,  0.4712, -0.2047, -0.1194]],
       dtype=torch.float64)
	q_value: tensor([[-5.9499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41898096744786895, distance: 0.872271881305105 entropy 0.07888543605804443
epoch: 16, step: 35
	action: tensor([[ 0.2008,  0.0649,  0.3972, -0.1376,  0.0836, -0.1741, -0.5563]],
       dtype=torch.float64)
	q_value: tensor([[-6.3167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37310980838489216, distance: 0.9060505549594028 entropy 0.07888543605804443
epoch: 16, step: 36
	action: tensor([[ 0.1493,  0.1323, -0.5320,  0.2125, -0.1485, -0.0031, -0.2694]],
       dtype=torch.float64)
	q_value: tensor([[-7.0679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5287399822848179, distance: 0.7855738003728682 entropy 0.07888543605804443
epoch: 16, step: 37
	action: tensor([[ 0.4077,  0.0908, -0.1298, -0.0639,  0.2829,  0.3914,  0.2122]],
       dtype=torch.float64)
	q_value: tensor([[-5.5421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7468943062011044, distance: 0.5757151403019237 entropy 0.07888543605804443
epoch: 16, step: 38
	action: tensor([[-0.0796, -0.3368, -0.0175, -0.1544,  0.5204,  0.1766, -0.0340]],
       dtype=torch.float64)
	q_value: tensor([[-6.5359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09005302200327181, distance: 1.19475953425205 entropy 0.07888543605804443
epoch: 16, step: 39
	action: tensor([[ 0.0231, -0.1901, -0.5348, -0.4077,  0.4075,  0.0944, -0.3438]],
       dtype=torch.float64)
	q_value: tensor([[-5.8507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08771612194078604, distance: 1.0930038539837281 entropy 0.07888543605804443
epoch: 16, step: 40
	action: tensor([[ 0.0660,  0.4673, -0.5214, -0.2499, -0.1404, -0.0537, -0.2728]],
       dtype=torch.float64)
	q_value: tensor([[-5.3519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6167457330257703, distance: 0.7084352992291966 entropy 0.07888543605804443
epoch: 16, step: 41
	action: tensor([[ 0.0602,  0.1184, -0.4821, -0.4174,  0.1630,  0.3234, -0.1385]],
       dtype=torch.float64)
	q_value: tensor([[-5.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4392025726922508, distance: 0.8569583199589393 entropy 0.07888543605804443
epoch: 16, step: 42
	action: tensor([[-0.1747, -0.0732, -0.0038,  0.0069,  0.0671, -0.4946,  0.2943]],
       dtype=torch.float64)
	q_value: tensor([[-5.4624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027443231498202714, distance: 1.1599402293570813 entropy 0.07888543605804443
epoch: 16, step: 43
	action: tensor([[ 0.1451, -0.3390, -0.7655, -0.2621, -0.3120,  0.1053,  0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-6.5996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10709990733733132, distance: 1.0813296893462678 entropy 0.07888543605804443
epoch: 16, step: 44
	action: tensor([[ 0.2886, -0.0344, -0.1772, -0.1140, -0.0223, -0.1546, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[-6.0122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44241393967174625, distance: 0.8545011414548641 entropy 0.07888543605804443
epoch: 16, step: 45
	action: tensor([[ 0.5182,  0.2205, -0.4238, -0.2894,  0.1935,  0.2742, -0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-5.9866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7779282246411202, distance: 0.5392664173187665 entropy 0.07888543605804443
epoch: 16, step: 46
	action: tensor([[-0.0215,  0.1914, -0.0445, -0.3086,  0.1371,  0.1101, -0.2402]],
       dtype=torch.float64)
	q_value: tensor([[-6.3301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3402528680567716, distance: 0.9294915559412158 entropy 0.07888543605804443
epoch: 16, step: 47
	action: tensor([[ 0.3308, -0.2987,  0.2489, -0.0089,  0.1941, -0.2290, -0.1885]],
       dtype=torch.float64)
	q_value: tensor([[-5.2679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23353671028300194, distance: 1.0018492391529243 entropy 0.07888543605804443
epoch: 16, step: 48
	action: tensor([[ 0.2663, -0.1466, -0.1419, -0.4842,  0.2537, -0.4377, -0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-7.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.057053516496912704, distance: 1.1112204261770715 entropy 0.07888543605804443
epoch: 16, step: 49
	action: tensor([[ 0.4937,  0.1458, -0.6461,  0.0620, -0.5643,  0.3021, -0.1334]],
       dtype=torch.float64)
	q_value: tensor([[-6.6290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7397319385027064, distance: 0.5838040883167107 entropy 0.07888543605804443
epoch: 16, step: 50
	action: tensor([[ 0.2799,  0.1582, -0.2326, -0.1739, -0.0221, -0.2972, -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-7.0588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.572606037701596, distance: 0.7481193444196731 entropy 0.07888543605804443
epoch: 16, step: 51
	action: tensor([[ 0.0786, -0.3454, -0.2695, -0.0700, -0.0643,  0.5396, -0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-6.0689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20512715894790234, distance: 1.020247460573601 entropy 0.07888543605804443
epoch: 16, step: 52
	action: tensor([[ 0.2621,  0.2230, -0.3504, -0.1303,  0.2902,  0.0034, -0.1509]],
       dtype=torch.float64)
	q_value: tensor([[-5.8395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6490044890232392, distance: 0.6779653128081151 entropy 0.07888543605804443
epoch: 16, step: 53
	action: tensor([[-0.2086,  0.2880, -0.3261, -0.3493,  0.3142,  0.2843, -0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-5.6362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973896473674299, distance: 0.9592105915745222 entropy 0.07888543605804443
epoch: 16, step: 54
	action: tensor([[ 0.0386,  0.2114, -0.7424, -0.4503,  0.2780, -0.1796, -0.1853]],
       dtype=torch.float64)
	q_value: tensor([[-5.2136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49593208116941445, distance: 0.8124585664766264 entropy 0.07888543605804443
epoch: 16, step: 55
	action: tensor([[ 0.4803, -0.1555, -0.1348, -0.4686,  0.1053,  0.0211, -0.2676]],
       dtype=torch.float64)
	q_value: tensor([[-5.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29584929076828803, distance: 0.9602614709401595 entropy 0.07888543605804443
epoch: 16, step: 56
	action: tensor([[-0.3134, -0.1185,  0.2506, -0.1128, -0.1785,  0.0368, -0.1372]],
       dtype=torch.float64)
	q_value: tensor([[-6.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18059614809438873, distance: 1.2433900114860006 entropy 0.07888543605804443
epoch: 16, step: 57
	action: tensor([[-0.0434, -0.2804, -0.1933,  0.2725,  0.0507,  0.0157, -0.6251]],
       dtype=torch.float64)
	q_value: tensor([[-5.9874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16325902194621, distance: 1.04677227775951 entropy 0.07888543605804443
epoch: 16, step: 58
	action: tensor([[ 0.5026, -0.0715, -0.0460, -0.1067,  0.2180,  0.2467, -0.6337]],
       dtype=torch.float64)
	q_value: tensor([[-6.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6144124694614161, distance: 0.7105885147517075 entropy 0.07888543605804443
epoch: 16, step: 59
	action: tensor([[-0.1001,  0.3744, -0.0266, -0.5042,  0.3671,  0.0986, -0.0365]],
       dtype=torch.float64)
	q_value: tensor([[-6.6856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089109421523951, distance: 0.9513135916129328 entropy 0.07888543605804443
epoch: 16, step: 60
	action: tensor([[ 0.3971,  0.1254, -0.1743, -0.3756,  0.1528,  0.1217, -0.5160]],
       dtype=torch.float64)
	q_value: tensor([[-5.6154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5723229655464821, distance: 0.7483670511260762 entropy 0.07888543605804443
epoch: 16, step: 61
	action: tensor([[ 0.4990,  0.2071, -0.1842,  0.1329, -0.0416, -0.1354, -0.6929]],
       dtype=torch.float64)
	q_value: tensor([[-6.0395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8103158145358176, distance: 0.4983933687097426 entropy 0.07888543605804443
epoch: 16, step: 62
	action: tensor([[ 0.2473,  0.0631, -0.5557,  0.0637, -0.0164,  0.0951, -0.3103]],
       dtype=torch.float64)
	q_value: tensor([[-7.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.601298316156285, distance: 0.7225713340531421 entropy 0.07888543605804443
epoch: 16, step: 63
	action: tensor([[ 0.5731,  0.1972, -0.2605, -0.5057,  0.4031, -0.0582, -0.6582]],
       dtype=torch.float64)
	q_value: tensor([[-5.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6414827044463778, distance: 0.685191156597728 entropy 0.07888543605804443
epoch: 16, step: 64
	action: tensor([[ 0.1641, -0.0065, -0.1879,  0.2929,  0.6202, -0.1349,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-6.9893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5202583545855454, distance: 0.7926115624483799 entropy 0.07888543605804443
epoch: 16, step: 65
	action: tensor([[-0.0104,  0.0891, -0.2263,  0.2697, -0.0267,  0.0620, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-6.8038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46264844243970715, distance: 0.8388531685374048 entropy 0.07888543605804443
epoch: 16, step: 66
	action: tensor([[ 0.1239, -0.2167,  0.1796,  0.1461, -0.0840, -0.1969,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-5.5464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28511466729866763, distance: 0.9675532735645496 entropy 0.07888543605804443
epoch: 16, step: 67
	action: tensor([[ 0.5082,  0.4329, -0.6336,  0.1855,  0.0585,  0.1954, -0.1919]],
       dtype=torch.float64)
	q_value: tensor([[-6.5231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8747452799866808, distance: 0.4049988066283798 entropy 0.07888543605804443
epoch: 16, step: 68
	action: tensor([[ 0.2240,  0.4339, -0.5717,  0.2584,  0.3843, -0.1521, -0.2668]],
       dtype=torch.float64)
	q_value: tensor([[-6.6701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7261570528419143, distance: 0.5988354106144005 entropy 0.07888543605804443
epoch: 16, step: 69
	action: tensor([[ 0.1002,  0.0969,  0.1519,  0.1423,  0.1798,  0.4224, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[-6.5591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.651442248118263, distance: 0.6756068841319517 entropy 0.07888543605804443
epoch: 16, step: 70
	action: tensor([[ 0.4683,  0.0900,  0.1180, -0.3708, -0.4514,  0.0791,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-6.2366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5440376878451039, distance: 0.7727182453839031 entropy 0.07888543605804443
epoch: 16, step: 71
	action: tensor([[-0.2182,  0.5581, -0.2774, -0.2721, -0.2260,  0.0766, -0.2975]],
       dtype=torch.float64)
	q_value: tensor([[-7.4817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.393219275801617, distance: 0.8913999004512856 entropy 0.07888543605804443
epoch: 16, step: 72
	action: tensor([[-0.0019,  0.1695, -0.4941, -0.1692,  0.2465,  0.5350,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[-5.7397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4987636145088026, distance: 0.8101734147841869 entropy 0.07888543605804443
epoch: 16, step: 73
	action: tensor([[ 0.1967,  0.2561, -0.2844, -0.4091,  0.6849, -0.0017,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[-5.6686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5351041348117478, distance: 0.7802513613008106 entropy 0.07888543605804443
epoch: 16, step: 74
	action: tensor([[ 0.3045,  0.6393,  0.1002, -0.0241,  0.1896, -0.2236,  0.2730]],
       dtype=torch.float64)
	q_value: tensor([[-6.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8096388712681178, distance: 0.49928190761829655 entropy 0.07888543605804443
epoch: 16, step: 75
	action: tensor([[ 0.0261,  0.1165, -0.9009, -0.1416,  0.2164, -0.4673, -0.2254]],
       dtype=torch.float64)
	q_value: tensor([[-7.2402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42383972426765215, distance: 0.8686170487385753 entropy 0.07888543605804443
epoch: 16, step: 76
	action: tensor([[ 0.4396,  0.2531,  0.0728,  0.0865, -0.1183,  0.2820, -0.2010]],
       dtype=torch.float64)
	q_value: tensor([[-6.4355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.845069879698991, distance: 0.4504272144095549 entropy 0.07888543605804443
epoch: 16, step: 77
	action: tensor([[ 0.0903,  0.0014, -0.1365,  0.0834,  0.2356, -0.5978, -0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-6.7424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072263664228727, distance: 0.952472331120686 entropy 0.07888543605804443
epoch: 16, step: 78
	action: tensor([[ 0.4450, -0.1733, -0.1799, -0.2166,  0.4657,  0.0162, -0.1865]],
       dtype=torch.float64)
	q_value: tensor([[-6.8129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3836806128451852, distance: 0.8983790338247486 entropy 0.07888543605804443
epoch: 16, step: 79
	action: tensor([[0.2786, 0.2016, 0.0176, 0.2350, 0.1778, 0.0702, 0.3322]],
       dtype=torch.float64)
	q_value: tensor([[-6.4652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7666172113822954, distance: 0.552829367519348 entropy 0.07888543605804443
epoch: 16, step: 80
	action: tensor([[ 0.3456, -0.0264,  0.2249,  0.4383,  0.4704, -0.2741, -0.3631]],
       dtype=torch.float64)
	q_value: tensor([[-6.8070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7145550696638503, distance: 0.6113893373095614 entropy 0.07888543605804443
epoch: 16, step: 81
	action: tensor([[ 0.3744, -0.1151, -0.0585,  0.1466, -0.1601, -0.1347, -0.5214]],
       dtype=torch.float64)
	q_value: tensor([[-7.7269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5476558938846166, distance: 0.7696462563604544 entropy 0.07888543605804443
epoch: 16, step: 82
	action: tensor([[ 0.8909, -0.1898, -0.2849, -0.4041,  0.3847, -0.1796, -0.0394]],
       dtype=torch.float64)
	q_value: tensor([[-6.6888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.287219559899236, distance: 0.9661278022540468 entropy 0.07888543605804443
epoch: 16, step: 83
	action: tensor([[ 0.1261,  0.2641, -0.1674, -0.2391, -0.4314,  0.2426,  0.2829]],
       dtype=torch.float64)
	q_value: tensor([[-8.4382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5505563977176189, distance: 0.7671747396848689 entropy 0.07888543605804443
epoch: 16, step: 84
	action: tensor([[ 0.0647,  0.1089, -0.6273, -0.1694,  0.4310,  0.2876,  0.3644]],
       dtype=torch.float64)
	q_value: tensor([[-6.6874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4945329884284996, distance: 0.8135853166351048 entropy 0.07888543605804443
epoch: 16, step: 85
	action: tensor([[-0.1269,  0.5510, -0.1580, -0.3655,  0.0211, -0.4142,  0.1202]],
       dtype=torch.float64)
	q_value: tensor([[-6.3843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.409128079957772, distance: 0.8796367575899432 entropy 0.07888543605804443
epoch: 16, step: 86
	action: tensor([[ 0.3139,  0.3143, -0.3623, -0.0027,  0.2263, -0.1207, -0.5483]],
       dtype=torch.float64)
	q_value: tensor([[-6.0630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7329491364071641, distance: 0.5913623712394142 entropy 0.07888543605804443
epoch: 16, step: 87
	action: tensor([[ 0.2433, -0.0824, -0.0503,  0.2263,  0.3589, -0.1043, -0.8516]],
       dtype=torch.float64)
	q_value: tensor([[-6.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5512887215600251, distance: 0.7665494672428911 entropy 0.07888543605804443
epoch: 16, step: 88
	action: tensor([[-0.0350,  0.2819, -0.1542, -0.2442,  0.0302,  0.3891, -0.1010]],
       dtype=torch.float64)
	q_value: tensor([[-7.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47080555564536997, distance: 0.8324618337883627 entropy 0.07888543605804443
epoch: 16, step: 89
	action: tensor([[-0.0420, -0.1214, -0.4403, -0.4266,  0.3199, -0.1434, -0.3780]],
       dtype=torch.float64)
	q_value: tensor([[-5.5634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07336644891080046, distance: 1.101566458626822 entropy 0.07888543605804443
epoch: 16, step: 90
	action: tensor([[-0.2524,  0.2820,  0.5521, -0.6964,  0.4039,  0.4057,  0.2408]],
       dtype=torch.float64)
	q_value: tensor([[-5.3300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08871935969082756, distance: 1.1940284260371103 entropy 0.07888543605804443
epoch: 16, step: 91
	action: tensor([[-0.0520,  0.7228, -0.5989,  0.2011,  0.5449, -0.1732, -0.3408]],
       dtype=torch.float64)
	q_value: tensor([[-7.5934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5607926509384182, distance: 0.7583880697144105 entropy 0.07888543605804443
epoch: 16, step: 92
	action: tensor([[ 0.2057, -0.5056, -0.4574,  0.4007,  0.3572,  0.3483, -0.5838]],
       dtype=torch.float64)
	q_value: tensor([[-6.6530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.330733461244588, distance: 0.9361732970335703 entropy 0.07888543605804443
epoch: 16, step: 93
	action: tensor([[-0.0014, -0.1130, -0.4923, -0.1965,  0.2077,  0.1200,  0.3874]],
       dtype=torch.float64)
	q_value: tensor([[-6.9754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20973800832914757, distance: 1.0172840625580368 entropy 0.07888543605804443
epoch: 16, step: 94
	action: tensor([[ 0.4133,  0.0838,  0.3655, -0.3365, -0.0477, -0.2372, -0.1992]],
       dtype=torch.float64)
	q_value: tensor([[-6.0329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4368814409050874, distance: 0.8587299572038352 entropy 0.07888543605804443
epoch: 16, step: 95
	action: tensor([[ 1.0948e-04,  1.6436e-01, -1.4566e-01, -3.2361e-01,  1.3783e-01,
          3.7463e-01, -2.6313e-01]], dtype=torch.float64)
	q_value: tensor([[-7.4015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4045680516192377, distance: 0.8830245131433868 entropy 0.07888543605804443
epoch: 16, step: 96
	action: tensor([[ 0.0478, -0.1482,  0.1470, -0.0135,  0.3084,  0.5923,  0.0861]],
       dtype=torch.float64)
	q_value: tensor([[-5.3127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4373883770748366, distance: 0.8583433429777959 entropy 0.07888543605804443
epoch: 16, step: 97
	action: tensor([[-0.2848, -0.1849, -0.2456, -0.1622,  0.5214, -0.0921,  0.1687]],
       dtype=torch.float64)
	q_value: tensor([[-6.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2500764249859715, distance: 1.2794548819377725 entropy 0.07888543605804443
epoch: 16, step: 98
	action: tensor([[ 0.0698, -0.2151, -0.4688, -0.0938, -0.3068,  0.0780,  0.0496]],
       dtype=torch.float64)
	q_value: tensor([[-5.8501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13950997840045465, distance: 1.0615234988276308 entropy 0.07888543605804443
epoch: 16, step: 99
	action: tensor([[-0.0089,  0.2899, -0.1210,  0.1185,  0.0444, -0.0890, -0.3979]],
       dtype=torch.float64)
	q_value: tensor([[-5.6009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47111823903518546, distance: 0.8322158604486675 entropy 0.07888543605804443
epoch: 16, step: 100
	action: tensor([[ 0.2848,  0.2157, -0.3906,  0.4378,  0.1401,  0.2195, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[-5.6407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7196105125235435, distance: 0.6059510699510096 entropy 0.07888543605804443
epoch: 16, step: 101
	action: tensor([[ 0.0379,  0.2637,  0.0470, -0.2986, -0.0843,  0.1583, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-6.2354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4182769081726123, distance: 0.8728002161280413 entropy 0.07888543605804443
epoch: 16, step: 102
	action: tensor([[ 0.1686,  0.0988, -0.2371, -0.3268,  0.6092, -0.3169, -0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-5.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31896297651038186, distance: 0.9443697219003492 entropy 0.07888543605804443
epoch: 16, step: 103
	action: tensor([[-0.3277, -0.2066, -0.3300, -0.6765,  0.6597, -0.4406, -0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-6.2330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4382468972496627, distance: 1.372376953125023 entropy 0.07888543605804443
epoch: 16, step: 104
	action: tensor([[ 0.0169,  0.0568, -0.5383, -0.0891, -0.6350, -0.1921, -0.1762]],
       dtype=torch.float64)
	q_value: tensor([[-6.2623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35980078969175644, distance: 0.9156178703467991 entropy 0.07888543605804443
epoch: 16, step: 105
	action: tensor([[-0.0692,  0.4126,  0.0695,  0.2447,  0.2413,  0.1290,  0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-6.3211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5619156599797346, distance: 0.757417888123692 entropy 0.07888543605804443
epoch: 16, step: 106
	action: tensor([[-0.0363,  0.0517,  0.2674, -0.0304, -0.0030, -0.1862, -0.3310]],
       dtype=torch.float64)
	q_value: tensor([[-6.1594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20677340415632883, distance: 1.0191904059460435 entropy 0.07888543605804443
epoch: 16, step: 107
	action: tensor([[ 0.3902, -0.0566, -0.2375, -0.1739,  0.2832,  0.3682, -0.1528]],
       dtype=torch.float64)
	q_value: tensor([[-6.3026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5795569468173711, distance: 0.7420108980150805 entropy 0.07888543605804443
epoch: 16, step: 108
	action: tensor([[-0.2242,  0.0184, -0.2725, -0.3895,  0.0991,  0.0995, -0.1080]],
       dtype=torch.float64)
	q_value: tensor([[-5.9930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013095795233452279, distance: 1.136826511168605 entropy 0.07888543605804443
epoch: 16, step: 109
	action: tensor([[ 0.2127,  0.1071,  0.1801,  0.1836, -0.1412, -0.2144, -0.5636]],
       dtype=torch.float64)
	q_value: tensor([[-4.9433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.607750989464722, distance: 0.716700358383589 entropy 0.07888543605804443
epoch: 16, step: 110
	action: tensor([[ 0.2378,  0.2983, -0.5389, -0.5871,  0.0322,  0.4568, -0.4997]],
       dtype=torch.float64)
	q_value: tensor([[-6.9699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6783164286973165, distance: 0.6490395084848491 entropy 0.07888543605804443
epoch: 16, step: 111
	action: tensor([[ 0.2077,  0.1161, -0.2316, -0.4885,  0.4410, -0.3087, -0.0425]],
       dtype=torch.float64)
	q_value: tensor([[-6.2085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3338103358829636, distance: 0.9340188443480818 entropy 0.07888543605804443
epoch: 16, step: 112
	action: tensor([[-0.1513,  0.2351, -0.4212, -0.1209,  0.1250, -0.2813, -0.3613]],
       dtype=torch.float64)
	q_value: tensor([[-6.2124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29267700223643234, distance: 0.9624220902572826 entropy 0.07888543605804443
epoch: 16, step: 113
	action: tensor([[ 0.3638,  0.4396, -0.2959, -0.4311,  0.1694,  0.5023,  0.1590]],
       dtype=torch.float64)
	q_value: tensor([[-5.3242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8208623174015187, distance: 0.48433981228399925 entropy 0.07888543605804443
epoch: 16, step: 114
	action: tensor([[-0.0109, -0.1626,  0.0304,  0.2109,  0.0015, -0.1172, -0.1428]],
       dtype=torch.float64)
	q_value: tensor([[-7.0171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26389105777557254, distance: 0.981810653016384 entropy 0.07888543605804443
epoch: 16, step: 115
	action: tensor([[ 0.2608, -0.0599,  0.0126, -0.0543, -0.1861,  0.0234, -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-5.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4648238016480646, distance: 0.8371534828034786 entropy 0.07888543605804443
epoch: 16, step: 116
	action: tensor([[ 0.3693, -0.2871,  0.1648, -0.0572,  0.2823, -0.3144,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-6.1269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20944518501771214, distance: 1.0174725170842727 entropy 0.07888543605804443
epoch: 16, step: 117
	action: tensor([[ 0.5288,  0.2269, -0.2893, -0.4789, -0.2412, -0.1464, -0.2921]],
       dtype=torch.float64)
	q_value: tensor([[-7.3481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6289998366231078, distance: 0.6970175989405354 entropy 0.07888543605804443
epoch: 16, step: 118
	action: tensor([[ 0.4858,  0.3344,  0.2268,  0.1972, -0.1745,  0.0145, -0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-6.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8976192589449373, distance: 0.3661557239373158 entropy 0.07888543605804443
epoch: 16, step: 119
	action: tensor([[ 0.2184, -0.0821,  0.2284, -0.4127, -0.0663, -0.0864, -0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-7.3171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16587611365343846, distance: 1.0451339903010501 entropy 0.07888543605804443
epoch: 16, step: 120
	action: tensor([[-0.3077, -0.5293, -0.3660,  0.2178,  0.4097,  0.4524, -0.3318]],
       dtype=torch.float64)
	q_value: tensor([[-6.4774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21211400832092497, distance: 1.2598778146557514 entropy 0.07888543605804443
epoch: 16, step: 121
	action: tensor([[ 0.3010,  0.0211, -0.0536, -0.2042,  0.5602, -0.0575, -0.5129]],
       dtype=torch.float64)
	q_value: tensor([[-6.3321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44586345396286486, distance: 0.8518538488395501 entropy 0.07888543605804443
epoch: 16, step: 122
	action: tensor([[ 0.2671,  0.3228,  0.3450, -0.1139,  0.4681,  0.0909,  0.1332]],
       dtype=torch.float64)
	q_value: tensor([[-6.1484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7000592781369883, distance: 0.6267212341777793 entropy 0.07888543605804443
epoch: 16, step: 123
	action: tensor([[-0.1953, -0.1878,  0.0172, -0.2985,  0.1867,  0.5528,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-7.0267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031064386601562965, distance: 1.1619824974455024 entropy 0.07888543605804443
epoch: 16, step: 124
	action: tensor([[ 0.1835,  0.1337, -0.0480, -0.0098,  0.3079,  0.1541, -0.2520]],
       dtype=torch.float64)
	q_value: tensor([[-6.0254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5750316784082321, distance: 0.7459933775363349 entropy 0.07888543605804443
epoch: 16, step: 125
	action: tensor([[-0.1163,  0.1046, -0.2949,  0.0068, -0.1569, -0.1255, -0.5457]],
       dtype=torch.float64)
	q_value: tensor([[-5.7113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2668021339648947, distance: 0.9798673557708817 entropy 0.07888543605804443
epoch: 16, step: 126
	action: tensor([[-0.2858,  0.3163, -0.2671, -0.4243, -0.1883, -0.0996, -0.2327]],
       dtype=torch.float64)
	q_value: tensor([[-5.4715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14381890282539544, distance: 1.0588623610264445 entropy 0.07888543605804443
epoch: 16, step: 127
	action: tensor([[ 0.9070,  0.2395, -0.6417,  0.1077,  0.2400, -0.0370, -0.4663]],
       dtype=torch.float64)
	q_value: tensor([[-5.4001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9051210644572927, distance: 0.35248577492324323 entropy 0.07888543605804443
LOSS epoch 16 actor 22.23218485858508 critic 52.51084803850398 
epoch: 17, step: 0
	action: tensor([[ 0.0637,  0.2620,  0.1484, -0.2985,  0.1093,  0.2112,  0.5995]],
       dtype=torch.float64)
	q_value: tensor([[-6.8971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45261475295585074, distance: 0.8466486834938308 entropy 0.07888543605804443
epoch: 17, step: 1
	action: tensor([[ 0.3440,  0.0998,  0.0117, -0.5125, -0.0819, -0.0986, -0.1495]],
       dtype=torch.float64)
	q_value: tensor([[-5.9101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3692664337756325, distance: 0.9088237444942074 entropy 0.07888543605804443
epoch: 17, step: 2
	action: tensor([[ 0.3064,  0.0954, -0.2471, -0.1662, -0.2988,  0.3220, -0.1912]],
       dtype=torch.float64)
	q_value: tensor([[-5.3285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5830957161304477, distance: 0.7388816347935856 entropy 0.07888543605804443
epoch: 17, step: 3
	action: tensor([[ 0.5532, -0.0537,  0.0949, -0.2585,  0.1713,  0.1875, -0.3407]],
       dtype=torch.float64)
	q_value: tensor([[-5.0454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.54929477025178, distance: 0.7682507483680346 entropy 0.07888543605804443
epoch: 17, step: 4
	action: tensor([[ 0.3719, -0.0017,  0.1166, -0.3192,  0.0569,  0.0231, -0.1270]],
       dtype=torch.float64)
	q_value: tensor([[-5.5433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42960855353862204, distance: 0.8642575763592201 entropy 0.07888543605804443
epoch: 17, step: 5
	action: tensor([[-0.0475,  0.2988, -0.6844, -0.6349,  0.4052,  0.0103,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-5.2266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47375026160405664, distance: 0.8301424831080664 entropy 0.07888543605804443
epoch: 17, step: 6
	action: tensor([[ 0.2025, -0.1619, -0.3889, -0.4160,  0.1797, -0.0325, -0.0900]],
       dtype=torch.float64)
	q_value: tensor([[-4.8400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20577986724646835, distance: 1.0198284874300911 entropy 0.07888543605804443
epoch: 17, step: 7
	action: tensor([[-0.2666, -0.0889, -0.4084,  0.0493, -0.2138,  0.3472, -0.2100]],
       dtype=torch.float64)
	q_value: tensor([[-4.5622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007238383437587093, distance: 1.1484783876254472 entropy 0.07888543605804443
epoch: 17, step: 8
	action: tensor([[ 0.3960,  0.4513, -0.6157, -0.3724, -0.2554, -0.1459, -0.4397]],
       dtype=torch.float64)
	q_value: tensor([[-4.0837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8462762859936914, distance: 0.44867009902550586 entropy 0.07888543605804443
epoch: 17, step: 9
	action: tensor([[ 0.4242,  0.1367, -0.4669, -0.6562,  0.0758, -0.1433, -0.3447]],
       dtype=torch.float64)
	q_value: tensor([[-5.4220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5225915279093507, distance: 0.790681821591928 entropy 0.07888543605804443
epoch: 17, step: 10
	action: tensor([[ 0.3459,  0.0994, -0.2381, -0.2471,  0.1941,  0.0863, -0.4965]],
       dtype=torch.float64)
	q_value: tensor([[-5.1761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6083404677879094, distance: 0.7161616213049057 entropy 0.07888543605804443
epoch: 17, step: 11
	action: tensor([[ 0.2635,  0.1789, -0.4271,  0.3395,  0.0019, -0.2072, -0.1509]],
       dtype=torch.float64)
	q_value: tensor([[-4.6985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7029206875803327, distance: 0.623724636103785 entropy 0.07888543605804443
epoch: 17, step: 12
	action: tensor([[ 0.5532, -0.1132,  0.1247,  0.0734, -0.0106, -0.0490, -0.3262]],
       dtype=torch.float64)
	q_value: tensor([[-5.1874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6373859690209651, distance: 0.6890948363654477 entropy 0.07888543605804443
epoch: 17, step: 13
	action: tensor([[ 0.0371, -0.2717, -0.1925, -0.3102,  0.0234,  0.0945,  0.2449]],
       dtype=torch.float64)
	q_value: tensor([[-5.9414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012683661641225585, distance: 1.1370638571478313 entropy 0.07888543605804443
epoch: 17, step: 14
	action: tensor([[-0.1689,  0.1356, -0.2090, -0.0739,  0.3171, -0.0638, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-4.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20035905030321433, distance: 1.0233029035544516 entropy 0.07888543605804443
epoch: 17, step: 15
	action: tensor([[ 4.3259e-01, -4.5800e-04, -2.3547e-01, -5.7885e-01,  1.5456e-01,
         -1.2631e-01, -4.9287e-01]], dtype=torch.float64)
	q_value: tensor([[-4.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3340575297472761, distance: 0.933845541487349 entropy 0.07888543605804443
epoch: 17, step: 16
	action: tensor([[ 0.4863,  0.4766,  0.2551, -0.4540,  0.4994,  0.1942, -0.1575]],
       dtype=torch.float64)
	q_value: tensor([[-5.1729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.804982220538837, distance: 0.5053517765886348 entropy 0.07888543605804443
epoch: 17, step: 17
	action: tensor([[ 0.5230,  0.0920, -0.4329, -0.6375, -0.0526,  0.0568, -0.3571]],
       dtype=torch.float64)
	q_value: tensor([[-6.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5063384657595398, distance: 0.8040283043795287 entropy 0.07888543605804443
epoch: 17, step: 18
	action: tensor([[ 0.0415,  0.2380, -0.3318, -0.3840,  0.3045,  0.2490, -0.4857]],
       dtype=torch.float64)
	q_value: tensor([[-5.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48399193478418134, distance: 0.8220248340347458 entropy 0.07888543605804443
epoch: 17, step: 19
	action: tensor([[ 0.3365,  0.1836, -0.0868, -0.4623,  0.5449,  0.2636,  0.1425]],
       dtype=torch.float64)
	q_value: tensor([[-4.1392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5950637891714912, distance: 0.7281988698748428 entropy 0.07888543605804443
epoch: 17, step: 20
	action: tensor([[ 0.2099, -0.4047,  0.1250, -0.8107,  0.3379,  0.0480, -0.4643]],
       dtype=torch.float64)
	q_value: tensor([[-5.4525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3595649723586276, distance: 1.3343098030156726 entropy 0.07888543605804443
epoch: 17, step: 21
	action: tensor([[ 0.3347, -0.1323, -0.0560, -0.3107,  0.6092,  0.0959, -0.2074]],
       dtype=torch.float64)
	q_value: tensor([[-5.3690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3276531452476168, distance: 0.9383252043406767 entropy 0.07888543605804443
epoch: 17, step: 22
	action: tensor([[ 0.7803, -0.0305, -0.4337, -0.1196,  0.2752, -0.1190, -0.2726]],
       dtype=torch.float64)
	q_value: tensor([[-5.0959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6254591893710273, distance: 0.7003357016292265 entropy 0.07888543605804443
epoch: 17, step: 23
	action: tensor([[ 0.0925,  0.0401, -0.0201, -0.3448,  0.3941,  0.4027, -0.2570]],
       dtype=torch.float64)
	q_value: tensor([[-6.2401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4095534388502564, distance: 0.8793200826332537 entropy 0.07888543605804443
epoch: 17, step: 24
	action: tensor([[ 0.4872,  0.3493, -0.3450,  0.0923,  0.5236,  0.0430, -0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-4.4755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8800604211677849, distance: 0.39631266651977193 entropy 0.07888543605804443
epoch: 17, step: 25
	action: tensor([[ 0.4497, -0.1036, -0.1092, -0.3061,  0.0099, -0.0822, -0.1785]],
       dtype=torch.float64)
	q_value: tensor([[-5.6000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38795858359237323, distance: 0.895255708737994 entropy 0.07888543605804443
epoch: 17, step: 26
	action: tensor([[-0.0926, -0.2883, -0.2188, -0.5843, -0.1802, -0.0128,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-5.2133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20928314437690143, distance: 1.258405747059632 entropy 0.07888543605804443
epoch: 17, step: 27
	action: tensor([[ 0.0660,  0.0129, -0.5240, -0.1474,  0.4872,  0.3013, -0.1437]],
       dtype=torch.float64)
	q_value: tensor([[-4.8115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42530945995013847, distance: 0.8675084573334346 entropy 0.07888543605804443
epoch: 17, step: 28
	action: tensor([[ 0.4052,  0.2184, -0.2070,  0.1991, -0.1069, -0.1285, -0.1393]],
       dtype=torch.float64)
	q_value: tensor([[-4.4718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.788848061176693, distance: 0.5258407395037143 entropy 0.07888543605804443
epoch: 17, step: 29
	action: tensor([[-0.2690,  0.1469, -0.0463, -0.6027,  0.3976,  0.3092,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-5.3172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003674773320496705, distance: 1.1464449287608538 entropy 0.07888543605804443
epoch: 17, step: 30
	action: tensor([[-0.0222, -0.1794,  0.1106,  0.2865,  0.3226, -0.3350, -0.1677]],
       dtype=torch.float64)
	q_value: tensor([[-4.5349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20021733204722092, distance: 1.0233935781741417 entropy 0.07888543605804443
epoch: 17, step: 31
	action: tensor([[ 0.1271,  0.2104, -0.6173,  0.3334,  0.3157,  0.3700,  0.0602]],
       dtype=torch.float64)
	q_value: tensor([[-5.5066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5917809766220672, distance: 0.7311446608971318 entropy 0.07888543605804443
epoch: 17, step: 32
	action: tensor([[ 0.0570,  0.0721, -0.0425, -0.8173,  0.2755,  0.0170, -0.4393]],
       dtype=torch.float64)
	q_value: tensor([[-5.0388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08373622615737064, distance: 1.095385408411307 entropy 0.07888543605804443
epoch: 17, step: 33
	action: tensor([[ 0.7548, -0.0158, -0.4270, -0.4640,  0.1785,  0.0379, -0.4191]],
       dtype=torch.float64)
	q_value: tensor([[-4.7825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5028500758153116, distance: 0.8068640801276361 entropy 0.07888543605804443
epoch: 17, step: 34
	action: tensor([[ 0.0422,  0.3686,  0.0637, -0.5849,  0.1994,  0.1148, -0.3450]],
       dtype=torch.float64)
	q_value: tensor([[-5.9125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3966264450035697, distance: 0.88889370200296 entropy 0.07888543605804443
epoch: 17, step: 35
	action: tensor([[ 0.1443,  0.8109, -0.0571, -0.2485,  0.4945,  0.3096, -0.4802]],
       dtype=torch.float64)
	q_value: tensor([[-4.8062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7809534816342819, distance: 0.5355806408934919 entropy 0.07888543605804443
epoch: 17, step: 36
	action: tensor([[-0.1070, -0.0931, -0.4425, -0.1478,  0.1817,  0.1247, -0.3934]],
       dtype=torch.float64)
	q_value: tensor([[-5.5139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13946066514691546, distance: 1.0615539154659186 entropy 0.07888543605804443
epoch: 17, step: 37
	action: tensor([[ 0.0915,  0.0352, -0.6779, -0.1148,  0.2942,  0.2696, -0.1049]],
       dtype=torch.float64)
	q_value: tensor([[-3.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4601197977767658, distance: 0.8408245706813364 entropy 0.07888543605804443
epoch: 17, step: 38
	action: tensor([[ 0.3261,  0.1570, -0.0578, -0.0174,  0.1584,  0.4205, -0.2260]],
       dtype=torch.float64)
	q_value: tensor([[-4.3987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7531687851615951, distance: 0.5685343819148521 entropy 0.07888543605804443
epoch: 17, step: 39
	action: tensor([[-0.0277,  0.5195, -0.0944, -0.1293,  0.3365,  0.1676, -0.5978]],
       dtype=torch.float64)
	q_value: tensor([[-4.9203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5935201913728977, distance: 0.7295854796376795 entropy 0.07888543605804443
epoch: 17, step: 40
	action: tensor([[ 0.7089,  0.5946,  0.2672, -0.4000, -0.0273,  0.2114,  0.1259]],
       dtype=torch.float64)
	q_value: tensor([[-4.7603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9386943178383926, distance: 0.2833394574157116 entropy 0.07888543605804443
epoch: 17, step: 41
	action: tensor([[ 0.1622, -0.2378, -0.3208, -0.5085, -0.3392,  0.0970,  0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-7.0805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07030268915912397, distance: 1.1033860290021447 entropy 0.07888543605804443
epoch: 17, step: 42
	action: tensor([[ 0.3264,  0.0808, -0.1510, -0.1681, -0.2140, -0.2209,  0.0191]],
       dtype=torch.float64)
	q_value: tensor([[-5.3969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.532549235679466, distance: 0.7823924130253896 entropy 0.07888543605804443
epoch: 17, step: 43
	action: tensor([[ 0.0343,  0.1416,  0.1438, -0.3336,  0.2322,  0.1389, -0.2541]],
       dtype=torch.float64)
	q_value: tensor([[-5.1833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3246854884469952, distance: 0.9403937506318953 entropy 0.07888543605804443
epoch: 17, step: 44
	action: tensor([[ 0.4009,  0.2393, -0.3127, -0.0331,  0.3184,  0.4167, -0.3087]],
       dtype=torch.float64)
	q_value: tensor([[-4.6334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8214525455705204, distance: 0.4835412453403401 entropy 0.07888543605804443
epoch: 17, step: 45
	action: tensor([[ 0.2919,  0.4502,  0.0310, -0.4248,  0.1894, -0.5241, -0.7057]],
       dtype=torch.float64)
	q_value: tensor([[-4.9576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5944217511014439, distance: 0.7287759314096739 entropy 0.07888543605804443
epoch: 17, step: 46
	action: tensor([[ 0.3336, -0.2646,  0.3271, -0.3109, -0.0638, -0.0066, -0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-6.2974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13954147883735335, distance: 1.0615040687591915 entropy 0.07888543605804443
epoch: 17, step: 47
	action: tensor([[-0.3736,  0.0821, -0.1528,  0.4088,  0.0369,  0.1466, -0.2986]],
       dtype=torch.float64)
	q_value: tensor([[-5.6466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1449411356280348, distance: 1.058168185623675 entropy 0.07888543605804443
epoch: 17, step: 48
	action: tensor([[ 0.0412,  0.0774, -0.1847, -0.7275,  0.1423,  0.3239, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-4.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21578432705001005, distance: 1.013384954416911 entropy 0.07888543605804443
epoch: 17, step: 49
	action: tensor([[ 0.2896,  0.8620, -0.9928, -0.0149, -0.0131, -0.1355,  0.0347]],
       dtype=torch.float64)
	q_value: tensor([[-4.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8425976041759274, distance: 0.4540068050682755 entropy 0.07888543605804443
epoch: 17, step: 50
	action: tensor([[ 0.1312,  0.4196,  0.0439, -0.8238,  0.1915, -0.3876, -0.1250]],
       dtype=torch.float64)
	q_value: tensor([[-6.2115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3125324416495491, distance: 0.9488177432504674 entropy 0.07888543605804443
epoch: 17, step: 51
	action: tensor([[-0.3057,  0.3872, -0.1828, -0.1977, -0.1003,  0.4238,  0.2518]],
       dtype=torch.float64)
	q_value: tensor([[-5.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2391935275827468, distance: 0.9981453607893611 entropy 0.07888543605804443
epoch: 17, step: 52
	action: tensor([[ 0.3306,  0.4758, -0.2295, -0.3791,  0.0735,  0.2109,  0.0746]],
       dtype=torch.float64)
	q_value: tensor([[-5.0106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7735147805033702, distance: 0.5445987319679577 entropy 0.07888543605804443
epoch: 17, step: 53
	action: tensor([[ 0.1441,  0.0447, -0.2016, -0.2213, -0.0827, -0.0502,  0.4079]],
       dtype=torch.float64)
	q_value: tensor([[-5.2759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3689987213292385, distance: 0.9090165974111779 entropy 0.07888543605804443
epoch: 17, step: 54
	action: tensor([[ 0.2845, -0.4672, -0.3494, -0.3306,  0.1586,  0.3435, -0.3451]],
       dtype=torch.float64)
	q_value: tensor([[-5.0967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045539814945809276, distance: 1.1179840345166623 entropy 0.07888543605804443
epoch: 17, step: 55
	action: tensor([[-0.3160,  0.1861,  0.0580,  0.1656, -0.2355,  0.2596, -0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-4.8648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20488929951884227, distance: 1.0204000996587317 entropy 0.07888543605804443
epoch: 17, step: 56
	action: tensor([[ 0.4985, -0.2052, -0.1907, -0.3001,  0.3178,  0.1466, -0.4788]],
       dtype=torch.float64)
	q_value: tensor([[-4.7669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3646421683685841, distance: 0.9121492114963868 entropy 0.07888543605804443
epoch: 17, step: 57
	action: tensor([[-0.0741,  0.0163, -0.0719, -0.6009,  0.0268,  0.3882, -0.1741]],
       dtype=torch.float64)
	q_value: tensor([[-5.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10443736227091494, distance: 1.0829407015710935 entropy 0.07888543605804443
epoch: 17, step: 58
	action: tensor([[ 0.3143,  0.1487, -0.3396, -0.1332, -0.0833, -0.1451, -0.2362]],
       dtype=torch.float64)
	q_value: tensor([[-4.6847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6133216223204738, distance: 0.711592950751588 entropy 0.07888543605804443
epoch: 17, step: 59
	action: tensor([[ 0.3598,  0.7233,  0.3748, -0.0085,  0.0697,  0.0034, -0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-4.8253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8635351816026131, distance: 0.422733868630167 entropy 0.07888543605804443
epoch: 17, step: 60
	action: tensor([[ 0.5758,  0.1551, -0.1817, -0.2665,  0.1051, -0.2559, -0.2651]],
       dtype=torch.float64)
	q_value: tensor([[-6.4587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6365767153319078, distance: 0.6898633418646402 entropy 0.07888543605804443
epoch: 17, step: 61
	action: tensor([[ 0.2805,  0.2033, -0.1039, -0.2923, -0.0927, -0.2299,  0.1374]],
       dtype=torch.float64)
	q_value: tensor([[-5.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5393877099341291, distance: 0.7766484039520756 entropy 0.07888543605804443
epoch: 17, step: 62
	action: tensor([[ 0.1827, -0.0826, -0.2722, -0.1405,  0.2485,  0.1460, -0.2214]],
       dtype=torch.float64)
	q_value: tensor([[-5.1503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4047745244927736, distance: 0.8828714004179863 entropy 0.07888543605804443
epoch: 17, step: 63
	action: tensor([[ 0.2077, -0.1008, -0.2700, -0.7832,  0.3088, -0.0054, -0.4383]],
       dtype=torch.float64)
	q_value: tensor([[-4.3435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11890513222882848, distance: 1.0741576580914252 entropy 0.07888543605804443
epoch: 17, step: 64
	action: tensor([[ 0.1761,  0.2993,  0.0318, -0.1957,  0.1471, -0.0484, -0.7236]],
       dtype=torch.float64)
	q_value: tensor([[-4.7435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5839802490837398, distance: 0.7380973874131288 entropy 0.07888543605804443
epoch: 17, step: 65
	action: tensor([[ 0.0286,  0.6195, -0.8704,  0.0922, -0.1443, -0.1536, -0.6269]],
       dtype=torch.float64)
	q_value: tensor([[-5.1917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.639599333780717, distance: 0.6869885295529083 entropy 0.07888543605804443
epoch: 17, step: 66
	action: tensor([[ 0.0642,  0.2601, -0.1150, -0.0584,  0.3745,  0.5192, -0.2959]],
       dtype=torch.float64)
	q_value: tensor([[-5.2425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6415302067995818, distance: 0.685145762315583 entropy 0.07888543605804443
epoch: 17, step: 67
	action: tensor([[-0.0024, -0.2465, -0.1522, -0.1367, -0.0701, -0.0693, -0.2758]],
       dtype=torch.float64)
	q_value: tensor([[-4.5932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04685963744228827, distance: 1.1172107959112456 entropy 0.07888543605804443
epoch: 17, step: 68
	action: tensor([[-0.5343, -0.1493, -0.1625, -0.5093,  0.2526,  0.2168, -0.8196]],
       dtype=torch.float64)
	q_value: tensor([[-4.4574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49702847230573854, distance: 1.400140839823064 entropy 0.07888543605804443
epoch: 17, step: 69
	action: tensor([[-0.0077,  0.3006, -0.2219, -0.1719,  0.0130, -0.2823, -0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-4.6124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40576827025616724, distance: 0.882134103170183 entropy 0.07888543605804443
epoch: 17, step: 70
	action: tensor([[ 0.1765,  0.2825, -0.4896,  0.1314,  0.2596,  0.2248,  0.2009]],
       dtype=torch.float64)
	q_value: tensor([[-4.4881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6537484910880997, distance: 0.6733680879684253 entropy 0.07888543605804443
epoch: 17, step: 71
	action: tensor([[ 0.1188,  0.2021, -0.3338, -0.4043, -0.1325,  0.2275, -0.2454]],
       dtype=torch.float64)
	q_value: tensor([[-4.8834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4857037888771364, distance: 0.8206601697057733 entropy 0.07888543605804443
epoch: 17, step: 72
	action: tensor([[ 0.4690,  0.3294, -0.0011, -0.6031, -0.1445, -0.1903, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-4.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6033224790333016, distance: 0.720734794074885 entropy 0.07888543605804443
epoch: 17, step: 73
	action: tensor([[ 0.3713,  0.1545, -0.0272, -0.3366,  0.1475, -0.1391, -0.2890]],
       dtype=torch.float64)
	q_value: tensor([[-5.9663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5462996716843669, distance: 0.7707991730279334 entropy 0.07888543605804443
epoch: 17, step: 74
	action: tensor([[ 0.0860,  0.0156, -0.4914, -0.1989,  0.3853,  0.6108, -0.1708]],
       dtype=torch.float64)
	q_value: tensor([[-5.1343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5227234439038476, distance: 0.7905725746984101 entropy 0.07888543605804443
epoch: 17, step: 75
	action: tensor([[-0.5163,  0.3694, -0.2749, -0.2448,  0.4905,  0.6120, -0.2266]],
       dtype=torch.float64)
	q_value: tensor([[-4.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12908911466961948, distance: 1.0679318832865898 entropy 0.07888543605804443
epoch: 17, step: 76
	action: tensor([[ 0.5065,  0.0771, -0.2100, -0.5699,  0.1498,  0.3905, -0.4176]],
       dtype=torch.float64)
	q_value: tensor([[-4.6662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5679034028249603, distance: 0.752223880008689 entropy 0.07888543605804443
epoch: 17, step: 77
	action: tensor([[-0.0382, -0.3873, -0.6145, -0.5804,  0.0606, -0.1119, -0.4490]],
       dtype=torch.float64)
	q_value: tensor([[-5.3512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0904548300800907, distance: 1.1949797161100333 entropy 0.07888543605804443
epoch: 17, step: 78
	action: tensor([[ 0.3768, -0.0039, -0.7163, -0.2045,  0.0748,  0.5661, -0.3254]],
       dtype=torch.float64)
	q_value: tensor([[-4.5264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6121036361558247, distance: 0.7127127818381795 entropy 0.07888543605804443
epoch: 17, step: 79
	action: tensor([[ 0.2318,  0.2692, -0.3583, -0.2440, -0.1340,  0.1598, -0.2194]],
       dtype=torch.float64)
	q_value: tensor([[-5.0291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6457274892069405, distance: 0.6811228042946317 entropy 0.07888543605804443
epoch: 17, step: 80
	action: tensor([[ 0.4930,  0.2908, -0.4160, -0.1241,  0.0035, -0.0550,  0.1566]],
       dtype=torch.float64)
	q_value: tensor([[-4.6146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8111812336132878, distance: 0.49725512880155154 entropy 0.07888543605804443
epoch: 17, step: 81
	action: tensor([[ 0.2610,  0.1904,  0.1235, -0.2409,  0.0473,  0.2634, -0.2234]],
       dtype=torch.float64)
	q_value: tensor([[-5.4021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6270885029752606, distance: 0.6988107541246512 entropy 0.07888543605804443
epoch: 17, step: 82
	action: tensor([[ 0.0761,  0.0030, -0.2938, -0.1590,  0.2705,  0.2140, -0.0805]],
       dtype=torch.float64)
	q_value: tensor([[-5.1130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3994800064488979, distance: 0.8867892686055517 entropy 0.07888543605804443
epoch: 17, step: 83
	action: tensor([[ 0.0371,  0.9337, -0.2138, -0.5056,  0.2261,  0.2021, -0.1441]],
       dtype=torch.float64)
	q_value: tensor([[-4.1677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7397272012763625, distance: 0.583809401299615 entropy 0.07888543605804443
epoch: 17, step: 84
	action: tensor([[ 0.6378,  0.3775, -0.0575, -0.3374, -0.3733, -0.0022, -0.2376]],
       dtype=torch.float64)
	q_value: tensor([[-5.6409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8375611005726276, distance: 0.46121318307577647 entropy 0.07888543605804443
epoch: 17, step: 85
	action: tensor([[-0.0237, -0.1071, -0.7433, -0.8261, -0.0151, -0.5973, -0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-6.2217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26102681947694417, distance: 0.9837189367477986 entropy 0.07888543605804443
epoch: 17, step: 86
	action: tensor([[ 0.3358,  0.5227, -0.4851, -0.1782, -0.2147,  0.0247, -0.0652]],
       dtype=torch.float64)
	q_value: tensor([[-5.3665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8312981532144834, distance: 0.4700202960394927 entropy 0.07888543605804443
epoch: 17, step: 87
	action: tensor([[-0.5373,  0.1518, -0.5609, -0.5307,  0.6006,  0.1303, -0.4369]],
       dtype=torch.float64)
	q_value: tensor([[-5.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1319365022686303, distance: 1.217496512411281 entropy 0.07888543605804443
epoch: 17, step: 88
	action: tensor([[ 0.3663,  0.1177, -0.8859, -0.6168,  0.2721,  0.2671, -0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-4.3801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6530599861540067, distance: 0.6740372359687221 entropy 0.07888543605804443
epoch: 17, step: 89
	action: tensor([[ 0.3199,  0.2081, -0.1652, -0.2064, -0.0789,  0.2081, -0.0813]],
       dtype=torch.float64)
	q_value: tensor([[-5.4960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6784616824611001, distance: 0.6488929575189244 entropy 0.07888543605804443
epoch: 17, step: 90
	action: tensor([[-0.2317,  0.6249, -0.5141, -0.1942,  0.2407, -0.2878, -0.3615]],
       dtype=torch.float64)
	q_value: tensor([[-4.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44537816388276996, distance: 0.8522267765558599 entropy 0.07888543605804443
epoch: 17, step: 91
	action: tensor([[ 0.3741,  0.1191, -0.1304, -0.0031,  0.4574,  0.3601,  0.2639]],
       dtype=torch.float64)
	q_value: tensor([[-4.6162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7677277488442438, distance: 0.5515124970352883 entropy 0.07888543605804443
epoch: 17, step: 92
	action: tensor([[ 0.4294,  0.0834,  0.1887, -0.1429,  0.0108,  0.5474, -0.5153]],
       dtype=torch.float64)
	q_value: tensor([[-5.5317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7429270165381767, distance: 0.5802096025537148 entropy 0.07888543605804443
epoch: 17, step: 93
	action: tensor([[-0.0883,  0.5373, -0.0152, -0.0634,  0.6261, -0.2273,  0.3314]],
       dtype=torch.float64)
	q_value: tensor([[-5.9080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4428870733320266, distance: 0.8541385256340769 entropy 0.07888543605804443
epoch: 17, step: 94
	action: tensor([[ 0.3474,  0.0117, -0.0609, -0.0581,  0.1312,  0.1741, -0.5402]],
       dtype=torch.float64)
	q_value: tensor([[-5.6108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5903732882351774, distance: 0.7324042029945677 entropy 0.07888543605804443
epoch: 17, step: 95
	action: tensor([[-0.2899,  0.1318, -0.4235, -0.3963,  0.4034,  0.5240, -0.3135]],
       dtype=torch.float64)
	q_value: tensor([[-5.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1813517567704036, distance: 1.0353933238818762 entropy 0.07888543605804443
epoch: 17, step: 96
	action: tensor([[ 0.0600,  0.3676, -0.2638,  0.0074,  0.0631, -0.0392, -0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-4.3150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5919664351187599, distance: 0.7309785584053107 entropy 0.07888543605804443
epoch: 17, step: 97
	action: tensor([[ 0.2524,  0.5203, -0.1940, -0.5980,  0.1254,  0.1540,  0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-4.3611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7039672777188253, distance: 0.6226249971027946 entropy 0.07888543605804443
epoch: 17, step: 98
	action: tensor([[ 0.3842, -0.0126, -0.4319, -0.5022, -0.3280, -0.2023,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-5.5539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39917453978071304, distance: 0.8870147816002888 entropy 0.07888543605804443
epoch: 17, step: 99
	action: tensor([[-0.1244,  0.4158, -0.2615, -0.6806,  0.1289,  0.0518, -0.2414]],
       dtype=torch.float64)
	q_value: tensor([[-5.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3425875120186985, distance: 0.9278455048118247 entropy 0.07888543605804443
epoch: 17, step: 100
	action: tensor([[ 0.4961,  0.0011, -0.2078, -0.2470,  0.2286,  0.1395, -0.1079]],
       dtype=torch.float64)
	q_value: tensor([[-4.5316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5889146433223147, distance: 0.7337070578588681 entropy 0.07888543605804443
epoch: 17, step: 101
	action: tensor([[ 5.5894e-01, -2.2168e-01, -1.4670e-01, -1.4801e-01,  5.1965e-01,
         -3.7602e-04, -3.7423e-02]], dtype=torch.float64)
	q_value: tensor([[-5.1036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41802067851005176, distance: 0.8729924146905768 entropy 0.07888543605804443
epoch: 17, step: 102
	action: tensor([[ 0.7027, -0.0113,  0.0085,  0.3510, -0.2854,  0.1835,  0.1906]],
       dtype=torch.float64)
	q_value: tensor([[-5.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8940719363197873, distance: 0.3724450529614417 entropy 0.07888543605804443
epoch: 17, step: 103
	action: tensor([[ 0.0016,  0.2960, -0.1991, -0.4817,  0.6794, -0.0775, -0.1811]],
       dtype=torch.float64)
	q_value: tensor([[-6.4275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35623645250764713, distance: 0.9181632037776403 entropy 0.07888543605804443
epoch: 17, step: 104
	action: tensor([[ 0.3157, -0.2944, -0.5539, -0.2109,  0.0819, -0.2579, -0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-4.6875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18533355136228447, distance: 1.0328722478928425 entropy 0.07888543605804443
epoch: 17, step: 105
	action: tensor([[ 0.5268, -0.0782, -0.5171, -0.1938,  0.0811, -0.2941, -0.4163]],
       dtype=torch.float64)
	q_value: tensor([[-5.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4806485275490321, distance: 0.8246836357189518 entropy 0.07888543605804443
epoch: 17, step: 106
	action: tensor([[-0.0594,  0.4632,  0.1316,  0.1599,  0.2451,  0.6251,  0.0504]],
       dtype=torch.float64)
	q_value: tensor([[-5.5125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6871448656363537, distance: 0.6400712737937544 entropy 0.07888543605804443
epoch: 17, step: 107
	action: tensor([[ 0.2015, -0.2392, -0.0587, -0.1258,  0.2530,  0.0817, -0.5484]],
       dtype=torch.float64)
	q_value: tensor([[-5.3761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23095478360395427, distance: 1.0035352471907484 entropy 0.07888543605804443
epoch: 17, step: 108
	action: tensor([[ 0.2137, -0.1524, -0.2048,  0.0481, -0.1070,  0.1617, -0.4569]],
       dtype=torch.float64)
	q_value: tensor([[-4.7977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4216071242499321, distance: 0.8702983512308476 entropy 0.07888543605804443
epoch: 17, step: 109
	action: tensor([[0.2289, 0.1832, 0.0633, 0.0512, 0.3366, 0.2629, 0.4188]],
       dtype=torch.float64)
	q_value: tensor([[-4.6660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7092742851417325, distance: 0.6170188284327853 entropy 0.07888543605804443
epoch: 17, step: 110
	action: tensor([[ 0.4256,  0.5077, -0.2441, -0.2132, -0.1060,  0.4755, -0.3356]],
       dtype=torch.float64)
	q_value: tensor([[-5.6133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8724028226133723, distance: 0.40876831700226735 entropy 0.07888543605804443
epoch: 17, step: 111
	action: tensor([[-0.1552,  0.3569, -0.4153, -0.2562,  0.1646, -0.3865, -0.1650]],
       dtype=torch.float64)
	q_value: tensor([[-5.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34467629669024225, distance: 0.9263703188348806 entropy 0.07888543605804443
epoch: 17, step: 112
	action: tensor([[-0.0419,  0.4884, -0.3446, -0.8601,  0.3320,  0.3761, -0.0928]],
       dtype=torch.float64)
	q_value: tensor([[-4.4252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5205131733648087, distance: 0.7924010333650193 entropy 0.07888543605804443
epoch: 17, step: 113
	action: tensor([[ 0.3330,  0.6300, -0.7848,  0.0668,  0.1019,  0.1080, -0.7516]],
       dtype=torch.float64)
	q_value: tensor([[-5.2581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8187260995884718, distance: 0.48721913116269794 entropy 0.07888543605804443
epoch: 17, step: 114
	action: tensor([[-0.1745, -0.1151, -0.6719,  0.4222,  0.5201,  0.3881, -0.0380]],
       dtype=torch.float64)
	q_value: tensor([[-5.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15041930410066973, distance: 1.0547730169216556 entropy 0.07888543605804443
epoch: 17, step: 115
	action: tensor([[ 0.3153,  0.6904, -0.0042,  0.0341,  0.0092,  0.1548, -0.2032]],
       dtype=torch.float64)
	q_value: tensor([[-5.2672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8269813222198961, distance: 0.4759958734009311 entropy 0.07888543605804443
epoch: 17, step: 116
	action: tensor([[ 0.1355, -0.2773, -0.3835, -0.4173,  0.4280, -0.0522, -0.4519]],
       dtype=torch.float64)
	q_value: tensor([[-5.5589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.056027336960312035, distance: 1.1118249151958797 entropy 0.07888543605804443
epoch: 17, step: 117
	action: tensor([[ 0.1640, -0.0149, -0.6111, -0.3116,  0.1473, -0.2353, -0.2141]],
       dtype=torch.float64)
	q_value: tensor([[-4.5929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3870115931230249, distance: 0.8959480401000482 entropy 0.07888543605804443
epoch: 17, step: 118
	action: tensor([[ 0.2348,  0.7266, -0.5182, -0.2474,  0.4185, -0.0285, -0.1412]],
       dtype=torch.float64)
	q_value: tensor([[-4.6065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8391525293685856, distance: 0.4589483482960515 entropy 0.07888543605804443
epoch: 17, step: 119
	action: tensor([[ 0.3911,  0.3450, -0.5911, -0.5255, -0.1189, -0.0667, -0.1936]],
       dtype=torch.float64)
	q_value: tensor([[-5.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7567432435712335, distance: 0.5644027862803845 entropy 0.07888543605804443
epoch: 17, step: 120
	action: tensor([[-0.0441,  0.1807, -0.3927, -0.3636,  0.1035,  0.1938, -0.2375]],
       dtype=torch.float64)
	q_value: tensor([[-5.3085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3729092327986424, distance: 0.9061954903002172 entropy 0.07888543605804443
epoch: 17, step: 121
	action: tensor([[ 0.4915,  0.1524, -0.2093, -0.6867,  0.0432, -0.2223, -0.2390]],
       dtype=torch.float64)
	q_value: tensor([[-4.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45264215257912876, distance: 0.8466274935317301 entropy 0.07888543605804443
epoch: 17, step: 122
	action: tensor([[ 0.4381,  0.1539, -0.3536, -0.2443, -0.1601, -0.2615, -0.3183]],
       dtype=torch.float64)
	q_value: tensor([[-5.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6539995744495046, distance: 0.6731238981523566 entropy 0.07888543605804443
epoch: 17, step: 123
	action: tensor([[ 0.5095,  0.1243,  0.1358,  0.0343,  0.5734,  0.2174, -0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-5.2645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8187515552514855, distance: 0.4871849207180927 entropy 0.07888543605804443
epoch: 17, step: 124
	action: tensor([[-0.2469,  0.0485, -0.1529, -0.0692,  0.4650, -0.8548,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-5.8352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11943593733771851, distance: 1.2107551242086028 entropy 0.07888543605804443
epoch: 17, step: 125
	action: tensor([[ 0.7100,  0.5695,  0.0738, -0.7980,  0.3156, -0.4397, -0.4367]],
       dtype=torch.float64)
	q_value: tensor([[-6.0665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7069002781753633, distance: 0.6195329314871976 entropy 0.07888543605804443
epoch: 17, step: 126
	action: tensor([[-0.0653,  0.1089, -0.5846, -0.3951,  0.0782, -0.2247, -0.2234]],
       dtype=torch.float64)
	q_value: tensor([[-7.4163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2673780335991891, distance: 0.9794824555060795 entropy 0.07888543605804443
epoch: 17, step: 127
	action: tensor([[ 0.1427, -0.2496, -0.0042, -0.5623,  0.3070,  0.0930,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-4.2199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0532165280499004, distance: 1.174398603622093 entropy 0.07888543605804443
LOSS epoch 17 actor 16.00257490304298 critic 61.47825782996003 
epoch: 18, step: 0
	action: tensor([[ 0.2452,  0.1942, -0.6904, -0.0770,  0.1705, -0.4409, -0.2263]],
       dtype=torch.float64)
	q_value: tensor([[-3.9008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.601870524283893, distance: 0.7220526384204244 entropy 0.03264415264129639
epoch: 18, step: 1
	action: tensor([[ 0.0622,  0.2927,  0.1358, -0.3703,  0.2869,  0.0647, -0.1371]],
       dtype=torch.float64)
	q_value: tensor([[-4.2889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4188942453534261, distance: 0.8723369759214318 entropy 0.03264415264129639
epoch: 18, step: 2
	action: tensor([[-0.0676,  0.1981, -0.1368, -0.3702, -0.1729,  0.4116,  0.2494]],
       dtype=torch.float64)
	q_value: tensor([[-3.7921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3219036617108677, distance: 0.942328644617752 entropy 0.03264415264129639
epoch: 18, step: 3
	action: tensor([[ 0.4019,  0.3009, -0.8598, -0.3505,  0.1506,  0.0505, -0.3761]],
       dtype=torch.float64)
	q_value: tensor([[-4.1710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7936212162431558, distance: 0.519863369835236 entropy 0.03264415264129639
epoch: 18, step: 4
	action: tensor([[ 0.2930,  0.2079, -0.5957, -0.4324, -0.0580, -0.0890, -0.2741]],
       dtype=torch.float64)
	q_value: tensor([[-4.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6437284893492199, distance: 0.6830417354477096 entropy 0.03264415264129639
epoch: 18, step: 5
	action: tensor([[-0.0740,  0.0599, -0.4498, -0.4679,  0.0141,  0.0491,  0.1636]],
       dtype=torch.float64)
	q_value: tensor([[-3.8145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22529262237944359, distance: 1.007222779268162 entropy 0.03264415264129639
epoch: 18, step: 6
	action: tensor([[ 0.4173,  0.3373, -0.0426, -0.1569, -0.1180,  0.1494,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[-3.5667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7966694110080293, distance: 0.5160099221828853 entropy 0.03264415264129639
epoch: 18, step: 7
	action: tensor([[ 0.2851,  0.2845,  0.4356,  0.1677,  0.0903, -0.0498, -0.4339]],
       dtype=torch.float64)
	q_value: tensor([[-4.3518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7816956952676458, distance: 0.5346724946201564 entropy 0.03264415264129639
epoch: 18, step: 8
	action: tensor([[ 0.2292,  0.0173, -0.0430, -0.3532, -0.1052,  0.0528,  0.1732]],
       dtype=torch.float64)
	q_value: tensor([[-5.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3616726732971688, distance: 0.9142782992171117 entropy 0.03264415264129639
epoch: 18, step: 9
	action: tensor([[ 0.3832, -0.0953, -0.2775, -0.2770,  0.7250, -0.4627, -0.1433]],
       dtype=torch.float64)
	q_value: tensor([[-4.1036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2782369179493386, distance: 0.9721964376883722 entropy 0.03264415264129639
epoch: 18, step: 10
	action: tensor([[ 0.1549,  0.4016,  0.0451, -0.4187, -0.0196,  0.4701, -0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-4.9314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6331977650727456, distance: 0.6930629452916154 entropy 0.03264415264129639
epoch: 18, step: 11
	action: tensor([[-0.0602, -0.0834, -0.2479, -0.2137, -0.1739,  0.1116,  0.1525]],
       dtype=torch.float64)
	q_value: tensor([[-4.4112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1390320291035395, distance: 1.061818263418564 entropy 0.03264415264129639
epoch: 18, step: 12
	action: tensor([[ 0.3562,  0.2154, -0.2132, -0.7090,  0.0419,  0.2971, -0.5845]],
       dtype=torch.float64)
	q_value: tensor([[-3.5742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5488046910712583, distance: 0.7686683177450045 entropy 0.03264415264129639
epoch: 18, step: 13
	action: tensor([[ 0.2579,  0.3278, -0.3551, -0.3603,  0.0737,  0.0960,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-4.2267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6808018827286317, distance: 0.6465272789580283 entropy 0.03264415264129639
epoch: 18, step: 14
	action: tensor([[ 0.0647,  0.1476, -0.8854, -0.2329,  0.0123,  0.5111, -0.5913]],
       dtype=torch.float64)
	q_value: tensor([[-3.8837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4974134130176372, distance: 0.8112638799874464 entropy 0.03264415264129639
epoch: 18, step: 15
	action: tensor([[ 0.3778, -0.0637, -0.2645, -0.3914,  0.1998, -0.6657, -0.5065]],
       dtype=torch.float64)
	q_value: tensor([[-3.7188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.265465850666045, distance: 0.9807598735933986 entropy 0.03264415264129639
epoch: 18, step: 16
	action: tensor([[ 0.2474,  0.2027,  0.0549, -0.4838,  0.6750, -0.0987,  0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-4.9466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40666788303627777, distance: 0.8814661148648976 entropy 0.03264415264129639
epoch: 18, step: 17
	action: tensor([[ 0.2591,  0.0998, -0.3308,  0.4134, -0.1732, -0.1101,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-4.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.699147325696882, distance: 0.6276732659340081 entropy 0.03264415264129639
epoch: 18, step: 18
	action: tensor([[-0.1239,  0.1732, -0.4073, -0.2679,  0.4853,  0.0191,  0.0958]],
       dtype=torch.float64)
	q_value: tensor([[-4.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2759201975458119, distance: 0.9737554692501992 entropy 0.03264415264129639
epoch: 18, step: 19
	action: tensor([[ 0.4548,  0.3819, -0.1831, -0.0868,  0.1790,  0.0196, -0.2609]],
       dtype=torch.float64)
	q_value: tensor([[-3.4341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8346686684931836, distance: 0.46530130970032924 entropy 0.03264415264129639
epoch: 18, step: 20
	action: tensor([[ 0.1360,  0.0250,  0.1950, -0.0914,  0.3890, -0.4383, -0.4395]],
       dtype=torch.float64)
	q_value: tensor([[-4.1333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27625741982488616, distance: 0.9735286915668656 entropy 0.03264415264129639
epoch: 18, step: 21
	action: tensor([[0.0969, 0.1384, 0.1936, 0.0640, 0.4849, 0.6542, 0.1933]],
       dtype=torch.float64)
	q_value: tensor([[-4.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7173729716279498, distance: 0.6083640457551885 entropy 0.03264415264129639
epoch: 18, step: 22
	action: tensor([[ 0.2935,  0.1730, -0.1835, -0.4088, -0.0791, -0.2186, -0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-4.5279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4739804168335292, distance: 0.8299609319218944 entropy 0.03264415264129639
epoch: 18, step: 23
	action: tensor([[ 0.4213,  0.1724, -0.0028, -0.3249,  0.2407, -0.3554, -0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-4.0377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5232374427036406, distance: 0.7901467599239638 entropy 0.03264415264129639
epoch: 18, step: 24
	action: tensor([[ 0.4164,  0.0439, -0.0411, -0.8601, -0.3175,  0.2333, -0.6294]],
       dtype=torch.float64)
	q_value: tensor([[-4.5571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28574613430564544, distance: 0.967125853303911 entropy 0.03264415264129639
epoch: 18, step: 25
	action: tensor([[ 0.3674,  0.0395,  0.3597, -0.0749,  0.0667, -0.0633, -0.9871]],
       dtype=torch.float64)
	q_value: tensor([[-5.0166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5694843994441607, distance: 0.7508464643318 entropy 0.03264415264129639
epoch: 18, step: 26
	action: tensor([[ 0.0748,  0.6195, -0.2965, -0.4763,  0.3709,  0.1429, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-5.3325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6853178086648838, distance: 0.6419375437472725 entropy 0.03264415264129639
epoch: 18, step: 27
	action: tensor([[ 0.4199,  0.5995, -0.4675, -0.1257,  0.0372,  0.1036, -0.0947]],
       dtype=torch.float64)
	q_value: tensor([[-3.8846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8929918352070865, distance: 0.3743390649642811 entropy 0.03264415264129639
epoch: 18, step: 28
	action: tensor([[ 0.0797,  0.2582, -0.4089,  0.2637, -0.2648,  0.2297,  0.1667]],
       dtype=torch.float64)
	q_value: tensor([[-4.3169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5763333609234544, distance: 0.7448500084790408 entropy 0.03264415264129639
epoch: 18, step: 29
	action: tensor([[ 0.5479,  0.5500, -0.3472, -0.2149,  0.0848, -0.0116, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-3.9450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9199394350375572, distance: 0.32379192854528366 entropy 0.03264415264129639
epoch: 18, step: 30
	action: tensor([[ 0.6055,  0.1610,  0.0943, -0.4202,  0.0991, -0.1632, -0.3350]],
       dtype=torch.float64)
	q_value: tensor([[-4.6561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5914940346377581, distance: 0.7314015808760075 entropy 0.03264415264129639
epoch: 18, step: 31
	action: tensor([[-0.2698,  0.1968, -0.3814, -0.3517,  0.0727, -0.0892, -0.2178]],
       dtype=torch.float64)
	q_value: tensor([[-4.9192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1151787113503141, distance: 1.0764267328341908 entropy 0.03264415264129639
epoch: 18, step: 32
	action: tensor([[ 0.1423,  0.6376, -0.4297, -0.3752,  0.2669, -0.4039,  0.0999]],
       dtype=torch.float64)
	q_value: tensor([[-3.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7146927762196824, distance: 0.6112418439271388 entropy 0.03264415264129639
epoch: 18, step: 33
	action: tensor([[ 0.1345, -0.0225, -0.1214, -0.3763,  0.2209,  0.4961,  0.1379]],
       dtype=torch.float64)
	q_value: tensor([[-4.2140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39163456048170187, distance: 0.8925631657800521 entropy 0.03264415264129639
epoch: 18, step: 34
	action: tensor([[ 0.1231,  0.1870, -0.1828, -0.4834,  0.0567,  0.1548,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-4.0005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41527082681296257, distance: 0.8750524282996817 entropy 0.03264415264129639
epoch: 18, step: 35
	action: tensor([[-0.0367,  0.1237, -0.1776,  0.0030,  0.0922, -0.4969, -0.2460]],
       dtype=torch.float64)
	q_value: tensor([[-3.6985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29897687724854516, distance: 0.9581265280370493 entropy 0.03264415264129639
epoch: 18, step: 36
	action: tensor([[-0.4365, -0.0374, -0.0295, -0.1637,  0.5118,  0.0289, -0.1946]],
       dtype=torch.float64)
	q_value: tensor([[-4.0141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2786779110014421, distance: 1.294008933800899 entropy 0.03264415264129639
epoch: 18, step: 37
	action: tensor([[ 0.4523, -0.1039, -0.3948, -0.6017,  0.3015, -0.0031, -0.3256]],
       dtype=torch.float64)
	q_value: tensor([[-3.5117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2864330099361748, distance: 0.9666607140886939 entropy 0.03264415264129639
epoch: 18, step: 38
	action: tensor([[ 0.0908,  0.1503, -0.3187, -0.3143,  0.0838, -0.0119,  0.1484]],
       dtype=torch.float64)
	q_value: tensor([[-4.0058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4215863375292982, distance: 0.8703139898105376 entropy 0.03264415264129639
epoch: 18, step: 39
	action: tensor([[-0.1785,  0.2836,  0.0520,  0.1891,  0.3235, -0.2100, -0.3501]],
       dtype=torch.float64)
	q_value: tensor([[-3.5555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34053937433930015, distance: 0.929289710358573 entropy 0.03264415264129639
epoch: 18, step: 40
	action: tensor([[ 0.4898,  0.2069, -0.0561,  0.3782, -0.3621,  0.2821, -0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-3.9597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8861232200163005, distance: 0.38616622124296546 entropy 0.03264415264129639
epoch: 18, step: 41
	action: tensor([[ 0.4744,  0.2382, -0.2886, -0.0916,  0.3373, -0.2847, -0.4490]],
       dtype=torch.float64)
	q_value: tensor([[-4.8641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7426005161719742, distance: 0.5805779386839793 entropy 0.03264415264129639
epoch: 18, step: 42
	action: tensor([[0.4999, 0.1719, 0.1664, 0.0958, 0.4558, 0.0070, 0.0368]],
       dtype=torch.float64)
	q_value: tensor([[-4.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8202339930919074, distance: 0.48518847820517064 entropy 0.03264415264129639
epoch: 18, step: 43
	action: tensor([[ 0.2082, -0.2955,  0.0120,  0.1400, -0.6585,  0.1310,  0.1168]],
       dtype=torch.float64)
	q_value: tensor([[-4.8657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35979068527681723, distance: 0.9156250960245516 entropy 0.03264415264129639
epoch: 18, step: 44
	action: tensor([[-0.1549,  0.3727, -0.1645,  0.3714, -0.0174, -0.6378, -0.2792]],
       dtype=torch.float64)
	q_value: tensor([[-4.8246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43782642959010787, distance: 0.8580091224761621 entropy 0.03264415264129639
epoch: 18, step: 45
	action: tensor([[-0.0031,  0.7898,  0.0776,  0.0483, -0.1613,  0.0083, -0.2442]],
       dtype=torch.float64)
	q_value: tensor([[-4.6039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6480563625725415, distance: 0.6788803720072751 entropy 0.03264415264129639
epoch: 18, step: 46
	action: tensor([[ 0.0256,  0.5861, -0.2100,  0.0669, -0.0268, -0.0493, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-4.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6457964186922924, distance: 0.6810565393053254 entropy 0.03264415264129639
epoch: 18, step: 47
	action: tensor([[ 0.0818,  0.2707, -0.5785, -0.3303, -0.0267, -0.1128,  0.2585]],
       dtype=torch.float64)
	q_value: tensor([[-3.8179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5555321915668116, distance: 0.7629162214045099 entropy 0.03264415264129639
epoch: 18, step: 48
	action: tensor([[ 0.6509,  0.0473, -0.4701, -0.3333,  0.0469,  0.2587, -0.5336]],
       dtype=torch.float64)
	q_value: tensor([[-3.9441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6894220011217956, distance: 0.6377376204102092 entropy 0.03264415264129639
epoch: 18, step: 49
	action: tensor([[ 0.7191,  0.3397,  0.0218, -0.5837,  0.1744, -0.2739, -0.4023]],
       dtype=torch.float64)
	q_value: tensor([[-4.4217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6719309197346122, distance: 0.6554496619535949 entropy 0.03264415264129639
epoch: 18, step: 50
	action: tensor([[-0.1013,  0.0948, -0.0886, -0.4484,  0.4333,  0.0699, -0.4652]],
       dtype=torch.float64)
	q_value: tensor([[-5.4276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13537123372118454, distance: 1.06407326961333 entropy 0.03264415264129639
epoch: 18, step: 51
	action: tensor([[ 0.2996,  0.2144, -0.0464, -0.2605, -0.2195,  0.1142, -0.4292]],
       dtype=torch.float64)
	q_value: tensor([[-3.3043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6145365622040813, distance: 0.7104741620218339 entropy 0.03264415264129639
epoch: 18, step: 52
	action: tensor([[ 0.5303,  0.4231, -0.3537, -0.1885,  0.1204,  0.3829, -0.8033]],
       dtype=torch.float64)
	q_value: tensor([[-4.2083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9008533835384256, distance: 0.3603260344970476 entropy 0.03264415264129639
epoch: 18, step: 53
	action: tensor([[-0.1984,  0.6344,  0.1409, -0.5715,  0.2139, -0.3367, -0.5670]],
       dtype=torch.float64)
	q_value: tensor([[-4.6035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20162528087664855, distance: 1.0224923830457873 entropy 0.03264415264129639
epoch: 18, step: 54
	action: tensor([[ 0.4085,  0.2449, -0.2047, -0.2561, -0.0082, -0.0889, -0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-4.3337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6706859433226486, distance: 0.6566921543323839 entropy 0.03264415264129639
epoch: 18, step: 55
	action: tensor([[ 0.5000, -0.2578, -0.0691, -0.3757,  0.2717, -0.1263, -0.7526]],
       dtype=torch.float64)
	q_value: tensor([[-4.1126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18267782768903862, distance: 1.034554403474412 entropy 0.03264415264129639
epoch: 18, step: 56
	action: tensor([[ 0.3147,  0.2815, -0.2855,  0.1007, -0.0271, -0.0165, -0.2846]],
       dtype=torch.float64)
	q_value: tensor([[-4.5491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.74388805947744, distance: 0.5791240577984873 entropy 0.03264415264129639
epoch: 18, step: 57
	action: tensor([[-0.0962,  0.2437, -0.2707, -0.3604,  0.2179,  0.4529, -0.2035]],
       dtype=torch.float64)
	q_value: tensor([[-3.9259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4040084005501894, distance: 0.8834393964448668 entropy 0.03264415264129639
epoch: 18, step: 58
	action: tensor([[ 0.2630,  0.2723, -0.1853,  0.0641,  0.0922,  0.0806, -0.5173]],
       dtype=torch.float64)
	q_value: tensor([[-3.3794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7343645230998546, distance: 0.5897931600126687 entropy 0.03264415264129639
epoch: 18, step: 59
	action: tensor([[ 0.3872,  0.0882, -0.4506, -0.4807, -0.0856,  0.1979,  0.1188]],
       dtype=torch.float64)
	q_value: tensor([[-3.8799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5789661178149282, distance: 0.7425320716284469 entropy 0.03264415264129639
epoch: 18, step: 60
	action: tensor([[ 0.3885,  0.3327, -0.2229, -0.3688, -0.0573,  0.0460,  0.1267]],
       dtype=torch.float64)
	q_value: tensor([[-4.3443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7349971518561633, distance: 0.5890904253191572 entropy 0.03264415264129639
epoch: 18, step: 61
	action: tensor([[ 0.2574,  0.0281,  0.0134,  0.1908,  0.2586, -0.1633, -0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-4.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6011167106135358, distance: 0.7227358781537231 entropy 0.03264415264129639
epoch: 18, step: 62
	action: tensor([[-0.1243,  0.1342, -0.0644,  0.0771,  0.4026, -0.1465, -0.3278]],
       dtype=torch.float64)
	q_value: tensor([[-4.3327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27348330629514683, distance: 0.9753926803684649 entropy 0.03264415264129639
epoch: 18, step: 63
	action: tensor([[-0.0852,  0.3419,  0.1393, -0.7919,  0.3291,  0.2037, -0.1431]],
       dtype=torch.float64)
	q_value: tensor([[-3.6749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15333064519684925, distance: 1.0529642191704072 entropy 0.03264415264129639
epoch: 18, step: 64
	action: tensor([[ 0.2277,  0.2574, -0.3284, -0.3932, -0.0567, -0.1614, -0.1725]],
       dtype=torch.float64)
	q_value: tensor([[-4.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5376632345284339, distance: 0.778100883496936 entropy 0.03264415264129639
epoch: 18, step: 65
	action: tensor([[ 0.4760,  0.4222, -0.2293,  0.2601, -0.0198,  0.3341, -0.3242]],
       dtype=torch.float64)
	q_value: tensor([[-3.7468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8833311626045596, distance: 0.3908716106109434 entropy 0.03264415264129639
epoch: 18, step: 66
	action: tensor([[ 0.1780,  0.4547, -0.3374, -0.3024,  0.0884,  0.6447, -0.1837]],
       dtype=torch.float64)
	q_value: tensor([[-4.4522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7425929809221192, distance: 0.5805864366959917 entropy 0.03264415264129639
epoch: 18, step: 67
	action: tensor([[ 0.5628,  0.2878, -0.3912, -0.4556,  0.0256,  0.2393,  0.1064]],
       dtype=torch.float64)
	q_value: tensor([[-4.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7845646305417739, distance: 0.5311475666540406 entropy 0.03264415264129639
epoch: 18, step: 68
	action: tensor([[ 0.0346,  0.1290,  0.0110, -0.4703, -0.3282,  0.3153, -0.3072]],
       dtype=torch.float64)
	q_value: tensor([[-4.6779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28866166021913797, distance: 0.9651499705425062 entropy 0.03264415264129639
epoch: 18, step: 69
	action: tensor([[ 0.3982, -0.1126,  0.0879, -0.0643,  0.2551,  0.2564, -0.3072]],
       dtype=torch.float64)
	q_value: tensor([[-4.2261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5630103540328085, distance: 0.7564709706026764 entropy 0.03264415264129639
epoch: 18, step: 70
	action: tensor([[-0.1568, -0.0815,  0.5383, -0.3869,  0.3140, -0.1286,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[-4.2235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25814263079412547, distance: 1.2835761306472249 entropy 0.03264415264129639
epoch: 18, step: 71
	action: tensor([[ 0.6043,  0.0736, -0.2031, -0.6847,  0.4738,  0.1368, -0.1294]],
       dtype=torch.float64)
	q_value: tensor([[-4.6210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47630966153923615, distance: 0.8281213360399045 entropy 0.03264415264129639
epoch: 18, step: 72
	action: tensor([[ 0.4367,  0.3291, -0.1216, -0.0141,  0.0083, -0.0356, -0.2161]],
       dtype=torch.float64)
	q_value: tensor([[-4.7172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8222567057644481, distance: 0.48245110535173796 entropy 0.03264415264129639
epoch: 18, step: 73
	action: tensor([[ 0.2318,  0.0267,  0.1632, -0.3871, -0.2329, -0.0210, -0.2897]],
       dtype=torch.float64)
	q_value: tensor([[-4.2487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3272204029580249, distance: 0.9386271226723618 entropy 0.03264415264129639
epoch: 18, step: 74
	action: tensor([[-0.4997,  0.3487, -0.4503, -0.4377,  0.7875,  0.1028, -0.7748]],
       dtype=torch.float64)
	q_value: tensor([[-4.3647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008933500354910251, distance: 1.139221286925373 entropy 0.03264415264129639
epoch: 18, step: 75
	action: tensor([[ 0.3804,  0.1487, -0.2224, -0.4434,  0.6400,  0.1190, -0.7539]],
       dtype=torch.float64)
	q_value: tensor([[-3.7810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5650745487501452, distance: 0.7546821962341912 entropy 0.03264415264129639
epoch: 18, step: 76
	action: tensor([[ 0.3435,  0.7555, -0.1957, -0.2479, -0.4972,  0.1643, -0.3970]],
       dtype=torch.float64)
	q_value: tensor([[-4.1297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8475221441229903, distance: 0.4468482705831153 entropy 0.03264415264129639
epoch: 18, step: 77
	action: tensor([[ 0.2219,  0.4438, -0.1743, -0.4491,  0.0132, -0.1336, -0.1583]],
       dtype=torch.float64)
	q_value: tensor([[-5.1896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6522264324468492, distance: 0.674846466884754 entropy 0.03264415264129639
epoch: 18, step: 78
	action: tensor([[ 0.2524,  0.3888, -0.1389,  0.1251,  0.1591,  0.0905, -0.1407]],
       dtype=torch.float64)
	q_value: tensor([[-3.9839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7873640108114153, distance: 0.5276854011184026 entropy 0.03264415264129639
epoch: 18, step: 79
	action: tensor([[ 0.2793, -0.1126, -0.2879, -0.1712, -0.3126,  0.0795, -0.3101]],
       dtype=torch.float64)
	q_value: tensor([[-3.9112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41630713206422376, distance: 0.874276664398601 entropy 0.03264415264129639
epoch: 18, step: 80
	action: tensor([[ 0.1167,  0.8313, -0.0143, -0.3188, -0.1918,  0.2586, -0.2577]],
       dtype=torch.float64)
	q_value: tensor([[-3.9024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.715211675266414, distance: 0.6106857466247763 entropy 0.03264415264129639
epoch: 18, step: 81
	action: tensor([[ 0.0696,  0.0938, -0.0464, -0.2467, -0.1140,  0.1437,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-4.8616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3777328560557186, distance: 0.9027035046568526 entropy 0.03264415264129639
epoch: 18, step: 82
	action: tensor([[ 0.0908,  0.2234, -0.2979, -0.2944,  0.3223, -0.1164, -0.1855]],
       dtype=torch.float64)
	q_value: tensor([[-3.7687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4619587937151244, distance: 0.8393912971696703 entropy 0.03264415264129639
epoch: 18, step: 83
	action: tensor([[ 0.2722,  0.2568, -0.3788, -0.3563,  0.1654,  0.1033, -0.1892]],
       dtype=torch.float64)
	q_value: tensor([[-3.3866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6478190233681562, distance: 0.67910924071671 entropy 0.03264415264129639
epoch: 18, step: 84
	action: tensor([[-0.2248,  0.6811, -1.0689, -0.2206, -0.2063, -0.2222, -0.2401]],
       dtype=torch.float64)
	q_value: tensor([[-3.5852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5966235519246659, distance: 0.7267950520494494 entropy 0.03264415264129639
epoch: 18, step: 85
	action: tensor([[ 0.4676,  0.2319, -0.3310,  0.1449,  0.4136,  0.2332, -0.5105]],
       dtype=torch.float64)
	q_value: tensor([[-4.1991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8643616832385101, distance: 0.4214517767758276 entropy 0.03264415264129639
epoch: 18, step: 86
	action: tensor([[ 0.0478, -0.1078,  0.0171, -0.0277,  0.0457, -0.0515, -0.5165]],
       dtype=torch.float64)
	q_value: tensor([[-4.2931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25961415540728106, distance: 0.9846587549485442 entropy 0.03264415264129639
epoch: 18, step: 87
	action: tensor([[ 0.1351,  0.4489, -0.3465, -0.1016,  0.1428, -0.0825, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-3.8378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.672315759733378, distance: 0.6550651129684829 entropy 0.03264415264129639
epoch: 18, step: 88
	action: tensor([[ 0.2231,  0.2607, -0.6319, -0.0127,  0.2850,  0.0515, -0.7425]],
       dtype=torch.float64)
	q_value: tensor([[-3.6342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.684400745289008, distance: 0.642872247311324 entropy 0.03264415264129639
epoch: 18, step: 89
	action: tensor([[ 0.0595, -0.0364, -0.3957, -0.2070,  0.3178,  0.0511, -0.0517]],
       dtype=torch.float64)
	q_value: tensor([[-3.8715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.310572088701663, distance: 0.950169584175907 entropy 0.03264415264129639
epoch: 18, step: 90
	action: tensor([[ 0.1190,  0.4157, -0.2871, -0.3212,  0.5902, -0.2548,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-3.3175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5635808588700215, distance: 0.7559770100461893 entropy 0.03264415264129639
epoch: 18, step: 91
	action: tensor([[ 0.1775,  0.5127, -0.2228, -0.5252, -0.0058, -0.1109, -0.5280]],
       dtype=torch.float64)
	q_value: tensor([[-4.0317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6487843367073156, distance: 0.6781778970610101 entropy 0.03264415264129639
epoch: 18, step: 92
	action: tensor([[ 0.3053,  0.4055, -0.4684, -0.1866,  0.0561,  0.4602,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-4.1326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7971481565632167, distance: 0.5154020868274513 entropy 0.03264415264129639
epoch: 18, step: 93
	action: tensor([[ 0.1807, -0.0215, -0.1054, -0.6241,  0.1246,  0.2740, -0.1840]],
       dtype=torch.float64)
	q_value: tensor([[-4.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2592990022710383, distance: 0.9848682979060912 entropy 0.03264415264129639
epoch: 18, step: 94
	action: tensor([[ 0.4742, -0.2172, -0.0352, -0.8665, -0.2511,  0.3610, -0.5142]],
       dtype=torch.float64)
	q_value: tensor([[-3.8218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08582879067268845, distance: 1.0941338720576468 entropy 0.03264415264129639
epoch: 18, step: 95
	action: tensor([[ 0.4969, -0.3334, -0.3738, -0.6334, -0.0815,  0.0385, -0.5525]],
       dtype=torch.float64)
	q_value: tensor([[-4.9606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08158410684198525, distance: 1.096671073915674 entropy 0.03264415264129639
epoch: 18, step: 96
	action: tensor([[ 0.3719,  0.6092, -0.2894, -0.1473, -0.0702,  0.1584,  0.1257]],
       dtype=torch.float64)
	q_value: tensor([[-4.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8622772697513092, distance: 0.4246777474261705 entropy 0.03264415264129639
epoch: 18, step: 97
	action: tensor([[ 0.1073,  0.2586,  0.1664, -0.0870, -0.3669, -0.0121,  0.2355]],
       dtype=torch.float64)
	q_value: tensor([[-4.4877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5428395148276768, distance: 0.7737328497602152 entropy 0.03264415264129639
epoch: 18, step: 98
	action: tensor([[ 0.7873,  0.1255, -0.2353, -0.6999, -0.3220, -0.0460, -0.5620]],
       dtype=torch.float64)
	q_value: tensor([[-4.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4920985359872396, distance: 0.8155421759707197 entropy 0.03264415264129639
epoch: 18, step: 99
	action: tensor([[ 0.5217,  0.4027,  0.2571, -0.3300,  0.3258, -0.1571, -0.3536]],
       dtype=torch.float64)
	q_value: tensor([[-5.3904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7644803187795438, distance: 0.5553545005243371 entropy 0.03264415264129639
epoch: 18, step: 100
	action: tensor([[ 0.4669, -0.1334, -0.1478, -0.3953,  0.4601,  0.0802,  0.0797]],
       dtype=torch.float64)
	q_value: tensor([[-5.0565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3587150573780259, distance: 0.9163939528242754 entropy 0.03264415264129639
epoch: 18, step: 101
	action: tensor([[ 0.4783,  0.3104, -0.0551, -0.4618,  0.3202,  0.3555, -0.6828]],
       dtype=torch.float64)
	q_value: tensor([[-4.3904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7585752408715493, distance: 0.5622734754073642 entropy 0.03264415264129639
epoch: 18, step: 102
	action: tensor([[ 0.3333,  0.2971, -0.2975, -0.1650,  0.0628,  0.3723,  0.1402]],
       dtype=torch.float64)
	q_value: tensor([[-4.5049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7753823063780313, distance: 0.5423487893727094 entropy 0.03264415264129639
epoch: 18, step: 103
	action: tensor([[ 0.2291,  0.5334, -0.4825, -0.1019,  0.3382, -0.0454, -0.2565]],
       dtype=torch.float64)
	q_value: tensor([[-4.1093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7863767018966223, distance: 0.528909053663657 entropy 0.03264415264129639
epoch: 18, step: 104
	action: tensor([[ 0.2446, -0.2896,  0.3004,  0.0292,  0.1656, -0.0115, -0.4664]],
       dtype=torch.float64)
	q_value: tensor([[-3.8637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984958706163563, distance: 0.958455180667051 entropy 0.03264415264129639
epoch: 18, step: 105
	action: tensor([[-0.0173,  0.0753, -0.3244, -0.2841,  0.3392,  0.0035, -0.3646]],
       dtype=torch.float64)
	q_value: tensor([[-4.5549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29242588908892597, distance: 0.9625929141912658 entropy 0.03264415264129639
epoch: 18, step: 106
	action: tensor([[ 0.2293,  0.5168, -0.4815,  0.3691,  0.0783, -0.0197,  0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-3.1874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7264078656367383, distance: 0.5985611110727612 entropy 0.03264415264129639
epoch: 18, step: 107
	action: tensor([[ 0.1442,  0.5992,  0.0792, -0.3282,  0.2039,  0.5144, -0.5682]],
       dtype=torch.float64)
	q_value: tensor([[-4.2871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7413555017205156, distance: 0.5819803423714475 entropy 0.03264415264129639
epoch: 18, step: 108
	action: tensor([[-0.2281,  0.2662, -0.1490, -0.1649, -0.0027, -0.3361, -0.0864]],
       dtype=torch.float64)
	q_value: tensor([[-4.4924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1725957407738562, distance: 1.040915724846012 entropy 0.03264415264129639
epoch: 18, step: 109
	action: tensor([[ 0.4987,  0.0631, -0.2711, -0.1612, -0.0496, -0.5074,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-3.5012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5507538385862915, distance: 0.76700621099149 entropy 0.03264415264129639
epoch: 18, step: 110
	action: tensor([[ 0.3035,  0.4955, -0.1769, -0.0017, -0.3353, -0.3308, -0.3598]],
       dtype=torch.float64)
	q_value: tensor([[-4.8727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.801424536142454, distance: 0.5099404773014463 entropy 0.03264415264129639
epoch: 18, step: 111
	action: tensor([[ 0.3424,  0.1576, -0.1313, -0.7841,  0.0823,  0.3216,  0.6212]],
       dtype=torch.float64)
	q_value: tensor([[-4.6258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.448593279030052, distance: 0.8497530282174889 entropy 0.03264415264129639
epoch: 18, step: 112
	action: tensor([[ 0.3121,  0.7696, -0.0797, -0.0500, -0.2425, -0.2972, -0.2608]],
       dtype=torch.float64)
	q_value: tensor([[-5.5415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8565607011247502, distance: 0.43340185932663816 entropy 0.03264415264129639
epoch: 18, step: 113
	action: tensor([[ 0.5794,  0.1467, -0.1621, -0.4061,  0.0733, -0.0667,  0.1024]],
       dtype=torch.float64)
	q_value: tensor([[-4.9923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6190384588301298, distance: 0.7063130993060003 entropy 0.03264415264129639
epoch: 18, step: 114
	action: tensor([[ 0.0858, -0.1627,  0.2680, -0.0941, -0.1048, -0.0445, -0.2151]],
       dtype=torch.float64)
	q_value: tensor([[-4.6733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1950291429912674, distance: 1.0267075887265689 entropy 0.03264415264129639
epoch: 18, step: 115
	action: tensor([[-0.0615,  0.1804, -0.3373, -0.3057,  0.1996,  0.4246, -0.3105]],
       dtype=torch.float64)
	q_value: tensor([[-4.1873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4073741552541511, distance: 0.8809413325508384 entropy 0.03264415264129639
epoch: 18, step: 116
	action: tensor([[ 0.2476,  0.2892, -0.5690, -0.0822, -0.2804,  0.3618, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-3.2475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6967480457906805, distance: 0.6301711218711215 entropy 0.03264415264129639
epoch: 18, step: 117
	action: tensor([[-0.0299, -0.0093, -0.1378,  0.3178,  0.0196,  0.0111, -0.1366]],
       dtype=torch.float64)
	q_value: tensor([[-3.9854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41141238586516726, distance: 0.8779347768575206 entropy 0.03264415264129639
epoch: 18, step: 118
	action: tensor([[-0.1532,  0.2940, -0.1401,  0.0359, -0.0953,  0.4378, -0.4452]],
       dtype=torch.float64)
	q_value: tensor([[-3.7985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42222733470390283, distance: 0.86983161578677 entropy 0.03264415264129639
epoch: 18, step: 119
	action: tensor([[ 0.4203,  0.4572,  0.3444, -0.1225, -0.0778,  0.1324, -0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-3.6331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8431078328557394, distance: 0.4532703635393617 entropy 0.03264415264129639
epoch: 18, step: 120
	action: tensor([[-0.0996,  0.3114, -0.1033, -0.1285, -0.1701, -0.0095,  0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-5.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38452119828717113, distance: 0.8977661843695687 entropy 0.03264415264129639
epoch: 18, step: 121
	action: tensor([[ 0.0134,  0.2196, -0.0922, -0.4172,  0.0730,  0.0590,  0.0523]],
       dtype=torch.float64)
	q_value: tensor([[-3.7226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3383537405866124, distance: 0.9308283972535505 entropy 0.03264415264129639
epoch: 18, step: 122
	action: tensor([[ 0.4528,  0.1100, -0.6550,  0.2144,  0.0202, -0.0060, -0.1599]],
       dtype=torch.float64)
	q_value: tensor([[-3.5988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7396099228917179, distance: 0.583940918140191 entropy 0.03264415264129639
epoch: 18, step: 123
	action: tensor([[ 0.0326,  0.1094, -0.5873, -0.7539, -0.0835, -0.2997, -0.2227]],
       dtype=torch.float64)
	q_value: tensor([[-4.2979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35952735249807954, distance: 0.9158133854686688 entropy 0.03264415264129639
epoch: 18, step: 124
	action: tensor([[ 0.1025,  0.2647, -0.5204, -0.3997,  0.4791,  0.1298,  0.1078]],
       dtype=torch.float64)
	q_value: tensor([[-3.8093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5756317572182761, distance: 0.745466499489712 entropy 0.03264415264129639
epoch: 18, step: 125
	action: tensor([[ 0.3276,  0.2101, -0.1133, -0.3830, -0.1417, -0.3138, -0.3441]],
       dtype=torch.float64)
	q_value: tensor([[-3.7795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5456127916804046, distance: 0.7713824283933454 entropy 0.03264415264129639
epoch: 18, step: 126
	action: tensor([[-0.4455, -0.4354, -0.0282,  0.1639, -0.1696,  0.2863, -0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-4.3642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37070089936495365, distance: 1.3397631923433366 entropy 0.03264415264129639
epoch: 18, step: 127
	action: tensor([[ 0.2275, -0.3848, -0.0313, -0.0401,  0.0408,  0.4699, -0.2634]],
       dtype=torch.float64)
	q_value: tensor([[-3.9475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2721887265027716, distance: 0.9762613194881831 entropy 0.03264415264129639
LOSS epoch 18 actor 11.370385769351287 critic 73.13366105569784 
epoch: 19, step: 0
	action: tensor([[ 0.3846,  0.6824, -0.0420,  0.0868,  0.1251,  0.1336, -0.2733]],
       dtype=torch.float64)
	q_value: tensor([[-3.2667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8608194562055373, distance: 0.4269194662366738 entropy 0.03264415264129639
epoch: 19, step: 1
	action: tensor([[ 0.5915, -0.1006, -0.4598, -0.4784,  0.1675,  0.1899, -0.0746]],
       dtype=torch.float64)
	q_value: tensor([[-3.6409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.452361598544582, distance: 0.8468444396589581 entropy 0.03264415264129639
epoch: 19, step: 2
	action: tensor([[ 0.6068,  0.1073, -0.4750, -0.2946, -0.1353,  0.3678, -0.6667]],
       dtype=torch.float64)
	q_value: tensor([[-3.5115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7329246378506311, distance: 0.5913894956456821 entropy 0.03264415264129639
epoch: 19, step: 3
	action: tensor([[ 0.6999,  0.9091, -0.5399, -0.4656, -0.0732, -0.0832, -0.5461]],
       dtype=torch.float64)
	q_value: tensor([[-3.5856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09858772602146768 entropy 0.03264415264129639
epoch: 19, step: 4
	action: tensor([[ 1.0755,  0.4565, -0.0871, -0.9055,  0.2130, -0.0059, -0.2777]],
       dtype=torch.float64)
	q_value: tensor([[-4.3638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6628967660222608, distance: 0.6644130480343248 entropy 0.03264415264129639
epoch: 19, step: 5
	action: tensor([[ 0.5302,  0.3852, -0.5393, -0.1625, -0.1264, -0.1360, -0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-5.3443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8324110832970026, distance: 0.4684673630999226 entropy 0.03264415264129639
epoch: 19, step: 6
	action: tensor([[ 0.4992,  0.6213,  0.0081, -0.6092, -0.1607,  0.2477, -0.9085]],
       dtype=torch.float64)
	q_value: tensor([[-3.5987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8196407010843005, distance: 0.4859884662937467 entropy 0.03264415264129639
epoch: 19, step: 7
	action: tensor([[ 0.7021,  0.8183, -0.5499, -0.6577, -0.0180,  0.3162, -0.0822]],
       dtype=torch.float64)
	q_value: tensor([[-4.4839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9863613175397395, distance: 0.1336419802901459 entropy 0.03264415264129639
epoch: 19, step: 8
	action: tensor([[ 0.5859,  0.1105, -0.2455, -0.6454,  0.3473,  0.2796, -0.4360]],
       dtype=torch.float64)
	q_value: tensor([[-4.6651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5691670652619943, distance: 0.7511231388092589 entropy 0.03264415264129639
epoch: 19, step: 9
	action: tensor([[ 0.5564,  0.3256,  0.2829, -0.3388, -0.0373,  0.1449, -0.2297]],
       dtype=torch.float64)
	q_value: tensor([[-3.4382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7486924703626555, distance: 0.5736664397173329 entropy 0.03264415264129639
epoch: 19, step: 10
	action: tensor([[ 0.8906,  0.6020, -0.2622, -0.0850, -0.0827,  0.1175, -0.3259]],
       dtype=torch.float64)
	q_value: tensor([[-4.1635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9849826700339497, distance: 0.14023391379208983 entropy 0.03264415264129639
epoch: 19, step: 11
	action: tensor([[ 0.6488,  0.2185, -0.4857, -0.3960, -0.1694, -0.0098, -0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-4.5140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.739872587294895, distance: 0.5836463232116511 entropy 0.03264415264129639
epoch: 19, step: 12
	action: tensor([[ 0.5458,  0.1469,  0.0590, -0.2838, -0.2987,  0.4866, -0.1680]],
       dtype=torch.float64)
	q_value: tensor([[-3.7736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7261274628905077, distance: 0.5988677631585181 entropy 0.03264415264129639
epoch: 19, step: 13
	action: tensor([[ 0.5438,  0.0861, -0.5827, -0.4655,  0.2030, -0.1286, -0.4647]],
       dtype=torch.float64)
	q_value: tensor([[-4.1395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5804222811758207, distance: 0.7412469203041049 entropy 0.03264415264129639
epoch: 19, step: 14
	action: tensor([[ 0.3482,  0.7183,  0.0380, -0.1800, -0.1404,  0.1393, -0.2008]],
       dtype=torch.float64)
	q_value: tensor([[-3.3746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8126877752822362, distance: 0.4952674136170873 entropy 0.03264415264129639
epoch: 19, step: 15
	action: tensor([[ 0.9164,  0.4488, -0.1364, -0.1909, -0.0521, -0.1456, -0.5783]],
       dtype=torch.float64)
	q_value: tensor([[-3.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.919873239369729, distance: 0.3239257596914395 entropy 0.03264415264129639
epoch: 19, step: 16
	action: tensor([[ 0.7108,  0.1392, -0.1436, -0.2850,  0.2705,  0.2650, -0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-4.7607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.763244692215264, distance: 0.5568093961235374 entropy 0.03264415264129639
epoch: 19, step: 17
	action: tensor([[ 0.8058,  0.5773, -0.5473, -0.2127,  0.1661, -0.2644, -0.0686]],
       dtype=torch.float64)
	q_value: tensor([[-3.6826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9551081724296718, distance: 0.24246013147883297 entropy 0.03264415264129639
epoch: 19, step: 18
	action: tensor([[ 0.5455,  0.1897, -0.6809, -0.4401,  0.1747, -0.1947, -0.7988]],
       dtype=torch.float64)
	q_value: tensor([[-4.5785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6856349370326484, distance: 0.6416139983946928 entropy 0.03264415264129639
epoch: 19, step: 19
	action: tensor([[ 0.5114,  0.2340, -0.0658, -0.7248,  0.0713,  0.2193, -0.7084]],
       dtype=torch.float64)
	q_value: tensor([[-3.6244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.557688936343522, distance: 0.7610629752705246 entropy 0.03264415264129639
epoch: 19, step: 20
	action: tensor([[ 0.4825,  0.4025,  0.2837, -0.6227,  0.1470,  0.3195, -0.3004]],
       dtype=torch.float64)
	q_value: tensor([[-3.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.675135980547183, distance: 0.6522401061199743 entropy 0.03264415264129639
epoch: 19, step: 21
	action: tensor([[ 0.3887, -0.1967, -0.4669, -0.7602,  0.2360,  0.0399, -0.1150]],
       dtype=torch.float64)
	q_value: tensor([[-4.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1440474429171673, distance: 1.058721030732621 entropy 0.03264415264129639
epoch: 19, step: 22
	action: tensor([[ 0.3388,  0.5628, -0.7611, -0.5435, -0.0604, -0.1707, -0.1033]],
       dtype=torch.float64)
	q_value: tensor([[-3.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8313210538942085, distance: 0.4699883931594109 entropy 0.03264415264129639
epoch: 19, step: 23
	action: tensor([[ 0.5182,  0.2126, -0.2736, -0.2471,  0.2374,  0.5387, -0.8070]],
       dtype=torch.float64)
	q_value: tensor([[-3.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7939873588787377, distance: 0.5194020127139801 entropy 0.03264415264129639
epoch: 19, step: 24
	action: tensor([[ 0.7549,  0.2063, -0.6620, -0.5496,  0.0224, -0.1115, -0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-3.6037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6964285392560069, distance: 0.630503008908494 entropy 0.03264415264129639
epoch: 19, step: 25
	action: tensor([[ 0.9158,  0.3460, -0.1749, -0.7027, -0.2575,  0.5484, -0.4548]],
       dtype=torch.float64)
	q_value: tensor([[-4.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8180671870716553, distance: 0.48810382438078853 entropy 0.03264415264129639
epoch: 19, step: 26
	action: tensor([[ 0.5325,  0.0065, -0.2975, -0.1452,  0.2782, -0.0413, -0.4121]],
       dtype=torch.float64)
	q_value: tensor([[-4.9479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5932751482811709, distance: 0.7298053588812847 entropy 0.03264415264129639
epoch: 19, step: 27
	action: tensor([[ 0.2918,  0.6678, -0.0427, -0.3474,  0.2875,  0.0045, -0.6291]],
       dtype=torch.float64)
	q_value: tensor([[-3.3883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7513654447984124, distance: 0.5706074486836724 entropy 0.03264415264129639
epoch: 19, step: 28
	action: tensor([[ 0.4649,  0.4523, -0.1228, -0.7750, -0.3660,  0.5590, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-3.5623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7127289510251154, distance: 0.6133418847128611 entropy 0.03264415264129639
epoch: 19, step: 29
	action: tensor([[ 0.7335,  0.7448, -0.1499,  0.3682,  0.4442,  0.2185, -0.5175]],
       dtype=torch.float64)
	q_value: tensor([[-4.5598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 19, step: 30
	action: tensor([[ 0.9034,  0.8718, -0.7760, -1.2404,  0.1825,  0.4279, -0.4973]],
       dtype=torch.float64)
	q_value: tensor([[-4.3638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8169582329841157, distance: 0.4895891595148602 entropy 0.03264415264129639
epoch: 19, step: 31
	action: tensor([[ 1.1024,  0.5194, -0.6871, -0.2174,  0.1191,  0.0838, -0.7135]],
       dtype=torch.float64)
	q_value: tensor([[-5.5252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8965872716339922, distance: 0.3679965028200632 entropy 0.03264415264129639
epoch: 19, step: 32
	action: tensor([[ 0.5777,  0.4553, -0.2395, -0.7137,  0.2179,  0.2461, -0.5382]],
       dtype=torch.float64)
	q_value: tensor([[-5.1619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8162913070819459, distance: 0.49048027527852367 entropy 0.03264415264129639
epoch: 19, step: 33
	action: tensor([[ 0.3779,  0.5104, -0.3659, -0.3228,  0.1815,  0.2782, -0.3771]],
       dtype=torch.float64)
	q_value: tensor([[-3.8036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8723489937453313, distance: 0.40885453056945276 entropy 0.03264415264129639
epoch: 19, step: 34
	action: tensor([[ 0.7238,  0.4136, -0.4958, -0.6594, -0.4299,  0.0971, -0.1219]],
       dtype=torch.float64)
	q_value: tensor([[-3.1634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8128718590629481, distance: 0.4950239882108131 entropy 0.03264415264129639
epoch: 19, step: 35
	action: tensor([[ 1.1056,  0.3962, -0.3565, -0.3774, -0.0971,  0.3301, -0.3129]],
       dtype=torch.float64)
	q_value: tensor([[-4.5215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8503487996402108, distance: 0.442687027513817 entropy 0.03264415264129639
epoch: 19, step: 36
	action: tensor([[ 0.4441,  0.1049, -0.5794, -0.6719, -0.0613, -0.0783, -0.2519]],
       dtype=torch.float64)
	q_value: tensor([[-5.0004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5434458826810403, distance: 0.7732195481748454 entropy 0.03264415264129639
epoch: 19, step: 37
	action: tensor([[ 0.8410,  0.3092, -0.0506, -0.3720,  0.5107,  0.1189, -0.4312]],
       dtype=torch.float64)
	q_value: tensor([[-3.3499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8221628039294702, distance: 0.48257852805550727 entropy 0.03264415264129639
epoch: 19, step: 38
	action: tensor([[ 0.2234,  0.4255, -0.6665, -0.6195, -0.1005,  0.2991, -0.2963]],
       dtype=torch.float64)
	q_value: tensor([[-4.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7585294715672503, distance: 0.5623267707824942 entropy 0.03264415264129639
epoch: 19, step: 39
	action: tensor([[-0.0982,  0.2625, -0.4884, -0.6714,  0.1277,  0.0762, -0.4492]],
       dtype=torch.float64)
	q_value: tensor([[-3.2658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3558765645845404, distance: 0.9184198117210142 entropy 0.03264415264129639
epoch: 19, step: 40
	action: tensor([[ 0.8452,  0.1545, -0.6307,  0.0423,  0.3750,  0.4485, -0.4609]],
       dtype=torch.float64)
	q_value: tensor([[-2.5802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9217672856924429, distance: 0.32007436496453706 entropy 0.03264415264129639
epoch: 19, step: 41
	action: tensor([[ 0.6052,  0.3678, -0.2820, -0.7951,  0.2097, -0.0905, -0.7485]],
       dtype=torch.float64)
	q_value: tensor([[-4.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6640678894300001, distance: 0.6632579320718875 entropy 0.03264415264129639
epoch: 19, step: 42
	action: tensor([[ 0.6453,  0.3124, -0.3700, -0.0826,  0.3709, -0.2649, -0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-4.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8537449404148468, distance: 0.4376350960020457 entropy 0.03264415264129639
epoch: 19, step: 43
	action: tensor([[ 0.6701,  0.5647, -0.2690,  0.1092, -0.0267, -0.3025, -0.4783]],
       dtype=torch.float64)
	q_value: tensor([[-4.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.973730229968353, distance: 0.1854747646785329 entropy 0.03264415264129639
epoch: 19, step: 44
	action: tensor([[ 0.2162,  0.2973, -0.5188, -0.1601, -0.4739, -0.1373, -0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-4.4104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6955936636900377, distance: 0.6313694113929511 entropy 0.03264415264129639
epoch: 19, step: 45
	action: tensor([[ 0.6567,  0.7770, -0.2853, -0.6430, -0.1578,  0.3479, -0.6774]],
       dtype=torch.float64)
	q_value: tensor([[-3.3653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9745829743483587, distance: 0.18243957710921144 entropy 0.03264415264129639
epoch: 19, step: 46
	action: tensor([[ 0.1269,  0.1583, -0.5644, -0.8722,  0.2395,  0.0552, -0.3559]],
       dtype=torch.float64)
	q_value: tensor([[-4.6142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43662651169211697, distance: 0.8589243128843057 entropy 0.03264415264129639
epoch: 19, step: 47
	action: tensor([[ 0.3271,  0.3007, -0.5792, -0.2421, -0.0714,  0.2002,  0.0839]],
       dtype=torch.float64)
	q_value: tensor([[-2.9344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7585268441437393, distance: 0.5623298300928146 entropy 0.03264415264129639
epoch: 19, step: 48
	action: tensor([[ 0.7706,  0.3418, -0.2802, -0.5247, -0.2017,  0.1803, -0.1860]],
       dtype=torch.float64)
	q_value: tensor([[-3.2477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8094197990250652, distance: 0.4995691179255921 entropy 0.03264415264129639
epoch: 19, step: 49
	action: tensor([[ 0.5193,  0.7644, -0.4966, -0.3921,  0.2568,  0.0327,  0.2978]],
       dtype=torch.float64)
	q_value: tensor([[-4.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9698297058997382, distance: 0.19876799873810488 entropy 0.03264415264129639
epoch: 19, step: 50
	action: tensor([[ 0.4612,  0.0817, -0.3642, -0.6640,  0.4519,  0.2156, -0.4399]],
       dtype=torch.float64)
	q_value: tensor([[-4.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5377599869441658, distance: 0.7780194633236018 entropy 0.03264415264129639
epoch: 19, step: 51
	action: tensor([[ 0.2570,  0.4335, -0.5807, -0.2925, -0.0064,  0.0380, -0.1295]],
       dtype=torch.float64)
	q_value: tensor([[-3.1748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7811406299999528, distance: 0.5353517980414464 entropy 0.03264415264129639
epoch: 19, step: 52
	action: tensor([[ 0.5432, -0.0964, -0.4365, -0.1069,  0.1666,  0.5336,  0.2708]],
       dtype=torch.float64)
	q_value: tensor([[-3.0118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7013264767202119, distance: 0.6253959375477027 entropy 0.03264415264129639
epoch: 19, step: 53
	action: tensor([[ 0.5826,  0.6105, -0.6172, -0.6050,  0.2657, -0.1586, -0.4017]],
       dtype=torch.float64)
	q_value: tensor([[-3.8442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9036109192392001, distance: 0.35527987922903054 entropy 0.03264415264129639
epoch: 19, step: 54
	action: tensor([[ 0.2021,  0.7060, -0.5537, -0.0723,  0.2274, -0.2424, -0.1471]],
       dtype=torch.float64)
	q_value: tensor([[-4.0153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8155049209494715, distance: 0.49152893279183235 entropy 0.03264415264129639
epoch: 19, step: 55
	action: tensor([[ 0.4276,  0.0034, -0.6448, -0.4794,  0.1704, -0.0035, -0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-3.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5354947954273359, distance: 0.7799234625583482 entropy 0.03264415264129639
epoch: 19, step: 56
	action: tensor([[ 0.6058,  0.5824, -0.3823, -0.0506,  0.0567,  0.5344,  0.1469]],
       dtype=torch.float64)
	q_value: tensor([[-3.2271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9526724488640612, distance: 0.24895090533496628 entropy 0.03264415264129639
epoch: 19, step: 57
	action: tensor([[ 0.3932,  0.0083, -0.5742, -0.1976,  0.0758,  0.2588, -0.4407]],
       dtype=torch.float64)
	q_value: tensor([[-4.1299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6355929083231671, distance: 0.6907964599745167 entropy 0.03264415264129639
epoch: 19, step: 58
	action: tensor([[ 0.3020,  0.4391, -0.1399, -0.4845,  0.0503,  0.0815, -0.2268]],
       dtype=torch.float64)
	q_value: tensor([[-2.8810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7142432428307619, distance: 0.6117231942851247 entropy 0.03264415264129639
epoch: 19, step: 59
	action: tensor([[ 0.2648,  0.3628, -0.5125, -0.8406,  0.0984,  0.1657, -0.4049]],
       dtype=torch.float64)
	q_value: tensor([[-3.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6616951013605068, distance: 0.6655962040444612 entropy 0.03264415264129639
epoch: 19, step: 60
	action: tensor([[ 0.3600,  0.4520, -0.2845, -0.6900,  0.4164,  0.2170, -0.3498]],
       dtype=torch.float64)
	q_value: tensor([[-3.2321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7558069077613603, distance: 0.5654879832165751 entropy 0.03264415264129639
epoch: 19, step: 61
	action: tensor([[ 0.5746,  0.3069, -0.0802, -0.4588,  0.1121, -0.0860,  0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-3.2536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7122735315715818, distance: 0.6138278668679066 entropy 0.03264415264129639
epoch: 19, step: 62
	action: tensor([[ 0.2667,  0.0836, -0.2481, -0.4477, -0.1155,  0.0722, -0.2671]],
       dtype=torch.float64)
	q_value: tensor([[-3.9018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46530028596397255, distance: 0.836780727688854 entropy 0.03264415264129639
epoch: 19, step: 63
	action: tensor([[ 0.2859,  0.2179,  0.1709, -0.6663, -0.1771,  0.1210, -0.2956]],
       dtype=torch.float64)
	q_value: tensor([[-2.9878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4137999170254777, distance: 0.8761523518235013 entropy 0.03264415264129639
epoch: 19, step: 64
	action: tensor([[ 0.7838,  0.6913, -0.3658, -0.4673, -0.2273, -0.1342, -0.1807]],
       dtype=torch.float64)
	q_value: tensor([[-3.7509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9478104215697118, distance: 0.261425885458046 entropy 0.03264415264129639
epoch: 19, step: 65
	action: tensor([[ 0.7905,  0.5388,  0.0308, -0.2848,  0.1424,  0.4880, -0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-4.5821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9810503541418301, distance: 0.1575278125153314 entropy 0.03264415264129639
epoch: 19, step: 66
	action: tensor([[ 0.8825,  0.2955, -0.2993, -1.0146, -0.0991,  0.0417, -0.4956]],
       dtype=torch.float64)
	q_value: tensor([[-4.4183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5093437252092936, distance: 0.8015772299372989 entropy 0.03264415264129639
epoch: 19, step: 67
	action: tensor([[ 0.3655,  0.4106, -0.2871, -0.3732,  0.4960,  0.2021,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[-4.7008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.804890974067444, distance: 0.5054699867694016 entropy 0.03264415264129639
epoch: 19, step: 68
	action: tensor([[ 0.3597,  0.1456, -0.5894, -0.5857,  0.1432,  0.2632,  0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-3.3611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6334819178348301, distance: 0.6927944437851355 entropy 0.03264415264129639
epoch: 19, step: 69
	action: tensor([[ 0.4611,  0.1642, -0.3014, -0.7757, -0.1775,  0.1889,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[-3.3996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5242137432803868, distance: 0.7893373254856259 entropy 0.03264415264129639
epoch: 19, step: 70
	action: tensor([[ 0.4790,  0.1590, -0.6036, -0.0899,  0.0333,  0.4000, -0.7737]],
       dtype=torch.float64)
	q_value: tensor([[-3.8941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.791298871746365, distance: 0.5227801532393758 entropy 0.03264415264129639
epoch: 19, step: 71
	action: tensor([[ 0.8229,  0.0894, -0.5041, -0.4829,  0.2968,  0.2864, -0.8711]],
       dtype=torch.float64)
	q_value: tensor([[-3.3801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6934098747470728, distance: 0.6336300633459467 entropy 0.03264415264129639
epoch: 19, step: 72
	action: tensor([[ 0.5950,  0.3396, -0.2445, -0.2754,  0.1742,  0.1849, -0.2327]],
       dtype=torch.float64)
	q_value: tensor([[-4.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8705940776332544, distance: 0.4116553552830406 entropy 0.03264415264129639
epoch: 19, step: 73
	action: tensor([[ 0.0016,  0.1538, -0.2023, -0.3328, -0.1619, -0.1286, -0.2053]],
       dtype=torch.float64)
	q_value: tensor([[-3.5116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3192514728394922, distance: 0.9441696769337254 entropy 0.03264415264129639
epoch: 19, step: 74
	action: tensor([[-0.0369,  0.2460, -0.4640, -0.3947, -0.1500,  0.0410, -0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-2.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41376791471685037, distance: 0.8761762673056992 entropy 0.03264415264129639
epoch: 19, step: 75
	action: tensor([[ 0.1959,  0.2088, -0.2117, -0.3376,  0.0920,  0.0583, -0.2461]],
       dtype=torch.float64)
	q_value: tensor([[-2.6185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5465719053348204, distance: 0.7705678871480633 entropy 0.03264415264129639
epoch: 19, step: 76
	action: tensor([[ 0.3398, -0.1461, -0.1684, -0.0453,  0.3381, -0.0069, -0.3056]],
       dtype=torch.float64)
	q_value: tensor([[-2.7235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4549236541006366, distance: 0.844861191158832 entropy 0.03264415264129639
epoch: 19, step: 77
	action: tensor([[ 0.2178,  0.0114, -0.4161, -0.5467,  0.2567, -0.1152, -0.3117]],
       dtype=torch.float64)
	q_value: tensor([[-3.1315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3429660396692302, distance: 0.9275783470105945 entropy 0.03264415264129639
epoch: 19, step: 78
	action: tensor([[ 0.6664,  0.2956,  0.2959, -0.6260, -0.0629, -0.2343, -0.6096]],
       dtype=torch.float64)
	q_value: tensor([[-2.7714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6043804840522359, distance: 0.7197729924547974 entropy 0.03264415264129639
epoch: 19, step: 79
	action: tensor([[ 0.2678,  0.1950, -0.4201, -0.2404, -0.2047, -0.1575, -0.3126]],
       dtype=torch.float64)
	q_value: tensor([[-4.7985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6200612070759657, distance: 0.7053643605335467 entropy 0.03264415264129639
epoch: 19, step: 80
	action: tensor([[ 0.4750,  0.2331, -0.5454, -0.3563,  0.1467, -0.0288, -0.5322]],
       dtype=torch.float64)
	q_value: tensor([[-3.0339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7482212724631541, distance: 0.5742039959008602 entropy 0.03264415264129639
epoch: 19, step: 81
	action: tensor([[ 0.7658,  0.2558,  0.4538, -0.2261,  0.5011, -0.1989, -0.1819]],
       dtype=torch.float64)
	q_value: tensor([[-3.2150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7862891184098417, distance: 0.5290174663518489 entropy 0.03264415264129639
epoch: 19, step: 82
	action: tensor([[ 0.7002,  0.5080, -0.2142, -0.1326, -0.0192, -0.0750, -0.1551]],
       dtype=torch.float64)
	q_value: tensor([[-4.8621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9482912507366751, distance: 0.2602188239987187 entropy 0.03264415264129639
epoch: 19, step: 83
	action: tensor([[ 0.0459,  0.3880, -0.1926, -0.7077, -0.0773,  0.0165, -0.4851]],
       dtype=torch.float64)
	q_value: tensor([[-4.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4335874478487174, distance: 0.8612378890935473 entropy 0.03264415264129639
epoch: 19, step: 84
	action: tensor([[ 0.3274,  0.0906, -0.2059, -0.5609, -0.1026,  0.2060, -0.2733]],
       dtype=torch.float64)
	q_value: tensor([[-3.1623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4788395350864155, distance: 0.8261186462338107 entropy 0.03264415264129639
epoch: 19, step: 85
	action: tensor([[ 0.7638,  0.3141, -0.2035, -0.5290,  0.2815,  0.6590, -0.2794]],
       dtype=torch.float64)
	q_value: tensor([[-3.1904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8759064138822085, distance: 0.4031172297634212 entropy 0.03264415264129639
epoch: 19, step: 86
	action: tensor([[ 0.1055,  0.1275, -0.0822, -0.6369,  0.2307, -0.1760, -0.0499]],
       dtype=torch.float64)
	q_value: tensor([[-4.1266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20503375402843882, distance: 1.0203074030748327 entropy 0.03264415264129639
epoch: 19, step: 87
	action: tensor([[ 0.9504,  0.7833, -0.3319, -0.1083,  0.3549,  0.0878, -0.3883]],
       dtype=torch.float64)
	q_value: tensor([[-2.9960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9744180587891629, distance: 0.18303048964823282 entropy 0.03264415264129639
epoch: 19, step: 88
	action: tensor([[ 0.8827,  0.0323, -0.4284, -0.2898,  0.2558,  0.0473, -0.3142]],
       dtype=torch.float64)
	q_value: tensor([[-4.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6491808694197866, distance: 0.6777949476418891 entropy 0.03264415264129639
epoch: 19, step: 89
	action: tensor([[ 0.3865,  0.3330, -0.4479, -0.7412,  0.0948, -0.2908, -0.1532]],
       dtype=torch.float64)
	q_value: tensor([[-4.1882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6153001846662207, distance: 0.7097700706101093 entropy 0.03264415264129639
epoch: 19, step: 90
	action: tensor([[ 0.9389,  0.4374,  0.3208, -0.4459, -0.1526,  0.2981, -0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-3.5250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.906437995236584, distance: 0.3500309548627421 entropy 0.03264415264129639
epoch: 19, step: 91
	action: tensor([[ 0.5468,  0.4120, -0.7167, -0.6348,  0.1900,  0.0329, -0.7248]],
       dtype=torch.float64)
	q_value: tensor([[-5.2474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8449009961059075, distance: 0.45067264456647216 entropy 0.03264415264129639
epoch: 19, step: 92
	action: tensor([[ 0.0899,  0.2528, -0.8858, -0.5030,  0.0175,  0.4766,  0.0483]],
       dtype=torch.float64)
	q_value: tensor([[-3.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6251331969544656, distance: 0.7006404140465853 entropy 0.03264415264129639
epoch: 19, step: 93
	action: tensor([[ 0.8291, -0.1245,  0.0508, -0.7222,  0.4222,  0.2293, -0.3629]],
       dtype=torch.float64)
	q_value: tensor([[-3.2888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2756624673571223, distance: 0.973928753915521 entropy 0.03264415264129639
epoch: 19, step: 94
	action: tensor([[ 7.9008e-01,  8.1526e-01, -3.3901e-02, -1.8372e-01,  1.7465e-04,
          2.2672e-02, -2.9253e-01]], dtype=torch.float64)
	q_value: tensor([[-4.2762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9841837343861172, distance: 0.14391586361590866 entropy 0.03264415264129639
epoch: 19, step: 95
	action: tensor([[ 0.5105,  0.5883, -0.5534, -0.0872,  0.0382,  0.0568, -0.5839]],
       dtype=torch.float64)
	q_value: tensor([[-4.7220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9372473300409897, distance: 0.2866637630511312 entropy 0.03264415264129639
epoch: 19, step: 96
	action: tensor([[ 0.1679,  0.4352, -0.1444, -0.4276,  0.1834,  0.5698, -0.4487]],
       dtype=torch.float64)
	q_value: tensor([[-3.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7261289846743231, distance: 0.5988660993397484 entropy 0.03264415264129639
epoch: 19, step: 97
	action: tensor([[ 0.8156,  0.4990, -0.0220, -0.5983, -0.0805,  0.0516, -0.4899]],
       dtype=torch.float64)
	q_value: tensor([[-3.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8531139277887418, distance: 0.4385781609779761 entropy 0.03264415264129639
epoch: 19, step: 98
	action: tensor([[ 0.8378,  0.3526, -0.0220, -0.1825, -0.2890,  0.0467, -0.1894]],
       dtype=torch.float64)
	q_value: tensor([[-4.6016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9051681187351602, distance: 0.35239835815141557 entropy 0.03264415264129639
epoch: 19, step: 99
	action: tensor([[ 0.4620,  0.3292, -0.0302, -0.6415,  0.2454,  0.2809, -0.2871]],
       dtype=torch.float64)
	q_value: tensor([[-4.4849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.696359066865915, distance: 0.6305751501517991 entropy 0.03264415264129639
epoch: 19, step: 100
	action: tensor([[ 0.6119,  0.6593, -0.3437, -0.4926, -0.0397,  0.3829, -0.4841]],
       dtype=torch.float64)
	q_value: tensor([[-3.5019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9631185893072787, distance: 0.21976597560718472 entropy 0.03264415264129639
epoch: 19, step: 101
	action: tensor([[ 0.6407,  0.6831, -0.3089, -0.8449, -0.0036,  0.0354, -0.0269]],
       dtype=torch.float64)
	q_value: tensor([[-4.0927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8791907502057102, distance: 0.3977468848933624 entropy 0.03264415264129639
epoch: 19, step: 102
	action: tensor([[ 0.2720,  0.4758, -0.5444, -0.7858, -0.0040,  0.3058, -0.3010]],
       dtype=torch.float64)
	q_value: tensor([[-4.5315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7712446475116778, distance: 0.547321269437398 entropy 0.03264415264129639
epoch: 19, step: 103
	action: tensor([[ 0.7614,  0.4259, -0.0434, -0.4349, -0.0319,  0.2331, -0.4100]],
       dtype=torch.float64)
	q_value: tensor([[-3.4461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.890370584659574, distance: 0.378896193517148 entropy 0.03264415264129639
epoch: 19, step: 104
	action: tensor([[ 0.8070,  0.0282, -0.6187, -0.2680, -0.1166,  0.3716, -0.8122]],
       dtype=torch.float64)
	q_value: tensor([[-4.2128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.752588404676422, distance: 0.5692023941106066 entropy 0.03264415264129639
epoch: 19, step: 105
	action: tensor([[ 0.3115,  0.0680, -0.4143, -0.6533,  0.0094, -0.1875, -0.1689]],
       dtype=torch.float64)
	q_value: tensor([[-4.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40145225293312414, distance: 0.8853318605242312 entropy 0.03264415264129639
epoch: 19, step: 106
	action: tensor([[ 0.1119, -0.1799, -0.4817, -0.7030,  0.2380,  0.2022, -0.6923]],
       dtype=torch.float64)
	q_value: tensor([[-3.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15765574477652688, distance: 1.0502713107282082 entropy 0.03264415264129639
epoch: 19, step: 107
	action: tensor([[ 0.2330,  0.4239, -0.1061, -0.3987, -0.1159,  0.2391, -0.6182]],
       dtype=torch.float64)
	q_value: tensor([[-2.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.672866995980338, distance: 0.6545138999740305 entropy 0.03264415264129639
epoch: 19, step: 108
	action: tensor([[ 0.5879, -0.2460, -0.0387, -0.6739, -0.0220, -0.0612, -0.3926]],
       dtype=torch.float64)
	q_value: tensor([[-3.3725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07182462791313593, distance: 1.1024825232264357 entropy 0.03264415264129639
epoch: 19, step: 109
	action: tensor([[ 0.2125,  0.4057, -0.3434, -0.4472,  0.0288,  0.1454, -0.2540]],
       dtype=torch.float64)
	q_value: tensor([[-3.7522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6816125740808984, distance: 0.6457057401321422 entropy 0.03264415264129639
epoch: 19, step: 110
	action: tensor([[ 0.7298,  0.3794,  0.0041, -0.5142,  0.2386,  0.2670, -0.6776]],
       dtype=torch.float64)
	q_value: tensor([[-2.9045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8338585637179482, distance: 0.4664398785154952 entropy 0.03264415264129639
epoch: 19, step: 111
	action: tensor([[ 0.4111,  0.0525, -0.5128, -0.4794, -0.2460, -0.0197, -0.3773]],
       dtype=torch.float64)
	q_value: tensor([[-4.2184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5447657262247181, distance: 0.7721010964792048 entropy 0.03264415264129639
epoch: 19, step: 112
	action: tensor([[ 5.6163e-01,  3.1742e-01, -1.4447e-01, -6.5457e-01,  3.5171e-06,
         -3.8972e-01, -3.1924e-01]], dtype=torch.float64)
	q_value: tensor([[-3.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5939923826345508, distance: 0.7291615914296615 entropy 0.03264415264129639
epoch: 19, step: 113
	action: tensor([[ 0.4675,  0.2031,  0.1622, -0.1404,  0.0696,  0.2344, -0.2125]],
       dtype=torch.float64)
	q_value: tensor([[-4.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7877457123648215, distance: 0.5272115659729244 entropy 0.03264415264129639
epoch: 19, step: 114
	action: tensor([[ 0.4004,  0.2070, -0.2783, -0.0368,  0.0534,  0.4642, -0.2955]],
       dtype=torch.float64)
	q_value: tensor([[-3.6306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8202835490668068, distance: 0.48512159780398423 entropy 0.03264415264129639
epoch: 19, step: 115
	action: tensor([[ 0.4380, -0.3540, -0.0648, -0.1076,  0.6680,  0.2413, -0.8234]],
       dtype=torch.float64)
	q_value: tensor([[-3.1528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35853587573083634, distance: 0.9165219688579979 entropy 0.03264415264129639
epoch: 19, step: 116
	action: tensor([[ 0.6694,  0.4097, -0.6843,  0.1214, -0.0534,  0.5980, -0.6429]],
       dtype=torch.float64)
	q_value: tensor([[-3.8583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9277173163800532, distance: 0.30766197763113795 entropy 0.03264415264129639
epoch: 19, step: 117
	action: tensor([[ 0.2712,  0.4064, -0.2839, -0.4461, -0.1523,  0.0797, -0.3950]],
       dtype=torch.float64)
	q_value: tensor([[-4.1868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7098835947284274, distance: 0.6163719082278879 entropy 0.03264415264129639
epoch: 19, step: 118
	action: tensor([[ 0.3552,  0.1717,  0.1096, -0.8090, -0.3026,  0.0140, -0.5737]],
       dtype=torch.float64)
	q_value: tensor([[-3.1913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3370729403952435, distance: 0.9317289001801766 entropy 0.03264415264129639
epoch: 19, step: 119
	action: tensor([[ 0.1503,  0.2687, -0.3945, -0.1850,  0.3397,  0.0441, -0.4055]],
       dtype=torch.float64)
	q_value: tensor([[-4.0804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5968621084126492, distance: 0.7265801072776925 entropy 0.03264415264129639
epoch: 19, step: 120
	action: tensor([[ 9.9829e-01,  8.5333e-04, -2.9720e-01, -2.6917e-01, -1.8354e-01,
          7.6948e-03, -3.4963e-01]], dtype=torch.float64)
	q_value: tensor([[-2.6098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5826707095818656, distance: 0.7392581595178522 entropy 0.03264415264129639
epoch: 19, step: 121
	action: tensor([[ 0.1347,  0.2257, -0.2451, -0.4046,  0.0743,  0.0116,  0.0489]],
       dtype=torch.float64)
	q_value: tensor([[-4.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48609716530546554, distance: 0.8203462551614408 entropy 0.03264415264129639
epoch: 19, step: 122
	action: tensor([[ 0.6179,  0.4719, -0.6555, -0.1693, -0.1231, -0.0299, -0.3583]],
       dtype=torch.float64)
	q_value: tensor([[-2.8506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9290698982397876, distance: 0.3047698383722718 entropy 0.03264415264129639
epoch: 19, step: 123
	action: tensor([[ 0.9463,  0.6374, -0.4873, -0.0486, -0.0083,  0.2563, -0.3271]],
       dtype=torch.float64)
	q_value: tensor([[-3.7916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9862484267047833, distance: 0.13419393476550948 entropy 0.03264415264129639
epoch: 19, step: 124
	action: tensor([[ 0.5786,  0.5351,  0.2100, -0.2841, -0.2731,  0.1655, -0.4200]],
       dtype=torch.float64)
	q_value: tensor([[-4.6578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9152091634613168, distance: 0.3332200715241833 entropy 0.03264415264129639
epoch: 19, step: 125
	action: tensor([[ 0.6486,  0.1766,  0.1874, -0.3473, -0.3509,  0.1160, -0.2204]],
       dtype=torch.float64)
	q_value: tensor([[-4.6183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7227670361029979, distance: 0.6025306270320735 entropy 0.03264415264129639
epoch: 19, step: 126
	action: tensor([[ 0.3716,  0.0403, -0.3513, -0.4009,  0.3983,  0.4366, -0.3381]],
       dtype=torch.float64)
	q_value: tensor([[-4.3834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6279880401974305, distance: 0.6979674094858133 entropy 0.03264415264129639
epoch: 19, step: 127
	action: tensor([[ 0.4905,  0.5500, -0.1395, -0.4650, -0.1135, -0.5192, -0.4889]],
       dtype=torch.float64)
	q_value: tensor([[-2.9561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.805362908042855, distance: 0.5048582957278239 entropy 0.03264415264129639
LOSS epoch 19 actor 60.29454284697498 critic 476.5848871087845 
epoch: 20, step: 0
	action: tensor([[ 1.4701,  0.6491, -1.3501, -0.9598, -0.4124, -0.0492, -0.7546]],
       dtype=torch.float64)
	q_value: tensor([[-3.4624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2152499265244333, distance: 1.013730179106056 entropy 0.03264415264129639
epoch: 20, step: 1
	action: tensor([[ 2.8294,  1.1018, -1.7420, -1.6020,  0.1256,  0.2128, -1.4004]],
       dtype=torch.float64)
	q_value: tensor([[-5.5176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 2
	action: tensor([[ 2.7435,  1.6489, -2.1881, -1.2465, -0.3802,  0.5691, -1.1546]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 3
	action: tensor([[ 2.7089,  1.7305, -1.4449, -1.7544, -0.1839,  0.7259, -1.5855]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 4
	action: tensor([[ 2.6667,  1.5206, -2.1333, -2.1213, -0.4673,  0.7622, -1.4260]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 5
	action: tensor([[ 2.4561,  1.4645, -1.6206, -1.2310,  0.4746,  0.7399, -1.3896]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 6
	action: tensor([[ 3.2742,  1.4682, -1.8829, -1.8087, -0.1346,  0.7312, -1.4266]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 7
	action: tensor([[ 2.8220,  1.0783, -1.4554, -1.6621, -0.3233,  0.4820, -1.0431]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 8
	action: tensor([[ 2.7753,  1.8870, -1.4733, -1.8695, -0.0180,  0.4574, -1.2121]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 9
	action: tensor([[ 2.8655,  1.4553, -1.8318, -1.6688,  0.1865,  0.3035, -0.9263]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 10
	action: tensor([[ 3.0146,  1.4959, -1.7785, -1.6465, -0.3918,  0.5653, -0.7794]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 11
	action: tensor([[ 3.3163,  2.1012, -2.2090, -1.8890,  0.0118,  0.0699, -1.1440]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 12
	action: tensor([[ 2.9569,  1.3783, -1.8646, -1.3986, -0.1470,  0.6828, -0.9401]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 13
	action: tensor([[ 3.0690,  1.4829, -1.7120, -1.8945,  0.0107,  0.8889, -1.2137]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 14
	action: tensor([[ 3.0084,  1.4049, -1.8146, -1.7049, -0.4291,  0.8525, -1.1516]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 15
	action: tensor([[ 3.2452,  1.4135, -1.4685, -1.7809, -0.0803,  0.6174, -1.4901]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 16
	action: tensor([[ 2.6634,  1.6472, -1.4857, -1.5595, -0.0727,  0.1156, -1.1292]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 17
	action: tensor([[ 3.1681,  1.2570, -1.8532, -2.4084, -0.2235,  0.2742, -1.3086]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 18
	action: tensor([[ 2.9126,  1.4330, -1.8364, -1.5768, -0.4926,  0.4504, -1.1313]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 19
	action: tensor([[ 2.4671,  1.1417, -1.7626, -1.8027, -0.6678,  0.5430, -1.0652]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 20
	action: tensor([[ 2.8071,  1.0657, -1.5501, -2.2018, -0.1155,  0.7656, -1.3673]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 21
	action: tensor([[ 3.0654,  1.8153, -1.9139, -1.6468, -0.4221,  0.8097, -1.0899]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 22
	action: tensor([[ 2.8963,  1.8161, -1.4236, -1.3766, -0.4056,  0.7581, -1.4913]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 23
	action: tensor([[ 2.8790,  1.5525, -1.9978, -1.6734,  0.2096,  0.5655, -1.5392]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 24
	action: tensor([[ 2.8486,  1.3609, -1.6671, -2.0758,  0.4107,  0.5564, -1.5332]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 25
	action: tensor([[ 2.2409,  1.8621, -2.3311, -2.0625,  0.3853,  0.8907, -0.9738]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 26
	action: tensor([[ 2.3786,  1.8003, -2.2791, -2.0994, -0.1278,  0.4862, -0.6746]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 27
	action: tensor([[ 3.1059,  1.4755, -1.7263, -1.4251, -0.0209,  0.3642, -1.0763]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 28
	action: tensor([[ 3.2400,  1.3984, -1.9451, -2.0787, -0.1136,  0.5321, -1.4831]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 29
	action: tensor([[ 2.7438,  1.3826, -1.8142, -1.7805, -0.3748,  0.6283, -0.8882]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 30
	action: tensor([[ 2.8911,  1.2986, -1.8460, -1.4961, -0.1668,  0.4580, -1.6360]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 31
	action: tensor([[ 3.1862,  1.5911, -1.7228, -1.4256,  0.0919,  0.3260, -1.1756]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 32
	action: tensor([[ 2.5854,  1.3973, -1.8080, -1.8742,  0.6586,  0.4536, -0.8239]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 33
	action: tensor([[ 2.9245,  1.1027, -1.5837, -1.6041, -0.2395,  0.5538, -1.0852]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 34
	action: tensor([[ 2.8274,  1.6691, -1.7366, -1.6666,  0.3579,  0.6077, -1.5051]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 35
	action: tensor([[ 2.6728,  1.5256, -2.0072, -1.7764, -0.1952,  0.1382, -0.9707]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 36
	action: tensor([[ 2.9238,  1.8421, -2.1236, -1.8045,  0.0758,  0.2481, -1.0501]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 37
	action: tensor([[ 2.7483,  1.4805, -1.7513, -2.1213,  0.4076,  0.4649, -1.4668]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 38
	action: tensor([[ 2.9038,  0.8868, -1.2434, -2.3144, -0.2899,  0.4713, -1.7454]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 39
	action: tensor([[ 2.8155,  1.7367, -1.3478, -1.8682, -0.2013,  0.6650, -1.3999]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 40
	action: tensor([[ 3.1471,  1.4811, -1.9231, -2.2603, -0.6791,  0.1871, -1.3693]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 41
	action: tensor([[ 2.6476,  1.5732, -2.2255, -2.0407,  0.0039,  0.3295, -1.0218]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 42
	action: tensor([[ 2.9724,  1.5003, -1.7782, -2.0310, -0.0776,  0.5862, -1.3262]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 43
	action: tensor([[ 2.7363,  1.8334, -2.0181, -1.4157,  0.0292,  0.4913, -1.4772]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 44
	action: tensor([[ 2.5106,  1.4841, -1.6063, -1.8571,  0.1072, -0.0986, -1.6060]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 45
	action: tensor([[ 2.7368,  1.8823, -1.7302, -1.8722,  0.2736,  0.5233, -1.5288]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 46
	action: tensor([[ 3.3615,  1.3485, -1.4746, -1.6750, -0.0877,  0.2172, -1.3650]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 47
	action: tensor([[ 2.5298,  1.6036, -1.5296, -1.5827,  0.2077,  0.3465, -0.8644]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 48
	action: tensor([[ 3.2512,  1.7414, -1.9391, -1.8998, -0.1793,  0.6358, -1.4203]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 49
	action: tensor([[ 2.9410,  1.4646, -1.4124, -1.6643, -0.4553,  0.1032, -1.2030]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 50
	action: tensor([[ 2.3955,  1.5834, -1.5823, -1.9525,  0.0089,  0.7863, -1.4304]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17575507636087528, distance: 1.2408381127882058 entropy 0.03264415264129639
epoch: 20, step: 51
	action: tensor([[ 3.6944,  1.7671, -2.3078, -2.6050, -0.2374,  0.4994, -1.7484]],
       dtype=torch.float64)
	q_value: tensor([[-7.2213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 52
	action: tensor([[ 2.7685,  1.5239, -1.6025, -1.6339, -0.2973,  0.2738, -1.0797]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 53
	action: tensor([[ 3.1493,  1.4424, -1.5324, -1.4186, -0.1251,  0.8910, -1.5248]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 54
	action: tensor([[ 2.6511,  1.2117, -1.3441, -1.3856,  0.0113,  0.1866, -1.2094]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 55
	action: tensor([[ 2.8297,  1.1865, -1.7731, -1.7591,  0.1563,  0.7309, -1.1512]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 56
	action: tensor([[ 2.8784,  1.8908, -1.7683, -1.7110,  0.4168,  0.4515, -1.3293]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 57
	action: tensor([[ 2.7161,  1.6138, -1.7993, -2.3185,  0.0733,  0.8375, -0.7940]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 58
	action: tensor([[ 2.4885,  1.3344, -1.7844, -1.9846, -0.0528,  0.8704, -1.3333]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 59
	action: tensor([[ 2.5611,  1.7445, -2.0948, -1.7354,  0.1730,  0.2014, -1.3055]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 60
	action: tensor([[ 2.4595,  1.3765, -1.7035, -1.8130, -0.6333,  0.1519, -1.0712]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 61
	action: tensor([[ 2.9417,  1.9279, -1.5192, -1.3123, -0.2145,  0.1708, -0.7979]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 62
	action: tensor([[ 2.6560,  1.3082, -2.1077, -1.5026, -0.3144,  0.1079, -1.1220]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 63
	action: tensor([[ 2.8424,  1.4077, -1.5844, -1.6411, -0.1903,  0.4928, -1.1298]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 64
	action: tensor([[ 2.3794,  1.8282, -1.2882, -1.5033,  0.4123,  0.4562, -0.6447]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 65
	action: tensor([[ 2.7693,  1.6647, -1.4407, -1.7996, -0.1333,  0.8587, -1.4058]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 66
	action: tensor([[ 2.7761,  1.1127, -1.8616, -1.6770,  0.3067,  0.3758, -0.8769]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 67
	action: tensor([[ 2.6202,  1.3493, -1.9465, -1.6310, -0.1717,  0.3956, -1.0980]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 68
	action: tensor([[ 3.0338,  2.0519, -1.8373, -1.9207, -0.1509,  0.8289, -1.1515]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 69
	action: tensor([[ 2.9664,  1.3921, -1.7005, -1.7105, -0.2968,  0.1406, -1.5724]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 70
	action: tensor([[ 2.4012,  1.5136, -1.6334, -1.8753, -0.2618, -0.1460, -1.2010]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 71
	action: tensor([[ 2.7275,  1.7012, -2.0748, -1.8248, -0.0663,  0.3485, -1.2456]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 72
	action: tensor([[ 2.2131,  1.8317, -2.0142, -1.4591,  0.2781,  0.2343, -1.3021]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 73
	action: tensor([[ 2.8562,  1.3568, -2.3016, -1.5482,  0.2577,  0.1364, -1.5669]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 74
	action: tensor([[ 2.6123,  1.2967, -1.8052, -2.0605,  0.2413,  0.1725, -1.3807]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 75
	action: tensor([[ 2.5766,  1.1966, -2.2896, -1.6233, -0.2636,  0.5597, -1.0155]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 76
	action: tensor([[ 2.8573,  1.3935, -1.1458, -1.1939,  0.3741,  0.3700, -1.5750]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 77
	action: tensor([[ 2.7442,  1.3922, -2.2437, -2.1409, -0.0812, -0.0448, -1.5258]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 78
	action: tensor([[ 2.9411,  1.3099, -2.0858, -1.7175, -0.2500,  0.5136, -1.1408]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 79
	action: tensor([[ 2.6943,  1.1717, -1.5663, -1.6940,  0.1961,  0.7206, -1.2641]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 80
	action: tensor([[ 2.8961,  1.6224, -1.8138, -1.4243, -0.0712,  0.7828, -1.1845]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 81
	action: tensor([[ 2.9735,  1.9851, -1.7891, -1.5763, -0.1583,  0.7206, -1.2041]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 82
	action: tensor([[ 2.8161,  1.2933, -1.5174, -1.7366, -0.9282,  0.2677, -1.2528]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 83
	action: tensor([[ 3.0559,  1.3444, -1.6826, -1.4481, -0.3735,  0.2368, -1.7407]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 84
	action: tensor([[ 2.6297,  1.5788, -1.8333, -1.9456,  0.1181,  0.2569, -0.9144]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 85
	action: tensor([[ 2.5591,  1.5233, -1.7507, -1.5981,  0.0519,  0.2551, -0.9627]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 86
	action: tensor([[ 2.7527,  1.5918, -1.7776, -1.9586,  0.1824,  0.3931, -0.9150]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 87
	action: tensor([[ 3.2283,  1.4373, -1.5641, -1.6662,  0.2640,  0.5984, -1.0333]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 88
	action: tensor([[ 2.7969,  0.8751, -1.7600, -2.1310, -0.2963,  0.5059, -1.0781]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 89
	action: tensor([[ 2.6350,  1.6905, -2.0994, -1.8806,  0.1800,  0.5480, -1.4588]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 90
	action: tensor([[ 2.7239,  1.1801, -1.8548, -1.7590,  0.3401,  0.3336, -1.3503]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 91
	action: tensor([[ 3.3341,  1.5113, -2.0660, -2.1580, -0.2607,  0.4277, -1.4991]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 92
	action: tensor([[ 2.4558,  1.6943, -1.7097, -2.0536, -0.1369, -0.0457, -1.3514]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 93
	action: tensor([[ 2.6795,  1.3667, -1.5559, -1.0645, -0.1810,  0.3389, -1.4966]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 94
	action: tensor([[ 2.2902,  1.9096, -2.4374, -1.8438, -0.2131,  0.4031, -1.3344]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 95
	action: tensor([[ 2.7616,  1.1960, -1.9496, -1.4030,  0.0625,  0.6870, -1.1296]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 96
	action: tensor([[ 3.1850,  1.1212, -2.0485, -1.7607,  0.0699,  0.3002, -1.5078]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 97
	action: tensor([[ 2.7032,  1.9361, -1.7570, -1.9143, -0.0380,  0.5367, -1.2552]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 98
	action: tensor([[ 2.3332,  1.4668, -1.7465, -1.5321, -0.2904,  0.5408, -1.1551]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16499283241183194, distance: 1.2351460674187849 entropy 0.03264415264129639
epoch: 20, step: 99
	action: tensor([[ 3.2838,  2.0058, -2.4653, -2.2084,  0.5640,  0.6284, -1.6182]],
       dtype=torch.float64)
	q_value: tensor([[-6.9633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 100
	action: tensor([[ 2.5688,  1.4373, -1.6126, -2.0247, -0.6792,  0.5771, -1.1289]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 101
	action: tensor([[ 2.3053,  1.6159, -1.6153, -2.1184, -0.2457,  0.2536, -1.3182]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26556275146467323, distance: 1.2873556259146073 entropy 0.03264415264129639
epoch: 20, step: 102
	action: tensor([[ 4.0158,  2.2237, -2.0423, -2.2328, -0.0250,  0.5555, -1.5580]],
       dtype=torch.float64)
	q_value: tensor([[-7.4119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 103
	action: tensor([[ 2.6378,  1.6812, -1.9206, -1.9600, -0.0829,  0.4184, -1.2883]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 104
	action: tensor([[ 3.1510,  1.0184, -2.0435, -1.8640,  0.1936,  0.6455, -1.1654]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 105
	action: tensor([[ 2.9930,  1.8079, -1.7780, -1.9079, -0.0117,  0.5171, -1.0385]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 106
	action: tensor([[ 2.3907,  1.2003, -1.5342, -1.3772, -0.2341,  0.6662, -1.2917]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08060427821890725, distance: 1.0972559196638831 entropy 0.03264415264129639
epoch: 20, step: 107
	action: tensor([[ 3.5290,  2.1981, -2.3980, -2.4407, -0.0906,  0.2160, -1.6011]],
       dtype=torch.float64)
	q_value: tensor([[-6.5787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 108
	action: tensor([[ 2.8457,  1.4576, -1.3335, -1.6927,  0.1974,  1.0175, -1.0588]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 109
	action: tensor([[ 2.5397,  1.1543, -1.7278, -1.8975,  0.2867,  0.6416, -1.2499]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 110
	action: tensor([[ 2.8319,  1.5218, -1.6290, -1.9145, -0.2536,  0.3387, -0.7408]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 111
	action: tensor([[ 2.4865,  0.9768, -1.6469, -1.7489, -0.1312, -0.0396, -0.8579]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 112
	action: tensor([[ 2.7901,  1.3859, -2.0519, -1.8342, -0.1741,  0.4105, -1.1597]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 113
	action: tensor([[ 2.9607,  1.5148, -1.8736, -1.9717,  0.0539,  0.7522, -0.6929]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 114
	action: tensor([[ 2.7725,  1.1920, -1.9538, -1.7730, -0.2521,  0.2066, -1.4153]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 115
	action: tensor([[ 2.7590,  1.5006, -2.0420, -1.9545, -0.2116,  0.6260, -1.0145]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 116
	action: tensor([[ 3.0490,  1.2030, -1.5713, -1.8914,  0.0372,  0.5509, -1.2788]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 117
	action: tensor([[ 2.5210,  1.3335, -1.8607, -2.2305,  0.0246,  0.1333, -1.0096]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 118
	action: tensor([[ 2.5826,  1.5292, -1.6525, -1.6229, -0.0897,  0.6203, -0.9189]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 119
	action: tensor([[ 2.4056,  1.7162, -1.2288, -1.8278, -0.2444,  0.2944, -1.0166]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 120
	action: tensor([[ 2.4535,  1.5414, -1.5181, -0.9952,  0.1116,  0.4407, -1.3244]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 121
	action: tensor([[ 2.7163,  1.7390, -1.6600, -1.6780, -0.0109,  0.0507, -1.3192]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 122
	action: tensor([[ 2.5621,  1.4469, -1.6261, -2.2726, -0.0252,  0.5263, -1.0215]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 123
	action: tensor([[ 2.9014,  1.4343, -1.7226, -1.8435,  0.0126,  0.1827, -1.2243]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 124
	action: tensor([[ 2.7935,  1.6686, -1.5542, -1.8518, -0.2513,  0.2793, -1.0494]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 125
	action: tensor([[ 2.7025,  1.8900, -1.8339, -2.5130, -0.1858,  0.3727, -1.2649]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 126
	action: tensor([[ 2.7853,  1.4064, -2.1317, -1.6867,  0.3574,  0.6592, -1.3331]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 127
	action: tensor([[ 2.3350,  1.4088, -1.5896, -1.2744, -0.3082,  0.4732, -1.4841]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004669625645542519, distance: 1.1470129717882098 entropy 0.03264415264129639
LOSS epoch 20 actor 1025.1773825329344 critic 2131.889272485293 
epoch: 21, step: 0
	action: tensor([[ 2.7063,  1.4163, -1.7914, -1.3368, -0.8290,  0.5865, -1.7590]],
       dtype=torch.float64)
	q_value: tensor([[-7.8367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 1
	action: tensor([[ 0.7824,  0.6083, -1.1692, -1.1864, -0.6402,  0.2773, -0.8521]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7805708948619343, distance: 0.5360481595649746 entropy 0.03264415264129639
epoch: 21, step: 2
	action: tensor([[ 2.0794,  0.7417, -1.3201, -0.8603, -0.3951,  0.8078, -1.0434]],
       dtype=torch.float64)
	q_value: tensor([[-5.1361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 3
	action: tensor([[ 1.0688,  0.4139, -1.1292, -0.8806, -0.2623,  0.2169, -1.3419]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6603338553952735, distance: 0.6669339478456238 entropy 0.03264415264129639
epoch: 21, step: 4
	action: tensor([[ 2.0194,  1.0702, -1.7298, -0.7752, -0.7685,  0.6200, -0.9783]],
       dtype=torch.float64)
	q_value: tensor([[-4.8924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 5
	action: tensor([[ 1.3748,  0.6384, -1.1061, -0.5743, -0.9351,  0.2996, -1.1543]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6592750982376068, distance: 0.6679725731310618 entropy 0.03264415264129639
epoch: 21, step: 6
	action: tensor([[ 2.4572,  0.6769, -1.0764, -1.0036, -0.5512,  0.9093, -1.5426]],
       dtype=torch.float64)
	q_value: tensor([[-6.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 7
	action: tensor([[ 1.2917,  0.5569, -1.3561, -1.0396, -0.4131,  0.1976, -0.6154]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5345028268323713, distance: 0.7807557965874172 entropy 0.03264415264129639
epoch: 21, step: 8
	action: tensor([[ 1.5347,  0.9016, -1.4129, -0.8152, -1.0243,  0.6939, -1.2949]],
       dtype=torch.float64)
	q_value: tensor([[-5.8072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.564421642190966, distance: 0.7552484451971959 entropy 0.03264415264129639
epoch: 21, step: 9
	action: tensor([[ 2.3339,  1.3363, -1.6857, -1.4134, -0.9740,  0.3225, -1.8108]],
       dtype=torch.float64)
	q_value: tensor([[-7.4185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 10
	action: tensor([[ 1.6971,  0.7352, -1.1824, -0.8551, -0.8687,  0.2472, -1.0694]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877581047262804, distance: 0.9657627519402029 entropy 0.03264415264129639
epoch: 21, step: 11
	action: tensor([[ 2.0840,  1.1763, -1.1836, -1.5243, -0.8049,  1.0153, -1.5731]],
       dtype=torch.float64)
	q_value: tensor([[-6.8327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 12
	action: tensor([[ 1.1134,  0.7068, -1.4588, -1.2068, -0.1256,  0.5734, -1.2090]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6408318201451104, distance: 0.6858128531493441 entropy 0.03264415264129639
epoch: 21, step: 13
	action: tensor([[ 2.6351,  1.2251, -1.5486, -1.0956, -0.6334,  0.3991, -1.5891]],
       dtype=torch.float64)
	q_value: tensor([[-5.6410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 14
	action: tensor([[ 1.3880,  0.6581, -0.9569, -1.1767, -0.5124,  0.2533, -1.0043]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34918023000843024, distance: 0.9231814338861495 entropy 0.03264415264129639
epoch: 21, step: 15
	action: tensor([[ 2.4545,  0.7430, -1.7901, -1.3977, -0.7867,  0.4807, -1.7967]],
       dtype=torch.float64)
	q_value: tensor([[-6.0862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 16
	action: tensor([[ 1.1044,  0.7385, -0.5478, -0.9785, -0.4774,  0.0583, -1.3425]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7383617955140347, distance: 0.5853387466668754 entropy 0.03264415264129639
epoch: 21, step: 17
	action: tensor([[ 1.8902,  1.0460, -1.0183, -1.6915, -0.7315,  0.4599, -1.2797]],
       dtype=torch.float64)
	q_value: tensor([[-5.6034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 18
	action: tensor([[ 1.3777,  0.3161, -1.4260, -0.8496, -0.6986, -0.0188, -1.2011]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3230159774805542, distance: 0.9415554527085113 entropy 0.03264415264129639
epoch: 21, step: 19
	action: tensor([[ 2.1203,  0.7619, -1.4523, -1.5217, -0.9302,  0.7867, -1.4988]],
       dtype=torch.float64)
	q_value: tensor([[-5.9728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 20
	action: tensor([[ 2.0797,  0.6768, -0.6392, -1.0785, -0.4422,  0.4222, -1.1295]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26422004993403925, distance: 0.9815912261854501 entropy 0.03264415264129639
epoch: 21, step: 21
	action: tensor([[ 1.8327,  1.0439, -1.3237, -1.3797, -0.7358,  0.6411, -1.5394]],
       dtype=torch.float64)
	q_value: tensor([[-6.5422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 22
	action: tensor([[ 1.9415,  0.4089, -0.9539, -1.1827, -0.7740,  0.7423, -1.1811]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14737422502882414, distance: 1.0566615923071994 entropy 0.03264415264129639
epoch: 21, step: 23
	action: tensor([[ 2.5894,  1.1478, -1.9439, -1.4146, -0.1763,  0.7464, -1.6910]],
       dtype=torch.float64)
	q_value: tensor([[-6.8266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 24
	action: tensor([[ 1.3959,  0.6260, -1.0406, -1.1092, -0.3623,  0.1354, -1.1499]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31273490167849693, distance: 0.9486780189749265 entropy 0.03264415264129639
epoch: 21, step: 25
	action: tensor([[ 2.2604,  0.9437, -1.7203, -1.7217, -0.2162,  0.5603, -1.8742]],
       dtype=torch.float64)
	q_value: tensor([[-5.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 26
	action: tensor([[ 1.3448,  1.0137, -0.5676, -0.8966, -0.1394, -0.1058, -1.4227]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.646460032736379, distance: 0.6804182473777476 entropy 0.03264415264129639
epoch: 21, step: 27
	action: tensor([[ 1.8444,  1.1044, -1.5104, -1.1938, -0.4168,  0.0046, -1.9024]],
       dtype=torch.float64)
	q_value: tensor([[-6.3508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 28
	action: tensor([[ 1.2752,  0.5960, -1.0264, -0.4097, -0.2027,  0.2714, -0.9073]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7934144574542472, distance: 0.5201237149424389 entropy 0.03264415264129639
epoch: 21, step: 29
	action: tensor([[ 2.3701,  0.6319, -1.2186, -1.2376, -0.8457,  0.3936, -1.3526]],
       dtype=torch.float64)
	q_value: tensor([[-5.2259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 30
	action: tensor([[ 1.2442,  0.6986, -1.3248, -0.3024, -0.7847,  0.3929, -1.2854]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8712300245489948, distance: 0.4106425987760263 entropy 0.03264415264129639
epoch: 21, step: 31
	action: tensor([[ 2.1941,  1.0921, -1.3278, -1.2303, -0.7723,  0.3301, -1.3509]],
       dtype=torch.float64)
	q_value: tensor([[-6.1355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 32
	action: tensor([[ 1.1133,  0.6630, -1.0762, -0.9763, -0.7940,  0.3135, -1.2979]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.656550369627151, distance: 0.670638095653724 entropy 0.03264415264129639
epoch: 21, step: 33
	action: tensor([[ 1.9417,  0.9801, -1.4755, -1.3375, -0.7715,  0.6122, -1.8786]],
       dtype=torch.float64)
	q_value: tensor([[-5.8079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 34
	action: tensor([[ 1.5663,  0.6700, -0.9607, -1.2504, -0.8998,  0.0866, -1.1311]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0848139844839424, distance: 1.0947409934827748 entropy 0.03264415264129639
epoch: 21, step: 35
	action: tensor([[ 2.4784,  1.0712, -1.4150, -0.8404, -0.9305,  0.0207, -1.1908]],
       dtype=torch.float64)
	q_value: tensor([[-6.8860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 36
	action: tensor([[ 1.4500,  0.3848, -1.0694, -0.8227, -0.2534,  0.2429, -0.7369]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34123762142345826, distance: 0.9287976069215085 entropy 0.03264415264129639
epoch: 21, step: 37
	action: tensor([[ 1.9710,  1.3284, -1.8355, -1.2568, -0.4806,  0.8704, -1.4222]],
       dtype=torch.float64)
	q_value: tensor([[-5.5244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 38
	action: tensor([[ 1.2585,  0.8263, -0.9199, -0.8944, -0.6558,  0.0893, -0.9888]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6209427430308749, distance: 0.7045455903079445 entropy 0.03264415264129639
epoch: 21, step: 39
	action: tensor([[ 1.8290,  0.8474, -1.0760, -1.1700, -0.2652,  0.7097, -1.4800]],
       dtype=torch.float64)
	q_value: tensor([[-5.8862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 40
	action: tensor([[ 1.3649,  0.7168, -0.7211, -0.8555, -0.1324,  0.5240, -0.9071]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6461791795809135, distance: 0.6806884568200302 entropy 0.03264415264129639
epoch: 21, step: 41
	action: tensor([[ 2.3961,  0.5410, -1.4851, -0.8401, -0.6532,  0.0783, -1.4932]],
       dtype=torch.float64)
	q_value: tensor([[-5.6114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 42
	action: tensor([[ 1.8900,  0.8044, -1.2346, -0.6769, -0.7150,  0.1403, -1.3892]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31279613259898476, distance: 0.948635757464503 entropy 0.03264415264129639
epoch: 21, step: 43
	action: tensor([[ 2.6462,  1.2799, -1.3015, -1.9869, -0.5819,  0.9964, -1.3616]],
       dtype=torch.float64)
	q_value: tensor([[-6.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 44
	action: tensor([[ 1.5799,  0.5441, -0.7314, -1.1778, -0.3717,  0.6211, -1.0357]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2714666500717102, distance: 0.9767454838289453 entropy 0.03264415264129639
epoch: 21, step: 45
	action: tensor([[ 2.4143,  1.0447, -1.5340, -1.0075, -0.6755,  0.5047, -1.8319]],
       dtype=torch.float64)
	q_value: tensor([[-6.3682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 46
	action: tensor([[ 1.3488,  0.6790, -1.1962, -0.7319, -0.6490,  0.3241, -1.1429]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6109051501372492, distance: 0.713812969430261 entropy 0.03264415264129639
epoch: 21, step: 47
	action: tensor([[ 2.0028,  1.0031, -1.6833, -1.4214, -0.3262,  0.1545, -1.9627]],
       dtype=torch.float64)
	q_value: tensor([[-5.9971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 48
	action: tensor([[ 1.5031,  0.5615, -0.8671, -0.9844, -0.7134,  0.5734, -1.5094]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40628980353332345, distance: 0.881746911393351 entropy 0.03264415264129639
epoch: 21, step: 49
	action: tensor([[ 2.2441,  1.1460, -1.3863, -1.4240, -0.1841,  0.5076, -1.8905]],
       dtype=torch.float64)
	q_value: tensor([[-6.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 50
	action: tensor([[ 1.2589,  1.0215, -1.2210, -1.2530, -0.1164,  0.3205, -0.9850]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4412946297008098, distance: 0.8553583832303662 entropy 0.03264415264129639
epoch: 21, step: 51
	action: tensor([[ 2.0060,  1.1694, -1.1753, -2.0593, -0.5942,  0.8096, -1.7316]],
       dtype=torch.float64)
	q_value: tensor([[-6.1692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 52
	action: tensor([[ 1.7670,  0.9499, -0.9430, -0.8013, -0.7199,  0.3351, -1.1060]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4399875410737534, distance: 0.8563583522050021 entropy 0.03264415264129639
epoch: 21, step: 53
	action: tensor([[ 2.5600,  1.1956, -1.5570, -1.5252, -1.0815,  0.4291, -1.7148]],
       dtype=torch.float64)
	q_value: tensor([[-6.9381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 54
	action: tensor([[ 1.5216,  0.8819, -0.8895, -0.6794, -0.0313,  0.7870, -0.9756]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6518288336450538, distance: 0.6752321222730528 entropy 0.03264415264129639
epoch: 21, step: 55
	action: tensor([[ 2.1158,  1.3763, -1.6242, -1.6342, -0.3816,  0.5384, -1.7302]],
       dtype=torch.float64)
	q_value: tensor([[-6.2168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 56
	action: tensor([[ 1.5157,  0.6882, -1.2182, -0.8027, -0.6430, -0.1450, -1.1249]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901317078329795, distance: 0.964152168146916 entropy 0.03264415264129639
epoch: 21, step: 57
	action: tensor([[ 2.4155,  1.2623, -1.6046, -1.4946, -0.5979,  0.6665, -1.1405]],
       dtype=torch.float64)
	q_value: tensor([[-6.4288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 58
	action: tensor([[ 1.6610,  0.8320, -1.0597, -1.2107, -0.3961,  0.7005, -0.9240]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098918951428875, distance: 0.9506381900216109 entropy 0.03264415264129639
epoch: 21, step: 59
	action: tensor([[ 2.1454,  1.0771, -1.6867, -1.8321, -0.7355,  0.7982, -1.2736]],
       dtype=torch.float64)
	q_value: tensor([[-6.8781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 60
	action: tensor([[ 1.6503,  0.5226, -1.2948, -0.6292, -0.4129,  0.0378, -1.3916]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2815275096060138, distance: 0.9699777345159576 entropy 0.03264415264129639
epoch: 21, step: 61
	action: tensor([[ 2.5480,  1.2796, -1.6043, -1.5503, -0.8930,  0.7407, -1.7090]],
       dtype=torch.float64)
	q_value: tensor([[-6.4381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 62
	action: tensor([[ 1.5745,  1.1094, -1.1261, -1.2299, -0.8823,  0.5246, -0.9823]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42332296452936113, distance: 0.8690064939463011 entropy 0.03264415264129639
epoch: 21, step: 63
	action: tensor([[ 2.4641,  1.5355, -1.3138, -1.2650, -0.8760,  0.3665, -1.2766]],
       dtype=torch.float64)
	q_value: tensor([[-7.5567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 64
	action: tensor([[ 0.9918,  0.2501, -0.7953, -0.6333, -0.8517, -0.0692, -0.8821]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6373172576463392, distance: 0.6891601212229125 entropy 0.03264415264129639
epoch: 21, step: 65
	action: tensor([[ 1.8639,  0.9821, -1.1392, -1.1438, -0.1395,  0.6417, -0.8747]],
       dtype=torch.float64)
	q_value: tensor([[-4.7177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 66
	action: tensor([[ 1.4343,  0.5936, -1.0982, -0.6991, -0.3115,  0.6858, -1.1848]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6630366468442519, distance: 0.6642751847966811 entropy 0.03264415264129639
epoch: 21, step: 67
	action: tensor([[ 2.2512,  0.9002, -1.4534, -1.4044, -0.7395,  0.6144, -1.5629]],
       dtype=torch.float64)
	q_value: tensor([[-5.9618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 68
	action: tensor([[ 1.3740,  0.7085, -1.4882, -0.5256, -0.0253,  0.8342, -0.6569]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.804144707282833, distance: 0.5064357428332931 entropy 0.03264415264129639
epoch: 21, step: 69
	action: tensor([[ 2.1743,  1.1703, -1.8269, -0.7930, -0.8650,  0.7644, -1.6617]],
       dtype=torch.float64)
	q_value: tensor([[-6.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 70
	action: tensor([[ 1.5460,  0.8741, -1.0172, -0.8442, -0.3884,  0.2941, -0.7870]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4058343243362137, distance: 0.8820850733278662 entropy 0.03264415264129639
epoch: 21, step: 71
	action: tensor([[ 1.6423e+00,  1.3067e+00, -1.3946e+00, -7.2198e-01, -2.9233e-04,
          4.7020e-01, -1.3547e+00]], dtype=torch.float64)
	q_value: tensor([[-6.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 72
	action: tensor([[ 1.1929,  0.7728, -1.0811, -1.1352, -0.3397,  0.4785, -0.7900]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6233111766786245, distance: 0.7023410629167541 entropy 0.03264415264129639
epoch: 21, step: 73
	action: tensor([[ 1.9986,  1.3183, -1.3249, -1.4037, -0.7944,  0.4230, -1.5687]],
       dtype=torch.float64)
	q_value: tensor([[-5.7350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 74
	action: tensor([[ 1.4212,  0.7521, -0.7601, -0.8248, -0.5467,  0.3204, -1.4652]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5932752401271763, distance: 0.7298052764794959 entropy 0.03264415264129639
epoch: 21, step: 75
	action: tensor([[ 2.5576,  0.8872, -1.7215, -1.6096, -0.8956,  0.7054, -1.3494]],
       dtype=torch.float64)
	q_value: tensor([[-6.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 76
	action: tensor([[ 1.1248,  0.5221, -1.0950, -0.5382, -0.8476,  0.4015, -0.8548]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8614517242637575, distance: 0.4259486595817672 entropy 0.03264415264129639
epoch: 21, step: 77
	action: tensor([[ 1.8713,  0.9818, -1.0805, -0.9238, -0.7437,  0.4964, -1.2387]],
       dtype=torch.float64)
	q_value: tensor([[-5.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 78
	action: tensor([[ 1.5939,  0.8406, -0.9973, -0.9131, -0.2085,  0.0993, -0.7477]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2466942118139448, distance: 0.993212886022344 entropy 0.03264415264129639
epoch: 21, step: 79
	action: tensor([[ 2.1935,  0.6212, -1.7176, -1.2555, -0.6252,  0.2854, -1.3466]],
       dtype=torch.float64)
	q_value: tensor([[-6.3944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 80
	action: tensor([[ 1.5061,  0.8980, -1.0810, -0.9139, -0.2126,  0.3064, -1.3124]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42343474515173474, distance: 0.8689222676043199 entropy 0.03264415264129639
epoch: 21, step: 81
	action: tensor([[ 2.4182,  1.2468, -1.3855, -1.6302, -0.5506,  0.3073, -2.0911]],
       dtype=torch.float64)
	q_value: tensor([[-6.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 82
	action: tensor([[ 1.5635,  0.3599, -1.1355, -0.3613, -0.0820,  0.7953, -1.2282]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6854816592539443, distance: 0.6417703980637512 entropy 0.03264415264129639
epoch: 21, step: 83
	action: tensor([[ 2.8418,  1.0571, -2.0462, -1.5258, -0.1433,  0.2884, -1.5171]],
       dtype=torch.float64)
	q_value: tensor([[-6.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 84
	action: tensor([[ 1.3075,  1.0546, -1.1489, -1.0294, -0.7034,  0.1797, -0.8667]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49444158805196936, distance: 0.8136588710326023 entropy 0.03264415264129639
epoch: 21, step: 85
	action: tensor([[ 2.1384,  1.4427, -1.0226, -1.3686, -0.5161,  0.3432, -0.9904]],
       dtype=torch.float64)
	q_value: tensor([[-6.5181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 86
	action: tensor([[ 1.3325,  0.4970, -1.1613, -1.2390, -0.7589, -0.1346, -1.2740]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18471966676616924, distance: 1.0332613304188267 entropy 0.03264415264129639
epoch: 21, step: 87
	action: tensor([[ 2.4569,  0.5793, -1.5703, -1.1683, -0.6896,  0.3169, -0.9168]],
       dtype=torch.float64)
	q_value: tensor([[-6.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 88
	action: tensor([[ 1.4581,  0.8956, -0.9531, -0.9309, -0.5001,  0.2875, -1.5050]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4936579826058509, distance: 0.8142892043797126 entropy 0.03264415264129639
epoch: 21, step: 89
	action: tensor([[ 2.4900,  1.5755, -1.5033, -1.3893, -0.6711,  0.2236, -1.7469]],
       dtype=torch.float64)
	q_value: tensor([[-6.6693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 90
	action: tensor([[ 1.6106,  0.6789, -1.1348, -1.0683, -0.0164,  0.5159, -1.2956]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24405603052248792, distance: 0.9949505505681941 entropy 0.03264415264129639
epoch: 21, step: 91
	action: tensor([[ 2.5689,  0.8615, -1.9164, -1.7437, -0.7919,  0.4578, -1.3537]],
       dtype=torch.float64)
	q_value: tensor([[-6.4166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 92
	action: tensor([[ 1.7941,  0.9190, -1.1546, -0.9232, -0.0649,  0.1283, -1.3269]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22688410393289382, distance: 1.0061876783931887 entropy 0.03264415264129639
epoch: 21, step: 93
	action: tensor([[ 2.3652,  0.8204, -1.6034, -1.1191, -0.8413,  0.2416, -1.2984]],
       dtype=torch.float64)
	q_value: tensor([[-6.7092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 94
	action: tensor([[ 1.6202,  0.6966, -1.0017, -0.8736, -0.4432,  0.4132, -1.0440]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3493107004200098, distance: 0.9230888937510975 entropy 0.03264415264129639
epoch: 21, step: 95
	action: tensor([[ 2.7931,  0.7997, -1.9373, -1.4995, -0.8037,  0.1776, -1.5141]],
       dtype=torch.float64)
	q_value: tensor([[-6.4115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 96
	action: tensor([[ 1.5571,  0.8290, -0.8604, -0.9029, -0.4955,  0.6163, -0.9830]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5410740684823112, distance: 0.7752253973876986 entropy 0.03264415264129639
epoch: 21, step: 97
	action: tensor([[ 2.1079,  1.1873, -1.3934, -0.9362, -0.9446,  0.4123, -1.2570]],
       dtype=torch.float64)
	q_value: tensor([[-6.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 98
	action: tensor([[ 1.1683,  0.7856, -1.1494, -0.7186, -0.4615,  0.7447, -0.8711]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8672093918631956, distance: 0.4170041462728006 entropy 0.03264415264129639
epoch: 21, step: 99
	action: tensor([[ 2.4140,  0.8492, -1.5053, -1.4811, -0.3601,  0.5052, -1.4075]],
       dtype=torch.float64)
	q_value: tensor([[-5.7325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 100
	action: tensor([[ 1.7187,  0.6119, -0.9859, -0.3282, -0.3398,  0.1485, -1.0181]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49556626168468154, distance: 0.8127533276170883 entropy 0.03264415264129639
epoch: 21, step: 101
	action: tensor([[ 2.3768,  0.8414, -1.4527, -1.3339, -0.7267,  0.3319, -1.5066]],
       dtype=torch.float64)
	q_value: tensor([[-6.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 102
	action: tensor([[ 1.5156,  0.7772, -1.2679, -0.6753, -0.4327,  0.0070, -0.8516]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39734768292882794, distance: 0.8883622770313792 entropy 0.03264415264129639
epoch: 21, step: 103
	action: tensor([[ 2.6227,  1.3088, -1.7664, -1.5072, -0.5584,  1.0218, -1.4498]],
       dtype=torch.float64)
	q_value: tensor([[-6.3220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 104
	action: tensor([[ 1.4824,  0.3735, -1.0307, -1.2119, -0.3825,  0.4886, -0.8249]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16860675669602943, distance: 1.0434218781041527 entropy 0.03264415264129639
epoch: 21, step: 105
	action: tensor([[ 2.2124,  1.0112, -1.7308, -0.8023, -0.9405,  0.8149, -1.7179]],
       dtype=torch.float64)
	q_value: tensor([[-5.9831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 106
	action: tensor([[ 1.4597,  1.0986, -1.0777, -0.5294, -0.3131, -0.0274, -1.0127]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5149772307634863, distance: 0.7969622614107541 entropy 0.03264415264129639
epoch: 21, step: 107
	action: tensor([[ 2.1597,  1.1445, -1.1511, -1.1185, -0.2731,  0.5417, -1.7378]],
       dtype=torch.float64)
	q_value: tensor([[-6.5033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 108
	action: tensor([[ 1.2319,  0.7164, -1.4083, -1.1247, -0.9577,  0.2930, -1.0301]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4782513764299602, distance: 0.8265846752736961 entropy 0.03264415264129639
epoch: 21, step: 109
	action: tensor([[ 2.0959,  1.1588, -1.6215, -1.0984, -0.6963,  0.5483, -1.9285]],
       dtype=torch.float64)
	q_value: tensor([[-6.4598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 110
	action: tensor([[ 1.6288,  0.6744, -0.9220, -0.6996, -0.5066,  0.7632, -1.4156]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.576975876842956, distance: 0.7442849892755918 entropy 0.03264415264129639
epoch: 21, step: 111
	action: tensor([[ 2.6998,  1.2387, -1.5560, -1.3295, -1.1445,  0.5035, -1.3981]],
       dtype=torch.float64)
	q_value: tensor([[-6.7671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 112
	action: tensor([[ 1.4425,  0.5632, -0.9333, -0.8569, -0.2545, -0.2360, -0.9205]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2781460240626523, distance: 0.9722576516331848 entropy 0.03264415264129639
epoch: 21, step: 113
	action: tensor([[ 1.8082,  0.8025, -0.9810, -1.1591, -0.7706,  0.6702, -1.2323]],
       dtype=torch.float64)
	q_value: tensor([[-5.7606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 114
	action: tensor([[ 1.3374,  1.0692, -0.6743, -0.7760, -0.8637,  0.1407, -1.0073]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7035984798576216, distance: 0.6230127098089088 entropy 0.03264415264129639
epoch: 21, step: 115
	action: tensor([[ 2.1279,  0.8690, -1.7246, -1.0374, -0.6948,  0.5154, -1.7164]],
       dtype=torch.float64)
	q_value: tensor([[-6.5371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 116
	action: tensor([[ 2.0723,  0.5859, -0.7087, -0.3037, -0.4084,  0.6747, -0.7618]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6779825123875953, distance: 0.6493762814575078 entropy 0.03264415264129639
epoch: 21, step: 117
	action: tensor([[ 2.2580,  0.7146, -1.4732, -0.9059, -0.8681,  0.6031, -1.2126]],
       dtype=torch.float64)
	q_value: tensor([[-6.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 118
	action: tensor([[ 0.9793,  0.8915, -1.4556, -1.2059, -0.5395,  0.0816, -1.2626]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6071006487769229, distance: 0.7172942494524248 entropy 0.03264415264129639
epoch: 21, step: 119
	action: tensor([[ 2.3895,  0.9003, -1.6211, -1.5459, -0.4002,  0.4657, -1.4921]],
       dtype=torch.float64)
	q_value: tensor([[-5.9347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 120
	action: tensor([[ 1.4881,  0.7160, -1.0009, -0.4954, -0.5447,  0.5691, -0.7076]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7076548232111337, distance: 0.6187349663110927 entropy 0.03264415264129639
epoch: 21, step: 121
	action: tensor([[ 2.2155,  0.9627, -1.4346, -1.2026, -0.7402,  0.8277, -1.4263]],
       dtype=torch.float64)
	q_value: tensor([[-6.0935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 122
	action: tensor([[ 1.9195,  0.9447, -0.7154, -0.2798, -0.6213,  0.1561, -1.2675]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6223868534329874, distance: 0.7032022411240583 entropy 0.03264415264129639
epoch: 21, step: 123
	action: tensor([[ 2.5777,  1.0618, -1.2853, -1.1055, -0.3334,  0.2291, -1.3988]],
       dtype=torch.float64)
	q_value: tensor([[-6.8868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 124
	action: tensor([[ 1.7215,  0.5496, -1.1796, -1.0053, -0.3288,  0.5470, -1.5365]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18980553127592326, distance: 1.0300334540508405 entropy 0.03264415264129639
epoch: 21, step: 125
	action: tensor([[ 2.4275,  1.5319, -1.5225, -1.3373, -0.4920,  0.6513, -1.4700]],
       dtype=torch.float64)
	q_value: tensor([[-6.5592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 126
	action: tensor([[ 0.7903,  0.6676, -1.2508, -1.1473, -0.4471,  0.4278, -1.0142]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8410936721002426, distance: 0.4561705973124293 entropy 0.03264415264129639
epoch: 21, step: 127
	action: tensor([[ 2.0698,  1.0737, -1.0702, -1.1857, -0.2551,  0.6556, -1.4318]],
       dtype=torch.float64)
	q_value: tensor([[-5.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 21 actor 485.0087898125499 critic 1829.377913687798 
epoch: 22, step: 0
	action: tensor([[ 0.5223,  0.4772, -0.9858, -0.1928,  0.1230,  0.1754, -0.4393]],
       dtype=torch.float64)
	q_value: tensor([[-8.9459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9236891263822843, distance: 0.3161184952494169 entropy 0.03264415264129639
epoch: 22, step: 1
	action: tensor([[ 0.4355,  0.2885, -0.2188, -0.1964, -0.0281,  0.1764, -0.6094]],
       dtype=torch.float64)
	q_value: tensor([[-5.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7989988277286564, distance: 0.5130456248925795 entropy 0.03264415264129639
epoch: 22, step: 2
	action: tensor([[ 0.0831, -0.0224, -0.4651,  0.0578, -0.5866,  0.0431, -0.7236]],
       dtype=torch.float64)
	q_value: tensor([[-4.3703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37629491064938403, distance: 0.9037458940497116 entropy 0.03264415264129639
epoch: 22, step: 3
	action: tensor([[ 0.2320,  0.1262, -0.6228,  0.1049, -0.5931,  0.3724, -0.4634]],
       dtype=torch.float64)
	q_value: tensor([[-4.6893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5575383042394899, distance: 0.7611925568794198 entropy 0.03264415264129639
epoch: 22, step: 4
	action: tensor([[ 0.7733,  0.2104, -0.5024, -0.2222, -0.1280, -0.2641, -0.3784]],
       dtype=torch.float64)
	q_value: tensor([[-4.9201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7783730888182688, distance: 0.5387260051572419 entropy 0.03264415264129639
epoch: 22, step: 5
	action: tensor([[-0.3240,  0.4682, -0.2765, -0.4065, -0.3724,  0.5755, -0.4011]],
       dtype=torch.float64)
	q_value: tensor([[-5.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18758881826987694, distance: 1.031441590610092 entropy 0.03264415264129639
epoch: 22, step: 6
	action: tensor([[ 0.0083, -0.1905, -0.2837, -0.2978, -0.3326,  0.6368, -0.2068]],
       dtype=torch.float64)
	q_value: tensor([[-5.3345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14261593250939486, distance: 1.0596059729087526 entropy 0.03264415264129639
epoch: 22, step: 7
	action: tensor([[ 0.2134,  0.0303,  0.2577,  0.0306, -0.1439,  0.2238, -0.3258]],
       dtype=torch.float64)
	q_value: tensor([[-4.5398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5722726988183575, distance: 0.7484110292389751 entropy 0.03264415264129639
epoch: 22, step: 8
	action: tensor([[ 0.1020, -0.0708, -0.5430, -0.1793, -0.3611, -0.0130, -0.1556]],
       dtype=torch.float64)
	q_value: tensor([[-4.3996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3313208195715821, distance: 0.9357624070174131 entropy 0.03264415264129639
epoch: 22, step: 9
	action: tensor([[-0.0598, -0.1822, -0.2612, -0.0802,  0.1655, -0.1958, -0.9429]],
       dtype=torch.float64)
	q_value: tensor([[-4.1575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04192415208139144, distance: 1.1200995928512014 entropy 0.03264415264129639
epoch: 22, step: 10
	action: tensor([[ 0.1722, -0.2775, -0.4540, -0.1331, -0.4246,  0.2971, -0.4163]],
       dtype=torch.float64)
	q_value: tensor([[-4.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21144832835927663, distance: 1.0161826406694707 entropy 0.03264415264129639
epoch: 22, step: 11
	action: tensor([[ 0.5323,  0.4921, -0.7094, -0.5148, -0.0442,  0.0443, -0.3267]],
       dtype=torch.float64)
	q_value: tensor([[-4.2134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8895760279886726, distance: 0.38026676992640657 entropy 0.03264415264129639
epoch: 22, step: 12
	action: tensor([[ 0.1678,  0.1249, -0.0635, -0.3992, -0.0885,  0.0925, -0.6884]],
       dtype=torch.float64)
	q_value: tensor([[-5.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4137663684111387, distance: 0.876177422851009 entropy 0.03264415264129639
epoch: 22, step: 13
	action: tensor([[ 0.9788,  0.1966, -0.4632, -0.4751, -0.7633,  0.3743, -0.7074]],
       dtype=torch.float64)
	q_value: tensor([[-4.2419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7329147066288869, distance: 0.5914004909796827 entropy 0.03264415264129639
epoch: 22, step: 14
	action: tensor([[ 0.6597,  0.3830, -0.7001, -0.2454, -0.3781, -0.0604, -0.5023]],
       dtype=torch.float64)
	q_value: tensor([[-7.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9073432098440966, distance: 0.34833356042036606 entropy 0.03264415264129639
epoch: 22, step: 15
	action: tensor([[ 0.6841,  0.0349, -0.4684,  0.1202, -0.3069,  0.3694, -0.6468]],
       dtype=torch.float64)
	q_value: tensor([[-5.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8412868632524448, distance: 0.45589321715298065 entropy 0.03264415264129639
epoch: 22, step: 16
	action: tensor([[ 0.1876,  0.1974, -0.0806, -0.3737, -0.1459, -0.1153, -0.2637]],
       dtype=torch.float64)
	q_value: tensor([[-5.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46278508422049647, distance: 0.8387465068213982 entropy 0.03264415264129639
epoch: 22, step: 17
	action: tensor([[ 0.1687,  0.3250, -0.1358, -0.1043,  0.3793,  0.0440, -0.0786]],
       dtype=torch.float64)
	q_value: tensor([[-4.1525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6464521169007982, distance: 0.6804258646902477 entropy 0.03264415264129639
epoch: 22, step: 18
	action: tensor([[ 0.3514,  0.0843, -0.2675, -0.8586, -0.3296,  0.0130, -0.2526]],
       dtype=torch.float64)
	q_value: tensor([[-3.7244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3350653603232213, distance: 0.9331386377483203 entropy 0.03264415264129639
epoch: 22, step: 19
	action: tensor([[ 0.1153,  0.0948, -0.8853, -0.1626, -0.3837,  0.5836, -0.3401]],
       dtype=torch.float64)
	q_value: tensor([[-5.3070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4069464404470813, distance: 0.8812591753302753 entropy 0.03264415264129639
epoch: 22, step: 20
	action: tensor([[ 0.7454, -0.1218, -0.5539, -0.3685, -0.4265,  0.2553, -0.3948]],
       dtype=torch.float64)
	q_value: tensor([[-4.9407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5309139212894816, distance: 0.7837597661449067 entropy 0.03264415264129639
epoch: 22, step: 21
	action: tensor([[ 0.9597, -0.3401, -0.6836, -0.3695, -0.1624,  0.1902, -0.1613]],
       dtype=torch.float64)
	q_value: tensor([[-5.3295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822509336187198, distance: 0.969489280247636 entropy 0.03264415264129639
epoch: 22, step: 22
	action: tensor([[ 0.3032,  0.4623, -0.6287, -0.0762, -0.4930,  0.0231, -0.7225]],
       dtype=torch.float64)
	q_value: tensor([[-5.6468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.794961957719946, distance: 0.5181719699684036 entropy 0.03264415264129639
epoch: 22, step: 23
	action: tensor([[ 0.1790,  0.0254, -0.0271, -0.1858, -0.4076,  0.4682, -0.2517]],
       dtype=torch.float64)
	q_value: tensor([[-5.2467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4818145792683547, distance: 0.8237573226243198 entropy 0.03264415264129639
epoch: 22, step: 24
	action: tensor([[ 0.7851,  0.0383, -0.3055, -0.2146, -0.3141,  0.0237,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[-4.7342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6790695631312049, distance: 0.6482792886787452 entropy 0.03264415264129639
epoch: 22, step: 25
	action: tensor([[ 0.5239,  0.1172, -0.1992, -0.5867, -0.2965,  0.1191, -0.1162]],
       dtype=torch.float64)
	q_value: tensor([[-5.2932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.547975753713837, distance: 0.7693740935849724 entropy 0.03264415264129639
epoch: 22, step: 26
	action: tensor([[ 0.6085, -0.1681,  0.1441, -0.2680, -0.3333,  0.1892, -0.5828]],
       dtype=torch.float64)
	q_value: tensor([[-5.0721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47773085495572654, distance: 0.8269968927641091 entropy 0.03264415264129639
epoch: 22, step: 27
	action: tensor([[ 0.7949,  0.1084, -0.3295, -0.1715,  0.1342,  0.0425, -0.6353]],
       dtype=torch.float64)
	q_value: tensor([[-5.2181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7637221851290692, distance: 0.5562476204803307 entropy 0.03264415264129639
epoch: 22, step: 28
	action: tensor([[ 0.2180,  0.3543, -0.3222, -0.4927, -0.4537,  0.1110, -0.7682]],
       dtype=torch.float64)
	q_value: tensor([[-4.9760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6231121513279058, distance: 0.7025265810536749 entropy 0.03264415264129639
epoch: 22, step: 29
	action: tensor([[ 0.7422,  0.2296, -0.7104,  0.1642, -0.5047,  0.0391, -0.4358]],
       dtype=torch.float64)
	q_value: tensor([[-5.3232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9118201304806616, distance: 0.3398141187733362 entropy 0.03264415264129639
epoch: 22, step: 30
	action: tensor([[ 0.3299, -0.4498, -0.5500, -0.3437, -0.2163,  0.0168, -0.6312]],
       dtype=torch.float64)
	q_value: tensor([[-5.6915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061663301229113676, distance: 1.1085008853691038 entropy 0.03264415264129639
epoch: 22, step: 31
	action: tensor([[ 0.1940,  0.2004, -0.4401, -0.1838, -0.1458,  0.3426, -0.4338]],
       dtype=torch.float64)
	q_value: tensor([[-4.2791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5707269359171834, distance: 0.749762150252321 entropy 0.03264415264129639
epoch: 22, step: 32
	action: tensor([[ 0.5132, -0.4770, -0.5998,  0.2199,  0.0615, -0.1732, -0.5950]],
       dtype=torch.float64)
	q_value: tensor([[-4.0128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065457232105747, distance: 0.9529401135522624 entropy 0.03264415264129639
epoch: 22, step: 33
	action: tensor([[ 0.5909,  0.1309, -0.6015, -0.4017, -0.5034,  0.2537, -0.3668]],
       dtype=torch.float64)
	q_value: tensor([[-4.8425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7067207894673895, distance: 0.6197225975603579 entropy 0.03264415264129639
epoch: 22, step: 34
	action: tensor([[ 0.4510,  0.1786, -0.3763, -0.3484, -0.2688,  0.1883, -0.4521]],
       dtype=torch.float64)
	q_value: tensor([[-5.4095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.700282907289224, distance: 0.6264875558847353 entropy 0.03264415264129639
epoch: 22, step: 35
	action: tensor([[ 1.2357, -0.2599, -0.4375, -0.8222, -0.4249,  0.1605, -0.2949]],
       dtype=torch.float64)
	q_value: tensor([[-4.5200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06209399854289965, distance: 1.1793376695553905 entropy 0.03264415264129639
epoch: 22, step: 36
	action: tensor([[ 1.1127,  0.3835, -0.6754, -0.3996, -0.1551,  0.2285, -0.3789]],
       dtype=torch.float64)
	q_value: tensor([[-7.0073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.79815888804404, distance: 0.514116459768782 entropy 0.03264415264129639
epoch: 22, step: 37
	action: tensor([[ 0.1575,  0.3980, -0.2592,  0.0568, -0.3426,  0.4795, -0.4577]],
       dtype=torch.float64)
	q_value: tensor([[-6.6160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6567101178354662, distance: 0.6704821110706715 entropy 0.03264415264129639
epoch: 22, step: 38
	action: tensor([[ 0.4402, -0.0604, -0.4191, -0.1687, -0.4707, -0.3060, -0.3712]],
       dtype=torch.float64)
	q_value: tensor([[-4.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5359991338157435, distance: 0.7794999450547905 entropy 0.03264415264129639
epoch: 22, step: 39
	action: tensor([[ 0.4056,  0.2157, -0.3105, -0.5704, -0.1279, -0.1229, -0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-4.8357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5791299588496859, distance: 0.7423875831848511 entropy 0.03264415264129639
epoch: 22, step: 40
	action: tensor([[ 0.7951, -0.3773, -0.3307, -0.4929, -0.4195, -0.0120, -0.2928]],
       dtype=torch.float64)
	q_value: tensor([[-4.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10080352579725438, distance: 1.085135548382729 entropy 0.03264415264129639
epoch: 22, step: 41
	action: tensor([[ 0.6175,  0.1469, -0.6872, -0.5125,  0.0623,  0.0220, -1.2528]],
       dtype=torch.float64)
	q_value: tensor([[-5.5342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.671356486049282, distance: 0.6560232419063105 entropy 0.03264415264129639
epoch: 22, step: 42
	action: tensor([[ 0.5353,  0.4605, -0.3772,  0.1745, -0.0833,  0.1408, -0.6687]],
       dtype=torch.float64)
	q_value: tensor([[-5.6289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9041273775995822, distance: 0.3543267956068243 entropy 0.03264415264129639
epoch: 22, step: 43
	action: tensor([[ 0.5127,  0.3826, -0.3370, -0.4002, -0.4748, -0.2050, -0.4697]],
       dtype=torch.float64)
	q_value: tensor([[-4.9654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8061607872689215, distance: 0.5038224458594049 entropy 0.03264415264129639
epoch: 22, step: 44
	action: tensor([[ 0.3923,  0.4601, -0.8176, -0.1763, -0.6668, -0.3431, -0.0376]],
       dtype=torch.float64)
	q_value: tensor([[-5.3271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9084350392651337, distance: 0.34627516909775646 entropy 0.03264415264129639
epoch: 22, step: 45
	action: tensor([[ 0.7293,  0.6398, -0.2594,  0.2021, -0.1256,  0.1128, -0.8963]],
       dtype=torch.float64)
	q_value: tensor([[-6.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.958158576389795, distance: 0.2340776342536324 entropy 0.03264415264129639
epoch: 22, step: 46
	action: tensor([[ 0.5557,  0.1842, -1.0102, -0.3585, -0.2118,  0.6460, -0.5358]],
       dtype=torch.float64)
	q_value: tensor([[-6.1497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.80729632074123, distance: 0.5023445519523886 entropy 0.03264415264129639
epoch: 22, step: 47
	action: tensor([[ 0.6586,  0.2125, -0.2201, -0.4762, -0.1229,  0.2556, -0.4783]],
       dtype=torch.float64)
	q_value: tensor([[-5.9408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7460342529769765, distance: 0.5766924508910746 entropy 0.03264415264129639
epoch: 22, step: 48
	action: tensor([[ 0.4309,  0.2647, -0.8323, -0.5350,  0.0308, -0.0603, -0.3013]],
       dtype=torch.float64)
	q_value: tensor([[-4.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7671757019472707, distance: 0.5521675044049204 entropy 0.03264415264129639
epoch: 22, step: 49
	action: tensor([[ 0.6580,  0.2818, -0.4401, -0.3430, -0.3405, -0.1656, -0.3362]],
       dtype=torch.float64)
	q_value: tensor([[-4.9589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7969786979851287, distance: 0.5156173204445426 entropy 0.03264415264129639
epoch: 22, step: 50
	action: tensor([[ 0.8970,  0.2734, -0.3300, -0.1246,  0.1262,  0.1933, -0.3726]],
       dtype=torch.float64)
	q_value: tensor([[-5.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9006060545879684, distance: 0.360775185228158 entropy 0.03264415264129639
epoch: 22, step: 51
	action: tensor([[ 0.9768,  0.1102, -0.3943, -0.1432, -0.1285,  0.2742, -0.4278]],
       dtype=torch.float64)
	q_value: tensor([[-5.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8156779521262969, distance: 0.49129838523262614 entropy 0.03264415264129639
epoch: 22, step: 52
	action: tensor([[ 0.9964,  0.0691, -0.7883, -0.4321, -0.5256,  0.0885, -0.2624]],
       dtype=torch.float64)
	q_value: tensor([[-5.6661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6171688536250822, distance: 0.7080441276579321 entropy 0.03264415264129639
epoch: 22, step: 53
	action: tensor([[ 0.6146,  0.2565, -0.4208, -0.3864, -0.2916, -0.3108, -0.9306]],
       dtype=torch.float64)
	q_value: tensor([[-6.4681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7386846161907363, distance: 0.5849775269182771 entropy 0.03264415264129639
epoch: 22, step: 54
	action: tensor([[ 0.2649, -0.2450, -0.2852, -0.2222, -0.4745, -0.2139, -0.4928]],
       dtype=torch.float64)
	q_value: tensor([[-5.6994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2314732499460972, distance: 1.0031969140474633 entropy 0.03264415264129639
epoch: 22, step: 55
	action: tensor([[ 0.4758,  0.4057, -0.1759, -0.1652,  0.1721,  0.1443, -0.5470]],
       dtype=torch.float64)
	q_value: tensor([[-4.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8471509129777367, distance: 0.4473919008348411 entropy 0.03264415264129639
epoch: 22, step: 56
	action: tensor([[-0.0762, -0.1736,  0.0232, -0.6645, -0.1875, -0.4004, -0.5267]],
       dtype=torch.float64)
	q_value: tensor([[-4.3092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2499892337696954, distance: 1.2794102609968412 entropy 0.03264415264129639
epoch: 22, step: 57
	action: tensor([[-0.1870, -0.2844, -0.6256, -0.0669, -0.0058,  0.2570, -0.1412]],
       dtype=torch.float64)
	q_value: tensor([[-4.6066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1373111919100456, distance: 1.2203835633047408 entropy 0.03264415264129639
epoch: 22, step: 58
	action: tensor([[ 0.2890, -0.0724, -0.5473, -0.4301, -0.6471, -0.0135, -0.5897]],
       dtype=torch.float64)
	q_value: tensor([[-3.6710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3921521704478955, distance: 0.8921833793296805 entropy 0.03264415264129639
epoch: 22, step: 59
	action: tensor([[ 0.7241, -0.0615, -0.5309, -0.3401, -0.8985, -0.0198, -0.5004]],
       dtype=torch.float64)
	q_value: tensor([[-5.1152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5559213075325748, distance: 0.7625821950944268 entropy 0.03264415264129639
epoch: 22, step: 60
	action: tensor([[ 0.2317,  0.2886, -0.3941, -0.6755, -0.2103,  0.1205, -0.7136]],
       dtype=torch.float64)
	q_value: tensor([[-6.1935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5708572772178865, distance: 0.7496483154915268 entropy 0.03264415264129639
epoch: 22, step: 61
	action: tensor([[ 0.6037,  0.5082, -0.5173,  0.0583, -0.8518,  0.0991, -0.5586]],
       dtype=torch.float64)
	q_value: tensor([[-4.8736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9339627027945591, distance: 0.29407041759906305 entropy 0.03264415264129639
epoch: 22, step: 62
	action: tensor([[ 0.1165, -0.0509, -0.5093, -0.4415, -0.4694,  0.1231, -0.3170]],
       dtype=torch.float64)
	q_value: tensor([[-6.4441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29241594388010017, distance: 0.9625996789624466 entropy 0.03264415264129639
epoch: 22, step: 63
	action: tensor([[ 0.3405,  0.1862, -0.6805, -0.3491, -0.1484, -0.3305, -0.1466]],
       dtype=torch.float64)
	q_value: tensor([[-4.5330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6743281126743055, distance: 0.6530505939169579 entropy 0.03264415264129639
epoch: 22, step: 64
	action: tensor([[ 0.8571,  0.0147, -0.0925, -0.3702,  0.0837,  0.1195, -0.5743]],
       dtype=torch.float64)
	q_value: tensor([[-4.7157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6057374061118411, distance: 0.7185375696329226 entropy 0.03264415264129639
epoch: 22, step: 65
	action: tensor([[ 0.5667,  0.2058, -0.6681, -0.0811, -0.1162,  0.1418, -0.5142]],
       dtype=torch.float64)
	q_value: tensor([[-5.1415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8356555459260778, distance: 0.46391051868948724 entropy 0.03264415264129639
epoch: 22, step: 66
	action: tensor([[ 0.4399,  0.3487, -0.3209, -0.0090, -0.3092,  0.1420, -0.1665]],
       dtype=torch.float64)
	q_value: tensor([[-4.6676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8500126205051503, distance: 0.4431839786114993 entropy 0.03264415264129639
epoch: 22, step: 67
	action: tensor([[ 0.0062, -0.0275, -0.6209, -0.2957, -0.2243,  0.0299, -0.2369]],
       dtype=torch.float64)
	q_value: tensor([[-4.5045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2818965265055521, distance: 0.9697286058762105 entropy 0.03264415264129639
epoch: 22, step: 68
	action: tensor([[ 0.7334,  0.0994, -0.0103, -0.2921, -0.0545,  0.4917, -0.4338]],
       dtype=torch.float64)
	q_value: tensor([[-3.9279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7773390122334937, distance: 0.5399813482909975 entropy 0.03264415264129639
epoch: 22, step: 69
	action: tensor([[ 0.5006, -0.1895, -0.4425, -0.6830, -0.2693,  0.5394, -0.5374]],
       dtype=torch.float64)
	q_value: tensor([[-5.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33502678151867094, distance: 0.9331657072141218 entropy 0.03264415264129639
epoch: 22, step: 70
	action: tensor([[ 0.4707,  0.2079, -0.5887, -0.4161, -0.0198,  0.3186,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[-5.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7471563617995476, distance: 0.5754170268050859 entropy 0.03264415264129639
epoch: 22, step: 71
	action: tensor([[ 0.5435,  0.1637, -0.7187, -0.3223, -0.1673,  0.3842, -0.6484]],
       dtype=torch.float64)
	q_value: tensor([[-4.8529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7805361833161546, distance: 0.5360905566854783 entropy 0.03264415264129639
epoch: 22, step: 72
	action: tensor([[ 0.4107,  0.3671, -0.9420, -0.4249, -0.1866,  0.6750, -0.2750]],
       dtype=torch.float64)
	q_value: tensor([[-5.0131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8389670807644786, distance: 0.4592128436348158 entropy 0.03264415264129639
epoch: 22, step: 73
	action: tensor([[ 0.5748,  0.2485, -0.7001,  0.2390, -0.5132, -0.0576, -0.3932]],
       dtype=torch.float64)
	q_value: tensor([[-5.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8757392099289343, distance: 0.4033887188353975 entropy 0.03264415264129639
epoch: 22, step: 74
	action: tensor([[ 0.6182,  0.1644, -0.2523, -0.3973, -0.1867, -0.1098, -0.1496]],
       dtype=torch.float64)
	q_value: tensor([[-5.3159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6629439754491824, distance: 0.6643665227353509 entropy 0.03264415264129639
epoch: 22, step: 75
	action: tensor([[ 0.4024,  0.3785, -0.7669, -0.0639, -0.3489,  0.2868, -0.5856]],
       dtype=torch.float64)
	q_value: tensor([[-4.8597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8062817897403964, distance: 0.5036651678833136 entropy 0.03264415264129639
epoch: 22, step: 76
	action: tensor([[ 0.5143,  0.1564, -0.5982, -0.5002,  0.0229, -0.3298, -0.7171]],
       dtype=torch.float64)
	q_value: tensor([[-5.0706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.62121632327577, distance: 0.7042912955505148 entropy 0.03264415264129639
epoch: 22, step: 77
	action: tensor([[ 0.3360,  0.4525, -0.3774, -0.6302, -0.3355,  0.1652, -0.0456]],
       dtype=torch.float64)
	q_value: tensor([[-5.0067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7464340675804072, distance: 0.5762383327992888 entropy 0.03264415264129639
epoch: 22, step: 78
	action: tensor([[ 0.1563, -0.3609, -0.3835, -0.4006, -0.5067, -0.0276, -0.5072]],
       dtype=torch.float64)
	q_value: tensor([[-5.4973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.001609556080042096, distance: 1.1434229399982216 entropy 0.03264415264129639
epoch: 22, step: 79
	action: tensor([[ 0.1337,  0.1591, -0.2405, -0.1223,  0.0461,  0.1025, -0.6446]],
       dtype=torch.float64)
	q_value: tensor([[-4.5569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.495391749857631, distance: 0.8128939038636032 entropy 0.03264415264129639
epoch: 22, step: 80
	action: tensor([[ 0.0752,  0.6142, -0.2427,  0.0349,  0.0800, -0.3890,  0.1798]],
       dtype=torch.float64)
	q_value: tensor([[-3.7613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6624226621189881, distance: 0.6648801010180975 entropy 0.03264415264129639
epoch: 22, step: 81
	action: tensor([[ 0.0738,  0.3484,  0.0486, -0.3153,  0.0991,  0.3408, -0.1498]],
       dtype=torch.float64)
	q_value: tensor([[-4.6852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5407232458748343, distance: 0.7755216485044392 entropy 0.03264415264129639
epoch: 22, step: 82
	action: tensor([[ 0.0898, -0.0372, -0.4952, -0.0138, -0.5703,  0.1972, -0.9241]],
       dtype=torch.float64)
	q_value: tensor([[-4.0745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33945375637218067, distance: 0.9300543039592943 entropy 0.03264415264129639
epoch: 22, step: 83
	action: tensor([[ 0.3568, -0.1662, -0.7532, -0.2591, -0.2602,  0.0142, -0.2366]],
       dtype=torch.float64)
	q_value: tensor([[-4.9852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42928871727518925, distance: 0.8644998504825309 entropy 0.03264415264129639
epoch: 22, step: 84
	action: tensor([[ 0.1300,  0.0262, -0.8103, -0.2414,  0.2482, -0.0799, -0.2731]],
       dtype=torch.float64)
	q_value: tensor([[-4.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4604336709122684, distance: 0.8405801177337153 entropy 0.03264415264129639
epoch: 22, step: 85
	action: tensor([[ 0.3764,  0.0537, -0.1258, -0.4555, -0.2726,  0.4552, -0.7039]],
       dtype=torch.float64)
	q_value: tensor([[-4.0122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5478590074562644, distance: 0.7694734419583577 entropy 0.03264415264129639
epoch: 22, step: 86
	action: tensor([[ 0.1407,  0.2409, -0.3122, -0.2510, -0.0958,  0.0914, -0.8289]],
       dtype=torch.float64)
	q_value: tensor([[-4.9838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5579327744623914, distance: 0.7608531661986602 entropy 0.03264415264129639
epoch: 22, step: 87
	action: tensor([[ 0.0544,  0.7785, -0.6521,  0.4776, -0.1564, -0.3277, -0.4751]],
       dtype=torch.float64)
	q_value: tensor([[-4.3030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 22, step: 88
	action: tensor([[ 0.6613,  0.0888, -0.1774, -0.3495, -0.3827,  0.1958, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[-8.9459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6736195525511999, distance: 0.6537606248009666 entropy 0.03264415264129639
epoch: 22, step: 89
	action: tensor([[ 0.4662, -0.0069, -0.4603, -0.6188,  0.1344,  0.6831, -0.4843]],
       dtype=torch.float64)
	q_value: tensor([[-5.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6156138776476399, distance: 0.7094806302625235 entropy 0.03264415264129639
epoch: 22, step: 90
	action: tensor([[ 0.5886,  0.2347, -0.5436, -0.1873, -0.1009,  0.0349, -0.4031]],
       dtype=torch.float64)
	q_value: tensor([[-4.8456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8310232024635658, distance: 0.4704031603140178 entropy 0.03264415264129639
epoch: 22, step: 91
	action: tensor([[ 0.6065, -0.0736, -0.1595, -0.2620, -0.0177, -0.0214, -0.2394]],
       dtype=torch.float64)
	q_value: tensor([[-4.6153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5252023966394737, distance: 0.7885168028197892 entropy 0.03264415264129639
epoch: 22, step: 92
	action: tensor([[ 0.4175, -0.1655,  0.0417, -0.2953, -0.5205, -0.3644, -0.2805]],
       dtype=torch.float64)
	q_value: tensor([[-4.4082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28199493898915673, distance: 0.969662155375732 entropy 0.03264415264129639
epoch: 22, step: 93
	action: tensor([[ 0.0077, -0.3160, -0.5260,  0.0093,  0.0588,  0.4573, -0.4092]],
       dtype=torch.float64)
	q_value: tensor([[-5.2025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1540279285399656, distance: 1.0525305400508576 entropy 0.03264415264129639
epoch: 22, step: 94
	action: tensor([[ 0.0947,  0.5026, -0.4134, -0.2074, -0.2903,  0.1150, -0.4929]],
       dtype=torch.float64)
	q_value: tensor([[-3.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6613700639485119, distance: 0.6659158738580144 entropy 0.03264415264129639
epoch: 22, step: 95
	action: tensor([[ 0.8305,  0.3958, -0.5630, -0.6444, -0.0978,  0.1025, -0.1550]],
       dtype=torch.float64)
	q_value: tensor([[-4.5289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7962655111981674, distance: 0.5165221739611686 entropy 0.03264415264129639
epoch: 22, step: 96
	action: tensor([[ 0.7127,  0.1658, -0.1952, -0.4637,  0.0403, -0.0584, -0.4814]],
       dtype=torch.float64)
	q_value: tensor([[-6.0175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6521574188378072, distance: 0.6749134232095582 entropy 0.03264415264129639
epoch: 22, step: 97
	action: tensor([[ 0.8230,  0.1202, -0.1575, -0.3933, -0.3778, -0.0142, -0.7042]],
       dtype=torch.float64)
	q_value: tensor([[-4.8572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6657820908041127, distance: 0.6615635245442868 entropy 0.03264415264129639
epoch: 22, step: 98
	action: tensor([[ 0.2009,  0.3349, -0.3597, -0.5629, -0.1148,  0.1546, -0.2559]],
       dtype=torch.float64)
	q_value: tensor([[-5.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6168598215765401, distance: 0.7083298465578404 entropy 0.03264415264129639
epoch: 22, step: 99
	action: tensor([[ 0.5014,  0.3655, -0.2181, -0.3853, -0.7397, -0.2631, -0.4310]],
       dtype=torch.float64)
	q_value: tensor([[-4.4537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7932956258831323, distance: 0.5202732855074556 entropy 0.03264415264129639
epoch: 22, step: 100
	action: tensor([[ 0.5788,  0.4038, -0.7319, -0.3085,  0.0164, -0.5963, -0.6123]],
       dtype=torch.float64)
	q_value: tensor([[-5.8940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8556995668749112, distance: 0.4347008712189846 entropy 0.03264415264129639
epoch: 22, step: 101
	action: tensor([[ 0.1114, -0.0288, -0.2703, -0.2296, -0.1026,  0.2476, -0.4562]],
       dtype=torch.float64)
	q_value: tensor([[-5.9021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3694852767037856, distance: 0.9086660654716867 entropy 0.03264415264129639
epoch: 22, step: 102
	action: tensor([[-0.0292, -0.2105, -0.4402, -0.2503, -0.3655,  0.0981, -0.5003]],
       dtype=torch.float64)
	q_value: tensor([[-3.6908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06279626956378115, distance: 1.1078314690860243 entropy 0.03264415264129639
epoch: 22, step: 103
	action: tensor([[ 0.5497, -0.0264, -0.3434, -0.5683, -0.3603,  0.4513, -0.6560]],
       dtype=torch.float64)
	q_value: tensor([[-4.0179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5218609571163851, distance: 0.7912865743940057 entropy 0.03264415264129639
epoch: 22, step: 104
	action: tensor([[ 0.2367, -0.0062, -0.4549, -0.4059, -0.1118,  0.4929, -0.6446]],
       dtype=torch.float64)
	q_value: tensor([[-5.3096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48549273387898084, distance: 0.8208285421930218 entropy 0.03264415264129639
epoch: 22, step: 105
	action: tensor([[ 0.4688,  0.1117,  0.0601, -0.3590, -0.2591,  0.4672, -0.4336]],
       dtype=torch.float64)
	q_value: tensor([[-4.3595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6828671602006362, distance: 0.6444323024897205 entropy 0.03264415264129639
epoch: 22, step: 106
	action: tensor([[ 0.8081,  0.4230, -0.3204, -0.1341, -0.3858,  0.0340, -0.3529]],
       dtype=torch.float64)
	q_value: tensor([[-5.0290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9439709367773882, distance: 0.27087153817632637 entropy 0.03264415264129639
epoch: 22, step: 107
	action: tensor([[ 0.7450,  0.5494, -0.4554, -0.0733, -0.0176, -0.0065, -0.8078]],
       dtype=torch.float64)
	q_value: tensor([[-5.5652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9801505562103183, distance: 0.16122443517306234 entropy 0.03264415264129639
epoch: 22, step: 108
	action: tensor([[ 0.4767, -0.1025, -0.1799, -0.5271, -0.4226,  0.1736, -0.8686]],
       dtype=torch.float64)
	q_value: tensor([[-5.7375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3600711976780079, distance: 0.9154244801251298 entropy 0.03264415264129639
epoch: 22, step: 109
	action: tensor([[ 0.4209, -0.0033, -0.5140, -0.2076, -0.3045, -0.1539, -0.5666]],
       dtype=torch.float64)
	q_value: tensor([[-5.2897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5788837311257076, distance: 0.7426047163362459 entropy 0.03264415264129639
epoch: 22, step: 110
	action: tensor([[ 0.4584,  0.2337, -0.3457, -0.4896, -0.2100, -0.3916, -0.1944]],
       dtype=torch.float64)
	q_value: tensor([[-4.4454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6277324365779999, distance: 0.69820714957676 entropy 0.03264415264129639
epoch: 22, step: 111
	action: tensor([[ 0.7065,  0.1707,  0.0645, -0.5148, -0.2311,  0.4052, -0.6720]],
       dtype=torch.float64)
	q_value: tensor([[-4.9776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7247274018623029, distance: 0.6003965447508004 entropy 0.03264415264129639
epoch: 22, step: 112
	action: tensor([[ 0.5785,  0.0736, -0.6561, -0.2304, -0.2402,  0.5270, -0.2375]],
       dtype=torch.float64)
	q_value: tensor([[-5.7052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7605085023958194, distance: 0.5600176865276694 entropy 0.03264415264129639
epoch: 22, step: 113
	action: tensor([[ 0.5501,  0.2834, -0.8176, -0.0498, -0.3303,  0.4981, -1.1008]],
       dtype=torch.float64)
	q_value: tensor([[-5.1986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8399112402449858, distance: 0.457864648641068 entropy 0.03264415264129639
epoch: 22, step: 114
	action: tensor([[ 0.5508,  0.4162, -0.2534, -0.3631, -0.1795,  0.0725, -0.7350]],
       dtype=torch.float64)
	q_value: tensor([[-6.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8591750172019761, distance: 0.4294341190660326 entropy 0.03264415264129639
epoch: 22, step: 115
	action: tensor([[ 0.4984,  0.2848, -0.7634, -0.3187, -0.4789,  0.2708, -0.2100]],
       dtype=torch.float64)
	q_value: tensor([[-5.1202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8198769703478718, distance: 0.4856700414410881 entropy 0.03264415264129639
epoch: 22, step: 116
	action: tensor([[ 0.7758,  0.1553, -0.3520, -0.2520, -0.7907, -0.0161, -0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7682479914640175, distance: 0.5508945128683649 entropy 0.03264415264129639
epoch: 22, step: 117
	action: tensor([[ 0.7424, -0.0085, -0.8114, -0.0488, -0.1694, -0.2099, -0.5918]],
       dtype=torch.float64)
	q_value: tensor([[-6.0577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7165493319893981, distance: 0.6092498566709625 entropy 0.03264415264129639
epoch: 22, step: 118
	action: tensor([[ 0.3320,  0.1822, -0.6606,  0.0451, -0.1766,  0.4073, -0.3295]],
       dtype=torch.float64)
	q_value: tensor([[-5.3938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7003226179896662, distance: 0.626446051605572 entropy 0.03264415264129639
epoch: 22, step: 119
	action: tensor([[ 0.5723, -0.5140, -0.1295, -0.8911, -0.1884,  0.1770, -0.5120]],
       dtype=torch.float64)
	q_value: tensor([[-4.3663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27443261591736046, distance: 1.2918590503325862 entropy 0.03264415264129639
epoch: 22, step: 120
	action: tensor([[ 0.7844,  0.4330, -0.4128, -0.5712, -0.6470, -0.2125, -0.4813]],
       dtype=torch.float64)
	q_value: tensor([[-5.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7827837200040366, distance: 0.5333384312487526 entropy 0.03264415264129639
epoch: 22, step: 121
	action: tensor([[ 0.3865,  0.0797, -0.3347,  0.0435, -0.1111,  0.2613, -0.5758]],
       dtype=torch.float64)
	q_value: tensor([[-6.4169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6952672292151864, distance: 0.6317078496800477 entropy 0.03264415264129639
epoch: 22, step: 122
	action: tensor([[ 0.5774,  0.1183, -0.4876, -0.2423, -0.2016, -0.1338, -0.3708]],
       dtype=torch.float64)
	q_value: tensor([[-4.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7113717942038832, distance: 0.6147889854758317 entropy 0.03264415264129639
epoch: 22, step: 123
	action: tensor([[ 0.1913,  0.3494,  0.0421,  0.3013, -0.2782,  0.3024, -0.5785]],
       dtype=torch.float64)
	q_value: tensor([[-4.6562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7580697211441562, distance: 0.5628618402829788 entropy 0.03264415264129639
epoch: 22, step: 124
	action: tensor([[ 0.3052,  0.2498, -0.0937, -0.2248,  0.0919, -0.3221, -0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-4.7962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6021209175423134, distance: 0.721825544522865 entropy 0.03264415264129639
epoch: 22, step: 125
	action: tensor([[ 0.2507, -0.1443, -0.4462, -0.3856,  0.0672, -0.0350, -0.6072]],
       dtype=torch.float64)
	q_value: tensor([[-4.2530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29753319394057165, distance: 0.9591126009697061 entropy 0.03264415264129639
epoch: 22, step: 126
	action: tensor([[ 0.0849,  0.3889, -0.1608, -0.2346, -0.1930,  0.0063, -0.4437]],
       dtype=torch.float64)
	q_value: tensor([[-3.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5762352119655345, distance: 0.7449362815199614 entropy 0.03264415264129639
epoch: 22, step: 127
	action: tensor([[ 0.3470,  0.3467, -0.1786, -0.1846,  0.2935,  0.4494, -0.3903]],
       dtype=torch.float64)
	q_value: tensor([[-4.2370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8164688049838028, distance: 0.49024326891390807 entropy 0.03264415264129639
LOSS epoch 22 actor 24.620144109708896 critic 157.87564940707995 
epoch: 23, step: 0
	action: tensor([[ 0.2409, -0.2331, -0.2056,  0.1471,  0.0321, -0.0985, -0.4698]],
       dtype=torch.float64)
	q_value: tensor([[-5.5814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39020390132220073, distance: 0.8936120450653194 entropy 0.03264415264129639
epoch: 23, step: 1
	action: tensor([[ 0.0745, -0.2614, -0.2319, -0.6001, -0.4911,  0.4794, -0.6133]],
       dtype=torch.float64)
	q_value: tensor([[-5.1508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030931665248570273, distance: 1.1619077082982692 entropy 0.03264415264129639
epoch: 23, step: 2
	action: tensor([[ 0.0566, -0.0485, -0.1875, -0.2603,  0.0584,  0.7751, -0.3493]],
       dtype=torch.float64)
	q_value: tensor([[-7.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37957142124932564, distance: 0.9013689436035522 entropy 0.03264415264129639
epoch: 23, step: 3
	action: tensor([[ 0.1710,  0.1129, -0.4542, -0.1852, -0.3287,  0.2827,  0.1120]],
       dtype=torch.float64)
	q_value: tensor([[-5.9082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5100147098091539, distance: 0.8010289540772021 entropy 0.03264415264129639
epoch: 23, step: 4
	action: tensor([[ 0.0102,  0.3135,  0.1989, -0.4321,  0.0616,  0.3087, -0.4213]],
       dtype=torch.float64)
	q_value: tensor([[-6.0805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4135119373144519, distance: 0.876367536986165 entropy 0.03264415264129639
epoch: 23, step: 5
	action: tensor([[ 0.3411,  0.3595,  0.1360,  0.2941,  0.0480,  0.0284, -0.1866]],
       dtype=torch.float64)
	q_value: tensor([[-5.7847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8577070651061831, distance: 0.43166651570758025 entropy 0.03264415264129639
epoch: 23, step: 6
	action: tensor([[ 0.5915, -0.0029, -0.3133,  0.4295,  0.3085, -0.3648,  0.2273]],
       dtype=torch.float64)
	q_value: tensor([[-5.8494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7757093705100898, distance: 0.5419537905377676 entropy 0.03264415264129639
epoch: 23, step: 7
	action: tensor([[ 0.0328, -0.0638, -0.2001, -0.2035,  0.0996,  0.2744, -0.6257]],
       dtype=torch.float64)
	q_value: tensor([[-6.9346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29104203053946887, distance: 0.9635337638704554 entropy 0.03264415264129639
epoch: 23, step: 8
	action: tensor([[ 0.4846,  0.4663, -0.2417, -0.1339,  0.1335,  0.3740, -0.3782]],
       dtype=torch.float64)
	q_value: tensor([[-4.9236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8975497506877708, distance: 0.3662799979348398 entropy 0.03264415264129639
epoch: 23, step: 9
	action: tensor([[ 0.2873, -0.0516, -0.2808, -0.2545,  0.1047,  0.3150,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-6.1111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4943538152424124, distance: 0.8137294998896558 entropy 0.03264415264129639
epoch: 23, step: 10
	action: tensor([[ 0.3356,  0.3327, -0.4035, -0.1577, -0.4170, -0.1747,  0.3230]],
       dtype=torch.float64)
	q_value: tensor([[-5.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.763883857445411, distance: 0.5560572826144449 entropy 0.03264415264129639
epoch: 23, step: 11
	action: tensor([[-0.2702, -0.0242, -0.0306, -0.4515,  0.0359,  0.2810, -0.3948]],
       dtype=torch.float64)
	q_value: tensor([[-7.1309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11652592534103223, distance: 1.2091804004828661 entropy 0.03264415264129639
epoch: 23, step: 12
	action: tensor([[ 0.2818, -0.0162,  0.0653, -0.2967, -0.0321, -0.0273, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-5.2166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35736593962296415, distance: 0.917357388668084 entropy 0.03264415264129639
epoch: 23, step: 13
	action: tensor([[ 0.0519,  0.3809, -0.6006,  0.2028,  0.2642,  0.3790, -0.2350]],
       dtype=torch.float64)
	q_value: tensor([[-5.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5703830199610945, distance: 0.7500624294490003 entropy 0.03264415264129639
epoch: 23, step: 14
	action: tensor([[-0.0475,  0.3843, -0.0921, -0.1972, -0.3153,  0.3369, -0.2516]],
       dtype=torch.float64)
	q_value: tensor([[-5.6265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4767007200324438, distance: 0.8278120842386576 entropy 0.03264415264129639
epoch: 23, step: 15
	action: tensor([[ 0.3745, -0.0218, -0.8134, -0.3852, -0.1114, -0.1690,  0.1981]],
       dtype=torch.float64)
	q_value: tensor([[-6.2800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5485567680346632, distance: 0.7688794728791369 entropy 0.03264415264129639
epoch: 23, step: 16
	action: tensor([[ 0.4126, -0.2494, -0.7955, -0.3984, -0.1747,  0.2501, -0.4309]],
       dtype=torch.float64)
	q_value: tensor([[-6.9912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3700932464010498, distance: 0.9082278721602732 entropy 0.03264415264129639
epoch: 23, step: 17
	action: tensor([[ 0.2348, -0.0901, -0.1391, -0.1308,  0.1035, -0.0363, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-6.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38301301434625834, distance: 0.8988654651949645 entropy 0.03264415264129639
epoch: 23, step: 18
	action: tensor([[ 0.1290,  0.1346, -0.3424,  0.2408, -0.2894, -0.1161, -0.4786]],
       dtype=torch.float64)
	q_value: tensor([[-4.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5795568341845474, distance: 0.7420109974040185 entropy 0.03264415264129639
epoch: 23, step: 19
	action: tensor([[ 0.1988, -0.2385, -0.1633, -0.0944,  0.2121, -0.0319, -0.1471]],
       dtype=torch.float64)
	q_value: tensor([[-5.6299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24110210476817384, distance: 0.9968925890340211 entropy 0.03264415264129639
epoch: 23, step: 20
	action: tensor([[-0.4250,  0.1291, -0.0830,  0.2888, -0.1562,  0.0077,  0.0584]],
       dtype=torch.float64)
	q_value: tensor([[-4.7382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04689641107930853, distance: 1.117189243838657 entropy 0.03264415264129639
epoch: 23, step: 21
	action: tensor([[ 0.5432,  0.5361, -0.3883, -0.0090, -0.0128,  0.3947, -0.1969]],
       dtype=torch.float64)
	q_value: tensor([[-5.4383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.917452088310783, distance: 0.328783291330555 entropy 0.03264415264129639
epoch: 23, step: 22
	action: tensor([[-0.1140,  0.2595, -0.3018, -0.3217,  0.2396,  0.2509, -0.4633]],
       dtype=torch.float64)
	q_value: tensor([[-6.6206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36800504020134417, distance: 0.9097320613386262 entropy 0.03264415264129639
epoch: 23, step: 23
	action: tensor([[ 0.0640,  0.1274, -0.2040, -0.2180, -0.3169, -0.1227,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-5.0053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3997080734533798, distance: 0.8866208590767919 entropy 0.03264415264129639
epoch: 23, step: 24
	action: tensor([[ 0.2016, -0.0907, -0.0750, -0.4908, -0.4217, -0.0095, -0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-5.7421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18956756428484456, distance: 1.0301847115423493 entropy 0.03264415264129639
epoch: 23, step: 25
	action: tensor([[ 0.0519,  0.0796, -0.3075, -0.5606, -0.3679, -0.3852, -0.2278]],
       dtype=torch.float64)
	q_value: tensor([[-6.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2804045540346055, distance: 0.9707354647235208 entropy 0.03264415264129639
epoch: 23, step: 26
	action: tensor([[ 0.1165, -0.0958, -0.1665,  0.0097, -0.4179, -0.0452,  0.3071]],
       dtype=torch.float64)
	q_value: tensor([[-6.5207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3582030534679378, distance: 0.9167597057377386 entropy 0.03264415264129639
epoch: 23, step: 27
	action: tensor([[ 0.0498,  0.0340, -0.4748, -0.3230,  0.1710,  0.1500, -0.4886]],
       dtype=torch.float64)
	q_value: tensor([[-5.9180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3662218153267236, distance: 0.9110145985608364 entropy 0.03264415264129639
epoch: 23, step: 28
	action: tensor([[-0.0133, -0.0704, -0.2803,  0.1760, -0.3440, -0.0484,  0.0907]],
       dtype=torch.float64)
	q_value: tensor([[-4.8199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3134989783767219, distance: 0.9481505192143765 entropy 0.03264415264129639
epoch: 23, step: 29
	action: tensor([[-0.0310, -0.0133, -0.2495, -0.2623, -0.0880,  0.3424, -0.0931]],
       dtype=torch.float64)
	q_value: tensor([[-5.3200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2514922164428173, distance: 0.9900448150678721 entropy 0.03264415264129639
epoch: 23, step: 30
	action: tensor([[ 0.2900,  0.1831, -0.2267,  0.0013,  0.2374, -0.5084, -0.1337]],
       dtype=torch.float64)
	q_value: tensor([[-5.0862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5896576753030456, distance: 0.7330436744901656 entropy 0.03264415264129639
epoch: 23, step: 31
	action: tensor([[ 0.1168,  0.0202, -0.1612,  0.2173,  0.1144, -0.0450,  0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-5.9869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5123294555014908, distance: 0.7991346386939732 entropy 0.03264415264129639
epoch: 23, step: 32
	action: tensor([[-0.4106,  0.1503,  0.0736, -0.1101,  0.0418, -0.2364, -0.7608]],
       dtype=torch.float64)
	q_value: tensor([[-5.1351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14972898065892482, distance: 1.2270278852342846 entropy 0.03264415264129639
epoch: 23, step: 33
	action: tensor([[ 0.3040,  0.3873, -0.7342, -0.2434, -0.0589,  0.3058, -0.3837]],
       dtype=torch.float64)
	q_value: tensor([[-5.6655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7666876869133981, distance: 0.5527458911729176 entropy 0.03264415264129639
epoch: 23, step: 34
	action: tensor([[ 0.0880, -0.1506, -0.6016, -0.0208, -0.0818, -0.1772, -0.3876]],
       dtype=torch.float64)
	q_value: tensor([[-6.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2809662958927537, distance: 0.9703564953949692 entropy 0.03264415264129639
epoch: 23, step: 35
	action: tensor([[ 0.5679,  0.0078, -0.2528, -0.4378, -0.1176, -0.1821, -0.2285]],
       dtype=torch.float64)
	q_value: tensor([[-5.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4804864463976378, distance: 0.8248123108582449 entropy 0.03264415264129639
epoch: 23, step: 36
	action: tensor([[-0.1009,  0.1872, -0.3526, -0.2044, -0.3514, -0.2330, -0.5805]],
       dtype=torch.float64)
	q_value: tensor([[-6.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31858183762199566, distance: 0.9446339407817135 entropy 0.03264415264129639
epoch: 23, step: 37
	action: tensor([[ 0.2711,  0.1141, -0.0984, -0.1692,  0.2356, -0.1854,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-5.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.514044337292408, distance: 0.7977283323849977 entropy 0.03264415264129639
epoch: 23, step: 38
	action: tensor([[ 0.3540, -0.2270, -0.3196, -0.0573, -0.3875,  0.3578, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-5.1768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4441456214377181, distance: 0.8531732076842137 entropy 0.03264415264129639
epoch: 23, step: 39
	action: tensor([[ 0.1873, -0.0117, -0.5863, -0.3382, -0.1135,  0.3407, -0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-6.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45436408610893864, distance: 0.8452947414442553 entropy 0.03264415264129639
epoch: 23, step: 40
	action: tensor([[ 0.2970,  0.3262, -0.5020, -0.0557, -0.1597, -0.2052, -0.3988]],
       dtype=torch.float64)
	q_value: tensor([[-5.6368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7599779408707659, distance: 0.5606376656568752 entropy 0.03264415264129639
epoch: 23, step: 41
	action: tensor([[-0.3810,  0.5362,  0.0444,  0.3009, -0.0307,  0.2093,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-5.9823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3721940218692248, distance: 0.9067121110662835 entropy 0.03264415264129639
epoch: 23, step: 42
	action: tensor([[-0.2245,  0.2104, -0.0551, -0.0516, -0.5383, -0.1482, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-6.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21254574719276131, distance: 1.0154752892848058 entropy 0.03264415264129639
epoch: 23, step: 43
	action: tensor([[ 0.1586, -0.0383, -0.0725, -0.1040,  0.2412, -0.0266, -0.0885]],
       dtype=torch.float64)
	q_value: tensor([[-6.2452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3661708406816755, distance: 0.9110512341797273 entropy 0.03264415264129639
epoch: 23, step: 44
	action: tensor([[ 0.5987, -0.1429, -0.5331, -0.3066, -0.1670,  0.4190, -0.4661]],
       dtype=torch.float64)
	q_value: tensor([[-4.7361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5382370303626285, distance: 0.7776178917783163 entropy 0.03264415264129639
epoch: 23, step: 45
	action: tensor([[ 0.1997, -0.5574, -0.3359, -0.0073, -0.0383,  0.1876, -0.1369]],
       dtype=torch.float64)
	q_value: tensor([[-6.5265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03351887797104214, distance: 1.1250022240690334 entropy 0.03264415264129639
epoch: 23, step: 46
	action: tensor([[ 0.2492,  0.1178,  0.1226, -0.0834, -0.2774,  0.0693,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[-5.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5823873351867557, distance: 0.7395091019861753 entropy 0.03264415264129639
epoch: 23, step: 47
	action: tensor([[ 0.3091, -0.4626, -0.4656, -0.0674, -0.5034,  0.0983, -0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-5.8164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1402927021882453, distance: 1.0610405944903791 entropy 0.03264415264129639
epoch: 23, step: 48
	action: tensor([[ 0.3039,  0.6097, -0.3264, -0.2695, -0.2249,  0.3849, -0.2402]],
       dtype=torch.float64)
	q_value: tensor([[-6.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8050680389178606, distance: 0.5052405732855514 entropy 0.03264415264129639
epoch: 23, step: 49
	action: tensor([[-0.0084,  0.1722, -0.1142, -0.3603, -0.2304, -0.1969, -0.3575]],
       dtype=torch.float64)
	q_value: tensor([[-7.1099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2896893787188376, distance: 0.9644525098895571 entropy 0.03264415264129639
epoch: 23, step: 50
	action: tensor([[-0.0256, -0.0476, -0.4373, -0.1969,  0.1181,  0.4272, -0.4837]],
       dtype=torch.float64)
	q_value: tensor([[-5.6469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294705747023307, distance: 0.9610408889793126 entropy 0.03264415264129639
epoch: 23, step: 51
	action: tensor([[ 0.5452,  0.1405,  0.0116, -0.6640, -0.0280,  0.2834, -0.2690]],
       dtype=torch.float64)
	q_value: tensor([[-5.0218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5525959031983952, distance: 0.7654321003858103 entropy 0.03264415264129639
epoch: 23, step: 52
	action: tensor([[ 0.1886,  0.3153, -0.5235, -0.0928, -0.1390, -0.0262, -0.1610]],
       dtype=torch.float64)
	q_value: tensor([[-6.5773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6809079966395809, distance: 0.6464198045759274 entropy 0.03264415264129639
epoch: 23, step: 53
	action: tensor([[ 0.1443,  0.1382, -0.1388,  0.5777, -0.0205, -0.1531,  0.3137]],
       dtype=torch.float64)
	q_value: tensor([[-5.6141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.683587201708737, distance: 0.6437003037289641 entropy 0.03264415264129639
epoch: 23, step: 54
	action: tensor([[ 0.2071, -0.1219, -0.0605, -0.3693, -0.3588,  0.1322, -0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-6.1507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2434817136514772, distance: 0.9953284280432304 entropy 0.03264415264129639
epoch: 23, step: 55
	action: tensor([[ 0.4303,  0.5548, -0.1455, -0.3391, -0.4325, -0.6093,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[-5.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8282869902902112, distance: 0.47419644465003785 entropy 0.03264415264129639
epoch: 23, step: 56
	action: tensor([[ 0.2330,  0.4285,  0.1045, -0.1770,  0.4249,  0.0267, -0.6190]],
       dtype=torch.float64)
	q_value: tensor([[-7.9252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7142662436141243, distance: 0.6116985747521537 entropy 0.03264415264129639
epoch: 23, step: 57
	action: tensor([[ 0.4463, -0.0280,  0.0426, -0.5482, -0.3067, -0.1988, -0.3152]],
       dtype=torch.float64)
	q_value: tensor([[-5.6493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995884718855527, distance: 0.9577084869690504 entropy 0.03264415264129639
epoch: 23, step: 58
	action: tensor([[ 0.4187, -0.2039, -0.3023,  0.1002, -0.0235,  0.2423, -0.5248]],
       dtype=torch.float64)
	q_value: tensor([[-6.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5596405549162333, distance: 0.7593820918440459 entropy 0.03264415264129639
epoch: 23, step: 59
	action: tensor([[ 0.2941, -0.2197,  0.2024, -0.2133, -0.2450,  0.1779, -0.8131]],
       dtype=torch.float64)
	q_value: tensor([[-5.4097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067460648081072, distance: 0.952802449583636 entropy 0.03264415264129639
epoch: 23, step: 60
	action: tensor([[ 0.2838,  0.1002, -0.2152, -0.4125, -0.0606,  0.0148,  0.3174]],
       dtype=torch.float64)
	q_value: tensor([[-6.4575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4753374437289717, distance: 0.8288896727178422 entropy 0.03264415264129639
epoch: 23, step: 61
	action: tensor([[ 0.5954, -0.0076, -0.1604, -0.6447,  0.2796, -0.0366, -0.3213]],
       dtype=torch.float64)
	q_value: tensor([[-6.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3603531102812948, distance: 0.9152228185293614 entropy 0.03264415264129639
epoch: 23, step: 62
	action: tensor([[ 0.0613, -0.0228, -0.1913, -0.7636,  0.0888, -0.0687,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[-6.0787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06327153652180362, distance: 1.1075505363268452 entropy 0.03264415264129639
epoch: 23, step: 63
	action: tensor([[ 0.1478,  0.1497,  0.1248, -0.7548, -0.2955,  0.2732, -0.2868]],
       dtype=torch.float64)
	q_value: tensor([[-5.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2429697233495045, distance: 0.9956651762225603 entropy 0.03264415264129639
epoch: 23, step: 64
	action: tensor([[-0.2586, -0.2110, -0.3458, -0.0403, -0.5543, -0.1014, -0.3756]],
       dtype=torch.float64)
	q_value: tensor([[-6.9463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12840372110614395, distance: 1.2155951204333104 entropy 0.03264415264129639
epoch: 23, step: 65
	action: tensor([[ 0.3319,  0.0365,  0.0385, -0.1051,  0.1667,  0.0654, -0.4790]],
       dtype=torch.float64)
	q_value: tensor([[-5.9906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5654078431656564, distance: 0.7543929747504201 entropy 0.03264415264129639
epoch: 23, step: 66
	action: tensor([[-0.2874,  0.2296, -0.2940,  0.4656, -0.0712, -0.5435, -0.1655]],
       dtype=torch.float64)
	q_value: tensor([[-5.1690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23965524438152708, distance: 0.997842438535035 entropy 0.03264415264129639
epoch: 23, step: 67
	action: tensor([[ 0.3135, -0.0375, -0.1503,  0.0537, -0.4042, -0.0752, -0.4951]],
       dtype=torch.float64)
	q_value: tensor([[-5.9813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5712377070049385, distance: 0.7493159647770028 entropy 0.03264415264129639
epoch: 23, step: 68
	action: tensor([[ 0.0284,  0.1040, -0.3064, -0.4847, -0.2039,  0.1769, -0.1824]],
       dtype=torch.float64)
	q_value: tensor([[-5.9605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3128084467444314, distance: 0.948627258028179 entropy 0.03264415264129639
epoch: 23, step: 69
	action: tensor([[ 0.3099,  0.3397, -0.0277, -0.2580, -0.4827,  0.1176,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[-5.7074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6982199033953274, distance: 0.6286399687567757 entropy 0.03264415264129639
epoch: 23, step: 70
	action: tensor([[ 0.2110, -0.0557, -0.6730, -0.4796,  0.0095,  0.1421, -0.1296]],
       dtype=torch.float64)
	q_value: tensor([[-6.9442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.416195388441996, distance: 0.8743603472491093 entropy 0.03264415264129639
epoch: 23, step: 71
	action: tensor([[-0.0020, -0.0306, -0.0903, -0.3887, -0.5174, -0.2085, -0.7315]],
       dtype=torch.float64)
	q_value: tensor([[-5.8746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12060668480689762, distance: 1.0731199610885724 entropy 0.03264415264129639
epoch: 23, step: 72
	action: tensor([[ 0.1042, -0.1793, -0.2923,  0.3838,  0.2692,  0.2164, -0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-6.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4411947278971756, distance: 0.8554348528959742 entropy 0.03264415264129639
epoch: 23, step: 73
	action: tensor([[ 0.2629, -0.1018, -0.3697,  0.0277, -0.2719,  0.0969, -0.8269]],
       dtype=torch.float64)
	q_value: tensor([[-5.0833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4875644622884323, distance: 0.8191742904157304 entropy 0.03264415264129639
epoch: 23, step: 74
	action: tensor([[ 0.3562,  0.0318, -0.4269, -0.3086, -0.0675, -0.1891,  0.1008]],
       dtype=torch.float64)
	q_value: tensor([[-5.9446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5141175610405164, distance: 0.7976682293102509 entropy 0.03264415264129639
epoch: 23, step: 75
	action: tensor([[ 0.1976,  0.0434, -0.2962, -0.6706,  0.2004,  0.5654,  0.3100]],
       dtype=torch.float64)
	q_value: tensor([[-5.9749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4374385801272366, distance: 0.858305046206633 entropy 0.03264415264129639
epoch: 23, step: 76
	action: tensor([[ 0.2005,  0.2223, -0.0318,  0.0329, -0.1427,  0.1063, -0.3872]],
       dtype=torch.float64)
	q_value: tensor([[-6.9538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6719297813686067, distance: 0.6554507991241044 entropy 0.03264415264129639
epoch: 23, step: 77
	action: tensor([[-0.0838, -0.4039, -0.2300,  0.1816, -0.2450,  0.2521,  0.1310]],
       dtype=torch.float64)
	q_value: tensor([[-5.4312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01843882642382133, distance: 1.1337449846148198 entropy 0.03264415264129639
epoch: 23, step: 78
	action: tensor([[ 0.4210,  0.0188, -0.3788, -0.0579, -0.3887,  0.1742, -0.6302]],
       dtype=torch.float64)
	q_value: tensor([[-5.2571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6443138882619879, distance: 0.6824803429925609 entropy 0.03264415264129639
epoch: 23, step: 79
	action: tensor([[-0.1146,  0.1082, -0.2237,  0.2721,  0.0984, -0.3379, -0.2932]],
       dtype=torch.float64)
	q_value: tensor([[-6.2541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30813200672483254, distance: 0.9518495595662592 entropy 0.03264415264129639
epoch: 23, step: 80
	action: tensor([[ 0.3248, -0.0638, -0.6019, -0.3871,  0.1604,  0.1750, -0.2450]],
       dtype=torch.float64)
	q_value: tensor([[-5.2061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4682600375977126, distance: 0.8344615755736674 entropy 0.03264415264129639
epoch: 23, step: 81
	action: tensor([[ 0.2035, -0.3598, -0.4510,  0.0931,  0.0789,  0.1339,  0.3957]],
       dtype=torch.float64)
	q_value: tensor([[-5.4295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24610044136163278, distance: 0.9936042438934363 entropy 0.03264415264129639
epoch: 23, step: 82
	action: tensor([[ 0.4275,  0.2934, -0.1158,  0.1205,  0.4492, -0.1511, -0.2578]],
       dtype=torch.float64)
	q_value: tensor([[-5.6764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8139245258086585, distance: 0.49362967574589 entropy 0.03264415264129639
epoch: 23, step: 83
	action: tensor([[-0.0737,  0.3458, -0.5386,  0.0022, -0.0849,  0.1705, -0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-5.5889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45962320970818826, distance: 0.8412111818930963 entropy 0.03264415264129639
epoch: 23, step: 84
	action: tensor([[-0.1284, -0.2459, -0.1038, -0.0742, -0.2225, -0.3241, -0.0301]],
       dtype=torch.float64)
	q_value: tensor([[-5.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07763383383849387, distance: 1.1879339723063926 entropy 0.03264415264129639
epoch: 23, step: 85
	action: tensor([[ 0.5714,  0.1717, -0.3270, -0.0276, -0.2667, -0.1934,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[-5.5098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7700631923793986, distance: 0.5487328271203002 entropy 0.03264415264129639
epoch: 23, step: 86
	action: tensor([[ 0.2655, -0.0026, -0.0263, -0.1033, -0.2243,  0.3962,  0.1315]],
       dtype=torch.float64)
	q_value: tensor([[-6.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5704873558507856, distance: 0.7499713446146782 entropy 0.03264415264129639
epoch: 23, step: 87
	action: tensor([[ 0.0031, -0.0684, -0.1511,  0.1267, -0.3626,  0.5430, -0.2725]],
       dtype=torch.float64)
	q_value: tensor([[-5.9282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3837601588070817, distance: 0.898321056798651 entropy 0.03264415264129639
epoch: 23, step: 88
	action: tensor([[ 0.2318,  0.0179, -0.2673,  0.2682, -0.1435,  0.1220, -0.3535]],
       dtype=torch.float64)
	q_value: tensor([[-5.8402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6348400339776155, distance: 0.6915096933275042 entropy 0.03264415264129639
epoch: 23, step: 89
	action: tensor([[ 0.0517,  0.1523, -0.2946, -0.2623,  0.2231,  0.3814, -0.2298]],
       dtype=torch.float64)
	q_value: tensor([[-5.1556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49345004497638967, distance: 0.8144563878060925 entropy 0.03264415264129639
epoch: 23, step: 90
	action: tensor([[-0.2063, -0.0315, -0.2220, -0.0608, -0.3651,  0.0764, -0.0822]],
       dtype=torch.float64)
	q_value: tensor([[-4.9149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061158933951212635, distance: 1.1087987616353778 entropy 0.03264415264129639
epoch: 23, step: 91
	action: tensor([[-0.0605,  0.5283, -0.1860, -0.3047, -0.0886,  0.0696,  0.5093]],
       dtype=torch.float64)
	q_value: tensor([[-5.2922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4990247542174625, distance: 0.8099623407090326 entropy 0.03264415264129639
epoch: 23, step: 92
	action: tensor([[-0.0236,  0.4006, -0.1867, -0.3120, -0.1003,  0.4783, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-6.9991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5111083399792036, distance: 0.8001345202932622 entropy 0.03264415264129639
epoch: 23, step: 93
	action: tensor([[ 0.4551,  0.6221, -0.4075, -0.0399, -0.1516, -0.0968, -0.3644]],
       dtype=torch.float64)
	q_value: tensor([[-6.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9108251476902386, distance: 0.3417258973712022 entropy 0.03264415264129639
epoch: 23, step: 94
	action: tensor([[-0.0454, -0.0107,  0.3233, -0.2206, -0.4085,  0.3659, -0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-6.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21277852904846206, distance: 1.01532518425104 entropy 0.03264415264129639
epoch: 23, step: 95
	action: tensor([[ 0.2888,  0.0524, -0.3413, -0.2132, -0.5558,  0.0446, -0.0943]],
       dtype=torch.float64)
	q_value: tensor([[-6.5435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5364366747997522, distance: 0.779132334087596 entropy 0.03264415264129639
epoch: 23, step: 96
	action: tensor([[ 0.1791, -0.0308, -0.6722, -0.1449, -0.2090,  0.2431, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[-6.4806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4495909277406114, distance: 0.8489839599559854 entropy 0.03264415264129639
epoch: 23, step: 97
	action: tensor([[-0.0868, -0.1596,  0.3078, -0.1146,  0.0183,  0.0533, -0.4190]],
       dtype=torch.float64)
	q_value: tensor([[-5.6442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.044894141396319864, distance: 1.1183621176953125 entropy 0.03264415264129639
epoch: 23, step: 98
	action: tensor([[-0.2036,  0.2158, -0.4639, -0.1412, -0.1554,  0.0689, -0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-5.2211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24339846871418858, distance: 0.9953831879709735 entropy 0.03264415264129639
epoch: 23, step: 99
	action: tensor([[ 0.0922,  0.3801,  0.0580, -0.1441, -0.1641,  0.0967, -0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-5.2203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5906337350417361, distance: 0.7321713291844099 entropy 0.03264415264129639
epoch: 23, step: 100
	action: tensor([[ 0.1886,  0.0907, -0.2993, -0.2395,  0.6126,  0.3228, -0.4770]],
       dtype=torch.float64)
	q_value: tensor([[-5.8598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5645397746494223, distance: 0.7551460234353542 entropy 0.03264415264129639
epoch: 23, step: 101
	action: tensor([[ 0.4078, -0.2665, -0.3257, -0.3106,  0.0399, -0.3504,  0.1417]],
       dtype=torch.float64)
	q_value: tensor([[-4.9593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16985202490454565, distance: 1.0426401619855477 entropy 0.03264415264129639
epoch: 23, step: 102
	action: tensor([[-0.1958,  0.3725, -0.1022, -0.2235, -0.2524, -0.2719, -0.6413]],
       dtype=torch.float64)
	q_value: tensor([[-6.0822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2770889753841098, distance: 0.9729692551831387 entropy 0.03264415264129639
epoch: 23, step: 103
	action: tensor([[ 0.0375, -0.4081, -0.0119,  0.2428, -0.3247, -0.2723,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[-6.0989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11088506527713038, distance: 1.0790352833712302 entropy 0.03264415264129639
epoch: 23, step: 104
	action: tensor([[-0.1066,  0.2518, -0.3444, -0.3236,  0.5213,  0.7992, -0.3561]],
       dtype=torch.float64)
	q_value: tensor([[-5.7803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.513464853946862, distance: 0.7982038207788938 entropy 0.03264415264129639
epoch: 23, step: 105
	action: tensor([[-0.2964, -0.3208,  0.1139, -0.1676, -0.3844,  0.3636, -0.4124]],
       dtype=torch.float64)
	q_value: tensor([[-6.2474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28271613980417687, distance: 1.296050646143558 entropy 0.03264415264129639
epoch: 23, step: 106
	action: tensor([[ 0.1302,  0.3202, -0.2079, -0.0745,  0.0201, -0.3515, -0.4334]],
       dtype=torch.float64)
	q_value: tensor([[-5.9941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.565225369728567, distance: 0.754551332684764 entropy 0.03264415264129639
epoch: 23, step: 107
	action: tensor([[-0.0530,  0.0568, -0.8682, -0.1848, -0.4562,  0.5131,  0.1734]],
       dtype=torch.float64)
	q_value: tensor([[-5.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2410726427116736, distance: 0.9969119396086612 entropy 0.03264415264129639
epoch: 23, step: 108
	action: tensor([[ 0.1997,  0.4026, -0.5312,  0.0462, -0.0128,  0.2559, -0.1355]],
       dtype=torch.float64)
	q_value: tensor([[-7.2315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6897589745595131, distance: 0.6373915576467626 entropy 0.03264415264129639
epoch: 23, step: 109
	action: tensor([[ 0.2039,  0.1023, -0.2600, -0.2411, -0.3124,  0.1494,  0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-5.5652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5140735759508381, distance: 0.7977043334290491 entropy 0.03264415264129639
epoch: 23, step: 110
	action: tensor([[ 8.0778e-02, -1.7030e-04, -4.0803e-01, -2.0284e-01, -1.8791e-01,
          2.6788e-01, -1.1420e-02]], dtype=torch.float64)
	q_value: tensor([[-5.8651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3737012774027467, distance: 0.9056230261003522 entropy 0.03264415264129639
epoch: 23, step: 111
	action: tensor([[ 0.3771, -0.0649,  0.0720, -0.4770, -0.0223,  0.4640, -0.4107]],
       dtype=torch.float64)
	q_value: tensor([[-5.2987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43839941899659185, distance: 0.857571752685227 entropy 0.03264415264129639
epoch: 23, step: 112
	action: tensor([[ 0.2571,  0.2221, -0.1501, -0.1501,  0.1085, -0.4040, -0.3946]],
       dtype=torch.float64)
	q_value: tensor([[-6.0182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5732713331462223, distance: 0.7475368441053621 entropy 0.03264415264129639
epoch: 23, step: 113
	action: tensor([[-0.0546,  0.0497, -0.3736,  0.3175,  0.3081,  0.3191,  0.1386]],
       dtype=torch.float64)
	q_value: tensor([[-5.7052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42530616491950757, distance: 0.8675109442917565 entropy 0.03264415264129639
epoch: 23, step: 114
	action: tensor([[-0.0650,  0.1172, -0.3597,  0.1976, -0.2772, -0.2594, -0.3157]],
       dtype=torch.float64)
	q_value: tensor([[-5.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3824757035401807, distance: 0.8992567741159861 entropy 0.03264415264129639
epoch: 23, step: 115
	action: tensor([[-0.1943,  0.2749, -0.0856, -0.5747, -0.0398,  0.0097, -0.0434]],
       dtype=torch.float64)
	q_value: tensor([[-5.5773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11258121633838447, distance: 1.0780055624276657 entropy 0.03264415264129639
epoch: 23, step: 116
	action: tensor([[ 0.3289,  0.0116, -0.1881, -0.4318, -0.0220,  0.4417, -0.3590]],
       dtype=torch.float64)
	q_value: tensor([[-5.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49031812619593773, distance: 0.816970335894803 entropy 0.03264415264129639
epoch: 23, step: 117
	action: tensor([[ 0.3371,  0.3725, -0.1209, -0.4066, -0.3472,  0.1854, -0.3222]],
       dtype=torch.float64)
	q_value: tensor([[-5.7420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7050657284441375, distance: 0.6214687761106413 entropy 0.03264415264129639
epoch: 23, step: 118
	action: tensor([[ 0.2275, -0.2823, -0.5488,  0.0485, -0.2000, -0.0903, -0.5450]],
       dtype=torch.float64)
	q_value: tensor([[-6.7862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29499570799265495, distance: 0.9608433168357534 entropy 0.03264415264129639
epoch: 23, step: 119
	action: tensor([[ 0.1332,  0.1502, -0.2270,  0.0960, -0.2513,  0.0287, -0.3856]],
       dtype=torch.float64)
	q_value: tensor([[-5.4870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5649562276508844, distance: 0.7547848445782636 entropy 0.03264415264129639
epoch: 23, step: 120
	action: tensor([[ 0.1416,  0.3387,  0.3158, -0.3820, -0.0779,  0.7343,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-5.3124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6397814771771497, distance: 0.6868149085669204 entropy 0.03264415264129639
epoch: 23, step: 121
	action: tensor([[-0.1176, -0.7575, -0.1724,  0.2722, -0.2790,  0.4123, -0.2982]],
       dtype=torch.float64)
	q_value: tensor([[-7.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21784198913819708, distance: 1.2628511531077597 entropy 0.03264415264129639
epoch: 23, step: 122
	action: tensor([[-0.0380, -0.1231, -0.4065, -0.1987, -0.1188,  0.0662, -0.5094]],
       dtype=torch.float64)
	q_value: tensor([[-5.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.147549943108216, distance: 1.0565527027342798 entropy 0.03264415264129639
epoch: 23, step: 123
	action: tensor([[ 0.1680,  0.2811, -0.0658,  0.0566, -0.3726, -0.3765,  0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-5.0092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.639726698084209, distance: 0.6868671291845193 entropy 0.03264415264129639
epoch: 23, step: 124
	action: tensor([[ 0.1509,  0.2609, -0.1685,  0.0539, -0.3341,  0.1378, -0.3161]],
       dtype=torch.float64)
	q_value: tensor([[-6.4351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6410951401468379, distance: 0.6855614091699062 entropy 0.03264415264129639
epoch: 23, step: 125
	action: tensor([[ 0.7948, -0.1489, -0.4873, -0.3579,  0.1507,  0.1065, -0.1606]],
       dtype=torch.float64)
	q_value: tensor([[-5.6861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47520980191425854, distance: 0.8289904942331466 entropy 0.03264415264129639
epoch: 23, step: 126
	action: tensor([[ 0.3149,  0.6153,  0.2568,  0.1854, -0.1645, -0.0480, -0.5347]],
       dtype=torch.float64)
	q_value: tensor([[-6.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8707001356410291, distance: 0.4114866292491775 entropy 0.03264415264129639
epoch: 23, step: 127
	action: tensor([[ 0.2061, -0.0723, -0.4465, -0.2192, -0.3489,  0.0343, -0.3462]],
       dtype=torch.float64)
	q_value: tensor([[-7.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39365016712491585, distance: 0.8910833407115738 entropy 0.03264415264129639
LOSS epoch 23 actor 20.151021832779893 critic 50.983004757721716 
epoch: 24, step: 0
	action: tensor([[-0.4456,  0.1348,  0.3178, -0.0021, -0.4499, -0.2253, -0.6657]],
       dtype=torch.float64)
	q_value: tensor([[-6.4376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13248005469122948, distance: 1.217788796365023 entropy 0.03264415264129639
epoch: 24, step: 1
	action: tensor([[ 0.0166,  0.4111,  0.0259, -0.3214, -0.4158, -0.3799, -0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-7.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4545579022611572, distance: 0.8451445989028433 entropy 0.03264415264129639
epoch: 24, step: 2
	action: tensor([[ 0.1921,  0.3317, -0.2863, -0.2484,  0.4512,  0.1325, -0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-7.6856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6551764022973129, distance: 0.6719781977281439 entropy 0.03264415264129639
epoch: 24, step: 3
	action: tensor([[ 0.1401,  0.1243,  0.1451, -0.0200, -0.0986,  0.0277, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-5.9177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.517371758688882, distance: 0.7949925501995142 entropy 0.03264415264129639
epoch: 24, step: 4
	action: tensor([[ 0.5159,  0.0039, -0.1731,  0.0293,  0.2086, -0.1804, -0.2293]],
       dtype=torch.float64)
	q_value: tensor([[-5.9231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6390881662428252, distance: 0.6874755455864877 entropy 0.03264415264129639
epoch: 24, step: 5
	action: tensor([[ 0.0265,  0.1265,  0.3418, -0.0296, -0.1652,  0.0605, -0.2455]],
       dtype=torch.float64)
	q_value: tensor([[-6.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43032252440723706, distance: 0.8637165023719223 entropy 0.03264415264129639
epoch: 24, step: 6
	action: tensor([[-0.0006, -0.1280, -0.3559, -0.4678, -0.2470,  0.0161, -0.4507]],
       dtype=torch.float64)
	q_value: tensor([[-6.3738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09141680528806073, distance: 1.0907847138682454 entropy 0.03264415264129639
epoch: 24, step: 7
	action: tensor([[ 0.2706, -0.1420,  0.2476,  0.2952,  0.2967,  0.1950, -0.2452]],
       dtype=torch.float64)
	q_value: tensor([[-6.3986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6502935421279045, distance: 0.6767192325385103 entropy 0.03264415264129639
epoch: 24, step: 8
	action: tensor([[ 0.1724,  0.2084, -0.1842, -0.2677, -0.5004,  0.2328, -0.3352]],
       dtype=torch.float64)
	q_value: tensor([[-5.9861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5426353586562043, distance: 0.7739055951198045 entropy 0.03264415264129639
epoch: 24, step: 9
	action: tensor([[-0.0109, -0.0212,  0.0863, -0.4436,  0.0387, -0.1006, -0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-7.5379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031815029971386855, distance: 1.1259934429734129 entropy 0.03264415264129639
epoch: 24, step: 10
	action: tensor([[-0.0820, -0.3923, -0.1569, -0.0414,  0.1344,  0.4668,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[-5.7572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03668223219496303, distance: 1.1231596131475958 entropy 0.03264415264129639
epoch: 24, step: 11
	action: tensor([[ 0.3511,  0.5692,  0.1373, -0.3662, -0.4880,  0.1991, -0.1277]],
       dtype=torch.float64)
	q_value: tensor([[-5.6036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7797156626258779, distance: 0.5370917764371419 entropy 0.03264415264129639
epoch: 24, step: 12
	action: tensor([[-0.0664,  0.3209, -0.4583,  0.0215,  0.3153, -0.2601, -0.1625]],
       dtype=torch.float64)
	q_value: tensor([[-8.9311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4332757233357052, distance: 0.8614748471489512 entropy 0.03264415264129639
epoch: 24, step: 13
	action: tensor([[ 0.1621, -0.0947, -0.6709,  0.1965,  0.2456, -0.4694,  0.0565]],
       dtype=torch.float64)
	q_value: tensor([[-6.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3706898179183855, distance: 0.907797688466615 entropy 0.03264415264129639
epoch: 24, step: 14
	action: tensor([[-0.3462, -0.1458,  0.2335, -0.1050, -0.1137,  0.2132, -0.4335]],
       dtype=torch.float64)
	q_value: tensor([[-6.9686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1803340261295785, distance: 1.2432519719345565 entropy 0.03264415264129639
epoch: 24, step: 15
	action: tensor([[ 0.2405, -0.3365,  0.1446, -0.1838, -0.1390,  0.1608, -0.1123]],
       dtype=torch.float64)
	q_value: tensor([[-6.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13563981077041898, distance: 1.063907991827821 entropy 0.03264415264129639
epoch: 24, step: 16
	action: tensor([[ 0.4402,  0.1258, -0.4537,  0.1459,  0.0692,  0.2116,  0.1766]],
       dtype=torch.float64)
	q_value: tensor([[-5.9648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7633470301962776, distance: 0.5566890420985284 entropy 0.03264415264129639
epoch: 24, step: 17
	action: tensor([[ 0.3375,  0.4066,  0.0614, -0.3057, -0.0011, -0.1150, -0.2731]],
       dtype=torch.float64)
	q_value: tensor([[-6.2875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6899748893006068, distance: 0.6371697201478201 entropy 0.03264415264129639
epoch: 24, step: 18
	action: tensor([[-0.0524,  0.2927, -0.0767, -0.6027,  0.0259, -0.0627, -0.1551]],
       dtype=torch.float64)
	q_value: tensor([[-6.7473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2482752269830628, distance: 0.9921700761530324 entropy 0.03264415264129639
epoch: 24, step: 19
	action: tensor([[-0.2304, -0.1689, -0.2468,  0.3006,  0.2346,  0.0920, -0.1798]],
       dtype=torch.float64)
	q_value: tensor([[-6.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07162549102076354, distance: 1.1026007838352487 entropy 0.03264415264129639
epoch: 24, step: 20
	action: tensor([[-0.0215,  0.0055, -0.1184,  0.2430, -0.2326,  0.3573, -0.3688]],
       dtype=torch.float64)
	q_value: tensor([[-5.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43333151046375573, distance: 0.8614324452329236 entropy 0.03264415264129639
epoch: 24, step: 21
	action: tensor([[ 0.1252,  0.0034, -0.1826, -0.1626, -0.3266,  0.1262,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[-5.8709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39382489959759903, distance: 0.8909549392461095 entropy 0.03264415264129639
epoch: 24, step: 22
	action: tensor([[ 0.3740, -0.2191, -0.3277,  0.0979, -0.3836,  0.0903, -0.3983]],
       dtype=torch.float64)
	q_value: tensor([[-6.2385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4975419644437129, distance: 0.8111601209526752 entropy 0.03264415264129639
epoch: 24, step: 23
	action: tensor([[ 0.1964, -0.3221, -0.4714, -0.2511,  0.1138,  0.2276, -0.3182]],
       dtype=torch.float64)
	q_value: tensor([[-6.3643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19372017485509097, distance: 1.0275420174569005 entropy 0.03264415264129639
epoch: 24, step: 24
	action: tensor([[ 0.0023,  0.0788, -0.1671, -0.3861, -0.0572,  0.2448, -0.2901]],
       dtype=torch.float64)
	q_value: tensor([[-5.5327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2903711350246747, distance: 0.963989557901584 entropy 0.03264415264129639
epoch: 24, step: 25
	action: tensor([[-0.2928, -0.2448, -0.6085,  0.1964, -0.2966,  0.0682, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-5.9081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18791978918364993, distance: 1.2472406354688221 entropy 0.03264415264129639
epoch: 24, step: 26
	action: tensor([[-0.2480, -0.0181, -0.2746, -0.1619,  0.4865, -0.0576, -0.0731]],
       dtype=torch.float64)
	q_value: tensor([[-6.2411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03263696814653083, distance: 1.1628682888812905 entropy 0.03264415264129639
epoch: 24, step: 27
	action: tensor([[ 0.4764,  0.1791, -0.2820, -0.3192,  0.3925,  0.0017, -0.1127]],
       dtype=torch.float64)
	q_value: tensor([[-5.5148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6631607514840168, distance: 0.6641528462339968 entropy 0.03264415264129639
epoch: 24, step: 28
	action: tensor([[ 0.2192, -0.4166, -0.0941, -0.3094,  0.0400,  0.3169, -0.2182]],
       dtype=torch.float64)
	q_value: tensor([[-6.3074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05006882588139239, distance: 1.1153284063233961 entropy 0.03264415264129639
epoch: 24, step: 29
	action: tensor([[-0.0909, -0.3379, -0.2419,  0.0659,  0.0075,  0.0903, -0.4791]],
       dtype=torch.float64)
	q_value: tensor([[-5.7518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0020466690553310185, distance: 1.143172607212985 entropy 0.03264415264129639
epoch: 24, step: 30
	action: tensor([[ 0.2964,  0.0270, -0.2640, -0.1419, -0.2653,  0.6104, -0.8573]],
       dtype=torch.float64)
	q_value: tensor([[-5.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5926775356970064, distance: 0.7303413241334828 entropy 0.03264415264129639
epoch: 24, step: 31
	action: tensor([[ 0.1779,  0.1874,  0.1188,  0.0276, -0.2456,  0.2111, -0.5451]],
       dtype=torch.float64)
	q_value: tensor([[-7.9932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6479337180455993, distance: 0.6789986490562638 entropy 0.03264415264129639
epoch: 24, step: 32
	action: tensor([[-0.1771,  0.1623, -0.3471, -0.5599,  0.0730,  0.0950, -0.4290]],
       dtype=torch.float64)
	q_value: tensor([[-6.6910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16301512914160288, distance: 1.0469248229511054 entropy 0.03264415264129639
epoch: 24, step: 33
	action: tensor([[ 0.5331,  0.2347, -0.0906, -0.1667, -0.1681,  0.3311, -0.2868]],
       dtype=torch.float64)
	q_value: tensor([[-6.2383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8093607649675096, distance: 0.49964648511388626 entropy 0.03264415264129639
epoch: 24, step: 34
	action: tensor([[ 0.2770, -0.0109, -0.2237, -0.0865,  0.0724,  0.0643,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-7.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5174091479571433, distance: 0.7949617555185494 entropy 0.03264415264129639
epoch: 24, step: 35
	action: tensor([[ 0.0859,  0.4177, -0.0297,  0.1531, -0.2184, -0.3875, -0.3065]],
       dtype=torch.float64)
	q_value: tensor([[-5.6046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6627842158786086, distance: 0.6645239540354271 entropy 0.03264415264129639
epoch: 24, step: 36
	action: tensor([[ 0.4595, -0.5699, -0.5280,  0.3228, -0.3304, -0.0051, -0.3422]],
       dtype=torch.float64)
	q_value: tensor([[-6.8361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2847383155734837, distance: 0.9678079244954088 entropy 0.03264415264129639
epoch: 24, step: 37
	action: tensor([[ 0.0786,  0.2105,  0.0116, -0.2115, -0.2077,  0.0890,  0.0514]],
       dtype=torch.float64)
	q_value: tensor([[-6.8439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46537038105411255, distance: 0.8367258780775655 entropy 0.03264415264129639
epoch: 24, step: 38
	action: tensor([[-0.0775,  0.2148, -0.4318, -0.3054,  0.1821,  0.1353,  0.3759]],
       dtype=torch.float64)
	q_value: tensor([[-6.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3652668769670101, distance: 0.9117006708747556 entropy 0.03264415264129639
epoch: 24, step: 39
	action: tensor([[ 0.0696,  0.1929, -0.2497, -0.1060, -0.0567, -0.2877, -0.2813]],
       dtype=torch.float64)
	q_value: tensor([[-6.8853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46113179622806977, distance: 0.8400361436465607 entropy 0.03264415264129639
epoch: 24, step: 40
	action: tensor([[-0.1302,  0.3289, -0.3909, -0.0763,  0.1034,  0.1055, -0.2873]],
       dtype=torch.float64)
	q_value: tensor([[-5.9937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4035670783822427, distance: 0.8837664222297419 entropy 0.03264415264129639
epoch: 24, step: 41
	action: tensor([[ 0.0835,  0.2602, -0.2526, -0.1360, -0.2199, -0.3690, -0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-5.5482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5293077167880971, distance: 0.7851004610903204 entropy 0.03264415264129639
epoch: 24, step: 42
	action: tensor([[ 0.4429,  0.1848, -0.2366, -0.1663,  0.2329,  0.3217, -0.0645]],
       dtype=torch.float64)
	q_value: tensor([[-6.5401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7767829591672815, distance: 0.5406551777340846 entropy 0.03264415264129639
epoch: 24, step: 43
	action: tensor([[-0.1159,  0.2562, -0.2436,  0.1577, -0.2400, -0.1610,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-6.0724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4063145726105739, distance: 0.8817285183412479 entropy 0.03264415264129639
epoch: 24, step: 44
	action: tensor([[-0.0372,  0.1425, -0.5189,  0.2477, -0.0341, -0.0783, -0.4963]],
       dtype=torch.float64)
	q_value: tensor([[-6.1027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3965910335709517, distance: 0.8889197857924691 entropy 0.03264415264129639
epoch: 24, step: 45
	action: tensor([[ 0.1947, -0.1949, -0.1169,  0.1351,  0.2653, -0.0839,  0.0654]],
       dtype=torch.float64)
	q_value: tensor([[-5.9894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3661606382984015, distance: 0.9110585664839717 entropy 0.03264415264129639
epoch: 24, step: 46
	action: tensor([[ 0.7943, -0.1031, -0.3285, -0.2491,  0.0179,  0.4751,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-5.6172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6556161683974574, distance: 0.6715495618789828 entropy 0.03264415264129639
epoch: 24, step: 47
	action: tensor([[ 0.4900,  0.1642, -0.4101, -0.0993,  0.1419, -0.0388, -0.2512]],
       dtype=torch.float64)
	q_value: tensor([[-7.6607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7464078496890456, distance: 0.5762681226124562 entropy 0.03264415264129639
epoch: 24, step: 48
	action: tensor([[ 0.3218,  0.2573, -0.4270, -0.0446, -0.3030, -0.2914, -0.1679]],
       dtype=torch.float64)
	q_value: tensor([[-6.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7442360529265238, distance: 0.5787304801261356 entropy 0.03264415264129639
epoch: 24, step: 49
	action: tensor([[ 0.0495, -0.0348, -0.2449, -0.0749, -0.2706,  0.1754,  0.3321]],
       dtype=torch.float64)
	q_value: tensor([[-7.0034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3327196574614495, distance: 0.9347831143733496 entropy 0.03264415264129639
epoch: 24, step: 50
	action: tensor([[ 0.3169,  0.1058, -0.2000, -0.4948,  0.0511,  0.1298, -0.1587]],
       dtype=torch.float64)
	q_value: tensor([[-6.3697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4814312768714899, distance: 0.8240619334598688 entropy 0.03264415264129639
epoch: 24, step: 51
	action: tensor([[-0.1479,  0.1672,  0.0014,  0.1945,  0.4215,  0.0157,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[-6.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3560050993235423, distance: 0.9183281718783627 entropy 0.03264415264129639
epoch: 24, step: 52
	action: tensor([[ 0.1634, -0.1668, -0.0267, -0.1790, -0.0907,  0.2978, -0.3160]],
       dtype=torch.float64)
	q_value: tensor([[-5.5997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2923942485054287, distance: 0.9626144360788864 entropy 0.03264415264129639
epoch: 24, step: 53
	action: tensor([[ 0.4429, -0.1109,  0.1108, -0.0185, -0.1739,  0.1221, -0.3561]],
       dtype=torch.float64)
	q_value: tensor([[-5.6704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5638864388139997, distance: 0.7557122967886613 entropy 0.03264415264129639
epoch: 24, step: 54
	action: tensor([[ 0.4151,  0.2665, -0.3360, -0.2030, -0.2351,  0.1399, -0.6101]],
       dtype=torch.float64)
	q_value: tensor([[-6.4042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7557365577252584, distance: 0.5655694335861533 entropy 0.03264415264129639
epoch: 24, step: 55
	action: tensor([[-0.2645, -0.2016, -0.3779, -0.2732, -0.0532, -0.0360, -0.0398]],
       dtype=torch.float64)
	q_value: tensor([[-7.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1786973171226276, distance: 1.242389695855236 entropy 0.03264415264129639
epoch: 24, step: 56
	action: tensor([[ 0.1088, -0.1551, -0.5034, -0.2520, -0.0097,  0.0668, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-5.8350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23450915533760908, distance: 1.0012134927554908 entropy 0.03264415264129639
epoch: 24, step: 57
	action: tensor([[ 0.6618, -0.1244, -0.2272, -0.2148,  0.2857,  0.1242, -0.6251]],
       dtype=torch.float64)
	q_value: tensor([[-5.7577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5352075382169734, distance: 0.7801645836581884 entropy 0.03264415264129639
epoch: 24, step: 58
	action: tensor([[ 0.2392,  0.1789, -0.0225, -0.2213, -0.0298,  0.0363, -0.3786]],
       dtype=torch.float64)
	q_value: tensor([[-6.5539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.572024312671148, distance: 0.7486283031323786 entropy 0.03264415264129639
epoch: 24, step: 59
	action: tensor([[ 0.2800, -0.5497, -0.1171, -0.7607, -0.1508, -0.1825,  0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-5.9595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3846286815500235, distance: 1.3465526998286772 entropy 0.03264415264129639
epoch: 24, step: 60
	action: tensor([[ 0.2621, -0.2239,  0.1867, -0.0344,  0.2313,  0.2766,  0.1351]],
       dtype=torch.float64)
	q_value: tensor([[-7.4564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41720263748028097, distance: 0.873605746479477 entropy 0.03264415264129639
epoch: 24, step: 61
	action: tensor([[ 0.3162,  0.0241, -0.3160,  0.0560, -0.1228,  0.0338, -0.6653]],
       dtype=torch.float64)
	q_value: tensor([[-5.8990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6272038924647737, distance: 0.6987026297365327 entropy 0.03264415264129639
epoch: 24, step: 62
	action: tensor([[-0.0465,  0.2524, -0.3766, -0.2428,  0.4601, -0.2761,  0.0325]],
       dtype=torch.float64)
	q_value: tensor([[-6.2493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3473466211789078, distance: 0.9244809972207528 entropy 0.03264415264129639
epoch: 24, step: 63
	action: tensor([[-0.0833,  0.2296, -0.1669, -0.3957,  0.1981,  0.4445, -0.2384]],
       dtype=torch.float64)
	q_value: tensor([[-6.0587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3799354327016432, distance: 0.9011044838657815 entropy 0.03264415264129639
epoch: 24, step: 64
	action: tensor([[ 0.0594,  0.2818, -0.0469,  0.0268, -0.2638, -0.6320, -0.4551]],
       dtype=torch.float64)
	q_value: tensor([[-6.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5327386138705044, distance: 0.7822339117659708 entropy 0.03264415264129639
epoch: 24, step: 65
	action: tensor([[ 0.0977,  0.0466,  0.1763, -0.4890,  0.6139,  0.1018, -0.1010]],
       dtype=torch.float64)
	q_value: tensor([[-7.3194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19008409534650272, distance: 1.0298563638641798 entropy 0.03264415264129639
epoch: 24, step: 66
	action: tensor([[ 0.0232, -0.2102, -0.4491,  0.1820, -0.0473, -0.0796, -0.2776]],
       dtype=torch.float64)
	q_value: tensor([[-5.5981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20873287346327818, distance: 1.0179307991878264 entropy 0.03264415264129639
epoch: 24, step: 67
	action: tensor([[ 0.0446,  0.0475, -0.2110, -0.0284,  0.2244, -0.0680, -0.3576]],
       dtype=torch.float64)
	q_value: tensor([[-5.4836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3764004438592947, distance: 0.9036694322398178 entropy 0.03264415264129639
epoch: 24, step: 68
	action: tensor([[ 0.5796,  0.2977, -0.2070,  0.3360,  0.2338,  0.5410,  0.1291]],
       dtype=torch.float64)
	q_value: tensor([[-5.1498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9457778766205894, distance: 0.2664679333716767 entropy 0.03264415264129639
epoch: 24, step: 69
	action: tensor([[ 0.1619, -0.1284,  0.3017,  0.2048, -0.2393, -0.0520, -0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-7.2231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46002772005042203, distance: 0.8408962698412579 entropy 0.03264415264129639
epoch: 24, step: 70
	action: tensor([[ 0.1550,  0.1204, -0.3028, -0.0984, -0.0271, -0.0491, -0.3666]],
       dtype=torch.float64)
	q_value: tensor([[-6.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5004858569176016, distance: 0.8087803438809466 entropy 0.03264415264129639
epoch: 24, step: 71
	action: tensor([[-0.0966, -0.0129,  0.5369, -0.1097,  0.3824,  0.3776, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-5.6147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25879909139485124, distance: 0.9852005934816122 entropy 0.03264415264129639
epoch: 24, step: 72
	action: tensor([[ 0.3796, -0.0727,  0.2315,  0.2017, -0.2904, -0.2272, -0.1104]],
       dtype=torch.float64)
	q_value: tensor([[-6.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5959298928525736, distance: 0.7274196936623976 entropy 0.03264415264129639
epoch: 24, step: 73
	action: tensor([[ 0.0579, -0.1519,  0.3340, -0.2329, -0.6247, -0.4428,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-6.7711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.037728560144034806, distance: 1.1225494756506693 entropy 0.03264415264129639
epoch: 24, step: 74
	action: tensor([[ 4.0280e-04, -9.7043e-02, -1.7676e-01, -2.0161e-01, -3.3468e-01,
         -4.2274e-01, -4.8060e-01]], dtype=torch.float64)
	q_value: tensor([[-7.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13873971125903817, distance: 1.0619985036220487 entropy 0.03264415264129639
epoch: 24, step: 75
	action: tensor([[-0.1361,  0.5450,  0.0053, -0.2866, -0.0538,  0.0322, -0.3391]],
       dtype=torch.float64)
	q_value: tensor([[-6.6596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4034094884338154, distance: 0.8838831692279242 entropy 0.03264415264129639
epoch: 24, step: 76
	action: tensor([[ 0.1765, -0.3245,  0.2045, -0.4554,  0.0959,  0.0631, -0.4812]],
       dtype=torch.float64)
	q_value: tensor([[-6.6824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0828671389174136, distance: 1.1908149562231969 entropy 0.03264415264129639
epoch: 24, step: 77
	action: tensor([[ 0.5808,  0.2116, -0.0300, -0.0130,  0.1383,  0.2052, -0.8643]],
       dtype=torch.float64)
	q_value: tensor([[-5.9721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8527466450641524, distance: 0.43912614217966406 entropy 0.03264415264129639
epoch: 24, step: 78
	action: tensor([[ 0.2550, -0.0552, -0.0897, -0.1754, -0.0031, -0.4832, -0.5049]],
       dtype=torch.float64)
	q_value: tensor([[-7.4340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31816527313431553, distance: 0.9449226334339957 entropy 0.03264415264129639
epoch: 24, step: 79
	action: tensor([[-0.0364,  0.3382, -0.1552, -0.0949, -0.2178,  0.5269, -0.4749]],
       dtype=torch.float64)
	q_value: tensor([[-6.6075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47790259740010876, distance: 0.8268609071832876 entropy 0.03264415264129639
epoch: 24, step: 80
	action: tensor([[-0.4381,  0.3996,  0.0388, -0.1934, -0.2822,  0.2123, -0.5835]],
       dtype=torch.float64)
	q_value: tensor([[-7.1729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03642794994654874, distance: 1.1233078408231947 entropy 0.03264415264129639
epoch: 24, step: 81
	action: tensor([[-0.1287,  0.1371, -0.1148, -0.1029,  0.1789, -0.0529, -0.3835]],
       dtype=torch.float64)
	q_value: tensor([[-7.3865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23678170974087576, distance: 0.9997262095504822 entropy 0.03264415264129639
epoch: 24, step: 82
	action: tensor([[-0.2633, -0.2238, -0.0237, -0.1686, -0.1880,  0.0254,  0.1986]],
       dtype=torch.float64)
	q_value: tensor([[-5.2744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22344566967880497, distance: 1.2657532096670066 entropy 0.03264415264129639
epoch: 24, step: 83
	action: tensor([[-0.2935,  0.0611, -0.2741,  0.1212,  0.1253, -0.0079, -0.2207]],
       dtype=torch.float64)
	q_value: tensor([[-6.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07335760142334291, distance: 1.1015717174867838 entropy 0.03264415264129639
epoch: 24, step: 84
	action: tensor([[ 0.0814,  0.1918, -0.3386, -0.2012,  0.5264,  0.1862,  0.4644]],
       dtype=torch.float64)
	q_value: tensor([[-5.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.506556119322549, distance: 0.8038510382705694 entropy 0.03264415264129639
epoch: 24, step: 85
	action: tensor([[-0.1843,  0.2446,  0.0119, -0.0738, -0.1265,  0.4043,  0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-6.8788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31819768551636674, distance: 0.9449001737702482 entropy 0.03264415264129639
epoch: 24, step: 86
	action: tensor([[ 0.2369, -0.1098, -0.4195, -0.2788, -0.0921, -0.2570, -0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-6.4902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2852465361947898, distance: 0.9674640312338548 entropy 0.03264415264129639
epoch: 24, step: 87
	action: tensor([[-0.1745,  0.3486, -0.1940, -0.1140, -0.2660,  0.0039,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[-6.2628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3248185382830512, distance: 0.9403011083181771 entropy 0.03264415264129639
epoch: 24, step: 88
	action: tensor([[ 0.2750, -0.4166, -0.2210,  0.1604,  0.2776, -0.0459,  0.2970]],
       dtype=torch.float64)
	q_value: tensor([[-6.4177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2301276585461104, distance: 1.00407476428779 entropy 0.03264415264129639
epoch: 24, step: 89
	action: tensor([[ 0.1724, -0.1335,  0.0895, -0.5747, -0.1208, -0.4272, -0.2916]],
       dtype=torch.float64)
	q_value: tensor([[-6.1218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06713446687108426, distance: 1.1821327979243328 entropy 0.03264415264129639
epoch: 24, step: 90
	action: tensor([[-0.1723, -0.0744, -0.3456,  0.3066, -0.4196, -0.2039, -0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-6.8737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11386477115572846, distance: 1.077225671379699 entropy 0.03264415264129639
epoch: 24, step: 91
	action: tensor([[-0.3397, -0.0394, -0.2028,  0.0938, -0.0581,  0.5972, -0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-6.3671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020394901091413176, distance: 1.1326147464442173 entropy 0.03264415264129639
epoch: 24, step: 92
	action: tensor([[ 0.0032, -0.2084, -0.8197, -0.4303,  0.1999, -0.2939, -0.0752]],
       dtype=torch.float64)
	q_value: tensor([[-6.1943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15265602992732918, distance: 1.053383629825957 entropy 0.03264415264129639
epoch: 24, step: 93
	action: tensor([[ 0.7307,  0.0172, -0.2714, -0.4130, -0.7908, -0.1779,  0.0439]],
       dtype=torch.float64)
	q_value: tensor([[-6.8998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5316566887254894, distance: 0.7831390038896348 entropy 0.03264415264129639
epoch: 24, step: 94
	action: tensor([[ 0.0547, -0.0885,  0.3918, -0.6919, -0.3064,  0.4163, -0.0795]],
       dtype=torch.float64)
	q_value: tensor([[-9.4971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03293996185347603, distance: 1.1630388792932116 entropy 0.03264415264129639
epoch: 24, step: 95
	action: tensor([[ 0.3052,  0.1155, -0.2320,  0.0688, -0.2908, -0.2143, -0.1407]],
       dtype=torch.float64)
	q_value: tensor([[-7.9696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6244020736406297, distance: 0.7013233299661829 entropy 0.03264415264129639
epoch: 24, step: 96
	action: tensor([[ 0.4011, -0.3228, -0.1521, -0.4551,  0.3064,  0.1247, -0.5461]],
       dtype=torch.float64)
	q_value: tensor([[-6.4132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12462807682141974, distance: 1.0706635057050127 entropy 0.03264415264129639
epoch: 24, step: 97
	action: tensor([[-0.0021,  0.2448, -0.0312, -0.1628,  0.0492, -0.5080,  0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-5.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3356047732895008, distance: 0.932760067303862 entropy 0.03264415264129639
epoch: 24, step: 98
	action: tensor([[ 0.0434,  0.0784, -0.3977,  0.1198,  0.1261, -0.0283,  0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-6.5137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4368696095046336, distance: 0.8587389783277843 entropy 0.03264415264129639
epoch: 24, step: 99
	action: tensor([[ 0.3310, -0.4082, -0.0939, -0.4431,  0.0722,  0.4482, -0.4042]],
       dtype=torch.float64)
	q_value: tensor([[-5.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10848214443446458, distance: 1.0804923991981277 entropy 0.03264415264129639
epoch: 24, step: 100
	action: tensor([[ 0.3300,  0.0416, -0.1709,  0.0455,  0.1040, -0.3865, -0.1560]],
       dtype=torch.float64)
	q_value: tensor([[-6.2827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5640814501331004, distance: 0.7555433167989032 entropy 0.03264415264129639
epoch: 24, step: 101
	action: tensor([[ 0.2413,  0.0636, -0.4114,  0.6450, -0.0869,  0.0230, -0.3793]],
       dtype=torch.float64)
	q_value: tensor([[-6.1692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.670221658770234, distance: 0.6571549111281985 entropy 0.03264415264129639
epoch: 24, step: 102
	action: tensor([[ 0.3200,  0.1043, -0.0739, -0.2475, -0.2422,  0.0458, -0.1136]],
       dtype=torch.float64)
	q_value: tensor([[-6.5421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5658924441554868, distance: 0.7539722566758233 entropy 0.03264415264129639
epoch: 24, step: 103
	action: tensor([[ 0.2888,  0.2824,  0.0638,  0.5453,  0.1849, -0.0041, -0.1709]],
       dtype=torch.float64)
	q_value: tensor([[-6.3814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8553795371415989, distance: 0.4351826443942311 entropy 0.03264415264129639
epoch: 24, step: 104
	action: tensor([[-0.1808,  0.1688, -0.1384,  0.2422, -0.2155, -0.0323, -0.1970]],
       dtype=torch.float64)
	q_value: tensor([[-6.5614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32511313426407595, distance: 0.9400959492861375 entropy 0.03264415264129639
epoch: 24, step: 105
	action: tensor([[ 0.5362,  0.1989,  0.2017,  0.0314, -0.4577, -0.0350, -0.3444]],
       dtype=torch.float64)
	q_value: tensor([[-5.8671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8177526447351658, distance: 0.48852558176822103 entropy 0.03264415264129639
epoch: 24, step: 106
	action: tensor([[-0.3300,  0.0668, -0.4060, -0.3978, -0.3738, -0.0047,  0.0590]],
       dtype=torch.float64)
	q_value: tensor([[-7.8139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04506027604210239, distance: 1.1698424143010193 entropy 0.03264415264129639
epoch: 24, step: 107
	action: tensor([[ 0.0454,  0.1740, -0.3693,  0.1092, -0.1248,  0.2016, -0.3002]],
       dtype=torch.float64)
	q_value: tensor([[-7.0987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48150666564304645, distance: 0.824002030816155 entropy 0.03264415264129639
epoch: 24, step: 108
	action: tensor([[ 0.6663, -0.1189, -0.1539,  0.0392, -0.0483,  0.1688, -0.0907]],
       dtype=torch.float64)
	q_value: tensor([[-5.6054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.683173986558375, distance: 0.6441204825830109 entropy 0.03264415264129639
epoch: 24, step: 109
	action: tensor([[ 0.0361,  0.3394, -0.0776,  0.3601,  0.3097, -0.2945, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-6.5124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5945021816852962, distance: 0.7287036657246307 entropy 0.03264415264129639
epoch: 24, step: 110
	action: tensor([[ 0.5191,  0.0105, -0.0114, -0.0566, -0.0211,  0.2155, -0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-6.1750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6967023137311488, distance: 0.6302186367122061 entropy 0.03264415264129639
epoch: 24, step: 111
	action: tensor([[-0.0279, -0.2900,  0.1138, -0.4347, -0.3803, -0.0715, -0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-6.1866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21120529997373438, distance: 1.2594054679409636 entropy 0.03264415264129639
epoch: 24, step: 112
	action: tensor([[ 0.2188,  0.3448,  0.1850, -0.2956, -0.2616,  0.1205, -0.0508]],
       dtype=torch.float64)
	q_value: tensor([[-6.6850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5891417631642213, distance: 0.7335043475807922 entropy 0.03264415264129639
epoch: 24, step: 113
	action: tensor([[ 0.3721, -0.3776, -0.1214, -0.2223,  0.3420,  0.3682,  0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-7.1998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26198154396192164, distance: 0.9830832681455589 entropy 0.03264415264129639
epoch: 24, step: 114
	action: tensor([[ 0.4108,  0.0203, -0.6081, -0.0661, -0.0781, -0.1671, -0.5209]],
       dtype=torch.float64)
	q_value: tensor([[-6.0986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5921161523116215, distance: 0.7308444394159589 entropy 0.03264415264129639
epoch: 24, step: 115
	action: tensor([[ 0.6499,  0.2720, -0.1335, -0.2443,  0.2056,  0.0508, -0.2750]],
       dtype=torch.float64)
	q_value: tensor([[-6.6003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8128039384678507, distance: 0.49511381776113345 entropy 0.03264415264129639
epoch: 24, step: 116
	action: tensor([[-0.3386, -0.1436, -0.0173,  0.2529, -0.3256,  0.3258, -0.5063]],
       dtype=torch.float64)
	q_value: tensor([[-6.7720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017975616190502253, distance: 1.1340124668411131 entropy 0.03264415264129639
epoch: 24, step: 117
	action: tensor([[-0.1657, -0.0291, -0.1324, -0.0589,  0.1693, -0.1622, -0.2442]],
       dtype=torch.float64)
	q_value: tensor([[-6.3502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.062352053218461934, distance: 1.1080939832709387 entropy 0.03264415264129639
epoch: 24, step: 118
	action: tensor([[ 0.3507,  0.2639,  0.3024,  0.0959,  0.2879, -0.0203, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-5.2548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7820088704929424, distance: 0.534288841452935 entropy 0.03264415264129639
epoch: 24, step: 119
	action: tensor([[-0.0536,  0.3348, -0.0159, -0.3926, -0.1344,  0.2751,  0.4695]],
       dtype=torch.float64)
	q_value: tensor([[-6.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3589220962516577, distance: 0.9162460119674307 entropy 0.03264415264129639
epoch: 24, step: 120
	action: tensor([[ 0.1654, -0.1128, -0.0168, -0.0872, -0.3107,  0.0813,  0.0544]],
       dtype=torch.float64)
	q_value: tensor([[-7.6302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052546838600507, distance: 0.9538267697735591 entropy 0.03264415264129639
epoch: 24, step: 121
	action: tensor([[ 0.1371, -0.2732, -0.3693, -0.0532, -0.0203,  0.9232,  0.3734]],
       dtype=torch.float64)
	q_value: tensor([[-6.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38087511994577017, distance: 0.9004214280318432 entropy 0.03264415264129639
epoch: 24, step: 122
	action: tensor([[-0.0152,  0.4060, -0.2086,  0.0433, -0.2935,  0.0931, -0.3545]],
       dtype=torch.float64)
	q_value: tensor([[-7.8868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5350508265349121, distance: 0.7802960946124946 entropy 0.03264415264129639
epoch: 24, step: 123
	action: tensor([[-0.1013,  0.0952,  0.0996, -0.0532, -0.2364,  0.2655,  0.1978]],
       dtype=torch.float64)
	q_value: tensor([[-6.5291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30486939217537223, distance: 0.9540912196204159 entropy 0.03264415264129639
epoch: 24, step: 124
	action: tensor([[ 0.1984,  0.1962, -0.0930, -0.2285,  0.1804,  0.0485, -0.2198]],
       dtype=torch.float64)
	q_value: tensor([[-6.4024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5341652846820615, distance: 0.781038816816822 entropy 0.03264415264129639
epoch: 24, step: 125
	action: tensor([[-0.1687, -0.0108, -0.2451, -0.1598, -0.2284,  0.3246, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-5.4280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1204188243295693, distance: 1.0732345776425378 entropy 0.03264415264129639
epoch: 24, step: 126
	action: tensor([[-0.0653, -0.0065, -0.0712, -0.0850, -0.1172,  0.3357,  0.2565]],
       dtype=torch.float64)
	q_value: tensor([[-6.0565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26880757771985264, distance: 0.9785263709453973 entropy 0.03264415264129639
epoch: 24, step: 127
	action: tensor([[-0.2215,  0.0857, -0.3757, -0.3282, -0.2538,  0.2899, -0.0421]],
       dtype=torch.float64)
	q_value: tensor([[-6.1082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0959287634110223, distance: 1.0880729642766906 entropy 0.03264415264129639
LOSS epoch 24 actor 23.31054682381194 critic 44.66025753101367 
epoch: 25, step: 0
	action: tensor([[-0.1397, -0.0577,  0.0029,  0.0130,  0.0208, -0.2873, -0.4465]],
       dtype=torch.float64)
	q_value: tensor([[-6.7807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07612208024663669, distance: 1.0999273148855888 entropy 0.03264415264129639
epoch: 25, step: 1
	action: tensor([[ 0.1180,  0.0299,  0.0184, -0.0850, -0.0581, -0.2877,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-5.6961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3391167052688091, distance: 0.9302915589992945 entropy 0.03264415264129639
epoch: 25, step: 2
	action: tensor([[ 0.0720,  0.0041, -0.1446, -0.2331,  0.0077,  0.1758, -0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-5.9231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32754743150645016, distance: 0.9383989683274496 entropy 0.03264415264129639
epoch: 25, step: 3
	action: tensor([[ 0.0841,  0.1793, -0.0837,  0.4754, -0.0377,  0.0368,  0.0911]],
       dtype=torch.float64)
	q_value: tensor([[-5.3691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.653729872338614, distance: 0.673386192009684 entropy 0.03264415264129639
epoch: 25, step: 4
	action: tensor([[-0.0567,  0.2307, -0.3449,  0.0084, -0.1051, -0.1422,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[-6.1969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.420258914498742, distance: 0.871312075477841 entropy 0.03264415264129639
epoch: 25, step: 5
	action: tensor([[-0.1745, -0.1939, -0.4065, -0.6630,  0.0067,  0.5652, -0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-5.9420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05175527424000492, distance: 1.173583628715496 entropy 0.03264415264129639
epoch: 25, step: 6
	action: tensor([[-0.2448, -0.1114, -0.5478,  0.0708,  0.1817,  0.0626, -0.1183]],
       dtype=torch.float64)
	q_value: tensor([[-7.4472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012003690986965854, distance: 1.1511919433169877 entropy 0.03264415264129639
epoch: 25, step: 7
	action: tensor([[ 0.1795, -0.0449, -0.1631,  0.3329,  0.1649,  0.1570,  0.2304]],
       dtype=torch.float64)
	q_value: tensor([[-5.7017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5850504868346361, distance: 0.737147374362791 entropy 0.03264415264129639
epoch: 25, step: 8
	action: tensor([[ 0.3290,  0.1208,  0.2067, -0.5349,  0.1076,  0.3743,  0.3652]],
       dtype=torch.float64)
	q_value: tensor([[-5.8344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49040467482946815, distance: 0.8169009684405167 entropy 0.03264415264129639
epoch: 25, step: 9
	action: tensor([[ 0.2348,  0.4692,  0.0655,  0.0442,  0.3542,  0.0196, -0.1718]],
       dtype=torch.float64)
	q_value: tensor([[-7.4876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7644335609656723, distance: 0.5554096251609363 entropy 0.03264415264129639
epoch: 25, step: 10
	action: tensor([[ 0.1189, -0.0219, -0.2828, -0.3067, -0.0535,  0.2488, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[-5.9391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3341558884756516, distance: 0.9337765751307117 entropy 0.03264415264129639
epoch: 25, step: 11
	action: tensor([[ 0.2567,  0.2422, -0.3126,  0.1147,  0.0111, -0.1349,  0.0932]],
       dtype=torch.float64)
	q_value: tensor([[-5.9148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6844210918701992, distance: 0.6428515240950823 entropy 0.03264415264129639
epoch: 25, step: 12
	action: tensor([[ 0.1529, -0.1886,  0.0335, -0.0901, -0.2765, -0.1065,  0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-6.1583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2292481674902297, distance: 1.0046481208484002 entropy 0.03264415264129639
epoch: 25, step: 13
	action: tensor([[ 0.6754, -0.2670, -0.2752, -0.0201, -0.5447,  0.0725, -0.1647]],
       dtype=torch.float64)
	q_value: tensor([[-6.0227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49194232148263906, distance: 0.8156675838810251 entropy 0.03264415264129639
epoch: 25, step: 14
	action: tensor([[ 0.2296,  0.0636,  0.1008, -0.1307, -0.0887,  0.1031, -0.3126]],
       dtype=torch.float64)
	q_value: tensor([[-7.5451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5301773766476074, distance: 0.7843748426262434 entropy 0.03264415264129639
epoch: 25, step: 15
	action: tensor([[-0.1792, -0.2274, -0.0021,  0.3099,  0.0309, -0.1204, -0.6271]],
       dtype=torch.float64)
	q_value: tensor([[-5.9205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09381313793389445, distance: 1.0893453251582341 entropy 0.03264415264129639
epoch: 25, step: 16
	action: tensor([[ 0.3613,  0.4193, -0.3341, -0.2054,  0.0795,  0.0575, -0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-6.0343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7920658272415089, distance: 0.5218186870631283 entropy 0.03264415264129639
epoch: 25, step: 17
	action: tensor([[-0.2395,  0.4250, -0.4608, -0.1733, -0.0871, -0.2596, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-6.6258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34701946525967564, distance: 0.9247126757407148 entropy 0.03264415264129639
epoch: 25, step: 18
	action: tensor([[ 0.0777, -0.1583, -0.1480,  0.2187,  0.0277, -0.3796, -0.1453]],
       dtype=torch.float64)
	q_value: tensor([[-6.7391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29384498998459363, distance: 0.9616271481681291 entropy 0.03264415264129639
epoch: 25, step: 19
	action: tensor([[ 0.2020,  0.3223, -0.4258, -0.1003,  0.2056, -0.0998, -0.1044]],
       dtype=torch.float64)
	q_value: tensor([[-5.8845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6641350708838972, distance: 0.6631916078671257 entropy 0.03264415264129639
epoch: 25, step: 20
	action: tensor([[ 0.1566,  0.1513, -0.4747, -0.3985, -0.6741,  0.4078,  0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-6.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4670744995062954, distance: 0.8353912924024347 entropy 0.03264415264129639
epoch: 25, step: 21
	action: tensor([[-0.0624, -0.1573, -0.0262, -0.2969,  0.1123,  0.2656, -0.1023]],
       dtype=torch.float64)
	q_value: tensor([[-9.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06342940635678751, distance: 1.107457202882415 entropy 0.03264415264129639
epoch: 25, step: 22
	action: tensor([[ 0.3925, -0.0874,  0.0229, -0.2994,  0.2796,  0.8560, -0.4998]],
       dtype=torch.float64)
	q_value: tensor([[-5.3536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6403219236416424, distance: 0.6862994908472706 entropy 0.03264415264129639
epoch: 25, step: 23
	action: tensor([[ 0.0067,  0.3236,  0.0637,  0.1709, -0.3309, -0.2164, -0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-7.2673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.579896836638331, distance: 0.7417109133435028 entropy 0.03264415264129639
epoch: 25, step: 24
	action: tensor([[-0.0472, -0.0632, -0.2548,  0.1109, -0.0961,  0.5871, -0.3067]],
       dtype=torch.float64)
	q_value: tensor([[-6.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3631416502394177, distance: 0.9132256830561846 entropy 0.03264415264129639
epoch: 25, step: 25
	action: tensor([[-0.2074, -0.1113, -0.0538,  0.2519, -0.1258,  0.4005,  0.0999]],
       dtype=torch.float64)
	q_value: tensor([[-5.9920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.220633637937664, distance: 1.0102468896042187 entropy 0.03264415264129639
epoch: 25, step: 26
	action: tensor([[ 0.5540, -0.1361, -0.2125, -0.1261, -0.0626,  0.1146, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[-6.0182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.529993247171497, distance: 0.7845285308202465 entropy 0.03264415264129639
epoch: 25, step: 27
	action: tensor([[ 0.1614, -0.3526, -0.1081, -0.3933,  0.1738,  0.2150, -0.0435]],
       dtype=torch.float64)
	q_value: tensor([[-6.2319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006145794277492178, distance: 1.1408223823017722 entropy 0.03264415264129639
epoch: 25, step: 28
	action: tensor([[ 0.0768, -0.0554, -0.3113, -0.3785, -0.4026,  0.0580, -0.2696]],
       dtype=torch.float64)
	q_value: tensor([[-5.5421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2342125435449537, distance: 1.001407448678652 entropy 0.03264415264129639
epoch: 25, step: 29
	action: tensor([[ 0.2383, -0.0950, -0.0631,  0.0500, -0.4474, -0.3541,  0.1365]],
       dtype=torch.float64)
	q_value: tensor([[-6.8068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44542039645902676, distance: 0.85219432884539 entropy 0.03264415264129639
epoch: 25, step: 30
	action: tensor([[ 0.0709, -0.2529, -0.2941, -0.3192,  0.1898, -0.1056, -0.0657]],
       dtype=torch.float64)
	q_value: tensor([[-6.8488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0351475953934387, distance: 1.1240538954953725 entropy 0.03264415264129639
epoch: 25, step: 31
	action: tensor([[-0.2801, -0.0297, -0.4646,  0.0612, -0.1294,  0.0974, -0.1890]],
       dtype=torch.float64)
	q_value: tensor([[-5.5658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005453261986547719, distance: 1.1412197840286187 entropy 0.03264415264129639
epoch: 25, step: 32
	action: tensor([[ 0.4829,  0.3065, -0.1449, -0.0972,  0.1181,  0.1532, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-5.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8293443425643318, distance: 0.4727342172730401 entropy 0.03264415264129639
epoch: 25, step: 33
	action: tensor([[-0.2628,  0.2034, -0.3889,  0.1553,  0.1755,  0.0789, -0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-6.3021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21107464360410488, distance: 1.0164233902601731 entropy 0.03264415264129639
epoch: 25, step: 34
	action: tensor([[ 0.3898,  0.0846,  0.4177, -0.2865,  0.0148, -0.0360, -0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-5.6691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4939694456622171, distance: 0.8140387214948925 entropy 0.03264415264129639
epoch: 25, step: 35
	action: tensor([[ 0.0407, -0.1188,  0.2665,  0.0742, -0.1408,  0.3382,  0.2843]],
       dtype=torch.float64)
	q_value: tensor([[-6.7003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3920111134732619, distance: 0.8922868932207679 entropy 0.03264415264129639
epoch: 25, step: 36
	action: tensor([[-0.2315, -0.0056, -0.3765, -0.1081, -0.2713,  0.1724, -0.4772]],
       dtype=torch.float64)
	q_value: tensor([[-6.4433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04764018550933369, distance: 1.1167532477015312 entropy 0.03264415264129639
epoch: 25, step: 37
	action: tensor([[ 0.3329,  0.3855, -0.1220, -0.2857,  0.3273,  0.0829, -0.3473]],
       dtype=torch.float64)
	q_value: tensor([[-6.2213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7355075891556799, distance: 0.5885228113038994 entropy 0.03264415264129639
epoch: 25, step: 38
	action: tensor([[ 0.3155, -0.0270,  0.2235,  0.0414,  0.0626,  0.4052, -0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-6.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6732023926909882, distance: 0.654178290142436 entropy 0.03264415264129639
epoch: 25, step: 39
	action: tensor([[ 0.2847,  0.1050, -0.2548,  0.0245,  0.0475,  0.7481, -0.1426]],
       dtype=torch.float64)
	q_value: tensor([[-6.0157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7429572351304673, distance: 0.5801755001123272 entropy 0.03264415264129639
epoch: 25, step: 40
	action: tensor([[ 0.2417, -0.0244,  0.0169, -0.0244,  0.2195,  0.4445, -0.2856]],
       dtype=torch.float64)
	q_value: tensor([[-6.9515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6128029472074079, distance: 0.7120700421655737 entropy 0.03264415264129639
epoch: 25, step: 41
	action: tensor([[ 0.0216,  0.3210,  0.0218, -0.1450,  0.3364,  0.0719, -0.3812]],
       dtype=torch.float64)
	q_value: tensor([[-5.4407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.512872419858508, distance: 0.7986896431204474 entropy 0.03264415264129639
epoch: 25, step: 42
	action: tensor([[ 0.3427, -0.2025, -0.4500, -0.5468, -0.2792,  0.1922, -0.3667]],
       dtype=torch.float64)
	q_value: tensor([[-5.5670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25663331311641036, distance: 0.9866389144648057 entropy 0.03264415264129639
epoch: 25, step: 43
	action: tensor([[-0.1096,  0.2369,  0.1407, -0.4458,  0.1428,  0.0525, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[-7.2766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16352884586278016, distance: 1.0466034877893926 entropy 0.03264415264129639
epoch: 25, step: 44
	action: tensor([[0.2737, 0.0209, 0.2565, 0.0470, 0.0305, 0.0045, 0.0659]],
       dtype=torch.float64)
	q_value: tensor([[-5.9896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5727444169509146, distance: 0.7479982236465766 entropy 0.03264415264129639
epoch: 25, step: 45
	action: tensor([[ 0.2536, -0.1282, -0.3432, -0.0219,  0.0207,  0.1629, -0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-5.9983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4327036828197619, distance: 0.8619095155086123 entropy 0.03264415264129639
epoch: 25, step: 46
	action: tensor([[ 0.3576, -0.0535, -0.0444, -0.2183, -0.1996, -0.1785, -0.1855]],
       dtype=torch.float64)
	q_value: tensor([[-5.3034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4167392084609969, distance: 0.8739530145260642 entropy 0.03264415264129639
epoch: 25, step: 47
	action: tensor([[ 0.2104,  0.1796, -0.3416,  0.2944,  0.1787,  0.1387, -0.1499]],
       dtype=torch.float64)
	q_value: tensor([[-6.2897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6926242846920185, distance: 0.6344413337950673 entropy 0.03264415264129639
epoch: 25, step: 48
	action: tensor([[ 0.1306,  0.0092, -0.0525,  0.1062,  0.0561,  0.0388, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-5.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5034406621765909, distance: 0.8063846829652094 entropy 0.03264415264129639
epoch: 25, step: 49
	action: tensor([[ 0.0344, -0.3243, -0.1716, -0.8057, -0.0988,  0.2292, -0.2057]],
       dtype=torch.float64)
	q_value: tensor([[-5.4448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20435553580319987, distance: 1.255839242764773 entropy 0.03264415264129639
epoch: 25, step: 50
	action: tensor([[ 0.3624, -0.1810, -0.2119, -0.2266, -0.3126,  0.0979, -0.3750]],
       dtype=torch.float64)
	q_value: tensor([[-7.0662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3471481319801749, distance: 0.9246215661266162 entropy 0.03264415264129639
epoch: 25, step: 51
	action: tensor([[ 0.1848, -0.1377, -0.0360, -0.2187, -0.0419, -0.3470, -0.1320]],
       dtype=torch.float64)
	q_value: tensor([[-6.4644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1863554444968032, distance: 1.0322242426505308 entropy 0.03264415264129639
epoch: 25, step: 52
	action: tensor([[-0.0845, -0.0598, -0.0854, -0.0167,  0.0952,  0.1695,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-5.9966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22203670702172484, distance: 1.0093371218954228 entropy 0.03264415264129639
epoch: 25, step: 53
	action: tensor([[-0.0859, -0.2707, -0.7206,  0.0891,  0.1797, -0.0029, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-5.4268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02137784249990826, distance: 1.1320463677439732 entropy 0.03264415264129639
epoch: 25, step: 54
	action: tensor([[-0.1642,  0.2249, -0.0875, -0.0166,  0.2115, -0.1572,  0.0646]],
       dtype=torch.float64)
	q_value: tensor([[-5.9555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25713297116500367, distance: 0.9863072713125229 entropy 0.03264415264129639
epoch: 25, step: 55
	action: tensor([[ 0.0106,  0.1754, -0.0332,  0.3820, -0.0941,  0.3207,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-5.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6124827164951085, distance: 0.7123644395110232 entropy 0.03264415264129639
epoch: 25, step: 56
	action: tensor([[-0.1460,  0.1817,  0.0175,  0.1725, -0.0404, -0.4185,  0.2103]],
       dtype=torch.float64)
	q_value: tensor([[-6.1290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28843181870253576, distance: 0.9653058834213671 entropy 0.03264415264129639
epoch: 25, step: 57
	action: tensor([[ 0.1207,  0.2541, -0.0035, -0.1328, -0.2112,  0.2676,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[-6.4051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5637191592744939, distance: 0.7558572167090085 entropy 0.03264415264129639
epoch: 25, step: 58
	action: tensor([[ 2.2039e-01, -3.4131e-04,  2.7740e-02, -5.2964e-01,  3.4036e-02,
          1.7322e-01, -1.4654e-01]], dtype=torch.float64)
	q_value: tensor([[-6.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2701715814450696, distance: 0.9776132483414701 entropy 0.03264415264129639
epoch: 25, step: 59
	action: tensor([[-0.3184, -0.3824, -0.5793,  0.2612, -0.2847,  0.0032, -0.1467]],
       dtype=torch.float64)
	q_value: tensor([[-6.0935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3079833606344413, distance: 1.3087533395144477 entropy 0.03264415264129639
epoch: 25, step: 60
	action: tensor([[ 0.3338, -0.0596, -0.0556,  0.0178, -0.1495,  0.3053, -0.6875]],
       dtype=torch.float64)
	q_value: tensor([[-6.4035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5859154669722377, distance: 0.7363786658219745 entropy 0.03264415264129639
epoch: 25, step: 61
	action: tensor([[ 0.4195, -0.1761,  0.0153, -0.2436,  0.2722,  0.0101,  0.2413]],
       dtype=torch.float64)
	q_value: tensor([[-6.5550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34197368648943616, distance: 0.9282785680611896 entropy 0.03264415264129639
epoch: 25, step: 62
	action: tensor([[-0.5587,  0.4824, -0.3395, -0.0302,  0.2175, -0.1799,  0.0050]],
       dtype=torch.float64)
	q_value: tensor([[-6.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027060234627758195, distance: 1.1597240155203474 entropy 0.03264415264129639
epoch: 25, step: 63
	action: tensor([[-0.1986,  0.1909, -0.2328, -0.4385,  0.1867,  0.3244,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-6.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16563189701026682, distance: 1.0452869774288134 entropy 0.03264415264129639
epoch: 25, step: 64
	action: tensor([[-0.4032, -0.0323, -0.2788, -0.0493,  0.2501, -0.1669, -0.0718]],
       dtype=torch.float64)
	q_value: tensor([[-6.4212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21789491257620908, distance: 1.2628785925043442 entropy 0.03264415264129639
epoch: 25, step: 65
	action: tensor([[-0.1368,  0.4876,  0.0446, -0.2631, -0.1862, -0.2245, -0.4381]],
       dtype=torch.float64)
	q_value: tensor([[-5.7926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3391913836653856, distance: 0.9302389970377458 entropy 0.03264415264129639
epoch: 25, step: 66
	action: tensor([[-0.1170, -0.3625, -0.1746,  0.1790,  0.1034, -0.1816,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-7.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06388660165228632, distance: 1.1803324935022497 entropy 0.03264415264129639
epoch: 25, step: 67
	action: tensor([[-0.0570, -0.1581, -0.0363, -0.2259, -0.1672, -0.0986, -0.3320]],
       dtype=torch.float64)
	q_value: tensor([[-5.6051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012618364805424442, distance: 1.1371014567709425 entropy 0.03264415264129639
epoch: 25, step: 68
	action: tensor([[ 0.0875,  0.2885,  0.2555, -0.5907, -0.0008, -0.2070, -0.1964]],
       dtype=torch.float64)
	q_value: tensor([[-5.6836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24633416402741748, distance: 0.9934502142011072 entropy 0.03264415264129639
epoch: 25, step: 69
	action: tensor([[ 0.1765,  0.5504, -0.1726, -0.0983, -0.1521,  0.0756, -0.4429]],
       dtype=torch.float64)
	q_value: tensor([[-6.9457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.71500128264562, distance: 0.6109112826499218 entropy 0.03264415264129639
epoch: 25, step: 70
	action: tensor([[-0.0607, -0.4121, -0.5007, -0.0086, -0.2813,  0.1574,  0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-7.1312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08036400379901232, distance: 1.1894378273048254 entropy 0.03264415264129639
epoch: 25, step: 71
	action: tensor([[ 0.0734, -0.0511, -0.2535, -0.3262, -0.1239,  0.5777, -0.2885]],
       dtype=torch.float64)
	q_value: tensor([[-6.1195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32507124952186817, distance: 0.9401251208907223 entropy 0.03264415264129639
epoch: 25, step: 72
	action: tensor([[ 0.1387, -0.0030, -0.1352, -0.1465,  0.3555, -0.1878, -0.4395]],
       dtype=torch.float64)
	q_value: tensor([[-6.6831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3347907216338476, distance: 0.9333313254954771 entropy 0.03264415264129639
epoch: 25, step: 73
	action: tensor([[ 0.7104, -0.1733, -0.1419,  0.0176, -0.1071,  0.1373, -0.2508]],
       dtype=torch.float64)
	q_value: tensor([[-5.3471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6256799797625388, distance: 0.7001292485750484 entropy 0.03264415264129639
epoch: 25, step: 74
	action: tensor([[ 0.4644, -0.1171,  0.2077,  0.0625,  0.2356,  0.1142,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[-6.7734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6288868507934311, distance: 0.6971237270834006 entropy 0.03264415264129639
epoch: 25, step: 75
	action: tensor([[-0.0442,  0.3256, -0.3004, -0.1740, -0.2494, -0.0930, -0.2122]],
       dtype=torch.float64)
	q_value: tensor([[-6.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4608779987583572, distance: 0.8402339415022233 entropy 0.03264415264129639
epoch: 25, step: 76
	action: tensor([[ 0.1656, -0.4135, -0.2285, -0.1584, -0.3058,  0.0757, -0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-6.5217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04740763204757137, distance: 1.1168895874428582 entropy 0.03264415264129639
epoch: 25, step: 77
	action: tensor([[ 0.2599,  0.2374, -0.2652,  0.1518,  0.1541,  0.3415,  0.1701]],
       dtype=torch.float64)
	q_value: tensor([[-5.9810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7483368625763578, distance: 0.5740721739533328 entropy 0.03264415264129639
epoch: 25, step: 78
	action: tensor([[-0.1394,  0.1961, -0.0705, -0.3831, -0.1296,  0.2730, -0.1353]],
       dtype=torch.float64)
	q_value: tensor([[-5.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21772034404978624, distance: 1.012133294361476 entropy 0.03264415264129639
epoch: 25, step: 79
	action: tensor([[-0.1182, -0.2144, -0.0325, -0.1490, -0.4720,  0.1064,  0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-6.4298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034693284688387616, distance: 1.1640255378123479 entropy 0.03264415264129639
epoch: 25, step: 80
	action: tensor([[ 0.0569,  0.3756,  0.0196, -0.3374,  0.0357,  0.2493,  0.4526]],
       dtype=torch.float64)
	q_value: tensor([[-6.5680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5209493238355188, distance: 0.7920405597370211 entropy 0.03264415264129639
epoch: 25, step: 81
	action: tensor([[ 0.1712,  0.2877, -0.1585,  0.2333, -0.1255,  0.0310, -0.2299]],
       dtype=torch.float64)
	q_value: tensor([[-7.3928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.683758368407358, distance: 0.6435261720862636 entropy 0.03264415264129639
epoch: 25, step: 82
	action: tensor([[ 0.4485, -0.0410, -0.0610, -0.0926, -0.3941,  0.1377, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-6.0150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.605621924937914, distance: 0.7186427932682719 entropy 0.03264415264129639
epoch: 25, step: 83
	action: tensor([[ 0.1146, -0.2987, -0.5289,  0.2743,  0.1815, -0.0385, -0.4078]],
       dtype=torch.float64)
	q_value: tensor([[-6.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24822999212772567, distance: 0.9921999275004437 entropy 0.03264415264129639
epoch: 25, step: 84
	action: tensor([[ 0.6736,  0.3686, -0.0744, -0.1392, -0.3309,  0.1044,  0.2965]],
       dtype=torch.float64)
	q_value: tensor([[-5.7127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9001569547109887, distance: 0.36158932677993616 entropy 0.03264415264129639
epoch: 25, step: 85
	action: tensor([[ 0.1574, -0.5222,  0.1331, -0.2272,  0.0569,  0.1208,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[-8.3231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1417541641103719, distance: 1.2227649893980204 entropy 0.03264415264129639
epoch: 25, step: 86
	action: tensor([[ 0.2139,  0.1103,  0.2193,  0.0806, -0.3500,  0.2895,  0.2906]],
       dtype=torch.float64)
	q_value: tensor([[-5.8003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.667370658517575, distance: 0.6599894164483125 entropy 0.03264415264129639
epoch: 25, step: 87
	action: tensor([[-0.2431, -0.3359,  0.2396,  0.0063, -0.0018, -0.0825,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-7.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24174225771143676, distance: 1.2751827338665551 entropy 0.03264415264129639
epoch: 25, step: 88
	action: tensor([[ 0.5688,  0.3452, -0.1555, -0.0061,  0.1087, -0.2718,  0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-5.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8502600432810424, distance: 0.44281828427836645 entropy 0.03264415264129639
epoch: 25, step: 89
	action: tensor([[ 0.2998, -0.2055,  0.0842,  0.0135,  0.2714, -0.0593, -0.1635]],
       dtype=torch.float64)
	q_value: tensor([[-7.0691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37004069888916336, distance: 0.9082657540525981 entropy 0.03264415264129639
epoch: 25, step: 90
	action: tensor([[ 0.0178, -0.2527, -0.0151, -0.1112,  0.1568, -0.0390,  0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-5.5108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.033857589030137136, distance: 1.1248050737690782 entropy 0.03264415264129639
epoch: 25, step: 91
	action: tensor([[-0.1162, -0.1942, -0.1512,  0.1512,  0.1565,  0.1989,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-5.9167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15512980784679276, distance: 1.051844855762463 entropy 0.03264415264129639
epoch: 25, step: 92
	action: tensor([[-0.2911,  0.1759, -0.4487, -0.1749,  0.1245, -0.0673,  0.0479]],
       dtype=torch.float64)
	q_value: tensor([[-5.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10185898319014264, distance: 1.0844985070611768 entropy 0.03264415264129639
epoch: 25, step: 93
	action: tensor([[ 0.0425,  0.2541, -0.0398, -0.0459, -0.1705, -0.3694, -0.0509]],
       dtype=torch.float64)
	q_value: tensor([[-6.1215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45803847688600474, distance: 0.8424437653899302 entropy 0.03264415264129639
epoch: 25, step: 94
	action: tensor([[ 0.4613,  0.1126, -0.3599, -0.0228,  0.2198, -0.1779, -0.4484]],
       dtype=torch.float64)
	q_value: tensor([[-6.4036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.682223534094806, distance: 0.6450859137958681 entropy 0.03264415264129639
epoch: 25, step: 95
	action: tensor([[ 0.6691, -0.2541, -0.1580, -0.3077,  0.3617,  0.2919, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-6.3463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41508212556221213, distance: 0.875193613463198 entropy 0.03264415264129639
epoch: 25, step: 96
	action: tensor([[ 0.2484, -0.6108, -0.3341, -0.1087,  0.2667,  0.1411, -0.1755]],
       dtype=torch.float64)
	q_value: tensor([[-6.4514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.050395292471131725, distance: 1.172824626759544 entropy 0.03264415264129639
epoch: 25, step: 97
	action: tensor([[ 0.0511,  0.1859, -0.2777, -0.2257, -0.0974,  0.0540, -0.1191]],
       dtype=torch.float64)
	q_value: tensor([[-5.5128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45189825074556567, distance: 0.8472026144545672 entropy 0.03264415264129639
epoch: 25, step: 98
	action: tensor([[ 0.0170,  0.0143, -0.2520,  0.0572, -0.5798, -0.0591,  0.6435]],
       dtype=torch.float64)
	q_value: tensor([[-5.8280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.372200406513321, distance: 0.9067075005265253 entropy 0.03264415264129639
epoch: 25, step: 99
	action: tensor([[-0.3392,  0.2858,  0.1254, -0.3680,  0.0620, -0.0974,  0.6232]],
       dtype=torch.float64)
	q_value: tensor([[-8.0364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07876739566678537, distance: 1.1885586012984142 entropy 0.03264415264129639
epoch: 25, step: 100
	action: tensor([[ 0.5658, -0.2733,  0.0716, -0.1758,  0.3307,  0.3415, -0.1905]],
       dtype=torch.float64)
	q_value: tensor([[-7.8261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4485981958087599, distance: 0.849749239673796 entropy 0.03264415264129639
epoch: 25, step: 101
	action: tensor([[ 0.5789,  0.0320,  0.2883, -0.2826, -0.2881, -0.1114, -0.4967]],
       dtype=torch.float64)
	q_value: tensor([[-6.0395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5336364770138742, distance: 0.78148200208754 entropy 0.03264415264129639
epoch: 25, step: 102
	action: tensor([[ 0.2758,  0.3791, -0.3549,  0.3454, -0.0070, -0.0388, -0.5140]],
       dtype=torch.float64)
	q_value: tensor([[-7.7898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7367493048716555, distance: 0.5871397135072094 entropy 0.03264415264129639
epoch: 25, step: 103
	action: tensor([[ 0.6148, -0.1262, -0.2381, -0.2076,  0.0145,  0.2876,  0.2097]],
       dtype=torch.float64)
	q_value: tensor([[-6.7517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5706120670374806, distance: 0.7498624577077978 entropy 0.03264415264129639
epoch: 25, step: 104
	action: tensor([[ 0.1174, -0.3628, -0.5112,  0.0995,  0.2409,  0.1666, -0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-6.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17350497588882774, distance: 1.040343636129242 entropy 0.03264415264129639
epoch: 25, step: 105
	action: tensor([[ 0.2951, -0.0367, -0.4381, -0.1144, -0.1164, -0.0373,  0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-5.4178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.476500418915078, distance: 0.8279704981979923 entropy 0.03264415264129639
epoch: 25, step: 106
	action: tensor([[-0.1169, -0.2670, -0.3670, -0.0600,  0.1862,  0.1171,  0.2866]],
       dtype=torch.float64)
	q_value: tensor([[-6.6726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.032627669324242436, distance: 1.1628630530964794 entropy 0.03264415264129639
epoch: 25, step: 107
	action: tensor([[ 0.1355, -0.3514, -0.3065, -0.3813,  0.3787, -0.1833,  0.4026]],
       dtype=torch.float64)
	q_value: tensor([[-5.8773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10868807420447801, distance: 1.2049287898169885 entropy 0.03264415264129639
epoch: 25, step: 108
	action: tensor([[-0.0353,  0.1293, -0.4697, -0.0570, -0.2744,  0.0795, -0.5649]],
       dtype=torch.float64)
	q_value: tensor([[-6.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30166380448156394, distance: 0.9562885802120427 entropy 0.03264415264129639
epoch: 25, step: 109
	action: tensor([[-0.4802, -0.0787, -0.0866, -0.1802, -0.2468,  0.4383, -0.3485]],
       dtype=torch.float64)
	q_value: tensor([[-6.5066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3226612381539724, distance: 1.3160761122441211 entropy 0.03264415264129639
epoch: 25, step: 110
	action: tensor([[ 0.0139,  0.2528, -0.0803,  0.0305, -0.0276, -0.2936, -0.1316]],
       dtype=torch.float64)
	q_value: tensor([[-6.8711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42364255488362546, distance: 0.8687656619334926 entropy 0.03264415264129639
epoch: 25, step: 111
	action: tensor([[ 0.1288,  0.1718, -0.1967, -0.0468,  0.0678, -0.1052,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-5.9567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4805064516620581, distance: 0.8247964298989183 entropy 0.03264415264129639
epoch: 25, step: 112
	action: tensor([[ 0.1034,  0.2375,  0.0613,  0.2290,  0.4122,  0.0032, -0.2690]],
       dtype=torch.float64)
	q_value: tensor([[-5.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6153629740112441, distance: 0.7097121451644774 entropy 0.03264415264129639
epoch: 25, step: 113
	action: tensor([[ 0.0963,  0.3066, -0.3334,  0.0647,  0.1246, -0.0141,  0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-5.6046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5666797386364941, distance: 0.7532882469537511 entropy 0.03264415264129639
epoch: 25, step: 114
	action: tensor([[-0.1393,  0.2992,  0.1249,  0.1473, -0.1359,  0.5543, -0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-5.9923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4956473326687505, distance: 0.8126880134293969 entropy 0.03264415264129639
epoch: 25, step: 115
	action: tensor([[-0.4318, -0.0035, -0.2117,  0.0437, -0.0645,  0.0583, -0.3259]],
       dtype=torch.float64)
	q_value: tensor([[-6.7665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17651582541923339, distance: 1.2412394777495541 entropy 0.03264415264129639
epoch: 25, step: 116
	action: tensor([[ 0.1982, -0.1664, -0.0138, -0.0305, -0.2822, -0.0574,  0.2264]],
       dtype=torch.float64)
	q_value: tensor([[-5.7149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29049417371661646, distance: 0.9639059838238164 entropy 0.03264415264129639
epoch: 25, step: 117
	action: tensor([[-0.0823, -0.3501,  0.1175, -0.1406, -0.7424, -0.2792,  0.0596]],
       dtype=torch.float64)
	q_value: tensor([[-6.1531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16118971207400867, distance: 1.2331283515397802 entropy 0.03264415264129639
epoch: 25, step: 118
	action: tensor([[-0.1396, -0.4752, -0.1989, -0.3569,  0.3118,  0.3980, -0.4374]],
       dtype=torch.float64)
	q_value: tensor([[-7.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25155850590599327, distance: 1.2802131131615142 entropy 0.03264415264129639
epoch: 25, step: 119
	action: tensor([[ 0.1152, -0.1808,  0.2192, -0.1454, -0.1782, -0.2018,  0.1142]],
       dtype=torch.float64)
	q_value: tensor([[-5.7689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1071695030412797, distance: 1.0812875472477106 entropy 0.03264415264129639
epoch: 25, step: 120
	action: tensor([[ 0.2815,  0.5600, -0.0597, -0.4832, -0.0089,  0.1423, -0.3355]],
       dtype=torch.float64)
	q_value: tensor([[-6.1393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7217174656546823, distance: 0.6036701035861953 entropy 0.03264415264129639
epoch: 25, step: 121
	action: tensor([[-0.1766,  0.2866, -0.0397,  0.4875, -0.1033,  0.6502, -0.3323]],
       dtype=torch.float64)
	q_value: tensor([[-7.6465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4984487738102644, distance: 0.810427821216728 entropy 0.03264415264129639
epoch: 25, step: 122
	action: tensor([[ 0.2641,  0.0230,  0.2216,  0.0272,  0.5180, -0.2799, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[-7.0005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4613501082463539, distance: 0.8398659642198881 entropy 0.03264415264129639
epoch: 25, step: 123
	action: tensor([[-0.0634,  0.5480, -0.0610, -0.1871, -0.1637, -0.0159, -0.1867]],
       dtype=torch.float64)
	q_value: tensor([[-6.1984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4657520623168956, distance: 0.8364271482668769 entropy 0.03264415264129639
epoch: 25, step: 124
	action: tensor([[-0.1511, -0.1869,  0.1949, -0.0549,  0.0816,  0.0490, -0.3235]],
       dtype=torch.float64)
	q_value: tensor([[-6.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03593014907543246, distance: 1.1647210636012744 entropy 0.03264415264129639
epoch: 25, step: 125
	action: tensor([[-0.1021,  0.3040,  0.0787, -0.0274, -0.1732, -0.4405, -0.2777]],
       dtype=torch.float64)
	q_value: tensor([[-5.3321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3189309295852111, distance: 0.9443919408023677 entropy 0.03264415264129639
epoch: 25, step: 126
	action: tensor([[ 0.1555,  0.3496,  0.1294, -0.1614, -0.1182, -0.3344,  0.1068]],
       dtype=torch.float64)
	q_value: tensor([[-6.5622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5293861706971159, distance: 0.7850350289925881 entropy 0.03264415264129639
epoch: 25, step: 127
	action: tensor([[-0.1653,  0.2075,  0.0082, -0.4498, -0.4100, -0.1990, -0.1749]],
       dtype=torch.float64)
	q_value: tensor([[-6.8728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08096847675802432, distance: 1.0970385711509214 entropy 0.03264415264129639
LOSS epoch 25 actor 22.438461744902842 critic 36.351776201445 
epoch: 26, step: 0
	action: tensor([[ 0.3423, -0.3250, -0.0339, -0.2996, -0.1195,  0.1522,  0.1881]],
       dtype=torch.float64)
	q_value: tensor([[-7.0046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14450566972141377, distance: 1.0584376042179195 entropy 0.03264415264129639
epoch: 26, step: 1
	action: tensor([[ 0.1051, -0.4032, -0.2859,  0.2825, -0.2286, -0.2601, -0.2403]],
       dtype=torch.float64)
	q_value: tensor([[-6.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1503107140014539, distance: 1.0548404232595097 entropy 0.03264415264129639
epoch: 26, step: 2
	action: tensor([[ 0.4910,  0.3379, -0.1460,  0.5255, -0.1652,  0.0563,  0.0751]],
       dtype=torch.float64)
	q_value: tensor([[-5.6962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.885145492636648, distance: 0.3878204580407428 entropy 0.03264415264129639
epoch: 26, step: 3
	action: tensor([[-0.1052, -0.0383, -0.2730,  0.1209, -0.1054, -0.0885, -0.1135]],
       dtype=torch.float64)
	q_value: tensor([[-6.6704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19433543788867658, distance: 1.0271498897780653 entropy 0.03264415264129639
epoch: 26, step: 4
	action: tensor([[-0.2806,  0.1615,  0.1481, -0.3723,  0.0236, -0.2676,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[-5.1039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1474496360892512, distance: 1.2258109867531464 entropy 0.03264415264129639
epoch: 26, step: 5
	action: tensor([[ 0.6430, -0.1227,  0.0191,  0.0189,  0.1188, -0.3447,  0.2353]],
       dtype=torch.float64)
	q_value: tensor([[-5.9638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5339791393832062, distance: 0.7811948508973005 entropy 0.03264415264129639
epoch: 26, step: 6
	action: tensor([[ 0.1516,  0.0985, -0.2305, -0.4671, -0.2023,  0.0796,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-6.7313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3467300210942973, distance: 0.9249175999464408 entropy 0.03264415264129639
epoch: 26, step: 7
	action: tensor([[ 0.5640,  0.2151,  0.0373, -0.4650, -0.0578,  0.0394,  0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-6.4823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6331452281637318, distance: 0.6931125770519042 entropy 0.03264415264129639
epoch: 26, step: 8
	action: tensor([[ 0.4814, -0.0648, -0.1119, -0.0189,  0.5671,  0.1415,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-6.9523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6307151317841079, distance: 0.6954044243777006 entropy 0.03264415264129639
epoch: 26, step: 9
	action: tensor([[ 0.2243,  0.1075, -0.1707, -0.1654, -0.0734,  0.5449,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-5.6672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6039352305335512, distance: 0.7201779160280242 entropy 0.03264415264129639
epoch: 26, step: 10
	action: tensor([[ 0.4862,  0.5197,  0.2402, -0.2126, -0.1835,  0.2132, -0.5032]],
       dtype=torch.float64)
	q_value: tensor([[-6.3049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.872254606455376, distance: 0.40900565957833246 entropy 0.03264415264129639
epoch: 26, step: 11
	action: tensor([[ 0.3591, -0.6621, -0.5615,  0.1066, -0.0050,  0.1597, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-8.0131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042270015340124645, distance: 1.1198973978657023 entropy 0.03264415264129639
epoch: 26, step: 12
	action: tensor([[ 0.5126,  0.0685, -0.2585,  0.0344,  0.0176,  0.0370, -0.1374]],
       dtype=torch.float64)
	q_value: tensor([[-5.7556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7284967537662852, distance: 0.5962717124152986 entropy 0.03264415264129639
epoch: 26, step: 13
	action: tensor([[ 0.1635,  0.0727, -0.1696, -0.3188,  0.1529,  0.0169, -0.1491]],
       dtype=torch.float64)
	q_value: tensor([[-5.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3900253756866594, distance: 0.8937428436913022 entropy 0.03264415264129639
epoch: 26, step: 14
	action: tensor([[ 0.5468,  0.3367,  0.1227, -0.3853, -0.2939, -0.3264, -0.6249]],
       dtype=torch.float64)
	q_value: tensor([[-5.1698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7008425541000127, distance: 0.6259023780718255 entropy 0.03264415264129639
epoch: 26, step: 15
	action: tensor([[ 0.1690,  0.2015, -0.4127, -0.1780, -0.0786,  0.3184, -0.2142]],
       dtype=torch.float64)
	q_value: tensor([[-8.1656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5901456865913355, distance: 0.7326076482860306 entropy 0.03264415264129639
epoch: 26, step: 16
	action: tensor([[-0.0184,  0.0499, -0.4794,  0.0364, -0.3859, -0.0517, -0.2221]],
       dtype=torch.float64)
	q_value: tensor([[-5.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3301890348672547, distance: 0.9365539927637615 entropy 0.03264415264129639
epoch: 26, step: 17
	action: tensor([[ 0.5053, -0.0292,  0.2295, -0.2280, -0.0439,  0.2152, -0.3834]],
       dtype=torch.float64)
	q_value: tensor([[-6.0265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.577155522297413, distance: 0.7441269349162718 entropy 0.03264415264129639
epoch: 26, step: 18
	action: tensor([[ 0.5821, -0.0585,  0.2345,  0.0355,  0.0047, -0.0537, -0.2860]],
       dtype=torch.float64)
	q_value: tensor([[-6.2410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6559676325519981, distance: 0.6712067962964882 entropy 0.03264415264129639
epoch: 26, step: 19
	action: tensor([[-0.2896,  0.0928,  0.2592, -0.3962, -0.0215, -0.1815, -0.6255]],
       dtype=torch.float64)
	q_value: tensor([[-6.3061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23120295846406624, distance: 1.269759639400419 entropy 0.03264415264129639
epoch: 26, step: 20
	action: tensor([[-0.2708, -0.2317,  0.2735,  0.3588,  0.1976,  0.0136, -0.0362]],
       dtype=torch.float64)
	q_value: tensor([[-6.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05422945068637941, distance: 1.1128832000101296 entropy 0.03264415264129639
epoch: 26, step: 21
	action: tensor([[-0.4566, -0.1866, -0.1499, -0.2700,  0.1353, -0.1734,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[-5.5001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5195494689734359, distance: 1.4106332449784116 entropy 0.03264415264129639
epoch: 26, step: 22
	action: tensor([[-0.2584,  0.1844, -0.0718,  0.0879,  0.2294,  0.1499, -0.4150]],
       dtype=torch.float64)
	q_value: tensor([[-5.7157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1626664118432889, distance: 1.0471428930680549 entropy 0.03264415264129639
epoch: 26, step: 23
	action: tensor([[-0.2340,  0.2867, -0.0182, -0.2818, -0.1721, -0.0597, -0.2285]],
       dtype=torch.float64)
	q_value: tensor([[-5.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09820743897247142, distance: 1.0867008768627362 entropy 0.03264415264129639
epoch: 26, step: 24
	action: tensor([[ 0.1141, -0.0719, -0.2110,  0.1390, -0.1413, -0.2208,  0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-5.9651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3402396280207869, distance: 0.9295008825750957 entropy 0.03264415264129639
epoch: 26, step: 25
	action: tensor([[ 0.2292, -0.1185, -0.0018, -0.2820,  0.0161, -0.1437, -0.2568]],
       dtype=torch.float64)
	q_value: tensor([[-5.4403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20025252650752978, distance: 1.023371060693856 entropy 0.03264415264129639
epoch: 26, step: 26
	action: tensor([[ 0.1583,  0.3975, -0.4386,  0.2425, -0.1284,  0.0116,  0.1596]],
       dtype=torch.float64)
	q_value: tensor([[-5.3135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.64734873091409, distance: 0.6795625210200013 entropy 0.03264415264129639
epoch: 26, step: 27
	action: tensor([[-0.2682, -0.3492,  0.0906, -0.0172, -0.0293,  0.3063,  0.2579]],
       dtype=torch.float64)
	q_value: tensor([[-6.4079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19962007445659946, distance: 1.2533678647806614 entropy 0.03264415264129639
epoch: 26, step: 28
	action: tensor([[ 0.2209,  0.0584,  0.0449, -0.4220,  0.2508, -0.2172, -0.3510]],
       dtype=torch.float64)
	q_value: tensor([[-5.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23665697351740456, distance: 0.999807901105006 entropy 0.03264415264129639
epoch: 26, step: 29
	action: tensor([[-0.0676,  0.1390, -0.7773, -0.3212,  0.4136,  0.2712,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[-5.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35169965507556034, distance: 0.9213928113762104 entropy 0.03264415264129639
epoch: 26, step: 30
	action: tensor([[ 0.2708, -0.1620, -0.4312, -0.1106,  0.2119,  0.0259,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-6.9936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33985464150937006, distance: 0.9297720364134167 entropy 0.03264415264129639
epoch: 26, step: 31
	action: tensor([[ 0.2854,  0.2073, -0.1499,  0.1319, -0.0054, -0.0104, -0.4664]],
       dtype=torch.float64)
	q_value: tensor([[-5.2952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6811308548581496, distance: 0.6461940309327021 entropy 0.03264415264129639
epoch: 26, step: 32
	action: tensor([[-0.0443,  0.2636, -0.2289,  0.2534, -0.1062,  0.1866,  0.2514]],
       dtype=torch.float64)
	q_value: tensor([[-5.6756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48009987133504217, distance: 0.8251191292033648 entropy 0.03264415264129639
epoch: 26, step: 33
	action: tensor([[-0.0506,  0.1161, -0.3247,  0.2556,  0.1920,  0.0909, -0.3319]],
       dtype=torch.float64)
	q_value: tensor([[-5.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40238561236376724, distance: 0.8846413097393564 entropy 0.03264415264129639
epoch: 26, step: 34
	action: tensor([[ 0.0690,  0.1250, -0.1440,  0.0786,  0.5256,  0.0608, -0.2752]],
       dtype=torch.float64)
	q_value: tensor([[-5.2179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4881843776668411, distance: 0.818678645228804 entropy 0.03264415264129639
epoch: 26, step: 35
	action: tensor([[-0.1093,  0.4394, -0.2394,  0.1416, -0.0866, -0.2567, -0.1592]],
       dtype=torch.float64)
	q_value: tensor([[-5.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4709781211495083, distance: 0.8323260935982034 entropy 0.03264415264129639
epoch: 26, step: 36
	action: tensor([[ 0.3417,  0.3520, -0.0061, -0.2923,  0.1308, -0.3361, -0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-6.0461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6301657202288147, distance: 0.695921533605739 entropy 0.03264415264129639
epoch: 26, step: 37
	action: tensor([[-0.0569,  0.2921,  0.0375,  0.3330, -0.1797, -0.0728, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-6.4264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5437466317745339, distance: 0.7729648320541345 entropy 0.03264415264129639
epoch: 26, step: 38
	action: tensor([[-0.0696,  0.4854,  0.1758, -0.3658,  0.2968,  0.0038,  0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-5.9610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3550448682441575, distance: 0.9190125552407865 entropy 0.03264415264129639
epoch: 26, step: 39
	action: tensor([[ 0.0934, -0.2402, -0.2005,  0.0118,  0.5104, -0.1529, -0.3114]],
       dtype=torch.float64)
	q_value: tensor([[-6.2685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10400065261367442, distance: 1.0832047104757083 entropy 0.03264415264129639
epoch: 26, step: 40
	action: tensor([[ 0.5037, -0.0735,  0.0157, -0.1556, -0.0487, -0.1277,  0.2799]],
       dtype=torch.float64)
	q_value: tensor([[-4.9512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4796064648365346, distance: 0.8255105722619231 entropy 0.03264415264129639
epoch: 26, step: 41
	action: tensor([[ 0.3712, -0.0252, -0.4110, -0.3230, -0.1674,  0.2565,  0.2624]],
       dtype=torch.float64)
	q_value: tensor([[-6.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4860709579464024, distance: 0.8203671723789814 entropy 0.03264415264129639
epoch: 26, step: 42
	action: tensor([[ 0.1664,  0.0131,  0.1352, -0.2601, -0.2265, -0.2011,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[-6.8875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27473204933054607, distance: 0.9745540632652582 entropy 0.03264415264129639
epoch: 26, step: 43
	action: tensor([[ 0.2158,  0.2239, -0.4150, -0.1276, -0.2796,  0.1139,  0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-5.9761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6079345666568008, distance: 0.7165326266194743 entropy 0.03264415264129639
epoch: 26, step: 44
	action: tensor([[ 0.2552,  0.1852,  0.2641, -0.2583, -0.1280, -0.0029, -0.2147]],
       dtype=torch.float64)
	q_value: tensor([[-6.5243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5179090230317691, distance: 0.7945499320017904 entropy 0.03264415264129639
epoch: 26, step: 45
	action: tensor([[ 0.3092,  0.4235,  0.0499,  0.0421,  0.3185, -0.1575, -0.4667]],
       dtype=torch.float64)
	q_value: tensor([[-6.1537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7749242372297326, distance: 0.5429015212474597 entropy 0.03264415264129639
epoch: 26, step: 46
	action: tensor([[-0.3549, -0.0558, -0.3436, -0.5882,  0.0587,  0.2168, -0.1300]],
       dtype=torch.float64)
	q_value: tensor([[-6.0075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20702865428192574, distance: 1.257232164661866 entropy 0.03264415264129639
epoch: 26, step: 47
	action: tensor([[ 0.2020,  0.2626, -0.3483, -0.1221, -0.3062,  0.2669, -0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-6.3870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5991411644239133, distance: 0.7245234118149405 entropy 0.03264415264129639
epoch: 26, step: 48
	action: tensor([[ 0.5252, -0.0555, -0.1268, -0.3861, -0.1085,  0.3780, -0.4423]],
       dtype=torch.float64)
	q_value: tensor([[-6.5302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5327179573854708, distance: 0.782251201900568 entropy 0.03264415264129639
epoch: 26, step: 49
	action: tensor([[ 0.3946,  0.0536, -0.6842,  0.0578, -0.3189,  0.1317, -0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-6.6591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6651987327204913, distance: 0.6621406333139154 entropy 0.03264415264129639
epoch: 26, step: 50
	action: tensor([[ 0.1376,  0.1092, -0.0506, -0.2590,  0.1102,  0.0267, -0.2662]],
       dtype=torch.float64)
	q_value: tensor([[-6.7206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40735735966896724, distance: 0.8809538158237944 entropy 0.03264415264129639
epoch: 26, step: 51
	action: tensor([[-0.0752,  0.1089, -0.2670, -0.1654,  0.0106, -0.5304, -0.2089]],
       dtype=torch.float64)
	q_value: tensor([[-5.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21330904069886314, distance: 1.0149830107726256 entropy 0.03264415264129639
epoch: 26, step: 52
	action: tensor([[-0.2697,  0.2298, -0.3798,  0.4424, -0.2623, -0.1188, -0.5184]],
       dtype=torch.float64)
	q_value: tensor([[-6.1577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1994810348507089, distance: 1.0238645489174407 entropy 0.03264415264129639
epoch: 26, step: 53
	action: tensor([[ 0.0819,  0.5983, -0.3514,  0.0917, -0.3947,  0.1896,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-6.3145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6283206574109466, distance: 0.6976553121394355 entropy 0.03264415264129639
epoch: 26, step: 54
	action: tensor([[ 0.0380,  0.1791, -0.7775,  0.0679, -0.3903, -0.5495,  0.1640]],
       dtype=torch.float64)
	q_value: tensor([[-7.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5623513291154132, distance: 0.7570411733584922 entropy 0.03264415264129639
epoch: 26, step: 55
	action: tensor([[ 0.4619,  0.2337, -0.0812,  0.1817,  0.4806, -0.0410, -0.5021]],
       dtype=torch.float64)
	q_value: tensor([[-7.9604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8350969600210862, distance: 0.46469823635543456 entropy 0.03264415264129639
epoch: 26, step: 56
	action: tensor([[ 0.1929, -0.0441,  0.5544,  0.0340,  0.0996,  0.2294, -0.2698]],
       dtype=torch.float64)
	q_value: tensor([[-5.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5275113722392657, distance: 0.7865971585843832 entropy 0.03264415264129639
epoch: 26, step: 57
	action: tensor([[ 0.3086, -0.0176, -0.1378, -0.0901, -0.1804,  0.4269,  0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-5.9298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5873471017238348, distance: 0.7351046046231764 entropy 0.03264415264129639
epoch: 26, step: 58
	action: tensor([[ 0.4199, -0.1698, -0.3315, -0.6491,  0.2411,  0.3368, -0.3849]],
       dtype=torch.float64)
	q_value: tensor([[-6.1318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30174045125984683, distance: 0.9562360994366305 entropy 0.03264415264129639
epoch: 26, step: 59
	action: tensor([[-0.1813, -0.0278,  0.1096,  0.3117,  0.0688,  0.6178,  0.1251]],
       dtype=torch.float64)
	q_value: tensor([[-6.3406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4361188439150572, distance: 0.8593112231816501 entropy 0.03264415264129639
epoch: 26, step: 60
	action: tensor([[ 0.2994,  0.1687, -0.2602,  0.2461,  0.0756,  0.2218,  0.0735]],
       dtype=torch.float64)
	q_value: tensor([[-6.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7382566138553832, distance: 0.5854563914006709 entropy 0.03264415264129639
epoch: 26, step: 61
	action: tensor([[ 0.0791, -0.2375, -0.0603, -0.1080, -0.2279, -0.2088, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-5.4618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09358961078975048, distance: 1.0894796700962195 entropy 0.03264415264129639
epoch: 26, step: 62
	action: tensor([[ 0.1485,  0.2345,  0.3585, -0.1121, -0.2738,  0.2805,  0.1673]],
       dtype=torch.float64)
	q_value: tensor([[-5.5639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5878892839855787, distance: 0.7346215210418687 entropy 0.03264415264129639
epoch: 26, step: 63
	action: tensor([[-0.1464,  0.3037, -0.0644,  0.4813,  0.0205,  0.5212,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-6.7943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5530867268027803, distance: 0.7650121274355054 entropy 0.03264415264129639
epoch: 26, step: 64
	action: tensor([[-0.4197,  0.3963, -0.1208, -0.3116, -0.1115, -0.0574, -0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-6.3722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00520931821112347, distance: 1.1473210090007235 entropy 0.03264415264129639
epoch: 26, step: 65
	action: tensor([[-0.0365,  0.1336,  0.0123, -0.1598, -0.1333, -0.0186, -0.5144]],
       dtype=torch.float64)
	q_value: tensor([[-6.4469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2767461251200364, distance: 0.9731999497033261 entropy 0.03264415264129639
epoch: 26, step: 66
	action: tensor([[ 0.0595,  0.1789, -0.3698,  0.1750,  0.0264, -0.6499,  0.1660]],
       dtype=torch.float64)
	q_value: tensor([[-5.6877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47090006019117914, distance: 0.8323874991643112 entropy 0.03264415264129639
epoch: 26, step: 67
	action: tensor([[-0.0538, -0.6066, -0.3297, -0.2386,  0.4135, -0.0771, -0.5168]],
       dtype=torch.float64)
	q_value: tensor([[-6.9865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3752627292983042, distance: 1.3419907735517462 entropy 0.03264415264129639
epoch: 26, step: 68
	action: tensor([[ 0.3961,  0.3097, -0.0507, -0.3971,  0.2391,  0.4150, -0.1820]],
       dtype=torch.float64)
	q_value: tensor([[-5.2756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7373059385799954, distance: 0.5865186425887485 entropy 0.03264415264129639
epoch: 26, step: 69
	action: tensor([[-0.2744,  0.2004, -0.1254,  0.0076, -0.1861,  0.1424,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[-6.3237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15844556064233617, distance: 1.049778806933744 entropy 0.03264415264129639
epoch: 26, step: 70
	action: tensor([[ 0.4635, -0.4462,  0.0805, -0.2176, -0.2639, -0.0513, -0.4239]],
       dtype=torch.float64)
	q_value: tensor([[-5.7330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07391120100822901, distance: 1.101242614935688 entropy 0.03264415264129639
epoch: 26, step: 71
	action: tensor([[ 0.0677, -0.2514, -0.0284, -0.1753,  0.0344,  0.0197, -0.1554]],
       dtype=torch.float64)
	q_value: tensor([[-6.2246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07018751234865073, distance: 1.1034543741217309 entropy 0.03264415264129639
epoch: 26, step: 72
	action: tensor([[-0.0358,  0.5819, -0.0623, -0.3002, -0.1393,  0.5561,  0.2071]],
       dtype=torch.float64)
	q_value: tensor([[-4.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.565535336624506, distance: 0.7542823109669833 entropy 0.03264415264129639
epoch: 26, step: 73
	action: tensor([[ 0.0446,  0.2274, -0.0477, -0.1462, -0.5277, -0.1275,  0.5629]],
       dtype=torch.float64)
	q_value: tensor([[-8.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42449281268930683, distance: 0.8681246123375134 entropy 0.03264415264129639
epoch: 26, step: 74
	action: tensor([[ 0.0215, -0.3760, -0.4117,  0.0160, -0.2069, -0.0707, -0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-7.7471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012179961691925478, distance: 1.1373538685428108 entropy 0.03264415264129639
epoch: 26, step: 75
	action: tensor([[ 0.2497,  0.3393, -0.1135, -0.1987,  0.3588, -0.0231, -0.4256]],
       dtype=torch.float64)
	q_value: tensor([[-5.5186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.650662263948868, distance: 0.6763623804572721 entropy 0.03264415264129639
epoch: 26, step: 76
	action: tensor([[-0.1044,  0.2256, -0.0956, -0.2247,  0.1847,  0.8003, -0.1675]],
       dtype=torch.float64)
	q_value: tensor([[-5.4716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4683507042666677, distance: 0.8343904307640359 entropy 0.03264415264129639
epoch: 26, step: 77
	action: tensor([[ 0.1251, -0.0549, -0.1713,  0.4135,  0.0016,  0.0876, -0.1527]],
       dtype=torch.float64)
	q_value: tensor([[-6.8545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5506041060420798, distance: 0.7671340209021881 entropy 0.03264415264129639
epoch: 26, step: 78
	action: tensor([[-0.0483,  0.0942, -0.1313, -0.1574,  0.3870, -0.1107,  0.3579]],
       dtype=torch.float64)
	q_value: tensor([[-5.2904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21195917695349753, distance: 1.0158534297919783 entropy 0.03264415264129639
epoch: 26, step: 79
	action: tensor([[ 0.2405, -0.2063, -0.5346,  0.0957,  0.2276, -0.6283,  0.0459]],
       dtype=torch.float64)
	q_value: tensor([[-5.8860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23034482627583386, distance: 1.003933138199487 entropy 0.03264415264129639
epoch: 26, step: 80
	action: tensor([[-0.0511,  0.5392,  0.0035,  0.0024, -0.1282,  0.0900, -0.0292]],
       dtype=torch.float64)
	q_value: tensor([[-6.9445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5311012335516163, distance: 0.78360326771111 entropy 0.03264415264129639
epoch: 26, step: 81
	action: tensor([[ 0.3420,  0.0361,  0.1369, -0.1070,  0.2598,  0.1370,  0.1518]],
       dtype=torch.float64)
	q_value: tensor([[-6.3684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5760249596552598, distance: 0.7451210599111656 entropy 0.03264415264129639
epoch: 26, step: 82
	action: tensor([[ 0.6437,  0.2752, -0.3949, -0.2521,  0.1370,  0.0192,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-5.5009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8105259898251154, distance: 0.49811717540784906 entropy 0.03264415264129639
epoch: 26, step: 83
	action: tensor([[-0.0732, -0.0181, -0.0832, -0.4684, -0.0810,  0.3194,  0.4790]],
       dtype=torch.float64)
	q_value: tensor([[-7.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0685340596982682, distance: 1.104435055274585 entropy 0.03264415264129639
epoch: 26, step: 84
	action: tensor([[ 0.3201, -0.0119, -0.1005, -0.2963,  0.5519,  0.6278, -0.2358]],
       dtype=torch.float64)
	q_value: tensor([[-7.0634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.601057589230482, distance: 0.7227894371303137 entropy 0.03264415264129639
epoch: 26, step: 85
	action: tensor([[ 0.1348, -0.2901, -0.2159, -0.0019, -0.0034,  0.4786,  0.0971]],
       dtype=torch.float64)
	q_value: tensor([[-5.8443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303846181322605, distance: 0.9547931578577084 entropy 0.03264415264129639
epoch: 26, step: 86
	action: tensor([[-0.2443,  0.4758,  0.0562,  0.3182, -0.1932, -0.0248, -0.5903]],
       dtype=torch.float64)
	q_value: tensor([[-5.3868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4361361734579281, distance: 0.859298018639402 entropy 0.03264415264129639
epoch: 26, step: 87
	action: tensor([[-0.1969,  0.5070,  0.1516, -0.0905,  0.1484, -0.1404, -0.4565]],
       dtype=torch.float64)
	q_value: tensor([[-6.6242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3302154989349846, distance: 0.9365354910711908 entropy 0.03264415264129639
epoch: 26, step: 88
	action: tensor([[ 0.3918,  0.3399,  0.0983, -0.1035,  0.0242, -0.0104,  0.2042]],
       dtype=torch.float64)
	q_value: tensor([[-6.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7544672156713089, distance: 0.567037051523955 entropy 0.03264415264129639
epoch: 26, step: 89
	action: tensor([[ 0.1856, -0.0480,  0.2437, -0.4675,  0.4729,  0.5367, -0.3344]],
       dtype=torch.float64)
	q_value: tensor([[-6.1875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3178243364914335, distance: 0.9451588479268034 entropy 0.03264415264129639
epoch: 26, step: 90
	action: tensor([[-0.3031,  0.4080, -0.4054, -0.1935, -0.1445, -0.0372, -0.1898]],
       dtype=torch.float64)
	q_value: tensor([[-5.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22397742197724335, distance: 1.00807738456324 entropy 0.03264415264129639
epoch: 26, step: 91
	action: tensor([[-0.0985,  0.3382, -0.6235,  0.0466, -0.3944, -0.5079, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-6.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5048786070115205, distance: 0.8052162653456655 entropy 0.03264415264129639
epoch: 26, step: 92
	action: tensor([[ 0.1350, -0.1585, -0.1975, -0.0181,  0.2333,  0.3493, -0.2779]],
       dtype=torch.float64)
	q_value: tensor([[-7.5391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39058440208909806, distance: 0.8933332033941392 entropy 0.03264415264129639
epoch: 26, step: 93
	action: tensor([[-0.1874, -0.1047, -0.3498, -0.2589, -0.4524,  0.1684,  0.0483]],
       dtype=torch.float64)
	q_value: tensor([[-4.7767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04664658638459285, distance: 1.1707299370052702 entropy 0.03264415264129639
epoch: 26, step: 94
	action: tensor([[ 1.6683e-04,  4.0186e-01, -1.9225e-01, -1.5695e-01, -6.3710e-02,
          3.2536e-01,  1.0376e-01]], dtype=torch.float64)
	q_value: tensor([[-6.5519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5164398034809223, distance: 0.7957597453827853 entropy 0.03264415264129639
epoch: 26, step: 95
	action: tensor([[ 0.4004,  0.2576,  0.0511, -0.0611, -0.0605,  0.2586,  0.1365]],
       dtype=torch.float64)
	q_value: tensor([[-6.4223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7743804018280569, distance: 0.5435570137153709 entropy 0.03264415264129639
epoch: 26, step: 96
	action: tensor([[ 0.7029, -0.1080, -0.4329, -0.2537, -0.2179,  0.3500, -0.2491]],
       dtype=torch.float64)
	q_value: tensor([[-6.1576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5866629005795485, distance: 0.735713774100525 entropy 0.03264415264129639
epoch: 26, step: 97
	action: tensor([[ 0.4215,  0.0210,  0.0582, -0.0245, -0.0883, -0.0124, -0.0583]],
       dtype=torch.float64)
	q_value: tensor([[-7.2646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6295957711312743, distance: 0.6964575671410504 entropy 0.03264415264129639
epoch: 26, step: 98
	action: tensor([[ 0.1018,  0.4461, -0.4668, -0.0257, -0.0009, -0.0798, -0.1242]],
       dtype=torch.float64)
	q_value: tensor([[-5.6851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6543845311464025, distance: 0.6727493384003999 entropy 0.03264415264129639
epoch: 26, step: 99
	action: tensor([[ 0.4095,  0.4305, -0.5216, -0.4237,  0.3666,  0.3388, -0.3454]],
       dtype=torch.float64)
	q_value: tensor([[-6.2455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8539999608930064, distance: 0.43725338406272957 entropy 0.03264415264129639
epoch: 26, step: 100
	action: tensor([[ 0.4623,  0.1929, -0.2341, -0.2321,  0.1077,  0.3987, -0.1329]],
       dtype=torch.float64)
	q_value: tensor([[-7.2761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7824479393055621, distance: 0.5337504988790158 entropy 0.03264415264129639
epoch: 26, step: 101
	action: tensor([[ 0.0048,  0.0870,  0.1137, -0.1523,  0.2547, -0.2868, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[-6.1567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19936373409145847, distance: 1.0239395600632817 entropy 0.03264415264129639
epoch: 26, step: 102
	action: tensor([[ 0.0622, -0.2513,  0.0587,  0.0704,  0.1615,  0.1298,  0.1773]],
       dtype=torch.float64)
	q_value: tensor([[-5.4137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21999269225916251, distance: 1.0106622143710235 entropy 0.03264415264129639
epoch: 26, step: 103
	action: tensor([[ 0.2283,  0.0331,  0.1423, -0.3461, -0.2788, -0.3426, -0.2144]],
       dtype=torch.float64)
	q_value: tensor([[-5.2038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2487222345373018, distance: 0.9918750390019527 entropy 0.03264415264129639
epoch: 26, step: 104
	action: tensor([[ 0.3085, -0.0932,  0.2054,  0.0321, -0.4244,  0.4288, -0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-6.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5623672276601664, distance: 0.7570274226457807 entropy 0.03264415264129639
epoch: 26, step: 105
	action: tensor([[-0.1771,  0.6866,  0.3925,  0.0983,  0.0184,  0.2129, -0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-6.7937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5361818631845907, distance: 0.7793464415060448 entropy 0.03264415264129639
epoch: 26, step: 106
	action: tensor([[ 0.1428,  0.0740,  0.0713, -0.0244, -0.4801,  0.1624, -0.4573]],
       dtype=torch.float64)
	q_value: tensor([[-6.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4818867088122518, distance: 0.8236999886031413 entropy 0.03264415264129639
epoch: 26, step: 107
	action: tensor([[-0.0291,  0.1363,  0.0848, -0.2792,  0.1466,  0.2412, -0.1287]],
       dtype=torch.float64)
	q_value: tensor([[-6.6755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30007445542345923, distance: 0.9573761742476269 entropy 0.03264415264129639
epoch: 26, step: 108
	action: tensor([[ 0.1416,  0.3745, -0.3760,  0.0636,  0.4125, -0.2887, -0.4112]],
       dtype=torch.float64)
	q_value: tensor([[-5.2160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6176746164727092, distance: 0.7075762702628536 entropy 0.03264415264129639
epoch: 26, step: 109
	action: tensor([[ 0.3444,  0.4871,  0.4066,  0.2918, -0.2104,  0.0513, -0.2115]],
       dtype=torch.float64)
	q_value: tensor([[-6.1622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8835060768149907, distance: 0.39057849616310386 entropy 0.03264415264129639
epoch: 26, step: 110
	action: tensor([[ 0.1239,  0.3282,  0.2638, -0.0823,  0.2513,  0.2070, -0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-7.2839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6135253132399681, distance: 0.7114055028309821 entropy 0.03264415264129639
epoch: 26, step: 111
	action: tensor([[ 0.3186, -0.1867,  0.2277,  0.1956,  0.1482,  0.0094, -0.3394]],
       dtype=torch.float64)
	q_value: tensor([[-5.6429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5100662499834747, distance: 0.8009868239781148 entropy 0.03264415264129639
epoch: 26, step: 112
	action: tensor([[-0.1302,  0.1816, -0.0522,  0.0284, -0.0623,  0.4214, -0.4818]],
       dtype=torch.float64)
	q_value: tensor([[-5.5145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3639495874173688, distance: 0.9126462267784257 entropy 0.03264415264129639
epoch: 26, step: 113
	action: tensor([[ 0.3609,  0.1779,  0.0720, -0.3318,  0.2583, -0.0745,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[-5.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5289701985024389, distance: 0.7853818957823474 entropy 0.03264415264129639
epoch: 26, step: 114
	action: tensor([[ 0.2369,  0.3814, -0.1143,  0.4355, -0.0642, -0.1675, -0.1908]],
       dtype=torch.float64)
	q_value: tensor([[-5.7555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7582822213787127, distance: 0.5626145902286092 entropy 0.03264415264129639
epoch: 26, step: 115
	action: tensor([[ 0.5125, -0.1935, -0.3670, -0.1018,  0.2537, -0.0072,  0.1454]],
       dtype=torch.float64)
	q_value: tensor([[-6.2146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4472082606356358, distance: 0.8508195598812188 entropy 0.03264415264129639
epoch: 26, step: 116
	action: tensor([[-0.0211,  0.0181, -0.3241, -0.0286,  0.2116,  0.1127, -0.1609]],
       dtype=torch.float64)
	q_value: tensor([[-5.9870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3116923644820211, distance: 0.9493972882454526 entropy 0.03264415264129639
epoch: 26, step: 117
	action: tensor([[ 0.1865, -0.3441, -0.0458, -0.1740,  0.1415,  0.6379,  0.4647]],
       dtype=torch.float64)
	q_value: tensor([[-4.7855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2807582767396706, distance: 0.9704968491350862 entropy 0.03264415264129639
epoch: 26, step: 118
	action: tensor([[-0.5586, -0.1697,  0.2764,  0.1239, -0.1926, -0.0183,  0.0388]],
       dtype=torch.float64)
	q_value: tensor([[-6.5118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41803877666895106, distance: 1.3627015400254199 entropy 0.03264415264129639
epoch: 26, step: 119
	action: tensor([[-0.0993,  0.0571, -0.7001,  0.1192, -0.0585,  0.0258,  0.3338]],
       dtype=torch.float64)
	q_value: tensor([[-6.0753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20268707754563764, distance: 1.0218122261002456 entropy 0.03264415264129639
epoch: 26, step: 120
	action: tensor([[ 0.0979,  0.0307, -0.2298,  0.1508,  0.4697, -0.2170,  0.0662]],
       dtype=torch.float64)
	q_value: tensor([[-6.5636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3786222636475449, distance: 0.9020581560717524 entropy 0.03264415264129639
epoch: 26, step: 121
	action: tensor([[ 0.4568, -0.0532, -0.0311,  0.2585,  0.0396, -0.1780, -0.0490]],
       dtype=torch.float64)
	q_value: tensor([[-5.5950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6646332623369058, distance: 0.6626995663155586 entropy 0.03264415264129639
epoch: 26, step: 122
	action: tensor([[-0.0402,  0.7646, -0.3863, -0.3117, -0.0362, -0.0557,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-5.7082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6278939993405698, distance: 0.6980556234584058 entropy 0.03264415264129639
epoch: 26, step: 123
	action: tensor([[ 0.5971,  0.3334, -0.1077, -0.2019, -0.1404,  0.2910, -0.0433]],
       dtype=torch.float64)
	q_value: tensor([[-7.8010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8629467116311204, distance: 0.42364435450458554 entropy 0.03264415264129639
epoch: 26, step: 124
	action: tensor([[ 0.4907, -0.5739,  0.1558, -0.2501,  0.5383, -0.0537, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-7.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1010865343870313, distance: 1.200790985741334 entropy 0.03264415264129639
epoch: 26, step: 125
	action: tensor([[-0.0214,  0.5999, -0.0887, -0.1436,  0.0935,  0.0109,  0.3001]],
       dtype=torch.float64)
	q_value: tensor([[-6.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.573269870838136, distance: 0.7475381249288928 entropy 0.03264415264129639
epoch: 26, step: 126
	action: tensor([[ 0.0452,  0.6261, -0.4625,  0.0908,  0.0103,  0.1297, -0.0327]],
       dtype=torch.float64)
	q_value: tensor([[-6.6417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6380128131697846, distance: 0.6884989660061345 entropy 0.03264415264129639
epoch: 26, step: 127
	action: tensor([[ 0.2964,  0.0178,  0.0065, -0.2983, -0.3698,  0.4781, -0.0979]],
       dtype=torch.float64)
	q_value: tensor([[-6.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5128586464614558, distance: 0.7987009344045641 entropy 0.03264415264129639
LOSS epoch 26 actor 22.093337465419953 critic 52.425975059117704 
epoch: 27, step: 0
	action: tensor([[ 0.4261, -0.0951, -0.3599,  0.3283, -0.2904, -0.2114, -0.6982]],
       dtype=torch.float64)
	q_value: tensor([[-6.4343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.663399527402325, distance: 0.6639174048174923 entropy 0.03264415264129639
epoch: 27, step: 1
	action: tensor([[ 0.0414,  0.0434, -0.2583, -0.1150, -0.0626,  0.0831, -0.1547]],
       dtype=torch.float64)
	q_value: tensor([[-6.3109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3541212469497895, distance: 0.9196703652565893 entropy 0.03264415264129639
epoch: 27, step: 2
	action: tensor([[-0.0450, -0.0379, -0.0257, -0.1922,  0.0237, -0.1991, -0.4222]],
       dtype=torch.float64)
	q_value: tensor([[-4.5724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0944864832451432, distance: 1.0889405290246483 entropy 0.03264415264129639
epoch: 27, step: 3
	action: tensor([[-0.1326,  0.1818, -0.5342, -0.4745, -0.2469,  0.1379, -0.0474]],
       dtype=torch.float64)
	q_value: tensor([[-4.7165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26002805031850673, distance: 0.984383491567995 entropy 0.03264415264129639
epoch: 27, step: 4
	action: tensor([[ 0.1879,  0.5056, -0.8171, -0.0852, -0.0390,  0.0811, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-6.7191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7242102091184728, distance: 0.6009603041310441 entropy 0.03264415264129639
epoch: 27, step: 5
	action: tensor([[ 0.1396,  0.0586, -0.3190,  0.0433,  0.3192,  0.0796,  0.0561]],
       dtype=torch.float64)
	q_value: tensor([[-7.2220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4990693596659893, distance: 0.8099262815044733 entropy 0.03264415264129639
epoch: 27, step: 6
	action: tensor([[ 0.1049, -0.1638, -0.0588,  0.2862, -0.1722,  0.2574,  0.1464]],
       dtype=torch.float64)
	q_value: tensor([[-4.6813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46316461336199133, distance: 0.8384501775730454 entropy 0.03264415264129639
epoch: 27, step: 7
	action: tensor([[ 0.0717,  0.0400, -0.4081,  0.0901, -0.2223,  0.3304,  0.1547]],
       dtype=torch.float64)
	q_value: tensor([[-4.8836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43358478497936037, distance: 0.8612399135554698 entropy 0.03264415264129639
epoch: 27, step: 8
	action: tensor([[-0.1429,  0.0271, -0.0720, -0.2466, -0.2254,  0.3353, -0.7502]],
       dtype=torch.float64)
	q_value: tensor([[-5.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12214814897611792, distance: 1.072179027454827 entropy 0.03264415264129639
epoch: 27, step: 9
	action: tensor([[-0.1023, -0.2220, -0.7177, -0.3933, -0.0891, -0.1398,  0.0699]],
       dtype=torch.float64)
	q_value: tensor([[-6.1313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04644241275298988, distance: 1.1174552913425215 entropy 0.03264415264129639
epoch: 27, step: 10
	action: tensor([[ 0.3516,  0.0027, -0.2448,  0.0268,  0.1322, -0.1231, -0.1183]],
       dtype=torch.float64)
	q_value: tensor([[-6.1785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.570035544448163, distance: 0.7503656945218051 entropy 0.03264415264129639
epoch: 27, step: 11
	action: tensor([[ 0.4413, -0.1959,  0.1047,  0.0664, -0.0959,  0.2078, -0.2803]],
       dtype=torch.float64)
	q_value: tensor([[-4.7672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5708997256827593, distance: 0.7496112390069016 entropy 0.03264415264129639
epoch: 27, step: 12
	action: tensor([[ 0.3296,  0.2411, -0.1669, -0.3490,  0.0229, -0.1815, -0.1593]],
       dtype=torch.float64)
	q_value: tensor([[-5.2085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5915340561308566, distance: 0.7313657521454863 entropy 0.03264415264129639
epoch: 27, step: 13
	action: tensor([[ 0.2557,  0.0141, -0.0064, -0.1219,  0.2977, -0.3493, -0.3293]],
       dtype=torch.float64)
	q_value: tensor([[-5.5962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38006300231736734, distance: 0.9010117842729272 entropy 0.03264415264129639
epoch: 27, step: 14
	action: tensor([[ 0.6532, -0.7438, -0.5141,  0.4006,  0.2072, -0.1473, -0.2655]],
       dtype=torch.float64)
	q_value: tensor([[-4.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15227080388187897, distance: 1.053623051308811 entropy 0.03264415264129639
epoch: 27, step: 15
	action: tensor([[-0.2262, -0.1258, -0.2293,  0.1878,  0.3073,  0.2147,  0.2162]],
       dtype=torch.float64)
	q_value: tensor([[-6.2584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07673162316085957, distance: 1.0995644078885956 entropy 0.03264415264129639
epoch: 27, step: 16
	action: tensor([[ 0.1662,  0.4697, -0.3775,  0.0899,  0.1680, -0.2637, -0.0479]],
       dtype=torch.float64)
	q_value: tensor([[-4.9268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6699374067519538, distance: 0.6574380670412929 entropy 0.03264415264129639
epoch: 27, step: 17
	action: tensor([[-0.1096,  0.3591, -0.4464, -0.1663,  0.0360,  0.2060, -0.2224]],
       dtype=torch.float64)
	q_value: tensor([[-5.8017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4127433340889394, distance: 0.8769415967314942 entropy 0.03264415264129639
epoch: 27, step: 18
	action: tensor([[ 0.4099,  0.0581, -0.1334, -0.0033, -0.0418,  0.3297, -0.0990]],
       dtype=torch.float64)
	q_value: tensor([[-5.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7114756172622558, distance: 0.6146784020115044 entropy 0.03264415264129639
epoch: 27, step: 19
	action: tensor([[ 0.3005,  0.1380, -0.2186, -0.0664,  0.5045,  0.0816, -0.1710]],
       dtype=torch.float64)
	q_value: tensor([[-5.0593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6368549210356749, distance: 0.6895992412018406 entropy 0.03264415264129639
epoch: 27, step: 20
	action: tensor([[-0.2931, -0.0425, -0.0287,  0.0325, -0.0033,  0.1680, -0.0928]],
       dtype=torch.float64)
	q_value: tensor([[-4.7336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0013671760610109995, distance: 1.1451262468456298 entropy 0.03264415264129639
epoch: 27, step: 21
	action: tensor([[ 0.0665, -0.0421,  0.0910, -0.1354, -0.3850,  0.5148, -0.3141]],
       dtype=torch.float64)
	q_value: tensor([[-4.6203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3497978264306236, distance: 0.9227433026990393 entropy 0.03264415264129639
epoch: 27, step: 22
	action: tensor([[-0.0135, -0.0878,  0.1182, -0.1549, -0.0317,  0.0757, -0.3408]],
       dtype=torch.float64)
	q_value: tensor([[-6.1401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15426537290296372, distance: 1.0523828194635052 entropy 0.03264415264129639
epoch: 27, step: 23
	action: tensor([[-0.1176,  0.0862, -0.5168, -0.0142, -0.2098,  0.2203, -0.4428]],
       dtype=torch.float64)
	q_value: tensor([[-4.6014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23395129604289355, distance: 1.001578248607328 entropy 0.03264415264129639
epoch: 27, step: 24
	action: tensor([[ 0.5109,  0.1505, -0.2726,  0.1058, -0.0797, -0.1595, -0.1817]],
       dtype=torch.float64)
	q_value: tensor([[-5.4355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7822186881751332, distance: 0.5340316515512686 entropy 0.03264415264129639
epoch: 27, step: 25
	action: tensor([[ 0.0955, -0.0429,  0.0944, -0.0092, -0.1194, -0.0330, -0.4103]],
       dtype=torch.float64)
	q_value: tensor([[-5.6148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3523503494780781, distance: 0.9209302976162594 entropy 0.03264415264129639
epoch: 27, step: 26
	action: tensor([[-0.1074, -0.5260, -0.4782, -0.4439,  0.2485,  0.0642, -0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-4.8286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30865980963494755, distance: 1.3090917193751934 entropy 0.03264415264129639
epoch: 27, step: 27
	action: tensor([[ 0.1622,  0.0848, -0.1161, -0.2129,  0.0888,  0.1386,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-5.1591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44902538136419134, distance: 0.8494200142958994 entropy 0.03264415264129639
epoch: 27, step: 28
	action: tensor([[ 0.2232,  0.2918,  0.2095, -0.3474,  0.0612, -0.2432, -0.3462]],
       dtype=torch.float64)
	q_value: tensor([[-4.6936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4700776805992436, distance: 0.8330341375898026 entropy 0.03264415264129639
epoch: 27, step: 29
	action: tensor([[-0.2804,  0.1205,  0.0915, -0.0669,  0.0167, -0.1791,  0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-5.6855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005080157498250637, distance: 1.1472472962141016 entropy 0.03264415264129639
epoch: 27, step: 30
	action: tensor([[-0.1463,  0.3297, -0.4246,  0.2802,  0.1833, -0.2355, -0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-4.9846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3477384774436221, distance: 0.9242034241056236 entropy 0.03264415264129639
epoch: 27, step: 31
	action: tensor([[ 0.1476, -0.1322, -0.0356,  0.3706, -0.6358, -0.1837,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[-5.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49720365668303346, distance: 0.811433154288575 entropy 0.03264415264129639
epoch: 27, step: 32
	action: tensor([[ 0.1405,  0.4977, -0.2493,  0.5299, -0.1244,  0.2746, -0.1104]],
       dtype=torch.float64)
	q_value: tensor([[-5.7554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6444416872552108, distance: 0.6823577234498219 entropy 0.03264415264129639
epoch: 27, step: 33
	action: tensor([[-0.0504, -0.3938, -0.0566, -0.1698, -0.0763,  0.7939, -0.3910]],
       dtype=torch.float64)
	q_value: tensor([[-5.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0835913577975328, distance: 1.0954719994237074 entropy 0.03264415264129639
epoch: 27, step: 34
	action: tensor([[-0.1159, -0.2070,  0.2466, -0.0111, -0.1996, -0.1165,  0.0492]],
       dtype=torch.float64)
	q_value: tensor([[-5.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02782288264852628, distance: 1.1601545146605456 entropy 0.03264415264129639
epoch: 27, step: 35
	action: tensor([[ 0.0259,  0.2894, -0.1220, -0.1695,  0.2386,  0.5869, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-4.9664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5800376028149183, distance: 0.7415866384437936 entropy 0.03264415264129639
epoch: 27, step: 36
	action: tensor([[-0.1080,  0.2941, -0.1690,  0.0798,  0.1228,  0.2459, -0.5861]],
       dtype=torch.float64)
	q_value: tensor([[-5.5465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4468394127551554, distance: 0.8511033653699692 entropy 0.03264415264129639
epoch: 27, step: 37
	action: tensor([[-0.1627,  0.1825,  0.1726,  0.1642,  0.1457,  0.2651, -0.3636]],
       dtype=torch.float64)
	q_value: tensor([[-5.2036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41592560693490155, distance: 0.8745623489141744 entropy 0.03264415264129639
epoch: 27, step: 38
	action: tensor([[-0.1297, -0.3717, -0.7346,  0.0141, -0.0279,  0.4595, -0.0514]],
       dtype=torch.float64)
	q_value: tensor([[-4.9531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11365725524459247, distance: 1.207626038650856 entropy 0.03264415264129639
epoch: 27, step: 39
	action: tensor([[ 0.3539,  0.0923, -0.0805, -0.0089,  0.4410,  0.2013, -0.6632]],
       dtype=torch.float64)
	q_value: tensor([[-5.5386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6708736599480858, distance: 0.656504962797968 entropy 0.03264415264129639
epoch: 27, step: 40
	action: tensor([[-0.0521, -0.2505, -0.1596, -0.2566, -0.0118, -0.2482, -0.5320]],
       dtype=torch.float64)
	q_value: tensor([[-5.0735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10356756592691352, distance: 1.202143070194794 entropy 0.03264415264129639
epoch: 27, step: 41
	action: tensor([[-0.0250,  0.0245, -0.0854, -0.1069,  0.0849,  0.0211,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-4.8743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24509934192295335, distance: 0.9942637260786478 entropy 0.03264415264129639
epoch: 27, step: 42
	action: tensor([[ 0.3391, -0.1299, -0.5066,  0.0849,  0.1610,  0.4702, -0.3315]],
       dtype=torch.float64)
	q_value: tensor([[-4.5549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5772029120038176, distance: 0.7440852352525323 entropy 0.03264415264129639
epoch: 27, step: 43
	action: tensor([[ 0.2386,  0.1965,  0.3771, -0.7286,  0.2149,  0.0544,  0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-5.1446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23635618537993952, distance: 1.0000048641522912 entropy 0.03264415264129639
epoch: 27, step: 44
	action: tensor([[ 0.4205,  0.0099, -0.1758, -0.0024, -0.4690,  0.2747,  0.2105]],
       dtype=torch.float64)
	q_value: tensor([[-6.1871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6475372111494088, distance: 0.6793808950996644 entropy 0.03264415264129639
epoch: 27, step: 45
	action: tensor([[ 0.7684, -0.0611, -0.2627, -0.0884,  0.0118, -0.6179, -0.1414]],
       dtype=torch.float64)
	q_value: tensor([[-6.1976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5070123520846619, distance: 0.8034793364370202 entropy 0.03264415264129639
epoch: 27, step: 46
	action: tensor([[ 0.1111,  0.2929,  0.2004,  0.1837, -0.0354, -0.2368, -0.1125]],
       dtype=torch.float64)
	q_value: tensor([[-7.2943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6223257627161076, distance: 0.7032591212831015 entropy 0.03264415264129639
epoch: 27, step: 47
	action: tensor([[ 0.3977, -0.3002, -0.2974,  0.2062, -0.4224,  0.3839,  0.4163]],
       dtype=torch.float64)
	q_value: tensor([[-5.3352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.507947078560469, distance: 0.8027172587787074 entropy 0.03264415264129639
epoch: 27, step: 48
	action: tensor([[ 0.2531, -0.4499,  0.0861,  0.3967, -0.0923, -0.0549, -0.1844]],
       dtype=torch.float64)
	q_value: tensor([[-6.0746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37306258587756524, distance: 0.9060846798937414 entropy 0.03264415264129639
epoch: 27, step: 49
	action: tensor([[-0.1998,  0.0860, -0.3191,  0.3348, -0.3267,  0.1350,  0.1524]],
       dtype=torch.float64)
	q_value: tensor([[-5.0034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22320682861316932, distance: 1.0085777727059044 entropy 0.03264415264129639
epoch: 27, step: 50
	action: tensor([[ 0.1426, -0.2217, -0.4241,  0.0485, -0.3380, -0.0226,  0.2619]],
       dtype=torch.float64)
	q_value: tensor([[-5.4788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2727219965008252, distance: 0.9759035988199052 entropy 0.03264415264129639
epoch: 27, step: 51
	action: tensor([[-0.1465,  0.7138, -0.3692, -0.3661, -0.3467,  0.0792,  0.0971]],
       dtype=torch.float64)
	q_value: tensor([[-5.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5251070136400984, distance: 0.7885960021672928 entropy 0.03264415264129639
epoch: 27, step: 52
	action: tensor([[-0.3639,  0.3833, -0.6857, -0.2587,  0.0403,  0.0082, -0.3434]],
       dtype=torch.float64)
	q_value: tensor([[-8.0940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18734881449474716, distance: 1.0315939343991403 entropy 0.03264415264129639
epoch: 27, step: 53
	action: tensor([[ 0.1532,  0.1507, -0.2166, -0.1800, -0.0011,  0.3743,  0.2679]],
       dtype=torch.float64)
	q_value: tensor([[-6.6099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5383920985724251, distance: 0.7774873118622551 entropy 0.03264415264129639
epoch: 27, step: 54
	action: tensor([[ 0.3879, -0.1786, -0.5017,  0.1212, -0.0933, -0.2034,  0.3153]],
       dtype=torch.float64)
	q_value: tensor([[-5.6962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45428801433736643, distance: 0.8453536642769917 entropy 0.03264415264129639
epoch: 27, step: 55
	action: tensor([[ 0.2131, -0.2178, -0.3132, -0.3152, -0.0953,  0.4902, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-5.9779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28368687401421266, distance: 0.9685190067481848 entropy 0.03264415264129639
epoch: 27, step: 56
	action: tensor([[ 0.1154,  0.3912, -0.2907,  0.3559, -0.1339,  0.2284, -0.2556]],
       dtype=torch.float64)
	q_value: tensor([[-5.4711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6368535243455302, distance: 0.689600567332351 entropy 0.03264415264129639
epoch: 27, step: 57
	action: tensor([[-0.1233, -0.0158, -0.6291, -0.0938, -0.1698,  0.0651, -0.2294]],
       dtype=torch.float64)
	q_value: tensor([[-5.4576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1571099201224727, distance: 1.0506115345351925 entropy 0.03264415264129639
epoch: 27, step: 58
	action: tensor([[ 0.0737, -0.0320, -0.4628, -0.2542,  0.3479,  0.0947,  0.2801]],
       dtype=torch.float64)
	q_value: tensor([[-5.2962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30348049039404834, distance: 0.9550439022662367 entropy 0.03264415264129639
epoch: 27, step: 59
	action: tensor([[-0.0368,  0.0694, -0.2286, -0.1442,  0.5844,  0.0323, -0.0950]],
       dtype=torch.float64)
	q_value: tensor([[-5.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2717132884542224, distance: 0.9765801356546918 entropy 0.03264415264129639
epoch: 27, step: 60
	action: tensor([[-0.3156,  0.4974, -0.1022,  0.5241, -0.3558,  0.0290, -0.2690]],
       dtype=torch.float64)
	q_value: tensor([[-4.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.379369984753708, distance: 0.9015152568717157 entropy 0.03264415264129639
epoch: 27, step: 61
	action: tensor([[ 0.0181, -0.3472,  0.1472, -0.2542,  0.0946, -0.0940,  0.0903]],
       dtype=torch.float64)
	q_value: tensor([[-6.2258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18319132255796888, distance: 1.2447558649449406 entropy 0.03264415264129639
epoch: 27, step: 62
	action: tensor([[0.4019, 0.0811, 0.1462, 0.2771, 0.1003, 0.1125, 0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-4.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8030617139326566, distance: 0.5078339956675983 entropy 0.03264415264129639
epoch: 27, step: 63
	action: tensor([[ 0.2163, -0.2704, -0.1012, -0.5199,  0.2258,  0.3962, -0.2142]],
       dtype=torch.float64)
	q_value: tensor([[-5.2453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09592610258900092, distance: 1.0880745654593502 entropy 0.03264415264129639
epoch: 27, step: 64
	action: tensor([[-0.0765, -0.2061, -0.2802,  0.0892,  0.2379, -0.2043, -0.1274]],
       dtype=torch.float64)
	q_value: tensor([[-5.0707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041350350689312365, distance: 1.1204349621769967 entropy 0.03264415264129639
epoch: 27, step: 65
	action: tensor([[ 0.4345, -0.2302, -0.1471, -0.0342,  0.2961, -0.4250, -0.2269]],
       dtype=torch.float64)
	q_value: tensor([[-4.5838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2878376150553932, distance: 0.9657088445121791 entropy 0.03264415264129639
epoch: 27, step: 66
	action: tensor([[-0.3317,  0.2195, -0.2444,  0.3978, -0.3512, -0.2120, -0.0887]],
       dtype=torch.float64)
	q_value: tensor([[-5.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18646028735980302, distance: 1.0321577364405474 entropy 0.03264415264129639
epoch: 27, step: 67
	action: tensor([[ 0.0080,  0.1390, -0.2627,  0.1211, -0.5181,  0.0720, -0.5650]],
       dtype=torch.float64)
	q_value: tensor([[-5.6373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4288924247713123, distance: 0.864799945616717 entropy 0.03264415264129639
epoch: 27, step: 68
	action: tensor([[ 0.3792, -0.2481, -0.8102,  0.0180,  0.1045, -0.1276, -0.5511]],
       dtype=torch.float64)
	q_value: tensor([[-6.0473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3969023359408651, distance: 0.8886904566381663 entropy 0.03264415264129639
epoch: 27, step: 69
	action: tensor([[-0.0083,  0.0210, -0.2541, -0.1517, -0.1154, -0.0335, -0.3750]],
       dtype=torch.float64)
	q_value: tensor([[-5.7978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2620639613225133, distance: 0.9830283742674268 entropy 0.03264415264129639
epoch: 27, step: 70
	action: tensor([[ 0.0263, -0.1058,  0.2166,  0.1065, -0.2009, -0.0297, -0.2881]],
       dtype=torch.float64)
	q_value: tensor([[-4.7787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30654867240368666, distance: 0.9529380871696291 entropy 0.03264415264129639
epoch: 27, step: 71
	action: tensor([[ 0.0402, -0.0413,  0.1216, -0.0664,  0.0458,  0.3795, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-4.8924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37943328847090285, distance: 0.9014692788165186 entropy 0.03264415264129639
epoch: 27, step: 72
	action: tensor([[ 0.3506, -0.4760, -0.4644, -0.2018,  0.0427, -0.1974,  0.3963]],
       dtype=torch.float64)
	q_value: tensor([[-4.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00889755877049514, distance: 1.1494239150037495 entropy 0.03264415264129639
epoch: 27, step: 73
	action: tensor([[ 0.1419, -0.1878, -0.2469, -0.2632, -0.6614,  0.0186, -0.1681]],
       dtype=torch.float64)
	q_value: tensor([[-6.0558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14259176198956758, distance: 1.0596209084829307 entropy 0.03264415264129639
epoch: 27, step: 74
	action: tensor([[-0.1458,  0.2363, -0.3233, -0.1473, -0.3764,  0.4356, -0.3637]],
       dtype=torch.float64)
	q_value: tensor([[-6.4296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22617077790279994, distance: 1.0066517578306953 entropy 0.03264415264129639
epoch: 27, step: 75
	action: tensor([[ 0.3578, -0.1325, -0.4746, -0.3513,  0.4185,  0.1189,  0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-6.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3731944902881559, distance: 0.9059893570991263 entropy 0.03264415264129639
epoch: 27, step: 76
	action: tensor([[-0.1145,  0.3125, -0.3590,  0.0027,  0.0900, -0.1478, -0.0738]],
       dtype=torch.float64)
	q_value: tensor([[-5.6465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3737571390426885, distance: 0.9055826374537728 entropy 0.03264415264129639
epoch: 27, step: 77
	action: tensor([[ 0.2606, -0.2058, -0.6040, -0.1548, -0.1126,  0.0788, -0.1040]],
       dtype=torch.float64)
	q_value: tensor([[-5.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3385735965515553, distance: 0.9306737336581381 entropy 0.03264415264129639
epoch: 27, step: 78
	action: tensor([[ 0.1044,  0.1470, -0.1343, -0.1000,  0.3444,  0.3167, -0.2213]],
       dtype=torch.float64)
	q_value: tensor([[-5.1968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5463431851307221, distance: 0.7707622092760885 entropy 0.03264415264129639
epoch: 27, step: 79
	action: tensor([[ 0.2296,  0.1998,  0.0130, -0.0571, -0.3189, -0.2945,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-4.4535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5783718455844877, distance: 0.7430559137840396 entropy 0.03264415264129639
epoch: 27, step: 80
	action: tensor([[ 0.0960,  0.0713, -0.0563, -0.2092,  0.0282,  0.0472,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-5.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3618346887842234, distance: 0.9141622642002123 entropy 0.03264415264129639
epoch: 27, step: 81
	action: tensor([[ 0.0518, -0.3142, -0.5869, -0.0268, -0.2216,  0.2632,  0.5272]],
       dtype=torch.float64)
	q_value: tensor([[-4.8551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0922606796869393, distance: 1.0902780463822064 entropy 0.03264415264129639
epoch: 27, step: 82
	action: tensor([[-0.0460,  0.4627, -0.1863,  0.1785,  0.1705, -0.1134, -0.5128]],
       dtype=torch.float64)
	q_value: tensor([[-6.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5240457333905764, distance: 0.7894766787875257 entropy 0.03264415264129639
epoch: 27, step: 83
	action: tensor([[-0.1249, -0.0120, -0.1525, -0.2279,  0.1563, -0.2230, -0.4824]],
       dtype=torch.float64)
	q_value: tensor([[-5.3945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031463931410248724, distance: 1.1261975872515122 entropy 0.03264415264129639
epoch: 27, step: 84
	action: tensor([[ 6.8862e-01,  1.9899e-01, -2.6802e-01,  2.3068e-02,  1.0574e-01,
         -3.9401e-04, -2.0368e-01]], dtype=torch.float64)
	q_value: tensor([[-4.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8496475142123323, distance: 0.44372306032878955 entropy 0.03264415264129639
epoch: 27, step: 85
	action: tensor([[ 0.8751,  0.2878, -0.2158, -0.2560,  0.1658,  0.1689, -0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-5.9495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8615194580945096, distance: 0.42584452742519596 entropy 0.03264415264129639
epoch: 27, step: 86
	action: tensor([[ 0.1518, -0.0107, -0.2954, -0.2740, -0.7357, -0.1279, -0.2140]],
       dtype=torch.float64)
	q_value: tensor([[-7.1940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35799496036839007, distance: 0.9169083165316594 entropy 0.03264415264129639
epoch: 27, step: 87
	action: tensor([[ 0.0150, -0.0092, -0.1943, -0.3884,  0.4321,  0.0042,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-7.0165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1602302891982007, distance: 1.0486650556862114 entropy 0.03264415264129639
epoch: 27, step: 88
	action: tensor([[-0.4063,  0.4714,  0.0730, -0.4007, -0.0150, -0.3385, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-4.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06419632281485277, distance: 1.1805042915814437 entropy 0.03264415264129639
epoch: 27, step: 89
	action: tensor([[-0.0550,  0.0299, -0.5403, -0.0869,  0.3513, -0.3774, -0.4944]],
       dtype=torch.float64)
	q_value: tensor([[-6.1496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17755506058031367, distance: 1.0377915007620095 entropy 0.03264415264129639
epoch: 27, step: 90
	action: tensor([[ 0.0091, -0.0614,  0.0133, -0.1240,  0.0229, -0.0221, -0.3361]],
       dtype=torch.float64)
	q_value: tensor([[-5.5301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17824292081474957, distance: 1.037357426215906 entropy 0.03264415264129639
epoch: 27, step: 91
	action: tensor([[ 0.3385, -0.0376, -0.4301,  0.0232,  0.0012, -0.0339, -0.2222]],
       dtype=torch.float64)
	q_value: tensor([[-4.4961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5295058312758578, distance: 0.7849352192206693 entropy 0.03264415264129639
epoch: 27, step: 92
	action: tensor([[-0.3791, -0.0581, -0.5728, -0.1677, -0.1971,  0.2302,  0.0768]],
       dtype=torch.float64)
	q_value: tensor([[-4.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18796645711940618, distance: 1.2472651344184678 entropy 0.03264415264129639
epoch: 27, step: 93
	action: tensor([[ 0.1980, -0.1953, -0.6321, -0.5792, -0.1899, -0.2894,  0.1863]],
       dtype=torch.float64)
	q_value: tensor([[-6.0004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2094718313413395, distance: 1.017455369550718 entropy 0.03264415264129639
epoch: 27, step: 94
	action: tensor([[ 0.1103,  0.2545, -0.0838,  0.2533,  0.0431,  0.0528, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[-7.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6466293627670427, distance: 0.68025528271686 entropy 0.03264415264129639
epoch: 27, step: 95
	action: tensor([[-0.0116, -0.3085, -0.5077,  0.1097, -0.0054, -0.0833, -0.1733]],
       dtype=torch.float64)
	q_value: tensor([[-4.9756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05690395502956358, distance: 1.111308548446367 entropy 0.03264415264129639
epoch: 27, step: 96
	action: tensor([[ 0.8517,  0.1357, -0.1329,  0.1196, -0.0783,  0.2665, -0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-4.7589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9151451714168988, distance: 0.33334578918886093 entropy 0.03264415264129639
epoch: 27, step: 97
	action: tensor([[-0.1100, -0.2289, -0.3877, -0.2651,  0.2050,  0.2812, -0.0842]],
       dtype=torch.float64)
	q_value: tensor([[-6.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.002828332054853999, distance: 1.1427248153439344 entropy 0.03264415264129639
epoch: 27, step: 98
	action: tensor([[ 0.4661,  0.4648, -0.0594, -0.4861, -0.1283, -0.2520,  0.2248]],
       dtype=torch.float64)
	q_value: tensor([[-4.6600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7212460168650259, distance: 0.6041812371804874 entropy 0.03264415264129639
epoch: 27, step: 99
	action: tensor([[-0.0407,  0.5580,  0.0349, -0.2382, -0.0271,  0.1734, -0.4728]],
       dtype=torch.float64)
	q_value: tensor([[-7.3098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5262702785451034, distance: 0.7876295649532156 entropy 0.03264415264129639
epoch: 27, step: 100
	action: tensor([[-0.0061, -0.1593, -0.6912,  0.1617, -0.2036,  0.1290, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-6.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17479895604798712, distance: 1.0395289238189256 entropy 0.03264415264129639
epoch: 27, step: 101
	action: tensor([[ 0.1369,  0.0557, -0.2342,  0.3257, -0.0930,  0.0353, -0.4386]],
       dtype=torch.float64)
	q_value: tensor([[-5.0627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5554275655100214, distance: 0.7630060099308767 entropy 0.03264415264129639
epoch: 27, step: 102
	action: tensor([[ 0.1085, -0.2894,  0.0465,  0.0969, -0.3652,  0.2298,  0.0790]],
       dtype=torch.float64)
	q_value: tensor([[-4.9171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2716151367267694, distance: 0.9766459406436222 entropy 0.03264415264129639
epoch: 27, step: 103
	action: tensor([[-0.0646,  0.2543, -0.2759, -0.2951, -0.0839, -0.2091, -0.2275]],
       dtype=torch.float64)
	q_value: tensor([[-5.0754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33226140386907643, distance: 0.9351040396074659 entropy 0.03264415264129639
epoch: 27, step: 104
	action: tensor([[-0.5117,  0.4155, -0.2105, -0.3408, -0.1221,  0.1431, -0.2329]],
       dtype=torch.float64)
	q_value: tensor([[-5.3834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07543928348613282, distance: 1.1867237701778446 entropy 0.03264415264129639
epoch: 27, step: 105
	action: tensor([[ 0.0788,  0.0520, -0.0220,  0.4034,  0.0974, -0.0738,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-6.4822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5314070938158046, distance: 0.7833476558213267 entropy 0.03264415264129639
epoch: 27, step: 106
	action: tensor([[ 0.3812,  0.1402, -0.3158, -0.0602, -0.2216,  0.1867, -0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-5.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6706164387602782, distance: 0.6567614509381399 entropy 0.03264415264129639
epoch: 27, step: 107
	action: tensor([[-0.0208,  0.1031, -0.0406, -0.3522, -0.2352, -0.0103, -0.4216]],
       dtype=torch.float64)
	q_value: tensor([[-5.4964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20846355816318296, distance: 1.0181040156696732 entropy 0.03264415264129639
epoch: 27, step: 108
	action: tensor([[-0.0662,  0.1934, -0.3880, -0.1269,  0.1206,  0.0762,  0.2892]],
       dtype=torch.float64)
	q_value: tensor([[-5.5376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35453996672732324, distance: 0.9193722082099516 entropy 0.03264415264129639
epoch: 27, step: 109
	action: tensor([[ 0.2893, -0.0128, -0.4087, -0.2838, -0.7778, -0.1307,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[-5.5646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4660159799609799, distance: 0.8362205258956339 entropy 0.03264415264129639
epoch: 27, step: 110
	action: tensor([[-0.3172,  0.0189, -0.5838,  0.0712, -0.0287,  0.3135,  0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-7.4761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028491785864588115, distance: 1.1605319653286683 entropy 0.03264415264129639
epoch: 27, step: 111
	action: tensor([[ 0.2657,  0.4491, -0.2606, -0.0534,  0.0304,  0.0151, -0.3930]],
       dtype=torch.float64)
	q_value: tensor([[-5.5979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.738392920307982, distance: 0.5853039293324336 entropy 0.03264415264129639
epoch: 27, step: 112
	action: tensor([[ 0.3573,  0.1396, -0.3074,  0.1756,  0.1480,  0.2629,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[-5.5459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7445423434125504, distance: 0.5783838465539485 entropy 0.03264415264129639
epoch: 27, step: 113
	action: tensor([[-0.3135, -0.2301, -0.3704, -0.3457,  0.0301,  0.4086, -0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-4.9603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23185370007210637, distance: 1.2700951552758637 entropy 0.03264415264129639
epoch: 27, step: 114
	action: tensor([[ 0.3822,  0.4534, -0.0445, -0.2965, -0.1764,  0.2148, -0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-5.4820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.769994958888063, distance: 0.548814239008396 entropy 0.03264415264129639
epoch: 27, step: 115
	action: tensor([[-0.0557, -0.0072, -0.3403,  0.0910, -0.1110,  0.3934, -0.7411]],
       dtype=torch.float64)
	q_value: tensor([[-6.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30290563456452824, distance: 0.9554379323796793 entropy 0.03264415264129639
epoch: 27, step: 116
	action: tensor([[ 0.4618,  0.2508, -0.3211,  0.0671, -0.2064,  0.2124, -0.3067]],
       dtype=torch.float64)
	q_value: tensor([[-5.5728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8301131152100913, distance: 0.47166822463054214 entropy 0.03264415264129639
epoch: 27, step: 117
	action: tensor([[ 0.2728, -0.1340,  0.3236,  0.0168,  0.0303, -0.2899, -0.1162]],
       dtype=torch.float64)
	q_value: tensor([[-5.8236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34461605983408017, distance: 0.9264128934839546 entropy 0.03264415264129639
epoch: 27, step: 118
	action: tensor([[ 0.4222,  0.1319, -0.3847, -0.1088, -0.2206, -0.1017,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-5.1739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6668254165948446, distance: 0.6605301182254346 entropy 0.03264415264129639
epoch: 27, step: 119
	action: tensor([[-0.0136, -0.0782, -0.5882,  0.2907, -0.1296,  0.0753, -0.2810]],
       dtype=torch.float64)
	q_value: tensor([[-5.8999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25842002646108353, distance: 0.9854524869004561 entropy 0.03264415264129639
epoch: 27, step: 120
	action: tensor([[ 0.6679,  0.7841,  0.2198, -0.2683,  0.2390, -0.0573, -0.6181]],
       dtype=torch.float64)
	q_value: tensor([[-4.8962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9509906602659425, distance: 0.25333553918715657 entropy 0.03264415264129639
epoch: 27, step: 121
	action: tensor([[ 0.2735,  0.1990, -0.4741,  0.0169,  0.0539, -0.3066,  0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-7.9864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6412640189524264, distance: 0.6854000984334474 entropy 0.03264415264129639
epoch: 27, step: 122
	action: tensor([[-0.0271,  0.3114, -0.1359, -0.5066,  0.4474,  0.6102,  0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-5.8777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49282859349369956, distance: 0.8149558350779912 entropy 0.03264415264129639
epoch: 27, step: 123
	action: tensor([[ 0.1194, -0.0591, -0.1102,  0.0625,  0.0402, -0.1203, -0.2806]],
       dtype=torch.float64)
	q_value: tensor([[-6.6019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3682347655057865, distance: 0.9095667060091339 entropy 0.03264415264129639
epoch: 27, step: 124
	action: tensor([[-0.2809,  0.3850,  0.0915, -0.1547,  0.2145,  0.0980,  0.1945]],
       dtype=torch.float64)
	q_value: tensor([[-4.5113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19285928296355925, distance: 1.028090441564452 entropy 0.03264415264129639
epoch: 27, step: 125
	action: tensor([[ 0.2261, -0.0621, -0.7132, -0.2815, -0.0935, -0.1794, -0.0478]],
       dtype=torch.float64)
	q_value: tensor([[-5.4533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3769443149454459, distance: 0.9032752795513667 entropy 0.03264415264129639
epoch: 27, step: 126
	action: tensor([[-0.0029,  0.1099, -0.4740, -0.3685, -0.1250,  0.0350, -0.1763]],
       dtype=torch.float64)
	q_value: tensor([[-6.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3168607164390169, distance: 0.9458261632984217 entropy 0.03264415264129639
epoch: 27, step: 127
	action: tensor([[-0.2717,  0.4845, -0.8084,  0.1223, -0.1933, -0.0996, -0.3605]],
       dtype=torch.float64)
	q_value: tensor([[-5.6084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2688955193409479, distance: 0.9784675247509376 entropy 0.03264415264129639
LOSS epoch 27 actor 17.958774814121274 critic 39.45483629296897 
epoch: 28, step: 0
	action: tensor([[-0.0918, -0.1018, -0.1533,  0.0226,  0.0934, -0.1529, -0.1691]],
       dtype=torch.float64)
	q_value: tensor([[-6.2092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10414453078644337, distance: 1.0831177373255867 entropy 0.03264415264129639
epoch: 28, step: 1
	action: tensor([[-0.1077,  0.6690, -0.1293, -0.2994,  0.1996,  0.4616, -0.0774]],
       dtype=torch.float64)
	q_value: tensor([[-3.9274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5594412237135006, distance: 0.7595539417064806 entropy 0.03264415264129639
epoch: 28, step: 2
	action: tensor([[ 0.0377, -0.2404, -0.1708, -0.2464, -0.1610, -0.1375,  0.0550]],
       dtype=torch.float64)
	q_value: tensor([[-6.0775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009967594974075844, distance: 1.1386267909626742 entropy 0.03264415264129639
epoch: 28, step: 3
	action: tensor([[ 0.0059, -0.1107, -0.3523,  0.1130, -0.3202,  0.2496, -0.2955]],
       dtype=torch.float64)
	q_value: tensor([[-4.5101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26149750395836413, distance: 0.9834055999427943 entropy 0.03264415264129639
epoch: 28, step: 4
	action: tensor([[-0.2631, -0.2825, -0.3838, -0.3317, -0.0290, -0.1917,  0.0574]],
       dtype=torch.float64)
	q_value: tensor([[-4.4386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2720933865676274, distance: 1.290672897943303 entropy 0.03264415264129639
epoch: 28, step: 5
	action: tensor([[ 0.3624, -0.1026, -0.1487, -0.3617,  0.1248, -0.0469, -0.5364]],
       dtype=torch.float64)
	q_value: tensor([[-4.9507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30901599621739073, distance: 0.9512412831622039 entropy 0.03264415264129639
epoch: 28, step: 6
	action: tensor([[ 0.4389,  0.3076, -0.0675, -0.2621, -0.2766, -0.0218, -0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-4.4304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7237394012854482, distance: 0.6014730427795644 entropy 0.03264415264129639
epoch: 28, step: 7
	action: tensor([[-0.0973,  0.0455,  0.4450, -0.1585,  0.0276,  0.1515,  0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-5.5001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15355874454199392, distance: 1.0528223712186104 entropy 0.03264415264129639
epoch: 28, step: 8
	action: tensor([[ 0.6213,  0.0462, -0.2177, -0.0195,  0.0063, -0.1266, -0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-4.9337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6951392269662522, distance: 0.6318405094129335 entropy 0.03264415264129639
epoch: 28, step: 9
	action: tensor([[ 0.2174, -0.4673,  0.2099,  0.0019, -0.0504, -0.1006, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[-5.0388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02988785834460206, distance: 1.1271135305178752 entropy 0.03264415264129639
epoch: 28, step: 10
	action: tensor([[-0.2037,  0.3339, -0.2992, -0.3470,  0.1907,  0.0708,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[-4.2713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2565621724083075, distance: 0.9866861243499291 entropy 0.03264415264129639
epoch: 28, step: 11
	action: tensor([[-0.2312,  0.0725,  0.4766, -0.1628, -0.1003,  0.2277, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[-5.0637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05040855085146456, distance: 1.1151289504033826 entropy 0.03264415264129639
epoch: 28, step: 12
	action: tensor([[ 0.2212,  0.4342, -0.8069, -0.2266,  0.0485,  0.3324, -0.1255]],
       dtype=torch.float64)
	q_value: tensor([[-4.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7304833814665694, distance: 0.5940862048791458 entropy 0.03264415264129639
epoch: 28, step: 13
	action: tensor([[-0.0554,  0.4636, -0.3120, -0.4705, -0.0916, -0.0427, -0.8317]],
       dtype=torch.float64)
	q_value: tensor([[-6.6764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4603026203700046, distance: 0.840682192109248 entropy 0.03264415264129639
epoch: 28, step: 14
	action: tensor([[-0.1998,  0.4217, -0.2644, -0.3235, -0.1733, -0.0073,  0.0855]],
       dtype=torch.float64)
	q_value: tensor([[-6.4342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051433908605061, distance: 0.9539031646688176 entropy 0.03264415264129639
epoch: 28, step: 15
	action: tensor([[ 0.1850,  0.2501, -0.3786,  0.2585, -0.2596, -0.2262,  0.2173]],
       dtype=torch.float64)
	q_value: tensor([[-5.6316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6490909805049493, distance: 0.6778817763590194 entropy 0.03264415264129639
epoch: 28, step: 16
	action: tensor([[ 0.1601,  0.1100, -0.2902,  0.1104, -0.0669, -0.2007,  0.1169]],
       dtype=torch.float64)
	q_value: tensor([[-5.2680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5368955586596735, distance: 0.7787466049349979 entropy 0.03264415264129639
epoch: 28, step: 17
	action: tensor([[ 0.4839,  0.2156, -0.1854,  0.0127, -0.1327, -0.1507,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[-4.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.777711567324908, distance: 0.5395294123156795 entropy 0.03264415264129639
epoch: 28, step: 18
	action: tensor([[-0.1152,  0.1367, -0.0629, -0.0136, -0.1496,  0.0359, -0.3352]],
       dtype=torch.float64)
	q_value: tensor([[-5.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2807679817100368, distance: 0.9704903014928755 entropy 0.03264415264129639
epoch: 28, step: 19
	action: tensor([[ 0.4242,  0.1024, -0.2438,  0.2438, -0.1553,  0.1428, -0.2456]],
       dtype=torch.float64)
	q_value: tensor([[-4.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7703030170637802, distance: 0.548446587585526 entropy 0.03264415264129639
epoch: 28, step: 20
	action: tensor([[ 0.3752, -0.1250,  0.1959, -0.0740,  0.4233, -0.1153, -0.4960]],
       dtype=torch.float64)
	q_value: tensor([[-4.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41791187854029177, distance: 0.8730740130447365 entropy 0.03264415264129639
epoch: 28, step: 21
	action: tensor([[ 0.2222, -0.2414, -0.6822, -0.0054,  0.4004, -0.0352,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[-4.4117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926406853139577, distance: 0.962446797328814 entropy 0.03264415264129639
epoch: 28, step: 22
	action: tensor([[ 0.2658,  0.2094, -0.3434, -0.2935,  0.2019,  0.2626, -0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-4.7645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6429344767626224, distance: 0.6838024500691466 entropy 0.03264415264129639
epoch: 28, step: 23
	action: tensor([[ 0.1035,  0.0257,  0.0123,  0.4100, -0.5290,  0.0485, -0.2011]],
       dtype=torch.float64)
	q_value: tensor([[-4.7558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6019044522799353, distance: 0.7220218716441725 entropy 0.03264415264129639
epoch: 28, step: 24
	action: tensor([[ 0.2704,  0.4058, -0.0072, -0.5721,  0.1300, -0.0234,  0.0829]],
       dtype=torch.float64)
	q_value: tensor([[-4.9307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5729734713688492, distance: 0.7477976934794225 entropy 0.03264415264129639
epoch: 28, step: 25
	action: tensor([[-0.3002, -0.5451,  0.1266, -0.4111,  0.4097, -0.3330,  0.1964]],
       dtype=torch.float64)
	q_value: tensor([[-5.5896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7650009376587525, distance: 1.520299537877749 entropy 0.03264415264129639
epoch: 28, step: 26
	action: tensor([[ 0.2168, -0.2488, -0.3354,  0.0143, -0.0158,  0.0337, -0.4814]],
       dtype=torch.float64)
	q_value: tensor([[-5.3064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2503731816929995, distance: 0.9907846074949792 entropy 0.03264415264129639
epoch: 28, step: 27
	action: tensor([[-0.1706, -0.3586,  0.3285, -0.1786, -0.1020, -0.2018, -0.8010]],
       dtype=torch.float64)
	q_value: tensor([[-4.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3551596919385094, distance: 1.3321463249180765 entropy 0.03264415264129639
epoch: 28, step: 28
	action: tensor([[ 0.3312, -0.0523,  0.1243, -0.2253,  0.0102,  0.0820,  0.1022]],
       dtype=torch.float64)
	q_value: tensor([[-4.9792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4066897507733097, distance: 0.8814498711404642 entropy 0.03264415264129639
epoch: 28, step: 29
	action: tensor([[-0.0895, -0.2960,  0.0333,  0.4457,  0.0730, -0.1717, -0.2328]],
       dtype=torch.float64)
	q_value: tensor([[-4.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16363694556262087, distance: 1.0465358577355437 entropy 0.03264415264129639
epoch: 28, step: 30
	action: tensor([[ 0.3479, -0.0485,  0.0927, -0.1622, -0.4940,  0.2432, -0.6160]],
       dtype=torch.float64)
	q_value: tensor([[-4.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48623592143744443, distance: 0.8202354990514061 entropy 0.03264415264129639
epoch: 28, step: 31
	action: tensor([[-0.1018,  0.2270, -0.0803,  0.5858, -0.3852, -0.0960, -0.2140]],
       dtype=torch.float64)
	q_value: tensor([[-5.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49204777314604353, distance: 0.8155829301391242 entropy 0.03264415264129639
epoch: 28, step: 32
	action: tensor([[ 0.0498, -0.0716, -0.3382,  0.1003,  0.1182, -0.0884, -0.1742]],
       dtype=torch.float64)
	q_value: tensor([[-5.0510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3123190009160641, distance: 0.9489650233919318 entropy 0.03264415264129639
epoch: 28, step: 33
	action: tensor([[ 0.1571,  0.3644, -0.3986,  0.2446, -0.1520,  0.0759, -0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-3.9348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6526202916586668, distance: 0.6744642213388875 entropy 0.03264415264129639
epoch: 28, step: 34
	action: tensor([[-0.1229, -0.2263,  0.3086, -0.4318,  0.1128, -0.0138, -0.2121]],
       dtype=torch.float64)
	q_value: tensor([[-4.8582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3050741987197174, distance: 1.3072970920977651 entropy 0.03264415264129639
epoch: 28, step: 35
	action: tensor([[ 0.0145, -0.0494, -0.0068,  0.0087,  0.6335,  0.1391,  0.2222]],
       dtype=torch.float64)
	q_value: tensor([[-4.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089532485111566, distance: 0.9512844729128124 entropy 0.03264415264129639
epoch: 28, step: 36
	action: tensor([[ 0.2386,  0.2403,  0.0256,  0.3411, -0.2738,  0.5285, -0.4839]],
       dtype=torch.float64)
	q_value: tensor([[-4.2948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7851007039728978, distance: 0.5304863208991303 entropy 0.03264415264129639
epoch: 28, step: 37
	action: tensor([[ 0.0971, -0.2668, -0.3382,  0.0921, -0.1070,  0.4807, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[-5.5871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29673885798207356, distance: 0.9596547208028597 entropy 0.03264415264129639
epoch: 28, step: 38
	action: tensor([[ 0.2557,  0.0325, -0.1319, -0.0716, -0.1429, -0.0218, -0.2593]],
       dtype=torch.float64)
	q_value: tensor([[-4.2369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5170044908190705, distance: 0.7952949772749814 entropy 0.03264415264129639
epoch: 28, step: 39
	action: tensor([[ 0.3797,  0.1948, -0.1544,  0.0148, -0.3945, -0.0656,  0.0701]],
       dtype=torch.float64)
	q_value: tensor([[-4.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7345035844951533, distance: 0.5896387600852451 entropy 0.03264415264129639
epoch: 28, step: 40
	action: tensor([[ 0.3107,  0.1758,  0.0279,  0.0167,  0.1278, -0.6253, -0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-5.2187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5631630922911793, distance: 0.756338756764703 entropy 0.03264415264129639
epoch: 28, step: 41
	action: tensor([[-0.2474,  0.2190, -0.0637, -0.0973, -0.3848, -0.2460, -0.2099]],
       dtype=torch.float64)
	q_value: tensor([[-5.3838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14470508832136875, distance: 1.0583142343488614 entropy 0.03264415264129639
epoch: 28, step: 42
	action: tensor([[ 0.1450,  0.0376,  0.0740, -0.4052,  0.3230,  0.1282, -0.2108]],
       dtype=torch.float64)
	q_value: tensor([[-5.1389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27901704167188335, distance: 0.9716708922749773 entropy 0.03264415264129639
epoch: 28, step: 43
	action: tensor([[-0.1689, -0.1031, -0.4313,  0.2465,  0.2392, -0.0723, -0.1854]],
       dtype=torch.float64)
	q_value: tensor([[-4.0035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07849199218999503, distance: 1.0985156539644056 entropy 0.03264415264129639
epoch: 28, step: 44
	action: tensor([[-0.0367, -0.2541, -0.3158, -0.5186, -0.0969,  0.1346,  0.2317]],
       dtype=torch.float64)
	q_value: tensor([[-4.1659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08763353331926327, distance: 1.1934328496983804 entropy 0.03264415264129639
epoch: 28, step: 45
	action: tensor([[ 0.2509,  0.1119, -0.3520,  0.0386,  0.4554, -0.0748, -0.1087]],
       dtype=torch.float64)
	q_value: tensor([[-5.2152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5798818594741196, distance: 0.7417241346521091 entropy 0.03264415264129639
epoch: 28, step: 46
	action: tensor([[ 0.2820, -0.0453, -0.5292,  0.1427,  0.1351, -0.1699, -0.2446]],
       dtype=torch.float64)
	q_value: tensor([[-4.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.516545131382768, distance: 0.7956730754401515 entropy 0.03264415264129639
epoch: 28, step: 47
	action: tensor([[-0.0670, -0.5837,  0.1690,  0.1826, -0.4981,  0.3658,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-4.4598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05113761665712713, distance: 1.1732389766375413 entropy 0.03264415264129639
epoch: 28, step: 48
	action: tensor([[ 0.0966,  0.1810, -0.0328,  0.2622,  0.3743, -0.2320, -0.3423]],
       dtype=torch.float64)
	q_value: tensor([[-4.8891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5404913683204747, distance: 0.775717394718329 entropy 0.03264415264129639
epoch: 28, step: 49
	action: tensor([[ 0.0957, -0.1498,  0.0788, -0.2177,  0.1097,  0.0881, -0.4450]],
       dtype=torch.float64)
	q_value: tensor([[-4.2477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1665771505399919, distance: 1.0446947081395797 entropy 0.03264415264129639
epoch: 28, step: 50
	action: tensor([[ 0.3750,  0.1564, -0.2361, -0.4282,  0.1020,  0.0746, -0.4062]],
       dtype=torch.float64)
	q_value: tensor([[-3.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.558330364656579, distance: 0.7605109379251934 entropy 0.03264415264129639
epoch: 28, step: 51
	action: tensor([[ 0.0556, -0.0056, -0.0614, -0.1743,  0.2638,  0.0089,  0.1917]],
       dtype=torch.float64)
	q_value: tensor([[-4.8947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.263326780209289, distance: 0.9821868932455384 entropy 0.03264415264129639
epoch: 28, step: 52
	action: tensor([[ 0.2265, -0.1810, -0.6770, -0.0346, -0.0313, -0.4066, -0.5252]],
       dtype=torch.float64)
	q_value: tensor([[-4.2277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3232902755544723, distance: 0.9413646852717668 entropy 0.03264415264129639
epoch: 28, step: 53
	action: tensor([[ 0.4255, -0.0041,  0.0527, -0.0774,  0.2891,  0.5457, -0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-5.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7268650068415627, distance: 0.5980608383884822 entropy 0.03264415264129639
epoch: 28, step: 54
	action: tensor([[ 0.0196, -0.1518, -0.8105, -0.4396,  0.0096, -0.1597, -0.2499]],
       dtype=torch.float64)
	q_value: tensor([[-4.5100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23113826672539994, distance: 1.0034155255152097 entropy 0.03264415264129639
epoch: 28, step: 55
	action: tensor([[ 0.3473, -0.1353, -0.1206, -0.1811, -0.4482,  0.3042, -0.1838]],
       dtype=torch.float64)
	q_value: tensor([[-5.6477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4379402622532007, distance: 0.8579222503665481 entropy 0.03264415264129639
epoch: 28, step: 56
	action: tensor([[ 0.0267,  0.2650, -0.3199, -0.3330, -0.6894, -0.2713, -0.5001]],
       dtype=torch.float64)
	q_value: tensor([[-5.2995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46838474701247956, distance: 0.8343637163525869 entropy 0.03264415264129639
epoch: 28, step: 57
	action: tensor([[ 0.3453, -0.1700, -0.2090, -0.0191, -0.1009,  0.1692, -0.2910]],
       dtype=torch.float64)
	q_value: tensor([[-6.7891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4832266808140482, distance: 0.8226341507833845 entropy 0.03264415264129639
epoch: 28, step: 58
	action: tensor([[ 0.1455,  0.2543, -0.1927,  0.2086, -0.2225, -0.2958, -0.0479]],
       dtype=torch.float64)
	q_value: tensor([[-4.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6403071841051673, distance: 0.6863135529119473 entropy 0.03264415264129639
epoch: 28, step: 59
	action: tensor([[ 0.4323,  0.2136, -0.0905, -0.1588,  0.0435,  0.0843,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-4.9240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7298635131792308, distance: 0.5947689896802592 entropy 0.03264415264129639
epoch: 28, step: 60
	action: tensor([[ 0.0125, -0.0374,  0.1653, -0.1623,  0.2076,  0.5340, -0.4423]],
       dtype=torch.float64)
	q_value: tensor([[-4.6119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3815466113580134, distance: 0.899933005335455 entropy 0.03264415264129639
epoch: 28, step: 61
	action: tensor([[-0.1203,  0.0280, -0.8223, -0.1849,  0.3795,  0.2331, -0.4387]],
       dtype=torch.float64)
	q_value: tensor([[-4.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24816309685026494, distance: 0.992244071329193 entropy 0.03264415264129639
epoch: 28, step: 62
	action: tensor([[-0.2025,  0.2126,  0.1056,  0.2568, -0.0307, -0.0762,  0.2388]],
       dtype=torch.float64)
	q_value: tensor([[-5.3880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.325097339361043, distance: 0.9401069501206747 entropy 0.03264415264129639
epoch: 28, step: 63
	action: tensor([[-0.2397,  0.0804,  0.1094, -0.2427,  0.2274, -0.0074,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-4.6964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05988152819241943, distance: 1.178108677633047 entropy 0.03264415264129639
epoch: 28, step: 64
	action: tensor([[-0.0491,  0.0962,  0.0570,  0.4744, -0.5001, -0.1240, -0.2646]],
       dtype=torch.float64)
	q_value: tensor([[-4.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4737358497712094, distance: 0.8301538501377518 entropy 0.03264415264129639
epoch: 28, step: 65
	action: tensor([[ 0.1596,  0.2653, -0.2102,  0.0829,  0.0201,  0.1908,  0.1804]],
       dtype=torch.float64)
	q_value: tensor([[-5.0872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6213410242124935, distance: 0.7041753546860572 entropy 0.03264415264129639
epoch: 28, step: 66
	action: tensor([[ 0.2917,  0.0521, -0.5466, -0.4887,  0.0022,  0.0994,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-4.5335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.456288558184749, distance: 0.8438027364154109 entropy 0.03264415264129639
epoch: 28, step: 67
	action: tensor([[ 0.0868, -0.2830, -0.4769, -0.0733, -0.0902,  0.1699, -0.3032]],
       dtype=torch.float64)
	q_value: tensor([[-5.7314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1564429339551875, distance: 1.0510271313052877 entropy 0.03264415264129639
epoch: 28, step: 68
	action: tensor([[ 0.3190,  0.1969,  0.2877, -0.1319, -0.2319,  0.0139, -0.6519]],
       dtype=torch.float64)
	q_value: tensor([[-4.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6319327459437262, distance: 0.6942570264371458 entropy 0.03264415264129639
epoch: 28, step: 69
	action: tensor([[-0.1402, -0.3953, -0.0345,  0.1310,  0.1825,  0.2753, -0.2058]],
       dtype=torch.float64)
	q_value: tensor([[-5.5673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0021527190426986476, distance: 1.1455753176561294 entropy 0.03264415264129639
epoch: 28, step: 70
	action: tensor([[ 0.1288,  0.0897, -0.0621,  0.0665,  0.0495,  0.0052, -0.1249]],
       dtype=torch.float64)
	q_value: tensor([[-3.8784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4984066661337624, distance: 0.81046184019079 entropy 0.03264415264129639
epoch: 28, step: 71
	action: tensor([[ 0.2082, -0.1477,  0.1991, -0.2930, -0.2070,  0.0414, -0.1745]],
       dtype=torch.float64)
	q_value: tensor([[-4.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19103763211301827, distance: 1.0292499458968376 entropy 0.03264415264129639
epoch: 28, step: 72
	action: tensor([[ 0.0542,  0.2691, -0.2397,  0.0272, -0.2837, -0.1723,  0.2588]],
       dtype=torch.float64)
	q_value: tensor([[-4.5962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.526764949858793, distance: 0.7872182338741623 entropy 0.03264415264129639
epoch: 28, step: 73
	action: tensor([[ 0.3862,  0.1895, -0.0877, -0.0537, -0.4376,  0.1773, -0.1664]],
       dtype=torch.float64)
	q_value: tensor([[-5.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7270719595236119, distance: 0.597834221968918 entropy 0.03264415264129639
epoch: 28, step: 74
	action: tensor([[ 1.4348e-01,  5.0996e-02, -5.0075e-02, -5.3342e-04, -6.2079e-01,
          1.4243e-01,  2.1937e-01]], dtype=torch.float64)
	q_value: tensor([[-5.5160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4797401911949174, distance: 0.8254044990615254 entropy 0.03264415264129639
epoch: 28, step: 75
	action: tensor([[-0.2235, -0.0507, -0.1215,  0.0450,  0.1220,  0.0408, -0.2317]],
       dtype=torch.float64)
	q_value: tensor([[-5.6016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048394934394347056, distance: 1.1163106443304016 entropy 0.03264415264129639
epoch: 28, step: 76
	action: tensor([[ 0.0767,  0.3288, -0.0635, -0.2463,  0.0285, -0.1460, -0.5096]],
       dtype=torch.float64)
	q_value: tensor([[-3.8799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47185619405317847, distance: 0.8316350574283388 entropy 0.03264415264129639
epoch: 28, step: 77
	action: tensor([[-0.2588,  0.3974, -0.0384,  0.0374, -0.3591, -0.3817, -0.2164]],
       dtype=torch.float64)
	q_value: tensor([[-4.8010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881875278657732, distance: 0.9654715703871586 entropy 0.03264415264129639
epoch: 28, step: 78
	action: tensor([[ 0.1882,  0.0531, -0.3406,  0.1091, -0.1264,  0.2832, -0.3191]],
       dtype=torch.float64)
	q_value: tensor([[-5.4629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5603844798075583, distance: 0.758740386485336 entropy 0.03264415264129639
epoch: 28, step: 79
	action: tensor([[-0.0350, -0.1008, -0.0558, -0.3630,  0.0914,  0.0362, -0.1231]],
       dtype=torch.float64)
	q_value: tensor([[-4.2728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02646487080847615, distance: 1.1291002587127201 entropy 0.03264415264129639
epoch: 28, step: 80
	action: tensor([[-0.0103, -0.2246, -0.1890,  0.1430,  0.0632, -0.0416, -0.4369]],
       dtype=torch.float64)
	q_value: tensor([[-4.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1675009969353064, distance: 1.0441155273238742 entropy 0.03264415264129639
epoch: 28, step: 81
	action: tensor([[ 0.0764,  0.3570, -0.1831,  0.2384, -0.1792, -0.1080, -0.2726]],
       dtype=torch.float64)
	q_value: tensor([[-3.9131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.611502555253977, distance: 0.7132647749084337 entropy 0.03264415264129639
epoch: 28, step: 82
	action: tensor([[ 0.1505,  0.1157, -0.3043, -0.4228,  0.4412, -0.0197, -0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-4.8326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39698696129380207, distance: 0.8886281048945033 entropy 0.03264415264129639
epoch: 28, step: 83
	action: tensor([[ 0.0014,  0.2328, -0.0039,  0.2689,  0.4190, -0.0350, -0.3528]],
       dtype=torch.float64)
	q_value: tensor([[-4.4195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5404458748035743, distance: 0.775755793604166 entropy 0.03264415264129639
epoch: 28, step: 84
	action: tensor([[-0.2510,  0.2863, -0.0394, -0.0906, -0.0200,  0.5187, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-4.2118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2936879584402695, distance: 0.9617340633637566 entropy 0.03264415264129639
epoch: 28, step: 85
	action: tensor([[ 0.2443,  0.3358, -0.4728,  0.1560,  0.1940,  0.1879, -0.3860]],
       dtype=torch.float64)
	q_value: tensor([[-5.1625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7221515802823604, distance: 0.6031990637751328 entropy 0.03264415264129639
epoch: 28, step: 86
	action: tensor([[ 0.3924,  0.0204, -0.0219, -0.0369,  0.1732, -0.5616, -0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-4.7218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47891552213210586, distance: 0.8260584185228755 entropy 0.03264415264129639
epoch: 28, step: 87
	action: tensor([[-0.0586,  0.1087, -0.4885,  0.0687, -0.0354, -0.0599,  0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-5.1825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3267475099729584, distance: 0.9389569425812848 entropy 0.03264415264129639
epoch: 28, step: 88
	action: tensor([[ 0.0196,  0.2107, -0.4869, -0.1415,  0.2287,  0.0939, -0.3747]],
       dtype=torch.float64)
	q_value: tensor([[-4.4974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4664396163989426, distance: 0.83588875213339 entropy 0.03264415264129639
epoch: 28, step: 89
	action: tensor([[ 0.4040,  0.2486, -0.0927,  0.3130,  0.2059,  0.1903, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[-4.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8548205795538346, distance: 0.4360228230614589 entropy 0.03264415264129639
epoch: 28, step: 90
	action: tensor([[ 0.2540, -0.1624, -0.4170, -0.0385, -0.1028,  0.1547, -0.3258]],
       dtype=torch.float64)
	q_value: tensor([[-4.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41022991655354557, distance: 0.878816217470237 entropy 0.03264415264129639
epoch: 28, step: 91
	action: tensor([[ 0.0177, -0.1146, -0.0649, -0.2803,  0.3015, -0.2673, -0.1488]],
       dtype=torch.float64)
	q_value: tensor([[-4.1413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016668053174442643, distance: 1.1347671830836243 entropy 0.03264415264129639
epoch: 28, step: 92
	action: tensor([[ 0.1295,  0.7105, -0.7857,  0.0238,  0.3651, -0.3715,  0.2386]],
       dtype=torch.float64)
	q_value: tensor([[-4.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7330892563211429, distance: 0.5912072088190943 entropy 0.03264415264129639
epoch: 28, step: 93
	action: tensor([[ 0.3042,  0.1337,  0.2373,  0.4734, -0.1457, -0.2418, -0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-7.2158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7879291637821628, distance: 0.5269836821986708 entropy 0.03264415264129639
epoch: 28, step: 94
	action: tensor([[ 0.4270,  0.1687, -0.0815, -0.0739,  0.2128, -0.3744, -0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-5.1110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6216725092954243, distance: 0.70386706312427 entropy 0.03264415264129639
epoch: 28, step: 95
	action: tensor([[ 0.6207,  0.2791, -0.0676,  0.1660,  0.3368, -0.0230,  0.0790]],
       dtype=torch.float64)
	q_value: tensor([[-4.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9060694525622149, distance: 0.3507196669066071 entropy 0.03264415264129639
epoch: 28, step: 96
	action: tensor([[-0.1942,  0.1448, -0.0207,  0.1552,  0.1390,  0.1954,  0.2072]],
       dtype=torch.float64)
	q_value: tensor([[-5.1329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30003719018806385, distance: 0.9574016600824491 entropy 0.03264415264129639
epoch: 28, step: 97
	action: tensor([[ 0.0839,  0.0585, -0.5040, -0.2879, -0.1413, -0.4119,  0.1714]],
       dtype=torch.float64)
	q_value: tensor([[-4.4518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3263743019922781, distance: 0.9392171552649947 entropy 0.03264415264129639
epoch: 28, step: 98
	action: tensor([[ 0.4739,  0.2283, -0.2933, -0.0955,  0.0892, -0.5322,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-5.7192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6798227166053894, distance: 0.6475181570826142 entropy 0.03264415264129639
epoch: 28, step: 99
	action: tensor([[-0.1568,  0.0971, -0.2871, -0.2994,  0.2247, -0.3090, -0.1023]],
       dtype=torch.float64)
	q_value: tensor([[-5.9102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08126880315461682, distance: 1.0968593082238303 entropy 0.03264415264129639
epoch: 28, step: 100
	action: tensor([[-0.1446, -0.0268, -0.1595, -0.3623, -0.2353,  0.4086, -0.3313]],
       dtype=torch.float64)
	q_value: tensor([[-4.5836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05245440214121011, distance: 1.1139270555698613 entropy 0.03264415264129639
epoch: 28, step: 101
	action: tensor([[ 0.3282,  0.0654,  0.0058, -0.2960, -0.2028,  0.3380,  0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-5.1694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5328156486236494, distance: 0.7821694278583159 entropy 0.03264415264129639
epoch: 28, step: 102
	action: tensor([[ 0.0169, -0.1398, -0.1007,  0.4264,  0.0285, -0.0035, -0.2086]],
       dtype=torch.float64)
	q_value: tensor([[-5.1844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4011118238790349, distance: 0.8855835946979773 entropy 0.03264415264129639
epoch: 28, step: 103
	action: tensor([[ 0.1373,  0.0594, -0.1062,  0.6845,  0.1392, -0.0988, -0.3100]],
       dtype=torch.float64)
	q_value: tensor([[-4.1131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6670766235158294, distance: 0.6602810580716904 entropy 0.03264415264129639
epoch: 28, step: 104
	action: tensor([[-0.0093, -0.2650, -0.3760,  0.0283, -0.4547, -0.4299, -0.0956]],
       dtype=torch.float64)
	q_value: tensor([[-4.8317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1183055705187036, distance: 1.0745230638513268 entropy 0.03264415264129639
epoch: 28, step: 105
	action: tensor([[ 0.2691,  0.5396,  0.2308, -0.0285,  0.3395, -0.0676, -0.5126]],
       dtype=torch.float64)
	q_value: tensor([[-5.3719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7705476183648916, distance: 0.5481544930180022 entropy 0.03264415264129639
epoch: 28, step: 106
	action: tensor([[ 0.1495, -0.0218, -0.0867,  0.2641, -0.4895, -0.1460, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-5.1422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5310453784760254, distance: 0.7836499376104 entropy 0.03264415264129639
epoch: 28, step: 107
	action: tensor([[-0.0911,  0.1061, -0.0523,  0.0387, -0.2088, -0.2665,  0.2024]],
       dtype=torch.float64)
	q_value: tensor([[-4.8001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26769974271445185, distance: 0.9792673766189813 entropy 0.03264415264129639
epoch: 28, step: 108
	action: tensor([[ 0.1286, -0.0230, -0.0828, -0.2391, -0.1651,  0.0279, -0.2082]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010589061030914, distance: 0.9567026588496629 entropy 0.03264415264129639
epoch: 28, step: 109
	action: tensor([[ 0.1935, -0.0756, -0.0900, -0.6706, -0.0480,  0.2275,  0.1039]],
       dtype=torch.float64)
	q_value: tensor([[-4.3109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14957940852035656, distance: 1.0552942624946589 entropy 0.03264415264129639
epoch: 28, step: 110
	action: tensor([[-0.2386,  0.0445, -0.3445, -0.4933, -0.1713,  0.1280,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-5.4431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012778014913532587, distance: 1.151632270278473 entropy 0.03264415264129639
epoch: 28, step: 111
	action: tensor([[ 0.0793, -0.0170,  0.0797, -0.0755,  0.2533,  0.2925, -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-5.4914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39354022532767086, distance: 0.8911641215234657 entropy 0.03264415264129639
epoch: 28, step: 112
	action: tensor([[-0.0844, -0.0357,  0.0301,  0.3388, -0.2739,  0.0978,  0.3853]],
       dtype=torch.float64)
	q_value: tensor([[-3.8146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3601833955458088, distance: 0.9153442265275663 entropy 0.03264415264129639
epoch: 28, step: 113
	action: tensor([[-0.0224,  0.4784, -0.5460, -0.1244,  0.3324, -0.2455, -0.0588]],
       dtype=torch.float64)
	q_value: tensor([[-4.9100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5287716232499994, distance: 0.7855474277446317 entropy 0.03264415264129639
epoch: 28, step: 114
	action: tensor([[ 0.2454, -0.0359, -0.5735, -0.0238, -0.1887, -0.2040, -0.0272]],
       dtype=torch.float64)
	q_value: tensor([[-5.4926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47230129269260224, distance: 0.8312845490533167 entropy 0.03264415264129639
epoch: 28, step: 115
	action: tensor([[ 0.0871, -0.2933, -0.1450, -0.1309,  0.0705, -0.0087,  0.1321]],
       dtype=torch.float64)
	q_value: tensor([[-5.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06791005492597735, distance: 1.104804933190616 entropy 0.03264415264129639
epoch: 28, step: 116
	action: tensor([[ 0.2302,  0.3952, -0.0773,  0.2994, -0.1066,  0.1639, -0.4952]],
       dtype=torch.float64)
	q_value: tensor([[-4.0970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7602247831776726, distance: 0.5603493073828536 entropy 0.03264415264129639
epoch: 28, step: 117
	action: tensor([[-0.3464, -0.1304,  0.1076,  0.0197, -0.1848,  0.1846, -0.1629]],
       dtype=torch.float64)
	q_value: tensor([[-5.0697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13403352543998315, distance: 1.2186237563030877 entropy 0.03264415264129639
epoch: 28, step: 118
	action: tensor([[ 0.1803,  0.1001, -0.1456, -0.3041, -0.0439, -0.0803, -0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-4.3228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39256828546729894, distance: 0.8918779455970833 entropy 0.03264415264129639
epoch: 28, step: 119
	action: tensor([[-0.1051, -0.2537, -0.0776, -0.2892,  0.5264, -0.2589,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-4.6121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24272878099903994, distance: 1.2756891786198536 entropy 0.03264415264129639
epoch: 28, step: 120
	action: tensor([[ 0.1550,  0.2711, -0.3276, -0.3380, -0.5720, -0.1728,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-4.3344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.51834228132651, distance: 0.7941928181542669 entropy 0.03264415264129639
epoch: 28, step: 121
	action: tensor([[-0.1384,  0.3080, -0.2507, -0.0349, -0.2460, -0.1728, -0.2712]],
       dtype=torch.float64)
	q_value: tensor([[-6.4430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3402973537256484, distance: 0.9294602183660846 entropy 0.03264415264129639
epoch: 28, step: 122
	action: tensor([[ 0.6786,  0.0771,  0.0502, -0.4057,  0.1481, -0.1634,  0.2703]],
       dtype=torch.float64)
	q_value: tensor([[-4.9034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5377920961934691, distance: 0.7779924405063925 entropy 0.03264415264129639
epoch: 28, step: 123
	action: tensor([[-0.0336, -0.1563,  0.1228, -0.1194, -0.2085,  0.5220,  0.1693]],
       dtype=torch.float64)
	q_value: tensor([[-5.8679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20882983747072592, distance: 1.0178684272844003 entropy 0.03264415264129639
epoch: 28, step: 124
	action: tensor([[ 0.5743,  0.2440, -0.2175,  0.3958, -0.4295,  0.0612,  0.3077]],
       dtype=torch.float64)
	q_value: tensor([[-4.9652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9025754556738216, distance: 0.35718308573972024 entropy 0.03264415264129639
epoch: 28, step: 125
	action: tensor([[ 0.0521,  0.3457, -0.2576,  0.2868,  0.2438,  0.1563,  0.5655]],
       dtype=torch.float64)
	q_value: tensor([[-5.9344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6056453793416456, distance: 0.7186214234332201 entropy 0.03264415264129639
epoch: 28, step: 126
	action: tensor([[ 0.2023,  0.2107, -0.2023,  0.0171, -0.0804,  0.2195, -0.4222]],
       dtype=torch.float64)
	q_value: tensor([[-5.5192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.637315663720053, distance: 0.6891616355894612 entropy 0.03264415264129639
epoch: 28, step: 127
	action: tensor([[ 0.2970, -0.2522, -0.3040,  0.2841, -0.0124, -0.3193, -0.3201]],
       dtype=torch.float64)
	q_value: tensor([[-4.4970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41340717544831984, distance: 0.8764458043925254 entropy 0.03264415264129639
LOSS epoch 28 actor 14.162719633714246 critic 45.03933739632453 
epoch: 29, step: 0
	action: tensor([[ 0.2725, -0.0322, -0.4976, -0.0128, -0.0897,  0.0903,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[-3.9464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5241682989044812, distance: 0.7893750210738478 entropy 0.03264415264129639
epoch: 29, step: 1
	action: tensor([[ 0.1661, -0.0820,  0.0470,  0.0479, -0.1684, -0.1034,  0.3579]],
       dtype=torch.float64)
	q_value: tensor([[-3.8458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3860872567103317, distance: 0.8966232938444956 entropy 0.03264415264129639
epoch: 29, step: 2
	action: tensor([[ 0.1360,  0.1156,  0.2127, -0.1441,  0.0070,  0.1175, -0.1282]],
       dtype=torch.float64)
	q_value: tensor([[-4.0734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46067817421212875, distance: 0.8403896426335878 entropy 0.03264415264129639
epoch: 29, step: 3
	action: tensor([[ 0.0556,  0.6786, -0.0814,  0.1683,  0.1219, -0.0535, -0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-3.7859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6810466120152472, distance: 0.6462793851102069 entropy 0.03264415264129639
epoch: 29, step: 4
	action: tensor([[ 0.2533,  0.0980, -0.5463,  0.1306, -0.2661, -0.2005, -0.1344]],
       dtype=torch.float64)
	q_value: tensor([[-4.5240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6185890048180134, distance: 0.7067296264940246 entropy 0.03264415264129639
epoch: 29, step: 5
	action: tensor([[-0.2386,  0.0640,  0.1636, -0.3095, -0.0760,  0.0179, -0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-4.4938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08259784808603476, distance: 1.1906668791982387 entropy 0.03264415264129639
epoch: 29, step: 6
	action: tensor([[-0.2920, -0.4529, -0.3148, -0.5005, -0.7057,  0.1957,  0.1011]],
       dtype=torch.float64)
	q_value: tensor([[-3.9119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5138340283115352, distance: 1.4079778572214963 entropy 0.03264415264129639
epoch: 29, step: 7
	action: tensor([[ 0.0091,  0.0225, -0.4041, -0.4199,  0.0342, -0.2756, -0.4223]],
       dtype=torch.float64)
	q_value: tensor([[-5.7402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19217562912144615, distance: 1.0285257493101598 entropy 0.03264415264129639
epoch: 29, step: 8
	action: tensor([[ 0.1047,  0.3012, -1.0191,  0.1026,  0.2283,  0.3829, -0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-4.3285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4932902618620413, distance: 0.8145848313249503 entropy 0.03264415264129639
epoch: 29, step: 9
	action: tensor([[-0.0767,  0.1290,  0.0573,  0.1929, -0.1737, -0.1612, -0.6000]],
       dtype=torch.float64)
	q_value: tensor([[-5.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37802811811839254, distance: 0.9024893155211436 entropy 0.03264415264129639
epoch: 29, step: 10
	action: tensor([[ 0.1376, -0.0190,  0.0869, -0.2142,  0.3347,  0.3525,  0.2010]],
       dtype=torch.float64)
	q_value: tensor([[-4.1406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3990481283706092, distance: 0.887108088974477 entropy 0.03264415264129639
epoch: 29, step: 11
	action: tensor([[-0.1487, -0.0129, -0.2626,  0.0055, -0.0207,  0.3993, -0.5130]],
       dtype=torch.float64)
	q_value: tensor([[-3.7289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20843086668953503, distance: 1.0181250399555974 entropy 0.03264415264129639
epoch: 29, step: 12
	action: tensor([[ 0.4739, -0.1721, -0.3422, -0.4989, -0.3239,  0.2607, -0.1614]],
       dtype=torch.float64)
	q_value: tensor([[-3.8638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33844966232802753, distance: 0.9307609216834424 entropy 0.03264415264129639
epoch: 29, step: 13
	action: tensor([[ 0.5127,  0.3596, -0.3333,  0.1954, -0.3615,  0.2441,  0.1095]],
       dtype=torch.float64)
	q_value: tensor([[-5.2106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8713272880178827, distance: 0.4104874847116602 entropy 0.03264415264129639
epoch: 29, step: 14
	action: tensor([[-0.0639,  0.1053, -0.5179, -0.4431,  0.1503,  0.0377, -0.5088]],
       dtype=torch.float64)
	q_value: tensor([[-5.3194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27856278221395225, distance: 0.9719769474905843 entropy 0.03264415264129639
epoch: 29, step: 15
	action: tensor([[ 0.2044, -0.1126, -0.0926, -0.2345, -0.2274, -0.3605, -0.2220]],
       dtype=torch.float64)
	q_value: tensor([[-4.3970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23207434938158733, distance: 1.0028045145263587 entropy 0.03264415264129639
epoch: 29, step: 16
	action: tensor([[-0.2909, -0.0806, -0.4594, -0.3686,  0.1291, -0.0732, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[-4.2153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12305490562587762, distance: 1.2127106397651617 entropy 0.03264415264129639
epoch: 29, step: 17
	action: tensor([[ 0.2225, -0.1420, -0.3043, -0.2144,  0.1574, -0.1305, -0.0806]],
       dtype=torch.float64)
	q_value: tensor([[-4.3088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25690201485079567, distance: 0.9864605801691345 entropy 0.03264415264129639
epoch: 29, step: 18
	action: tensor([[-0.2935,  0.4147, -0.6070,  0.1019,  0.3127,  0.3887,  0.3884]],
       dtype=torch.float64)
	q_value: tensor([[-3.6673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26716716337790947, distance: 0.9796234073416786 entropy 0.03264415264129639
epoch: 29, step: 19
	action: tensor([[ 0.1084, -0.2669, -0.3591, -0.3696, -0.1244, -0.1237,  0.0457]],
       dtype=torch.float64)
	q_value: tensor([[-5.7187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008698555876775482, distance: 1.1393563121179615 entropy 0.03264415264129639
epoch: 29, step: 20
	action: tensor([[-0.1487,  0.0065, -0.1508,  0.1153,  0.0200, -0.2497, -0.4451]],
       dtype=torch.float64)
	q_value: tensor([[-4.2444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13783255137014117, distance: 1.0625576544991242 entropy 0.03264415264129639
epoch: 29, step: 21
	action: tensor([[ 0.2055, -0.5682,  0.2502, -0.0829, -0.0408, -0.4443,  0.1292]],
       dtype=torch.float64)
	q_value: tensor([[-3.7254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24524170616316598, distance: 1.276978314493488 entropy 0.03264415264129639
epoch: 29, step: 22
	action: tensor([[ 0.3422,  0.1786,  0.4600,  0.0135, -0.0636, -0.0710, -0.1036]],
       dtype=torch.float64)
	q_value: tensor([[-4.3900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6856620197460885, distance: 0.641586360109825 entropy 0.03264415264129639
epoch: 29, step: 23
	action: tensor([[ 0.1410,  0.3173,  0.2337, -0.2189, -0.3279,  0.1340, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[-4.4199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5533180985233445, distance: 0.7648140743705156 entropy 0.03264415264129639
epoch: 29, step: 24
	action: tensor([[ 0.4502, -0.0964,  0.2897,  0.0505, -0.5787,  0.0201, -0.4175]],
       dtype=torch.float64)
	q_value: tensor([[-4.8703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6041504724841593, distance: 0.7199821985879148 entropy 0.03264415264129639
epoch: 29, step: 25
	action: tensor([[ 0.0271, -0.5386, -0.4358, -0.3015, -0.2054,  0.1106, -0.6032]],
       dtype=torch.float64)
	q_value: tensor([[-5.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22061079179999332, distance: 1.2642859042994443 entropy 0.03264415264129639
epoch: 29, step: 26
	action: tensor([[-0.0369,  0.4291, -0.2251, -0.1810, -0.2223,  0.1665, -0.2490]],
       dtype=torch.float64)
	q_value: tensor([[-4.2285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44644018953438513, distance: 0.8514104361833286 entropy 0.03264415264129639
epoch: 29, step: 27
	action: tensor([[ 0.3082,  0.0314, -0.2437, -0.2080, -0.5284,  0.2267,  0.2345]],
       dtype=torch.float64)
	q_value: tensor([[-4.7465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.501351586280773, distance: 0.808079174009133 entropy 0.03264415264129639
epoch: 29, step: 28
	action: tensor([[-0.0704,  0.0563, -0.3790,  0.3673, -0.1232,  0.4315, -0.0485]],
       dtype=torch.float64)
	q_value: tensor([[-5.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3668714183670403, distance: 0.9105475979692236 entropy 0.03264415264129639
epoch: 29, step: 29
	action: tensor([[ 0.1189,  0.0835, -0.2734,  0.1085,  0.0652,  0.3306, -0.1615]],
       dtype=torch.float64)
	q_value: tensor([[-4.0322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5494706868335412, distance: 0.7681008042063224 entropy 0.03264415264129639
epoch: 29, step: 30
	action: tensor([[-0.2024, -0.1099,  0.0421,  0.1992, -0.0726,  0.2661, -0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-3.4908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17414224018660207, distance: 1.0399424832594566 entropy 0.03264415264129639
epoch: 29, step: 31
	action: tensor([[-0.2878,  0.1276, -0.4514,  0.2095, -0.2276,  0.3957, -0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-3.5527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09633467164869469, distance: 1.0878286763564926 entropy 0.03264415264129639
epoch: 29, step: 32
	action: tensor([[ 0.1819,  0.4624, -0.3565,  0.0556, -0.1893, -0.1795,  0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-4.5286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7055211686524008, distance: 0.6209887518314374 entropy 0.03264415264129639
epoch: 29, step: 33
	action: tensor([[-0.2916,  0.0946, -0.0069,  0.3439, -0.0125, -0.1067, -0.2144]],
       dtype=torch.float64)
	q_value: tensor([[-4.9184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18080903633433454, distance: 1.03573647249267 entropy 0.03264415264129639
epoch: 29, step: 34
	action: tensor([[ 0.3069,  0.2105,  0.0019, -0.4494,  0.3588, -0.0343, -0.5561]],
       dtype=torch.float64)
	q_value: tensor([[-3.7689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4833306363041999, distance: 0.8225514049891055 entropy 0.03264415264129639
epoch: 29, step: 35
	action: tensor([[ 0.3310, -0.1549, -0.2010, -0.3370,  0.3890,  0.0573, -0.1143]],
       dtype=torch.float64)
	q_value: tensor([[-4.1375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2831525308699473, distance: 0.9688801791151117 entropy 0.03264415264129639
epoch: 29, step: 36
	action: tensor([[ 0.2713,  0.0207, -0.3365,  0.2293,  0.0811, -0.1419, -0.1063]],
       dtype=torch.float64)
	q_value: tensor([[-3.6630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5771236826480012, distance: 0.7441549502930633 entropy 0.03264415264129639
epoch: 29, step: 37
	action: tensor([[ 0.4138,  0.1940, -0.1824, -0.2008,  0.2951, -0.0765, -0.3204]],
       dtype=torch.float64)
	q_value: tensor([[-3.7474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6569543958804827, distance: 0.6702435179536911 entropy 0.03264415264129639
epoch: 29, step: 38
	action: tensor([[-0.0208,  0.2270,  0.2245, -0.0283, -0.0832,  0.0485,  0.2574]],
       dtype=torch.float64)
	q_value: tensor([[-3.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.419621042242442, distance: 0.8717912833679927 entropy 0.03264415264129639
epoch: 29, step: 39
	action: tensor([[ 0.5886,  0.3294, -0.2602,  0.1444, -0.0379,  0.0557,  0.1567]],
       dtype=torch.float64)
	q_value: tensor([[-4.1577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9014582590594821, distance: 0.3592252110621514 entropy 0.03264415264129639
epoch: 29, step: 40
	action: tensor([[ 0.5064,  0.5810, -0.3065,  0.0432,  0.1176,  0.3845, -0.2520]],
       dtype=torch.float64)
	q_value: tensor([[-4.7696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9015032521773052, distance: 0.359143192480794 entropy 0.03264415264129639
epoch: 29, step: 41
	action: tensor([[ 0.3179,  0.0865, -0.1258, -0.3204,  0.1805,  0.3918, -0.5251]],
       dtype=torch.float64)
	q_value: tensor([[-5.2460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6062991094092776, distance: 0.71802553934618 entropy 0.03264415264129639
epoch: 29, step: 42
	action: tensor([[-0.1270, -0.2464, -0.7516, -0.1934, -0.2916,  0.0202, -0.4005]],
       dtype=torch.float64)
	q_value: tensor([[-4.1414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02622621058340746, distance: 1.1592530430680021 entropy 0.03264415264129639
epoch: 29, step: 43
	action: tensor([[ 0.1408, -0.0696, -0.3396,  0.1515, -0.0470,  0.1339,  0.0548]],
       dtype=torch.float64)
	q_value: tensor([[-4.6990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42358132862690934, distance: 0.8688118050494935 entropy 0.03264415264129639
epoch: 29, step: 44
	action: tensor([[-0.0406,  0.2186, -0.1043,  0.0330, -0.2344,  0.0389,  0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-3.5185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4162906229410197, distance: 0.8742890282974097 entropy 0.03264415264129639
epoch: 29, step: 45
	action: tensor([[ 0.0719, -0.2594,  0.2391, -0.0673,  0.0287, -0.2698, -0.2203]],
       dtype=torch.float64)
	q_value: tensor([[-4.0975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02108615647266665, distance: 1.132215062834291 entropy 0.03264415264129639
epoch: 29, step: 46
	action: tensor([[ 0.4907,  0.2345,  0.2701,  0.1613, -0.1048,  0.3944, -0.2870]],
       dtype=torch.float64)
	q_value: tensor([[-3.6038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9003856601867486, distance: 0.3611749520439513 entropy 0.03264415264129639
epoch: 29, step: 47
	action: tensor([[ 0.1810, -0.0638, -0.6066,  0.2634,  0.1102,  0.2838, -0.2813]],
       dtype=torch.float64)
	q_value: tensor([[-4.8286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4852773492610212, distance: 0.8210003331034587 entropy 0.03264415264129639
epoch: 29, step: 48
	action: tensor([[-0.3701, -0.2696, -0.0214, -0.1404, -0.2477, -0.2126, -0.0894]],
       dtype=torch.float64)
	q_value: tensor([[-3.8243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4111015549558461, distance: 1.359364200810454 entropy 0.03264415264129639
epoch: 29, step: 49
	action: tensor([[ 0.3341,  0.1295,  0.0632, -0.1627, -0.0963,  0.3357,  0.2702]],
       dtype=torch.float64)
	q_value: tensor([[-4.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6399704632828722, distance: 0.6866347186327164 entropy 0.03264415264129639
epoch: 29, step: 50
	action: tensor([[-0.0867,  0.2591, -0.3013, -0.0986, -0.1643,  0.0226, -0.4535]],
       dtype=torch.float64)
	q_value: tensor([[-4.4778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35747265998975775, distance: 0.9172812140705212 entropy 0.03264415264129639
epoch: 29, step: 51
	action: tensor([[-0.1387,  0.1607, -0.3235,  0.3117,  0.2245,  0.2698, -0.3404]],
       dtype=torch.float64)
	q_value: tensor([[-4.1929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3667280810955631, distance: 0.9106506639386408 entropy 0.03264415264129639
epoch: 29, step: 52
	action: tensor([[ 0.2893, -0.0759, -0.3039, -0.3461, -0.0852, -0.0076, -0.0955]],
       dtype=torch.float64)
	q_value: tensor([[-3.8435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3589537190240041, distance: 0.9162234136247053 entropy 0.03264415264129639
epoch: 29, step: 53
	action: tensor([[ 0.3144, -0.1448, -0.1522,  0.2300, -0.3295,  0.1922, -0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-4.1317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5868917051813719, distance: 0.7355101170861517 entropy 0.03264415264129639
epoch: 29, step: 54
	action: tensor([[ 0.5641, -0.1654,  0.0728, -0.1976, -0.0618,  0.2060,  0.1844]],
       dtype=torch.float64)
	q_value: tensor([[-3.8750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4936127856009038, distance: 0.8143255460328699 entropy 0.03264415264129639
epoch: 29, step: 55
	action: tensor([[-0.0330,  0.1487, -0.2166, -0.3053,  0.5402,  0.0537, -0.1637]],
       dtype=torch.float64)
	q_value: tensor([[-4.3692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937920433868386, distance: 0.9616631982780446 entropy 0.03264415264129639
epoch: 29, step: 56
	action: tensor([[-0.0185,  0.1299, -0.0857, -0.0274, -0.3567, -0.1349, -0.2334]],
       dtype=torch.float64)
	q_value: tensor([[-3.7438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35939944592373185, distance: 0.9159048278632691 entropy 0.03264415264129639
epoch: 29, step: 57
	action: tensor([[ 0.1698, -0.3750, -0.3185,  0.2030,  0.0532,  0.1135,  0.3895]],
       dtype=torch.float64)
	q_value: tensor([[-4.1434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24070743478168277, distance: 0.9971517757093078 entropy 0.03264415264129639
epoch: 29, step: 58
	action: tensor([[-0.0442,  0.2296, -0.3715,  0.0629,  0.0068,  0.6454,  0.1176]],
       dtype=torch.float64)
	q_value: tensor([[-3.9199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4775457777319696, distance: 0.8271434117887868 entropy 0.03264415264129639
epoch: 29, step: 59
	action: tensor([[-0.0367, -0.0166, -0.0522, -0.0398, -0.2800,  0.1045, -0.1503]],
       dtype=torch.float64)
	q_value: tensor([[-4.8679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2518556514489665, distance: 0.9898044297045604 entropy 0.03264415264129639
epoch: 29, step: 60
	action: tensor([[-0.0056, -0.4539,  0.2650, -0.0515,  0.4981,  0.0779, -0.1949]],
       dtype=torch.float64)
	q_value: tensor([[-3.7327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12194136266298328, distance: 1.2121092709896042 entropy 0.03264415264129639
epoch: 29, step: 61
	action: tensor([[ 2.3840e-01,  2.7535e-01, -3.1276e-01, -1.4093e-01,  2.2845e-01,
          2.8469e-04,  3.6528e-02]], dtype=torch.float64)
	q_value: tensor([[-3.4392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.634254686362939, distance: 0.6920637128054061 entropy 0.03264415264129639
epoch: 29, step: 62
	action: tensor([[ 0.1145,  0.2235, -0.0059, -0.5589, -0.3486,  0.1959, -0.2016]],
       dtype=torch.float64)
	q_value: tensor([[-4.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3604214754447378, distance: 0.9151739079268739 entropy 0.03264415264129639
epoch: 29, step: 63
	action: tensor([[ 0.0303,  0.0625, -0.0804, -0.1806,  0.2644, -0.1682, -0.0510]],
       dtype=torch.float64)
	q_value: tensor([[-5.2350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2528230873106573, distance: 0.9891642577470776 entropy 0.03264415264129639
epoch: 29, step: 64
	action: tensor([[ 0.4321, -0.2059, -0.3508, -0.4661,  0.1853, -0.0486, -0.4634]],
       dtype=torch.float64)
	q_value: tensor([[-3.5768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2170165476678081, distance: 1.012588486823559 entropy 0.03264415264129639
epoch: 29, step: 65
	action: tensor([[ 0.1947,  0.3593, -0.1886, -0.0256,  0.2929,  0.0345,  0.1440]],
       dtype=torch.float64)
	q_value: tensor([[-4.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6823584376552377, distance: 0.6449489722129828 entropy 0.03264415264129639
epoch: 29, step: 66
	action: tensor([[-0.0496,  0.4023, -0.1849,  0.1074,  0.1058, -0.1086,  0.1369]],
       dtype=torch.float64)
	q_value: tensor([[-4.1048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5020918739796877, distance: 0.8074791186945234 entropy 0.03264415264129639
epoch: 29, step: 67
	action: tensor([[ 0.0877, -0.6011, -0.2316,  0.1736, -0.2053, -0.0620, -0.5691]],
       dtype=torch.float64)
	q_value: tensor([[-4.1118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06956076234879771, distance: 1.1834759159076937 entropy 0.03264415264129639
epoch: 29, step: 68
	action: tensor([[-0.0386, -0.3272, -0.3585, -0.2292, -0.0613,  0.1100, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-3.8170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05995743262991793, distance: 1.1781508625721775 entropy 0.03264415264129639
epoch: 29, step: 69
	action: tensor([[ 0.1123,  0.0709, -0.3365,  0.3517,  0.2536,  0.2675, -0.1991]],
       dtype=torch.float64)
	q_value: tensor([[-3.6956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5611838577149628, distance: 0.7580502422080143 entropy 0.03264415264129639
epoch: 29, step: 70
	action: tensor([[ 0.1261, -0.0300, -0.1922, -0.8834, -0.2613, -0.1142, -0.3597]],
       dtype=torch.float64)
	q_value: tensor([[-3.6981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.055276143793869026, distance: 1.1122672104125397 entropy 0.03264415264129639
epoch: 29, step: 71
	action: tensor([[ 0.0675,  0.2704, -0.2763,  0.1962, -0.2591, -0.1185, -0.1985]],
       dtype=torch.float64)
	q_value: tensor([[-5.6666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5313439864473263, distance: 0.7834004023996651 entropy 0.03264415264129639
epoch: 29, step: 72
	action: tensor([[-0.0802,  0.2362, -0.0903,  0.1211,  0.0808,  0.1151, -0.1398]],
       dtype=torch.float64)
	q_value: tensor([[-4.1894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4151532322506075, distance: 0.875140414532429 entropy 0.03264415264129639
epoch: 29, step: 73
	action: tensor([[-0.4636,  0.0642, -0.3560,  0.1365, -0.1258,  0.0119, -0.0415]],
       dtype=torch.float64)
	q_value: tensor([[-3.6256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16361551353460446, distance: 1.234415722538076 entropy 0.03264415264129639
epoch: 29, step: 74
	action: tensor([[-0.1928, -0.0512,  0.0772,  0.1804,  0.0251, -0.2705,  0.2448]],
       dtype=torch.float64)
	q_value: tensor([[-4.1838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05863767666776498, distance: 1.110286602707292 entropy 0.03264415264129639
epoch: 29, step: 75
	action: tensor([[-0.0972, -0.0040, -0.0592, -0.0220,  0.6241, -0.0461, -0.0890]],
       dtype=torch.float64)
	q_value: tensor([[-3.9169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1357648908347383, distance: 1.0638310109023517 entropy 0.03264415264129639
epoch: 29, step: 76
	action: tensor([[-0.2282,  0.1034,  0.1712, -0.2200, -0.5924,  0.3465,  0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-3.5467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015218516300992024, distance: 1.1356032594164343 entropy 0.03264415264129639
epoch: 29, step: 77
	action: tensor([[ 0.0795, -0.1869, -0.2298,  0.1773,  0.0071,  0.3003, -0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-5.3597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3233581442397254, distance: 0.9413174783356155 entropy 0.03264415264129639
epoch: 29, step: 78
	action: tensor([[-0.0890, -0.1258, -0.2982,  0.0795, -0.4500, -0.3938,  0.5087]],
       dtype=torch.float64)
	q_value: tensor([[-3.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1214930754204524, distance: 1.0725789953692504 entropy 0.03264415264129639
epoch: 29, step: 79
	action: tensor([[ 0.2890, -0.3470, -0.0363,  0.2687, -0.0612, -0.0964, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[-5.2552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3551458142654307, distance: 0.9189406321823449 entropy 0.03264415264129639
epoch: 29, step: 80
	action: tensor([[ 0.3276, -0.3635, -0.1858, -0.7899,  0.3617,  0.2191,  0.2514]],
       dtype=torch.float64)
	q_value: tensor([[-3.6343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10573986531465329, distance: 1.2033256578768605 entropy 0.03264415264129639
epoch: 29, step: 81
	action: tensor([[0.4150, 0.3281, 0.2294, 0.2371, 0.0983, 0.2806, 0.2101]],
       dtype=torch.float64)
	q_value: tensor([[-4.9481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9137446562857335, distance: 0.33608544006960317 entropy 0.03264415264129639
epoch: 29, step: 82
	action: tensor([[ 0.2295, -0.4140, -0.1933, -0.1255, -0.1374,  0.0372, -0.5872]],
       dtype=torch.float64)
	q_value: tensor([[-4.5108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08816593538570017, distance: 1.092734360869262 entropy 0.03264415264129639
epoch: 29, step: 83
	action: tensor([[ 0.1490,  0.0311,  0.0634, -0.0627,  0.1503, -0.1376, -0.1566]],
       dtype=torch.float64)
	q_value: tensor([[-3.7714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39046432552978194, distance: 0.8934212082748421 entropy 0.03264415264129639
epoch: 29, step: 84
	action: tensor([[ 0.1471, -0.0267, -0.6904, -0.3573,  0.0870, -0.1280, -0.2308]],
       dtype=torch.float64)
	q_value: tensor([[-3.4716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37591257812017875, distance: 0.9040228506776733 entropy 0.03264415264129639
epoch: 29, step: 85
	action: tensor([[ 0.1899, -0.0109, -0.1713, -0.2629,  0.2309, -0.1228, -0.2186]],
       dtype=torch.float64)
	q_value: tensor([[-4.6124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3267085414695584, distance: 0.938984116054762 entropy 0.03264415264129639
epoch: 29, step: 86
	action: tensor([[ 0.1851,  0.0554, -0.1630, -0.0267, -0.2486, -0.1108, -0.2132]],
       dtype=torch.float64)
	q_value: tensor([[-3.5831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4918602993213599, distance: 0.8157334229785533 entropy 0.03264415264129639
epoch: 29, step: 87
	action: tensor([[ 0.2471, -0.0321, -0.2733, -0.0943,  0.2515,  0.6232, -0.4495]],
       dtype=torch.float64)
	q_value: tensor([[-3.8904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6087152617120439, distance: 0.7158188781227708 entropy 0.03264415264129639
epoch: 29, step: 88
	action: tensor([[ 0.2313, -0.0200,  0.1734,  0.1071,  0.1724, -0.2310, -0.1428]],
       dtype=torch.float64)
	q_value: tensor([[-4.1001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47832344537961624, distance: 0.8265275853802753 entropy 0.03264415264129639
epoch: 29, step: 89
	action: tensor([[ 0.1341, -0.1333, -0.2034,  0.2992, -0.0913, -0.1783, -0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-3.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41979454109991066, distance: 0.8716609667056339 entropy 0.03264415264129639
epoch: 29, step: 90
	action: tensor([[ 0.4331,  0.0646,  0.0141, -0.4450, -0.0173, -0.0303,  0.2099]],
       dtype=torch.float64)
	q_value: tensor([[-3.6248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44573149848367344, distance: 0.8519552679686636 entropy 0.03264415264129639
epoch: 29, step: 91
	action: tensor([[ 0.0550,  0.1286, -0.1069,  0.0751, -0.4131,  0.1900, -0.2559]],
       dtype=torch.float64)
	q_value: tensor([[-4.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.484100156485048, distance: 0.8219386284145287 entropy 0.03264415264129639
epoch: 29, step: 92
	action: tensor([[ 0.2473, -0.2243,  0.2097,  0.1838,  0.2894,  0.3396,  0.2209]],
       dtype=torch.float64)
	q_value: tensor([[-4.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5508982249090829, distance: 0.7668829443777737 entropy 0.03264415264129639
epoch: 29, step: 93
	action: tensor([[ 0.0958,  0.0553, -0.1963, -0.0894, -0.1217,  0.1735, -0.2342]],
       dtype=torch.float64)
	q_value: tensor([[-3.8250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4124800865514707, distance: 0.8771381265149347 entropy 0.03264415264129639
epoch: 29, step: 94
	action: tensor([[ 0.4830,  0.5397, -0.6276,  0.3079, -0.3107,  0.6015,  0.2358]],
       dtype=torch.float64)
	q_value: tensor([[-3.6579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7715873802403073, distance: 0.5469111036750831 entropy 0.03264415264129639
epoch: 29, step: 95
	action: tensor([[-0.1819,  0.0894, -0.2170, -0.0292,  0.0603, -0.0975, -0.2977]],
       dtype=torch.float64)
	q_value: tensor([[-6.8178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14815808361499327, distance: 1.056175761511774 entropy 0.03264415264129639
epoch: 29, step: 96
	action: tensor([[-0.2331,  0.2114, -0.0266, -0.3518, -0.1473, -0.0745, -0.1521]],
       dtype=torch.float64)
	q_value: tensor([[-3.5292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048014793983317317, distance: 1.1165335899784519 entropy 0.03264415264129639
epoch: 29, step: 97
	action: tensor([[ 0.2221, -0.1097, -0.2675, -0.0111, -0.2668, -0.4113, -0.3101]],
       dtype=torch.float64)
	q_value: tensor([[-4.1568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3624426270584614, distance: 0.9137267292061891 entropy 0.03264415264129639
epoch: 29, step: 98
	action: tensor([[ 0.1342,  0.2344,  0.0940, -0.0622,  0.3212,  0.0377, -0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-4.3433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5596849527403029, distance: 0.7593438097511624 entropy 0.03264415264129639
epoch: 29, step: 99
	action: tensor([[ 0.1810, -0.3466, -0.4656,  0.0057, -0.2555, -0.0330, -0.7352]],
       dtype=torch.float64)
	q_value: tensor([[-3.5486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17141766714084006, distance: 1.0416564988799821 entropy 0.03264415264129639
epoch: 29, step: 100
	action: tensor([[ 0.2015,  0.1205, -0.2652,  0.1186,  0.5324,  0.5462,  0.2553]],
       dtype=torch.float64)
	q_value: tensor([[-4.2519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.705004329878246, distance: 0.62153346053293 entropy 0.03264415264129639
epoch: 29, step: 101
	action: tensor([[ 0.0221,  0.0046,  0.1614,  0.1444, -0.4740,  0.3211, -0.8172]],
       dtype=torch.float64)
	q_value: tensor([[-4.3936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4510371474950442, distance: 0.8478678582863417 entropy 0.03264415264129639
epoch: 29, step: 102
	action: tensor([[ 0.0871,  0.3330,  0.0044,  0.1341, -0.0652, -0.3079,  0.2262]],
       dtype=torch.float64)
	q_value: tensor([[-5.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5909880281119682, distance: 0.731854425471392 entropy 0.03264415264129639
epoch: 29, step: 103
	action: tensor([[ 0.3882, -0.0395, -0.1679, -0.3445,  0.0549, -0.3189, -0.1766]],
       dtype=torch.float64)
	q_value: tensor([[-4.3898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33317271494577827, distance: 0.9344657197414362 entropy 0.03264415264129639
epoch: 29, step: 104
	action: tensor([[-0.0678, -0.0931, -0.3847,  0.2203, -0.1720,  0.1699,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-4.3199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2284133686717651, distance: 1.0051920391753792 entropy 0.03264415264129639
epoch: 29, step: 105
	action: tensor([[ 0.0662,  0.1362,  0.1523,  0.1509,  0.7835,  0.0591, -0.2980]],
       dtype=torch.float64)
	q_value: tensor([[-3.6673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5269786921939643, distance: 0.7870404354764994 entropy 0.03264415264129639
epoch: 29, step: 106
	action: tensor([[ 0.0665, -0.0403, -0.0990, -0.1054,  0.0403,  0.3227, -0.6060]],
       dtype=torch.float64)
	q_value: tensor([[-3.7091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.338004837935055, distance: 0.9310737894763359 entropy 0.03264415264129639
epoch: 29, step: 107
	action: tensor([[-0.1040,  0.0656, -0.0708,  0.0680, -0.2677,  0.1807, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-3.7744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.293281331676677, distance: 0.9620108606701282 entropy 0.03264415264129639
epoch: 29, step: 108
	action: tensor([[ 0.0007, -0.0145, -0.3866, -0.5264, -0.3944, -0.0581, -0.1716]],
       dtype=torch.float64)
	q_value: tensor([[-3.9496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1751346550493742, distance: 1.0393174575815098 entropy 0.03264415264129639
epoch: 29, step: 109
	action: tensor([[ 0.2253, -0.1358, -0.4280, -0.2965,  0.1082,  0.0180, -0.1984]],
       dtype=torch.float64)
	q_value: tensor([[-5.1264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30941101346662514, distance: 0.9509693445710997 entropy 0.03264415264129639
epoch: 29, step: 110
	action: tensor([[ 0.2706,  0.2379, -0.0252,  0.1045,  0.1417,  0.1557, -0.6312]],
       dtype=torch.float64)
	q_value: tensor([[-3.7745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7344240289205713, distance: 0.5897270956221602 entropy 0.03264415264129639
epoch: 29, step: 111
	action: tensor([[ 0.6766, -0.2132,  0.1213, -0.1586,  0.2265, -0.0129, -0.1418]],
       dtype=torch.float64)
	q_value: tensor([[-4.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4365329649277897, distance: 0.8589956210455472 entropy 0.03264415264129639
epoch: 29, step: 112
	action: tensor([[ 0.2306,  0.2980, -0.2309, -0.0342,  0.4167,  0.1339, -0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-4.3099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.705480633182322, distance: 0.6210314903954791 entropy 0.03264415264129639
epoch: 29, step: 113
	action: tensor([[-0.0557,  0.2242, -0.0540, -0.1069, -0.0030, -0.0006, -0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-3.8924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35710968898542084, distance: 0.9175402687867671 entropy 0.03264415264129639
epoch: 29, step: 114
	action: tensor([[ 0.2040,  0.1622, -0.1540, -0.3306, -0.1409,  0.0688,  0.1582]],
       dtype=torch.float64)
	q_value: tensor([[-3.7075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47581611388746425, distance: 0.8285114722120388 entropy 0.03264415264129639
epoch: 29, step: 115
	action: tensor([[ 0.1764, -0.3297, -0.7002, -0.2164,  0.0711,  0.2409, -0.4813]],
       dtype=torch.float64)
	q_value: tensor([[-4.4443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1859507945569866, distance: 1.0324808888453354 entropy 0.03264415264129639
epoch: 29, step: 116
	action: tensor([[ 0.2023,  0.3985, -0.6013, -0.2892, -0.0392,  0.4204, -0.3670]],
       dtype=torch.float64)
	q_value: tensor([[-4.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6930982084547095, distance: 0.6339520420303607 entropy 0.03264415264129639
epoch: 29, step: 117
	action: tensor([[ 0.4244,  0.0995, -0.1217,  0.1230, -0.2583, -0.0857, -0.1333]],
       dtype=torch.float64)
	q_value: tensor([[-5.6068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7355511735597307, distance: 0.5884743194157814 entropy 0.03264415264129639
epoch: 29, step: 118
	action: tensor([[ 0.0088, -0.3369, -0.0741, -0.0569, -0.4456, -0.2677, -0.3323]],
       dtype=torch.float64)
	q_value: tensor([[-4.2002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016027377612689797, distance: 1.1534782198394775 entropy 0.03264415264129639
epoch: 29, step: 119
	action: tensor([[ 0.1497, -0.0755,  0.1453, -0.3414,  0.1212, -0.1381, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-4.1996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13259304617538548, distance: 1.0657814155533787 entropy 0.03264415264129639
epoch: 29, step: 120
	action: tensor([[ 0.1430, -0.2029, -0.5407, -0.1948,  0.0919, -0.0139,  0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-3.7273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20892320757844685, distance: 1.0178083635374564 entropy 0.03264415264129639
epoch: 29, step: 121
	action: tensor([[ 0.0318, -0.1663, -0.2464,  0.1742,  0.0644,  0.2066, -0.2246]],
       dtype=torch.float64)
	q_value: tensor([[-3.9356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3159476499921796, distance: 0.9464580357295583 entropy 0.03264415264129639
epoch: 29, step: 122
	action: tensor([[ 0.0347,  0.2048, -0.1513, -0.2849,  0.2411, -0.0017,  0.0740]],
       dtype=torch.float64)
	q_value: tensor([[-3.2316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3747899908976562, distance: 0.904835548152837 entropy 0.03264415264129639
epoch: 29, step: 123
	action: tensor([[-0.1400,  0.0704, -0.2753, -0.0363,  0.0019, -0.0012,  0.1260]],
       dtype=torch.float64)
	q_value: tensor([[-3.8153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19434964588473613, distance: 1.0271408327792757 entropy 0.03264415264129639
epoch: 29, step: 124
	action: tensor([[ 0.1023,  0.0946, -0.2964,  0.1395, -0.2268, -0.1374,  0.2352]],
       dtype=torch.float64)
	q_value: tensor([[-3.8153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48079629214761066, distance: 0.8245663088970953 entropy 0.03264415264129639
epoch: 29, step: 125
	action: tensor([[ 0.2793,  0.3346, -0.6330,  0.0640,  0.2941,  0.2970,  0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-4.1418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7405763905948776, distance: 0.582856228796242 entropy 0.03264415264129639
epoch: 29, step: 126
	action: tensor([[ 0.5274,  0.4089, -0.2179,  0.1883, -0.1197, -0.3053,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-5.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8948960702229236, distance: 0.3709933888066753 entropy 0.03264415264129639
epoch: 29, step: 127
	action: tensor([[ 0.2188, -0.1404, -0.2858, -0.0072,  0.3169,  0.2602,  0.0533]],
       dtype=torch.float64)
	q_value: tensor([[-5.0938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45031223243614915, distance: 0.8484274857741984 entropy 0.03264415264129639
LOSS epoch 29 actor 10.83202457109597 critic 37.668047260646716 
epoch: 30, step: 0
	action: tensor([[ 0.1925,  0.1815,  0.0125, -0.3539, -0.0797,  0.2476,  0.3038]],
       dtype=torch.float64)
	q_value: tensor([[-2.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49212174612524406, distance: 0.815523541388963 entropy 0.03264415264129639
epoch: 30, step: 1
	action: tensor([[ 0.0810,  0.1555, -0.0937, -0.4227, -0.3630,  0.3052,  0.2160]],
       dtype=torch.float64)
	q_value: tensor([[-4.0894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35808550481166557, distance: 0.9168436566962006 entropy 0.03264415264129639
epoch: 30, step: 2
	action: tensor([[ 0.1092, -0.0338, -0.2843,  0.1820,  0.1324,  0.1794,  0.0769]],
       dtype=torch.float64)
	q_value: tensor([[-4.5125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4682628314878041, distance: 0.8344593833395352 entropy 0.03264415264129639
epoch: 30, step: 3
	action: tensor([[ 0.0121, -0.1532, -0.1480, -0.7510,  0.1551, -0.3278, -0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-3.0036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15850529649574296, distance: 1.2317021658495428 entropy 0.03264415264129639
epoch: 30, step: 4
	action: tensor([[-0.4257,  0.4750, -0.0079, -0.2604,  0.1430,  0.2703, -0.2928]],
       dtype=torch.float64)
	q_value: tensor([[-4.1183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10354131978085745, distance: 1.0834823266027196 entropy 0.03264415264129639
epoch: 30, step: 5
	action: tensor([[-0.0517,  0.0695,  0.1113,  0.0568,  0.2402,  0.3457, -0.0503]],
       dtype=torch.float64)
	q_value: tensor([[-4.2114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40294438592288984, distance: 0.8842276401689309 entropy 0.03264415264129639
epoch: 30, step: 6
	action: tensor([[ 0.1810, -0.2706, -0.7098,  0.0953, -0.4085,  0.0172, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-3.0359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24846352912770964, distance: 0.992045802308968 entropy 0.03264415264129639
epoch: 30, step: 7
	action: tensor([[ 0.2903, -0.3114, -0.1398,  0.3341,  0.2766,  0.0041, -0.1128]],
       dtype=torch.float64)
	q_value: tensor([[-3.8560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45442367212719925, distance: 0.8452485850954969 entropy 0.03264415264129639
epoch: 30, step: 8
	action: tensor([[ 0.3196,  0.5985, -0.6188,  0.1076,  0.1009, -0.2590, -0.5124]],
       dtype=torch.float64)
	q_value: tensor([[-3.1158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8255165684085992, distance: 0.4780064868077024 entropy 0.03264415264129639
epoch: 30, step: 9
	action: tensor([[ 2.1944e-04,  3.0608e-01, -2.5250e-01, -4.5917e-01, -1.6637e-01,
          9.1687e-02,  5.2298e-03]], dtype=torch.float64)
	q_value: tensor([[-5.1665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40693854821860553, distance: 0.8812650391139984 entropy 0.03264415264129639
epoch: 30, step: 10
	action: tensor([[ 0.3887,  0.2775, -0.5912, -0.1584,  0.2890,  0.0973,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-4.3107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7639118784592407, distance: 0.5560242866681676 entropy 0.03264415264129639
epoch: 30, step: 11
	action: tensor([[ 0.3544,  0.2559, -0.2366,  0.0305,  0.3383,  0.3102, -0.3584]],
       dtype=torch.float64)
	q_value: tensor([[-4.4080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7959441977778411, distance: 0.5169293218184119 entropy 0.03264415264129639
epoch: 30, step: 12
	action: tensor([[ 0.0635,  0.1425, -0.4522, -0.2855,  0.2770, -0.0101, -0.3991]],
       dtype=torch.float64)
	q_value: tensor([[-3.4141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42094138445738005, distance: 0.8707990711060849 entropy 0.03264415264129639
epoch: 30, step: 13
	action: tensor([[ 0.2745, -0.3272, -0.6217, -0.0330, -0.2710,  0.0591, -0.2459]],
       dtype=torch.float64)
	q_value: tensor([[-3.4298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2584675501687925, distance: 0.9854209103288457 entropy 0.03264415264129639
epoch: 30, step: 14
	action: tensor([[ 0.1874,  0.2794, -0.0541, -0.1629,  0.0276, -0.0157, -0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-3.5268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5927843745446195, distance: 0.7302455352320634 entropy 0.03264415264129639
epoch: 30, step: 15
	action: tensor([[ 0.3777,  0.2699, -0.5411, -0.2867,  0.2978, -0.3913, -0.4290]],
       dtype=torch.float64)
	q_value: tensor([[-3.3608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6799370741299908, distance: 0.647402509923101 entropy 0.03264415264129639
epoch: 30, step: 16
	action: tensor([[ 0.4306, -0.1724, -0.4455,  0.1764,  0.2472, -0.0662,  0.0614]],
       dtype=torch.float64)
	q_value: tensor([[-4.7239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5449168732327312, distance: 0.7719729092295801 entropy 0.03264415264129639
epoch: 30, step: 17
	action: tensor([[ 0.4268, -0.1047, -0.0649,  0.0029,  0.1054, -0.0073, -0.5438]],
       dtype=torch.float64)
	q_value: tensor([[-3.5503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5575469807671218, distance: 0.7611850934755878 entropy 0.03264415264129639
epoch: 30, step: 18
	action: tensor([[-0.0299, -0.1079, -0.1237, -0.2048, -0.2875,  0.1838, -0.2082]],
       dtype=torch.float64)
	q_value: tensor([[-3.3910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12678008491840287, distance: 1.0693466400979326 entropy 0.03264415264129639
epoch: 30, step: 19
	action: tensor([[-0.3068,  0.3117, -0.1450, -0.0707,  0.1374,  0.1273, -0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-3.3934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17774042301813908, distance: 1.037674545575422 entropy 0.03264415264129639
epoch: 30, step: 20
	action: tensor([[ 0.1860,  0.3360, -0.1306,  0.0425, -0.5924, -0.4547, -0.3678]],
       dtype=torch.float64)
	q_value: tensor([[-3.4727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6929254897098552, distance: 0.6341304052769136 entropy 0.03264415264129639
epoch: 30, step: 21
	action: tensor([[-0.0268,  0.0744, -0.2655,  0.1556, -0.0043,  0.3295, -0.3275]],
       dtype=torch.float64)
	q_value: tensor([[-4.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42080524484962634, distance: 0.8709014297220087 entropy 0.03264415264129639
epoch: 30, step: 22
	action: tensor([[ 0.2218,  0.3348, -0.2344,  0.2138,  0.0974, -0.0112, -0.4807]],
       dtype=torch.float64)
	q_value: tensor([[-3.0937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7257606457871268, distance: 0.599268682201245 entropy 0.03264415264129639
epoch: 30, step: 23
	action: tensor([[-0.2312,  0.1362, -0.3452,  0.2119, -0.4586,  0.0850, -0.2757]],
       dtype=torch.float64)
	q_value: tensor([[-3.6028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17724792941147638, distance: 1.0379852574196202 entropy 0.03264415264129639
epoch: 30, step: 24
	action: tensor([[ 0.0097, -0.0161, -0.3775,  0.3805,  0.2953, -0.0028, -0.4689]],
       dtype=torch.float64)
	q_value: tensor([[-3.8910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3868760858861179, distance: 0.8960470637790626 entropy 0.03264415264129639
epoch: 30, step: 25
	action: tensor([[ 0.0836, -0.1935,  0.0374, -0.1007,  0.3636,  0.3893, -0.2451]],
       dtype=torch.float64)
	q_value: tensor([[-3.2932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938654103479307, distance: 0.9616132440554015 entropy 0.03264415264129639
epoch: 30, step: 26
	action: tensor([[-0.0920, -0.0862, -0.7343, -0.1666, -0.2364,  0.1588, -0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-2.8125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13187692988676414, distance: 1.0662212702071687 entropy 0.03264415264129639
epoch: 30, step: 27
	action: tensor([[ 0.5951, -0.0708, -0.1920,  0.3442, -0.1069,  0.1578, -0.3140]],
       dtype=torch.float64)
	q_value: tensor([[-4.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7978852023017056, distance: 0.514464898890816 entropy 0.03264415264129639
epoch: 30, step: 28
	action: tensor([[-0.0591,  0.2183, -0.3315, -0.4552,  0.3038,  0.2245,  0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-3.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.347330427843332, distance: 0.9244924660483484 entropy 0.03264415264129639
epoch: 30, step: 29
	action: tensor([[ 0.2471, -0.0589, -0.1756, -0.1958,  0.0662,  0.3480, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[-4.0269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46495432632223244, distance: 0.8370513894656544 entropy 0.03264415264129639
epoch: 30, step: 30
	action: tensor([[ 0.0815,  0.2344, -0.0997, -0.0264, -0.1459,  0.1854, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-3.2037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5492842275263143, distance: 0.7682597336302748 entropy 0.03264415264129639
epoch: 30, step: 31
	action: tensor([[ 0.0705,  0.0692, -0.2931, -0.1370,  0.5049,  0.0680, -0.1433]],
       dtype=torch.float64)
	q_value: tensor([[-3.4800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3943450369251654, distance: 0.8905726088152842 entropy 0.03264415264129639
epoch: 30, step: 32
	action: tensor([[ 0.1274,  0.0392, -0.0317,  0.1663, -0.4018, -0.0767,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-3.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5141618975381672, distance: 0.7976318350926856 entropy 0.03264415264129639
epoch: 30, step: 33
	action: tensor([[ 0.1643,  0.0231, -0.3463,  0.2852,  0.4424,  0.0470,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-3.5143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5467388643814146, distance: 0.770426006741349 entropy 0.03264415264129639
epoch: 30, step: 34
	action: tensor([[-0.2011,  0.1873, -0.1533,  0.2347, -0.3127, -0.2728, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-3.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2859509518931106, distance: 0.9669871781100191 entropy 0.03264415264129639
epoch: 30, step: 35
	action: tensor([[ 0.6376, -0.0321, -0.2973,  0.1517,  0.1222, -0.0861, -0.4337]],
       dtype=torch.float64)
	q_value: tensor([[-3.6612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7268772453401178, distance: 0.5980474394303072 entropy 0.03264415264129639
epoch: 30, step: 36
	action: tensor([[-0.0838, -0.0069, -0.1701, -0.4323,  0.1112,  0.1093, -0.1891]],
       dtype=torch.float64)
	q_value: tensor([[-3.9202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07934733992247178, distance: 1.0980057121054196 entropy 0.03264415264129639
epoch: 30, step: 37
	action: tensor([[-0.1456,  0.3739, -0.0442, -0.2402, -0.1399,  0.0685, -0.0178]],
       dtype=torch.float64)
	q_value: tensor([[-3.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3188019905615811, distance: 0.9444813320383846 entropy 0.03264415264129639
epoch: 30, step: 38
	action: tensor([[ 0.0598, -0.1133, -0.2001, -0.3749,  0.2925,  0.1056,  0.1130]],
       dtype=torch.float64)
	q_value: tensor([[-3.8631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12695705955018743, distance: 1.0692382728846896 entropy 0.03264415264129639
epoch: 30, step: 39
	action: tensor([[ 0.1314,  0.3725, -0.1464,  0.0827,  0.2685,  0.3717, -0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-3.2758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7023905072172824, distance: 0.6242809507517285 entropy 0.03264415264129639
epoch: 30, step: 40
	action: tensor([[-0.0329, -0.5200, -0.3495, -0.3423, -0.2366, -0.2660, -0.1839]],
       dtype=torch.float64)
	q_value: tensor([[-3.4381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2793908007611461, distance: 1.2943696021011337 entropy 0.03264415264129639
epoch: 30, step: 41
	action: tensor([[-0.1185, -0.1751, -0.0851,  0.2618, -0.3561,  0.3100, -0.2735]],
       dtype=torch.float64)
	q_value: tensor([[-3.8963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1629693721090546, distance: 1.046953439668186 entropy 0.03264415264129639
epoch: 30, step: 42
	action: tensor([[ 0.3617,  0.0198, -0.3378,  0.0902, -0.3031,  0.0021, -0.2307]],
       dtype=torch.float64)
	q_value: tensor([[-3.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6203906156891177, distance: 0.7050585172178165 entropy 0.03264415264129639
epoch: 30, step: 43
	action: tensor([[ 0.3028,  0.3634, -0.2775, -0.4331,  0.3164,  0.2336, -0.3333]],
       dtype=torch.float64)
	q_value: tensor([[-3.5781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7135415222046343, distance: 0.6124738249650972 entropy 0.03264415264129639
epoch: 30, step: 44
	action: tensor([[-0.0855,  0.0335,  0.3291,  0.0213, -0.1834,  0.2572,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-4.0126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31734371178921494, distance: 0.9454917436377358 entropy 0.03264415264129639
epoch: 30, step: 45
	action: tensor([[ 0.5926,  0.2245, -0.6070, -0.0943, -0.1194,  0.3774, -0.1320]],
       dtype=torch.float64)
	q_value: tensor([[-3.5096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8463399490238794, distance: 0.4485771834457812 entropy 0.03264415264129639
epoch: 30, step: 46
	action: tensor([[ 0.1478,  0.3475,  0.2483, -0.2117, -0.4567,  0.0943,  0.0431]],
       dtype=torch.float64)
	q_value: tensor([[-4.9773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5693306304962886, distance: 0.7509805437507334 entropy 0.03264415264129639
epoch: 30, step: 47
	action: tensor([[-0.0144,  0.0248, -0.1570, -0.0510,  0.3753, -0.1927, -0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-4.6297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22842759315200778, distance: 1.0051827735922159 entropy 0.03264415264129639
epoch: 30, step: 48
	action: tensor([[ 0.0068,  0.3306, -0.5078,  0.1395,  0.1801,  0.1452,  0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-3.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5240774094974198, distance: 0.7894504073939521 entropy 0.03264415264129639
epoch: 30, step: 49
	action: tensor([[-0.0331,  0.3592, -0.2589, -0.3642, -0.0932,  0.5198,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[-3.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4754117675980396, distance: 0.828830960247592 entropy 0.03264415264129639
epoch: 30, step: 50
	action: tensor([[ 0.1480, -0.1226, -0.1700,  0.2626, -0.0137,  0.2115, -0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-4.7688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4967532540028031, distance: 0.8117965119999522 entropy 0.03264415264129639
epoch: 30, step: 51
	action: tensor([[ 0.0747, -0.2735, -0.0753,  0.3611, -0.3237,  0.0823, -0.1088]],
       dtype=torch.float64)
	q_value: tensor([[-2.9004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3571234873942053, distance: 0.9175304221119747 entropy 0.03264415264129639
epoch: 30, step: 52
	action: tensor([[-0.2642, -0.0411, -0.4399, -0.3811, -0.1488, -0.0120,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-3.0319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06366839670478242, distance: 1.1802114431962551 entropy 0.03264415264129639
epoch: 30, step: 53
	action: tensor([[-0.0705,  0.2822, -0.2858, -0.3794,  0.4950,  0.1933,  0.1115]],
       dtype=torch.float64)
	q_value: tensor([[-4.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3719402528604584, distance: 0.9068953461694284 entropy 0.03264415264129639
epoch: 30, step: 54
	action: tensor([[ 0.3248, -0.1277, -0.2712, -0.5355, -0.2598, -0.0822,  0.1684]],
       dtype=torch.float64)
	q_value: tensor([[-3.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18493902415759955, distance: 1.0331223176470035 entropy 0.03264415264129639
epoch: 30, step: 55
	action: tensor([[ 0.4055, -0.0290, -0.1628,  0.0153, -0.2717,  0.1651, -0.2391]],
       dtype=torch.float64)
	q_value: tensor([[-4.5393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5953025899240081, distance: 0.7279841198988385 entropy 0.03264415264129639
epoch: 30, step: 56
	action: tensor([[ 0.0260,  0.1911,  0.0930, -0.4235,  0.0359, -0.1942,  0.2081]],
       dtype=torch.float64)
	q_value: tensor([[-3.5569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20881302623064946, distance: 1.0178792413547542 entropy 0.03264415264129639
epoch: 30, step: 57
	action: tensor([[ 0.3706, -0.1824,  0.2769, -0.0529, -0.5450,  0.3424, -0.4717]],
       dtype=torch.float64)
	q_value: tensor([[-3.8794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4740907584924232, distance: 0.829873878063006 entropy 0.03264415264129639
epoch: 30, step: 58
	action: tensor([[ 0.1541, -0.1452, -0.2192,  0.2612, -0.0311, -0.0485, -0.2672]],
       dtype=torch.float64)
	q_value: tensor([[-4.5892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43192039323759335, distance: 0.8625043473816444 entropy 0.03264415264129639
epoch: 30, step: 59
	action: tensor([[ 0.1882, -0.0930, -0.3317, -0.1029, -0.0907, -0.1016,  0.6375]],
       dtype=torch.float64)
	q_value: tensor([[-2.9895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33921413596154126, distance: 0.9302229823741808 entropy 0.03264415264129639
epoch: 30, step: 60
	action: tensor([[ 0.1663,  0.2980,  0.1113, -0.2233, -0.4009, -0.0108, -0.0840]],
       dtype=torch.float64)
	q_value: tensor([[-4.3775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5532743952883483, distance: 0.7648514880552658 entropy 0.03264415264129639
epoch: 30, step: 61
	action: tensor([[ 0.0214, -0.3915, -0.2500, -0.0276,  0.1582, -0.1139, -0.2630]],
       dtype=torch.float64)
	q_value: tensor([[-4.3052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.046838364657692644, distance: 1.170837189197489 entropy 0.03264415264129639
epoch: 30, step: 62
	action: tensor([[-0.0400,  0.1251,  0.1481, -0.1096, -0.0667,  0.0107, -0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-2.8481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29445583329843483, distance: 0.9612111413491484 entropy 0.03264415264129639
epoch: 30, step: 63
	action: tensor([[ 0.1634, -0.6325,  0.1880,  0.4033,  0.1982, -0.2565,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-3.2533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05661648957563126, distance: 1.1114779047265975 entropy 0.03264415264129639
epoch: 30, step: 64
	action: tensor([[-0.0119,  0.1979, -0.3838,  0.0738, -0.0542,  0.6708, -0.0270]],
       dtype=torch.float64)
	q_value: tensor([[-3.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4854274819773843, distance: 0.8208805909479212 entropy 0.03264415264129639
epoch: 30, step: 65
	action: tensor([[ 0.3365,  0.5187, -0.2981, -0.0922,  0.4407, -0.2242,  0.5069]],
       dtype=torch.float64)
	q_value: tensor([[-4.2492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7844513848414492, distance: 0.5312871497590756 entropy 0.03264415264129639
epoch: 30, step: 66
	action: tensor([[ 0.6289, -0.0206, -0.4078,  0.3058,  0.0828,  0.0823,  0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-5.1077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7953759417248404, distance: 0.5176485956374642 entropy 0.03264415264129639
epoch: 30, step: 67
	action: tensor([[-0.0209, -0.4715, -0.5042, -0.5000,  0.0709,  0.0088, -0.0983]],
       dtype=torch.float64)
	q_value: tensor([[-3.8892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2046779802863843, distance: 1.2560073456726955 entropy 0.03264415264129639
epoch: 30, step: 68
	action: tensor([[ 0.2061, -0.3249, -0.1418, -0.0487, -0.4266,  0.3878, -0.1990]],
       dtype=torch.float64)
	q_value: tensor([[-3.7704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23779939457512522, distance: 0.9990594634853643 entropy 0.03264415264129639
epoch: 30, step: 69
	action: tensor([[ 1.3382e-01, -2.3871e-01, -1.5918e-01,  2.2292e-04, -1.6533e-01,
          3.2327e-02, -2.9191e-01]], dtype=torch.float64)
	q_value: tensor([[-3.6674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.243232223253239, distance: 0.995492538032457 entropy 0.03264415264129639
epoch: 30, step: 70
	action: tensor([[ 0.1454,  0.0624,  0.0249,  0.0930, -0.0596,  0.5530,  0.4263]],
       dtype=torch.float64)
	q_value: tensor([[-2.9833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6262173107986297, distance: 0.6996265553459564 entropy 0.03264415264129639
epoch: 30, step: 71
	action: tensor([[ 0.4283,  0.0519,  0.1844,  0.0244,  0.1003, -0.1460, -0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-3.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6257275640920658, distance: 0.7000847462269034 entropy 0.03264415264129639
epoch: 30, step: 72
	action: tensor([[ 0.0976,  0.0698, -0.0596, -0.3171, -0.1380,  0.1315, -0.6908]],
       dtype=torch.float64)
	q_value: tensor([[-3.4414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3174322437860503, distance: 0.9454304324130449 entropy 0.03264415264129639
epoch: 30, step: 73
	action: tensor([[-0.1781,  0.1575, -0.6231, -0.3276,  0.1619,  0.0344, -0.4926]],
       dtype=torch.float64)
	q_value: tensor([[-3.8978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23633208356519297, distance: 1.0000206449006905 entropy 0.03264415264129639
epoch: 30, step: 74
	action: tensor([[-0.1895,  0.0126, -0.1159,  0.0148,  0.0437,  0.0246, -0.0599]],
       dtype=torch.float64)
	q_value: tensor([[-4.1315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11734905615241897, distance: 1.0751057590065403 entropy 0.03264415264129639
epoch: 30, step: 75
	action: tensor([[-0.2043,  0.3747,  0.0932, -0.1846,  0.0236,  0.2015, -0.1929]],
       dtype=torch.float64)
	q_value: tensor([[-2.9663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2930638874575735, distance: 0.9621588457324537 entropy 0.03264415264129639
epoch: 30, step: 76
	action: tensor([[ 0.5377, -0.1943, -0.2053, -0.7938,  0.2479, -0.0605,  0.3151]],
       dtype=torch.float64)
	q_value: tensor([[-3.6474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06392007920805964, distance: 1.107167064382468 entropy 0.03264415264129639
epoch: 30, step: 77
	action: tensor([[ 0.0300, -0.0965, -0.5319,  0.2150,  0.4415, -0.1565, -0.2076]],
       dtype=torch.float64)
	q_value: tensor([[-4.9872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2588254901886754, distance: 0.985183048752091 entropy 0.03264415264129639
epoch: 30, step: 78
	action: tensor([[-0.1316,  0.0991, -0.5078, -0.0244,  0.0171,  0.0189, -0.3566]],
       dtype=torch.float64)
	q_value: tensor([[-3.3826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2357771920154088, distance: 1.0003838932643458 entropy 0.03264415264129639
epoch: 30, step: 79
	action: tensor([[ 0.3241,  0.1719, -0.3509,  0.0237, -0.4664,  0.2631,  0.1953]],
       dtype=torch.float64)
	q_value: tensor([[-3.3542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6837112358305022, distance: 0.6435741257926797 entropy 0.03264415264129639
epoch: 30, step: 80
	action: tensor([[ 0.4623, -0.3647, -0.1759,  0.1937, -0.3650,  0.5465, -0.1335]],
       dtype=torch.float64)
	q_value: tensor([[-4.4249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5607628909155994, distance: 0.7584137628909936 entropy 0.03264415264129639
epoch: 30, step: 81
	action: tensor([[ 0.2962, -0.1275,  0.1035, -0.4759, -0.2701,  0.3520,  0.2177]],
       dtype=torch.float64)
	q_value: tensor([[-4.0096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2570075720794591, distance: 0.9863905142120031 entropy 0.03264415264129639
epoch: 30, step: 82
	action: tensor([[ 0.2516,  0.2007, -0.3677, -0.3031,  0.0798, -0.3747,  0.1265]],
       dtype=torch.float64)
	q_value: tensor([[-4.3211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5226149782259687, distance: 0.790662402197591 entropy 0.03264415264129639
epoch: 30, step: 83
	action: tensor([[ 0.0658, -0.0791, -0.2026, -0.1982,  0.2618,  0.0068, -0.1071]],
       dtype=torch.float64)
	q_value: tensor([[-4.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22676816105629938, distance: 1.0062631236924715 entropy 0.03264415264129639
epoch: 30, step: 84
	action: tensor([[ 0.3348, -0.1146, -0.5757, -0.1944,  0.3144, -0.0721,  0.2966]],
       dtype=torch.float64)
	q_value: tensor([[-2.8752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4048283711407392, distance: 0.8828314653491238 entropy 0.03264415264129639
epoch: 30, step: 85
	action: tensor([[ 0.1291,  0.3702, -0.3980,  0.5431,  0.3146,  0.0662, -0.1692]],
       dtype=torch.float64)
	q_value: tensor([[-4.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6404397394103301, distance: 0.686187079875051 entropy 0.03264415264129639
epoch: 30, step: 86
	action: tensor([[ 0.3226,  0.1261,  0.1205, -0.0361, -0.1806,  0.2383,  0.0991]],
       dtype=torch.float64)
	q_value: tensor([[-3.8813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6743699424920597, distance: 0.6530086531391278 entropy 0.03264415264129639
epoch: 30, step: 87
	action: tensor([[ 0.3483, -0.0025, -0.2164,  0.3878,  0.0818, -0.3622, -0.2829]],
       dtype=torch.float64)
	q_value: tensor([[-3.6958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6556463269191144, distance: 0.6715201566288429 entropy 0.03264415264129639
epoch: 30, step: 88
	action: tensor([[-0.1434, -0.2291, -0.7513, -0.1054,  0.0592, -0.0586, -0.4636]],
       dtype=torch.float64)
	q_value: tensor([[-3.6508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02460772093675989, distance: 1.1583385373870405 entropy 0.03264415264129639
epoch: 30, step: 89
	action: tensor([[-0.1007, -0.4068,  0.1466, -0.4881, -0.1893, -0.5196, -0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-3.7601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4640929914508096, distance: 1.384653233206697 entropy 0.03264415264129639
epoch: 30, step: 90
	action: tensor([[ 0.0496,  0.0026, -0.0759, -0.4083, -0.2840,  0.1485,  0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-4.1466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17242126569824057, distance: 1.041025468223895 entropy 0.03264415264129639
epoch: 30, step: 91
	action: tensor([[ 0.1507, -0.3792, -0.3638,  0.0811, -0.0353,  0.4191, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-4.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2238385187466182, distance: 1.0081676003276077 entropy 0.03264415264129639
epoch: 30, step: 92
	action: tensor([[ 0.5744, -0.2617, -0.3420, -0.1903,  0.6090,  0.1600, -0.1254]],
       dtype=torch.float64)
	q_value: tensor([[-3.1173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.411566642681106, distance: 0.8778197249142057 entropy 0.03264415264129639
epoch: 30, step: 93
	action: tensor([[ 0.0137,  0.1295,  0.0871, -0.4019, -0.0970,  0.0690,  0.1122]],
       dtype=torch.float64)
	q_value: tensor([[-3.7725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22299080886363454, distance: 1.0087180015215191 entropy 0.03264415264129639
epoch: 30, step: 94
	action: tensor([[ 0.3551, -0.0490, -0.4744,  0.1231,  0.3953,  0.0326,  0.1409]],
       dtype=torch.float64)
	q_value: tensor([[-3.7009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5777771890549771, distance: 0.7435797252956825 entropy 0.03264415264129639
epoch: 30, step: 95
	action: tensor([[ 0.2854,  0.1943,  0.0489,  0.2583,  0.1626,  0.1137, -0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-3.6894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7646866587112282, distance: 0.555111172835723 entropy 0.03264415264129639
epoch: 30, step: 96
	action: tensor([[-0.0615,  0.0470, -0.1018,  0.2813, -0.2885,  0.3812, -0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-3.3389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4212721527546215, distance: 0.8705503278335321 entropy 0.03264415264129639
epoch: 30, step: 97
	action: tensor([[ 0.3892, -0.0236, -0.4095, -0.1786,  0.1622,  0.1100,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-3.4727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5481253756495033, distance: 0.7692467500082016 entropy 0.03264415264129639
epoch: 30, step: 98
	action: tensor([[-0.0589,  0.4422, -0.2329, -0.4161,  0.0115,  0.3024,  0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-3.4840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47119064588015536, distance: 0.8321588910163713 entropy 0.03264415264129639
epoch: 30, step: 99
	action: tensor([[ 0.1695,  0.1452,  0.2618,  0.2002, -0.0885,  0.3392,  0.0664]],
       dtype=torch.float64)
	q_value: tensor([[-4.5744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7065758761938217, distance: 0.619875685362415 entropy 0.03264415264129639
epoch: 30, step: 100
	action: tensor([[-0.1560,  0.1650, -0.2861,  0.3345, -0.0649, -0.2345,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-3.6218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31062316089063524, distance: 0.9501343896773161 entropy 0.03264415264129639
epoch: 30, step: 101
	action: tensor([[-0.2271,  0.0555, -0.1577, -0.6761,  0.0741, -0.4487, -0.2012]],
       dtype=torch.float64)
	q_value: tensor([[-3.4003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1472513508025164, distance: 1.225705068908574 entropy 0.03264415264129639
epoch: 30, step: 102
	action: tensor([[ 0.0653, -0.0374, -0.0495,  0.0332,  0.3420, -0.2013,  0.2344]],
       dtype=torch.float64)
	q_value: tensor([[-4.3086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2772510974896907, distance: 0.9728601486238716 entropy 0.03264415264129639
epoch: 30, step: 103
	action: tensor([[ 0.0052,  0.2136,  0.1130, -0.2044,  0.0152, -0.0924, -0.0520]],
       dtype=torch.float64)
	q_value: tensor([[-3.4268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32714302226709535, distance: 0.9386810998792066 entropy 0.03264415264129639
epoch: 30, step: 104
	action: tensor([[-0.0145, -0.0037, -0.3010, -0.0055,  0.1178,  0.1104,  0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-3.3231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29528089895189613, distance: 0.9606489552210747 entropy 0.03264415264129639
epoch: 30, step: 105
	action: tensor([[-5.2943e-02,  3.4257e-01, -4.6529e-01, -1.3991e-01,  1.7736e-04,
          4.3988e-01, -2.3714e-01]], dtype=torch.float64)
	q_value: tensor([[-3.1329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4781298489283762, distance: 0.8266809351554907 entropy 0.03264415264129639
epoch: 30, step: 106
	action: tensor([[ 0.0540, -0.1427, -0.2046,  0.0825,  0.1149, -0.1983, -0.0232]],
       dtype=torch.float64)
	q_value: tensor([[-4.2835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23089454091830497, distance: 1.0035745520802744 entropy 0.03264415264129639
epoch: 30, step: 107
	action: tensor([[ 0.8216,  0.2460, -0.6244, -0.3780, -0.0614,  0.1972, -0.0854]],
       dtype=torch.float64)
	q_value: tensor([[-3.0540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8044919095214437, distance: 0.5059866520315277 entropy 0.03264415264129639
epoch: 30, step: 108
	action: tensor([[ 0.2548, -0.3287, -0.4724,  0.2317,  0.2576,  0.2249, -0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-5.8466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3828622679178909, distance: 0.8989752669298529 entropy 0.03264415264129639
epoch: 30, step: 109
	action: tensor([[ 0.0371,  0.2159, -0.4013, -0.1200,  0.4213,  0.0324,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-3.1409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46709890968273915, distance: 0.8353721600074344 entropy 0.03264415264129639
epoch: 30, step: 110
	action: tensor([[ 0.5156,  0.1289, -0.4962, -0.0624, -0.2006, -0.1596, -0.1013]],
       dtype=torch.float64)
	q_value: tensor([[-3.7618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.720270542337283, distance: 0.6052374529419633 entropy 0.03264415264129639
epoch: 30, step: 111
	action: tensor([[-0.0446, -0.3023,  0.3003,  0.0960,  0.0450,  0.2147, -0.3065]],
       dtype=torch.float64)
	q_value: tensor([[-4.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.156351056123336, distance: 1.0510843671869166 entropy 0.03264415264129639
epoch: 30, step: 112
	action: tensor([[-0.0751,  0.2043,  0.1966, -0.1452,  0.1397, -0.0388, -0.1996]],
       dtype=torch.float64)
	q_value: tensor([[-2.8765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2733136066475268, distance: 0.9755065897465685 entropy 0.03264415264129639
epoch: 30, step: 113
	action: tensor([[ 0.1637, -0.3748, -0.0371, -0.2385, -0.0418,  0.0336, -0.2069]],
       dtype=torch.float64)
	q_value: tensor([[-3.2230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021138662858316115, distance: 1.1563759566754004 entropy 0.03264415264129639
epoch: 30, step: 114
	action: tensor([[ 0.3229, -0.1863, -0.1727, -0.0899, -0.2265,  0.0869, -0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-3.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3683724164766433, distance: 0.9094676110020503 entropy 0.03264415264129639
epoch: 30, step: 115
	action: tensor([[ 0.3227,  0.4246, -0.6212, -0.0101,  0.3100,  0.1929,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[-3.3061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7846362726303399, distance: 0.5310592439120893 entropy 0.03264415264129639
epoch: 30, step: 116
	action: tensor([[-0.0578, -0.0101,  0.0953, -0.7067, -0.2625,  0.1827,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-4.5315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09176068892981037, distance: 1.1956950176029673 entropy 0.03264415264129639
epoch: 30, step: 117
	action: tensor([[ 0.3252,  0.2491,  0.0192,  0.3637, -0.2147,  0.2813, -0.2505]],
       dtype=torch.float64)
	q_value: tensor([[-4.4308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8098588396449925, distance: 0.4989933561245419 entropy 0.03264415264129639
epoch: 30, step: 118
	action: tensor([[ 0.1481,  0.2831, -0.0271,  0.2047,  0.3069, -0.0060, -0.4108]],
       dtype=torch.float64)
	q_value: tensor([[-3.8803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6564513100705361, distance: 0.6707348031720517 entropy 0.03264415264129639
epoch: 30, step: 119
	action: tensor([[ 0.1563,  0.0832, -0.0283, -0.0844,  0.1947,  0.1819,  0.0749]],
       dtype=torch.float64)
	q_value: tensor([[-3.2638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4933926582363548, distance: 0.814502521136922 entropy 0.03264415264129639
epoch: 30, step: 120
	action: tensor([[-0.0112,  0.5586, -0.1589,  0.1437, -0.1873,  0.2533, -0.3374]],
       dtype=torch.float64)
	q_value: tensor([[-3.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5835442691262606, distance: 0.7384840414148257 entropy 0.03264415264129639
epoch: 30, step: 121
	action: tensor([[ 0.0899,  0.0540, -0.3228,  0.0818, -0.1008, -0.1845, -0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-4.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4374162297904547, distance: 0.8583220960927728 entropy 0.03264415264129639
epoch: 30, step: 122
	action: tensor([[ 0.0152, -0.0598,  0.0984, -0.4564,  0.2192,  0.1587,  0.1658]],
       dtype=torch.float64)
	q_value: tensor([[-3.3038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05684252947629176, distance: 1.111344738626214 entropy 0.03264415264129639
epoch: 30, step: 123
	action: tensor([[ 0.5328, -0.2126, -0.1776,  0.3045, -0.4137,  0.0190,  0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-3.3281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6387847109478441, distance: 0.6877645001756443 entropy 0.03264415264129639
epoch: 30, step: 124
	action: tensor([[-0.1863, -0.3754, -0.4079, -0.3425, -0.1707, -0.0269, -0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-3.8202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26405701660550385, distance: 1.286589566249504 entropy 0.03264415264129639
epoch: 30, step: 125
	action: tensor([[ 0.1242,  0.1996, -0.3076, -0.0029,  0.0584, -0.1175, -0.1141]],
       dtype=torch.float64)
	q_value: tensor([[-3.7482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.510980412021994, distance: 0.8002391987811862 entropy 0.03264415264129639
epoch: 30, step: 126
	action: tensor([[ 0.5480,  0.3107, -0.4314, -0.4632,  0.1101,  0.1170, -0.1372]],
       dtype=torch.float64)
	q_value: tensor([[-3.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7621814701791324, distance: 0.5580582569430619 entropy 0.03264415264129639
epoch: 30, step: 127
	action: tensor([[ 0.5782,  0.5240,  0.3423,  0.3992,  0.1281, -0.2908, -0.4244]],
       dtype=torch.float64)
	q_value: tensor([[-4.8675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9555623835834284, distance: 0.24123041900542802 entropy 0.03264415264129639
LOSS epoch 30 actor 8.848219541955496 critic 41.61897407670193 
epoch: 31, step: 0
	action: tensor([[ 0.2786,  0.5141, -0.1018,  0.0099, -0.5286,  0.5144, -0.5095]],
       dtype=torch.float64)
	q_value: tensor([[-4.3679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.756796922541521, distance: 0.5643405100410847 entropy 0.03264415264129639
epoch: 31, step: 1
	action: tensor([[ 0.2540,  0.0604, -0.0149, -0.1702,  0.2664,  0.2560, -0.0959]],
       dtype=torch.float64)
	q_value: tensor([[-5.1845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5560938287716815, distance: 0.7624340519751908 entropy 0.03264415264129639
epoch: 31, step: 2
	action: tensor([[ 0.5224, -0.0821, -0.2397,  0.2004,  0.1681, -0.3630,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-2.6069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6111018945797557, distance: 0.7136324781023086 entropy 0.03264415264129639
epoch: 31, step: 3
	action: tensor([[ 0.1593,  0.2082, -0.4888, -0.2046,  0.3946, -0.1854, -0.5116]],
       dtype=torch.float64)
	q_value: tensor([[-3.4143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5343087400067743, distance: 0.7809185458271165 entropy 0.03264415264129639
epoch: 31, step: 4
	action: tensor([[ 0.2629,  0.2106, -0.1985, -0.0713,  0.0208,  0.3454,  0.2066]],
       dtype=torch.float64)
	q_value: tensor([[-3.3921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6954350264540555, distance: 0.6315339047701802 entropy 0.03264415264129639
epoch: 31, step: 5
	action: tensor([[-0.0466,  0.3939, -0.0339,  0.2342,  0.2539, -0.1796,  0.4284]],
       dtype=torch.float64)
	q_value: tensor([[-3.2757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5118619663816933, distance: 0.7995175788410882 entropy 0.03264415264129639
epoch: 31, step: 6
	action: tensor([[-0.1538,  0.1662,  0.3982, -0.5701, -0.4859,  0.2039, -0.1977]],
       dtype=torch.float64)
	q_value: tensor([[-3.5444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06301240541633235, distance: 1.1798474538961206 entropy 0.03264415264129639
epoch: 31, step: 7
	action: tensor([[ 0.5014,  0.2265, -0.0438,  0.2592,  0.1721,  0.0657, -0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-4.3355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8682712114076279, distance: 0.4153335768681084 entropy 0.03264415264129639
epoch: 31, step: 8
	action: tensor([[ 0.3816,  0.1867, -0.3294, -0.3808,  0.1420, -0.0426, -0.2039]],
       dtype=torch.float64)
	q_value: tensor([[-3.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5974037110760578, distance: 0.7260918748540942 entropy 0.03264415264129639
epoch: 31, step: 9
	action: tensor([[ 0.2129,  0.5027, -0.4424, -0.0974,  0.0137, -0.0470, -0.1273]],
       dtype=torch.float64)
	q_value: tensor([[-3.4131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.742677999309511, distance: 0.5804905484812231 entropy 0.03264415264129639
epoch: 31, step: 10
	action: tensor([[-0.0864,  0.5480, -0.4073,  0.0586,  0.6018,  0.2203, -0.4641]],
       dtype=torch.float64)
	q_value: tensor([[-3.8295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5589476217426289, distance: 0.759979324637485 entropy 0.03264415264129639
epoch: 31, step: 11
	action: tensor([[ 0.5323,  0.2410, -0.2337, -0.0348, -0.0429,  0.3816,  0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-3.7407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8591407346518176, distance: 0.42948638678356876 entropy 0.03264415264129639
epoch: 31, step: 12
	action: tensor([[ 0.3835,  0.2028, -0.1855, -0.4015, -0.3444,  0.2833, -0.2632]],
       dtype=torch.float64)
	q_value: tensor([[-3.7983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6463204689096409, distance: 0.6805525354543787 entropy 0.03264415264129639
epoch: 31, step: 13
	action: tensor([[-0.0175, -0.0455, -0.1168, -0.4525, -0.0793, -0.2238,  0.1757]],
       dtype=torch.float64)
	q_value: tensor([[-4.1523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029523206968843807, distance: 1.1273253435993507 entropy 0.03264415264129639
epoch: 31, step: 14
	action: tensor([[ 0.2911, -0.2740, -0.2921,  0.0922,  0.3678, -0.1699, -0.2058]],
       dtype=torch.float64)
	q_value: tensor([[-3.4228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30084847265452797, distance: 0.9568466674725168 entropy 0.03264415264129639
epoch: 31, step: 15
	action: tensor([[ 0.3791,  0.6804, -0.2872, -0.0235, -0.1975, -0.2600, -0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-2.6353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8729443241019985, distance: 0.4079000219577333 entropy 0.03264415264129639
epoch: 31, step: 16
	action: tensor([[-0.0973, -0.6141,  0.0518, -0.0652, -0.0753,  0.0542,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-4.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3531235230837617, distance: 1.3311451534202894 entropy 0.03264415264129639
epoch: 31, step: 17
	action: tensor([[ 0.3061, -0.0404, -0.3533, -0.1525, -0.1959, -0.5732,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-2.6959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3920286121461426, distance: 0.8922740525676293 entropy 0.03264415264129639
epoch: 31, step: 18
	action: tensor([[ 0.0073, -0.1446, -0.4427,  0.4605,  0.1932,  0.6043,  0.2738]],
       dtype=torch.float64)
	q_value: tensor([[-3.9014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4126983226155969, distance: 0.8769752035675366 entropy 0.03264415264129639
epoch: 31, step: 19
	action: tensor([[ 0.0612, -0.0172, -0.4390,  0.2103, -0.0092, -0.0980, -0.1212]],
       dtype=torch.float64)
	q_value: tensor([[-3.3391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3835959656147415, distance: 0.8984407247984477 entropy 0.03264415264129639
epoch: 31, step: 20
	action: tensor([[-0.2074,  0.4526, -0.3205, -0.0674, -0.3048,  0.0764,  0.0461]],
       dtype=torch.float64)
	q_value: tensor([[-2.6991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35220366326794217, distance: 0.9210345824922755 entropy 0.03264415264129639
epoch: 31, step: 21
	action: tensor([[ 0.2437, -0.1229, -0.2896,  0.1015, -0.0343, -0.0444,  0.0739]],
       dtype=torch.float64)
	q_value: tensor([[-3.9292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4458416357342273, distance: 0.851870618857002 entropy 0.03264415264129639
epoch: 31, step: 22
	action: tensor([[ 0.2554, -0.0317, -0.2678,  0.1052,  0.0931,  0.1772, -0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-2.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5753479947396769, distance: 0.7457156935271487 entropy 0.03264415264129639
epoch: 31, step: 23
	action: tensor([[ 0.4867,  0.1011, -0.4333, -0.3695,  0.1986,  0.3831, -0.1903]],
       dtype=torch.float64)
	q_value: tensor([[-2.5527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7003760572850644, distance: 0.6263901943234315 entropy 0.03264415264129639
epoch: 31, step: 24
	action: tensor([[-0.0983, -0.1795, -0.0271, -0.2706, -0.1844, -0.1740,  0.2223]],
       dtype=torch.float64)
	q_value: tensor([[-3.6921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10349932116033767, distance: 1.2021058992756182 entropy 0.03264415264129639
epoch: 31, step: 25
	action: tensor([[ 0.4950,  0.2803,  0.2655,  0.3835,  0.1962,  0.0316, -0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-3.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9281638251213896, distance: 0.3067102519787488 entropy 0.03264415264129639
epoch: 31, step: 26
	action: tensor([[ 0.3770, -0.1171,  0.0217,  0.1949,  0.2017, -0.2975, -0.2400]],
       dtype=torch.float64)
	q_value: tensor([[-3.4275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5166397899803105, distance: 0.7955951767681319 entropy 0.03264415264129639
epoch: 31, step: 27
	action: tensor([[ 0.2802,  0.4175, -0.3919, -0.0532, -0.4037,  0.1234, -0.1977]],
       dtype=torch.float64)
	q_value: tensor([[-2.8882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7352456526659025, distance: 0.5888141570012347 entropy 0.03264415264129639
epoch: 31, step: 28
	action: tensor([[ 0.3560, -0.0133,  0.0615, -0.1218,  0.0962, -0.5169, -0.2544]],
       dtype=torch.float64)
	q_value: tensor([[-4.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39258722464835216, distance: 0.8918640415078236 entropy 0.03264415264129639
epoch: 31, step: 29
	action: tensor([[ 0.3403, -0.0844,  0.1681, -0.0802, -0.0066,  0.1274,  0.0471]],
       dtype=torch.float64)
	q_value: tensor([[-3.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4986558162881567, distance: 0.8102605299257449 entropy 0.03264415264129639
epoch: 31, step: 30
	action: tensor([[-0.2734,  0.0717, -0.5002, -0.0354,  0.1874, -0.0792, -0.4835]],
       dtype=torch.float64)
	q_value: tensor([[-2.8014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0420646332593797, distance: 1.1200174706063206 entropy 0.03264415264129639
epoch: 31, step: 31
	action: tensor([[ 0.2038,  0.1107, -0.4038, -0.1368,  0.2874,  0.0723, -0.1567]],
       dtype=torch.float64)
	q_value: tensor([[-3.1214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5271983254334585, distance: 0.7868576949657878 entropy 0.03264415264129639
epoch: 31, step: 32
	action: tensor([[-0.0286, -0.1129,  0.1524, -0.0431,  0.4515,  0.3786, -0.3072]],
       dtype=torch.float64)
	q_value: tensor([[-2.8321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28782467635026776, distance: 0.965717617065664 entropy 0.03264415264129639
epoch: 31, step: 33
	action: tensor([[ 0.0081,  0.2944, -0.2103, -0.1371, -0.1258, -0.2598, -0.2604]],
       dtype=torch.float64)
	q_value: tensor([[-2.4531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44646594974351606, distance: 0.8513906255272404 entropy 0.03264415264129639
epoch: 31, step: 34
	action: tensor([[ 0.4568,  0.1651, -0.4721,  0.0037,  0.2178,  0.0946, -0.1976]],
       dtype=torch.float64)
	q_value: tensor([[-3.2926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7610261875147106, distance: 0.5594120916233208 entropy 0.03264415264129639
epoch: 31, step: 35
	action: tensor([[-0.0084, -0.0468,  0.1514, -0.5483, -0.2166,  0.4758,  0.1894]],
       dtype=torch.float64)
	q_value: tensor([[-3.2862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06267117499319452, distance: 1.1079054012916392 entropy 0.03264415264129639
epoch: 31, step: 36
	action: tensor([[-0.2227,  0.2271, -0.0126, -0.2985, -0.2024,  0.3604,  0.0691]],
       dtype=torch.float64)
	q_value: tensor([[-3.8765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15199666185377747, distance: 1.0537933999375304 entropy 0.03264415264129639
epoch: 31, step: 37
	action: tensor([[ 0.1534, -0.5801,  0.2198, -0.1442, -0.1316,  0.3930, -0.1788]],
       dtype=torch.float64)
	q_value: tensor([[-3.6438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07237428071376129, distance: 1.1850314817398246 entropy 0.03264415264129639
epoch: 31, step: 38
	action: tensor([[ 0.2201, -0.0286, -0.2089, -0.4487,  0.0643,  0.2715, -0.4111]],
       dtype=torch.float64)
	q_value: tensor([[-2.9068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.329775772913257, distance: 0.9368428671417783 entropy 0.03264415264129639
epoch: 31, step: 39
	action: tensor([[-0.1704,  0.3305, -0.3322, -0.4413, -0.0468, -0.1594, -0.5942]],
       dtype=torch.float64)
	q_value: tensor([[-3.0878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26591172407464536, distance: 0.9804621603219701 entropy 0.03264415264129639
epoch: 31, step: 40
	action: tensor([[ 0.0459, -0.4105, -0.4163, -0.2440,  0.1999,  0.1539, -0.1740]],
       dtype=torch.float64)
	q_value: tensor([[-3.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.040610595391295856, distance: 1.1673492672328616 entropy 0.03264415264129639
epoch: 31, step: 41
	action: tensor([[ 0.3356,  0.0614, -0.2220, -0.1140,  0.1865,  0.2213, -0.2056]],
       dtype=torch.float64)
	q_value: tensor([[-2.5976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.615480178477493, distance: 0.7096040071471316 entropy 0.03264415264129639
epoch: 31, step: 42
	action: tensor([[ 0.3148,  0.1784, -0.3121, -0.0068,  0.1426,  0.1681,  0.2168]],
       dtype=torch.float64)
	q_value: tensor([[-2.6865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6959972021746955, distance: 0.6309507828699806 entropy 0.03264415264129639
epoch: 31, step: 43
	action: tensor([[ 0.3030, -0.0857, -0.3178,  0.2729,  0.3373,  0.1046, -0.2283]],
       dtype=torch.float64)
	q_value: tensor([[-3.1664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6033726127905894, distance: 0.7206892479031692 entropy 0.03264415264129639
epoch: 31, step: 44
	action: tensor([[ 0.2615, -0.0042, -0.1195, -0.0723, -0.0230, -0.2449, -0.8128]],
       dtype=torch.float64)
	q_value: tensor([[-2.6783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4520477897926314, distance: 0.847087035080652 entropy 0.03264415264129639
epoch: 31, step: 45
	action: tensor([[-0.0640,  0.3680, -0.4701,  0.0136,  0.0063,  0.2978,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-3.4711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47267815606658903, distance: 0.8309876593234532 entropy 0.03264415264129639
epoch: 31, step: 46
	action: tensor([[-0.0532, -0.0554, -0.4133,  0.1275, -0.0359,  0.2400,  0.2282]],
       dtype=torch.float64)
	q_value: tensor([[-3.8925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.258175309740578, distance: 0.9856150700456638 entropy 0.03264415264129639
epoch: 31, step: 47
	action: tensor([[-0.1967, -0.0755,  0.3196,  0.3172,  0.3417,  0.5409, -0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-2.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42997517278688846, distance: 0.8639797808245047 entropy 0.03264415264129639
epoch: 31, step: 48
	action: tensor([[ 0.3809,  0.6312, -0.0552, -0.0528, -0.0465,  0.2290, -0.1880]],
       dtype=torch.float64)
	q_value: tensor([[-2.9453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8512580982652465, distance: 0.4413400686137555 entropy 0.03264415264129639
epoch: 31, step: 49
	action: tensor([[-0.2082,  0.1007, -0.0617,  0.1092, -0.1698,  0.0637, -0.4405]],
       dtype=torch.float64)
	q_value: tensor([[-4.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2057397320024491, distance: 1.0198542551904524 entropy 0.03264415264129639
epoch: 31, step: 50
	action: tensor([[ 0.1409, -0.0861, -0.1643, -0.1924, -0.0384, -0.1075, -0.5280]],
       dtype=torch.float64)
	q_value: tensor([[-2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26768235671673724, distance: 0.9792790012491451 entropy 0.03264415264129639
epoch: 31, step: 51
	action: tensor([[-0.3059, -0.1437,  0.1326,  0.0359,  0.2785,  0.2116, -0.0272]],
       dtype=torch.float64)
	q_value: tensor([[-2.8105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.061311333840176285, distance: 1.1789030582702218 entropy 0.03264415264129639
epoch: 31, step: 52
	action: tensor([[ 0.2883, -0.0716,  0.1075,  0.1075,  0.1047,  0.2680, -0.1626]],
       dtype=torch.float64)
	q_value: tensor([[-2.6531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5933997123097436, distance: 0.7296935948021818 entropy 0.03264415264129639
epoch: 31, step: 53
	action: tensor([[ 0.1061, -0.0950, -0.2614, -0.0314,  0.1549, -0.1476, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-2.6282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2610074882286624, distance: 0.9837318035142405 entropy 0.03264415264129639
epoch: 31, step: 54
	action: tensor([[ 0.3802,  0.1842, -0.0725, -0.2759,  0.5925,  0.1715, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[-2.6151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.653129109421503, distance: 0.673970086064948 entropy 0.03264415264129639
epoch: 31, step: 55
	action: tensor([[-0.0127,  0.1677, -0.1622, -0.2922,  0.0775, -0.2282, -0.3231]],
       dtype=torch.float64)
	q_value: tensor([[-3.1655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2554029541163766, distance: 0.987455078418514 entropy 0.03264415264129639
epoch: 31, step: 56
	action: tensor([[ 0.3047, -0.0490, -0.0769, -0.4290, -0.5036, -0.4213, -0.3310]],
       dtype=torch.float64)
	q_value: tensor([[-2.9776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27796422164774826, distance: 0.9723800778210844 entropy 0.03264415264129639
epoch: 31, step: 57
	action: tensor([[ 0.0579, -0.3063, -0.0750, -0.0567,  0.0166, -0.3998, -0.1409]],
       dtype=torch.float64)
	q_value: tensor([[-4.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017036097231192038, distance: 1.1540506687158159 entropy 0.03264415264129639
epoch: 31, step: 58
	action: tensor([[ 0.0032,  0.0710, -0.3203, -0.4954, -0.1498,  0.2291, -0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-2.8392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24727469376813194, distance: 0.9928301375576812 entropy 0.03264415264129639
epoch: 31, step: 59
	action: tensor([[ 0.2864,  0.3145, -0.1999,  0.1456,  0.2747,  0.1016, -0.0984]],
       dtype=torch.float64)
	q_value: tensor([[-3.5856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7558654959497905, distance: 0.5654201416087523 entropy 0.03264415264129639
epoch: 31, step: 60
	action: tensor([[ 0.1628,  0.0014, -0.5946,  0.0727,  0.1160, -0.2611, -0.0411]],
       dtype=torch.float64)
	q_value: tensor([[-2.9191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4399846042564507, distance: 0.8563605976592672 entropy 0.03264415264129639
epoch: 31, step: 61
	action: tensor([[ 4.2466e-01, -4.5079e-01, -3.3466e-01,  1.2039e-01, -1.1353e-01,
         -9.0497e-06,  6.7597e-02]], dtype=torch.float64)
	q_value: tensor([[-3.2624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28817640992482507, distance: 0.9654791103042104 entropy 0.03264415264129639
epoch: 31, step: 62
	action: tensor([[ 0.1102,  0.0117, -0.5486,  0.2426, -0.1543,  0.5058, -0.3719]],
       dtype=torch.float64)
	q_value: tensor([[-2.8782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45445471169933116, distance: 0.8452245403136202 entropy 0.03264415264129639
epoch: 31, step: 63
	action: tensor([[ 0.5473,  0.3486, -0.3754,  0.3110, -0.1818,  0.1932, -0.3156]],
       dtype=torch.float64)
	q_value: tensor([[-3.3311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8963745690308561, distance: 0.3683747618973078 entropy 0.03264415264129639
epoch: 31, step: 64
	action: tensor([[-0.3274,  0.0399, -0.3224,  0.0720,  0.4028,  0.5271, -0.2593]],
       dtype=torch.float64)
	q_value: tensor([[-3.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14054967041224253, distance: 1.0608820090653865 entropy 0.03264415264129639
epoch: 31, step: 65
	action: tensor([[-0.0493, -0.1257, -0.3964,  0.0857, -0.0568,  0.1042,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[-3.1735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18963035634205294, distance: 1.0301448015711225 entropy 0.03264415264129639
epoch: 31, step: 66
	action: tensor([[ 0.4248,  0.3713, -0.2576,  0.1525, -0.5159, -0.1537, -0.2207]],
       dtype=torch.float64)
	q_value: tensor([[-2.6574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8443923619401054, distance: 0.4514110112654937 entropy 0.03264415264129639
epoch: 31, step: 67
	action: tensor([[-0.4076, -0.1559, -0.2280,  0.1368,  0.1185, -0.3758, -0.1842]],
       dtype=torch.float64)
	q_value: tensor([[-4.1137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3033995904128244, distance: 1.3064580925621654 entropy 0.03264415264129639
epoch: 31, step: 68
	action: tensor([[ 0.0266,  0.2110,  0.0825, -0.5011,  0.3512, -0.1162, -0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-2.9530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18613782193977302, distance: 1.0323622760577524 entropy 0.03264415264129639
epoch: 31, step: 69
	action: tensor([[ 0.0242, -0.0282, -0.7500, -0.3563, -0.0976, -0.5041, -0.1552]],
       dtype=torch.float64)
	q_value: tensor([[-3.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2951657342927403, distance: 0.9607274462750414 entropy 0.03264415264129639
epoch: 31, step: 70
	action: tensor([[ 0.2622, -0.1109,  0.1218,  0.1637,  0.0862, -0.1877, -0.5574]],
       dtype=torch.float64)
	q_value: tensor([[-4.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46368803668386493, distance: 0.838041326540425 entropy 0.03264415264129639
epoch: 31, step: 71
	action: tensor([[ 0.3083,  0.0589, -0.2367,  0.0213,  0.2374,  0.3497, -0.3865]],
       dtype=torch.float64)
	q_value: tensor([[-2.9589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6609777729370359, distance: 0.6663014824094424 entropy 0.03264415264129639
epoch: 31, step: 72
	action: tensor([[ 0.4124,  0.2434, -0.6759,  0.0047, -0.0416,  0.1787, -0.0897]],
       dtype=torch.float64)
	q_value: tensor([[-2.7610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7750984662386963, distance: 0.5426913531132173 entropy 0.03264415264129639
epoch: 31, step: 73
	action: tensor([[ 0.3493, -0.3407,  0.2246, -0.2605,  0.4358, -0.2761, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-3.8963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0015958846519816827, distance: 1.145257010697345 entropy 0.03264415264129639
epoch: 31, step: 74
	action: tensor([[ 0.4789,  0.0274, -0.0609, -0.1203, -0.2188,  0.5825, -0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-3.0486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7318358314940403, distance: 0.5925937506983577 entropy 0.03264415264129639
epoch: 31, step: 75
	action: tensor([[-0.2054,  0.0464, -0.1336, -0.3545,  0.1734,  0.1147,  0.0439]],
       dtype=torch.float64)
	q_value: tensor([[-3.7924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009585239359997666, distance: 1.1388466415035088 entropy 0.03264415264129639
epoch: 31, step: 76
	action: tensor([[ 0.3932, -0.1757, -0.3974,  0.0431,  0.3170,  0.1720, -0.0858]],
       dtype=torch.float64)
	q_value: tensor([[-2.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5052236909381853, distance: 0.8049356113205794 entropy 0.03264415264129639
epoch: 31, step: 77
	action: tensor([[ 0.4264,  0.1129,  0.1272,  0.0487,  0.2583, -0.3743, -0.2928]],
       dtype=torch.float64)
	q_value: tensor([[-2.7680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6281843148371806, distance: 0.6977832603307488 entropy 0.03264415264129639
epoch: 31, step: 78
	action: tensor([[-0.4032,  0.2829,  0.0701, -0.2352,  0.0763,  0.0403,  0.3604]],
       dtype=torch.float64)
	q_value: tensor([[-3.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07596255305741995, distance: 1.1870124433415623 entropy 0.03264415264129639
epoch: 31, step: 79
	action: tensor([[-0.1306, -0.1364, -0.2039, -0.1655, -0.0190, -0.2230, -0.3077]],
       dtype=torch.float64)
	q_value: tensor([[-3.7269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07546730445039618, distance: 1.1867392303382935 entropy 0.03264415264129639
epoch: 31, step: 80
	action: tensor([[ 0.2372,  0.7244, -0.4286, -0.6245,  0.4019, -0.1348, -0.1039]],
       dtype=torch.float64)
	q_value: tensor([[-2.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7855985055284671, distance: 0.5298715445185808 entropy 0.03264415264129639
epoch: 31, step: 81
	action: tensor([[ 0.4354,  0.3072,  0.0328, -0.2082,  0.0888,  0.2779, -0.1987]],
       dtype=torch.float64)
	q_value: tensor([[-5.1756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.778664699739531, distance: 0.5383714676681997 entropy 0.03264415264129639
epoch: 31, step: 82
	action: tensor([[ 0.2916,  0.2289, -0.2701, -0.1567, -0.1459,  0.0831, -0.2720]],
       dtype=torch.float64)
	q_value: tensor([[-3.2852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6551253340896319, distance: 0.6720279557086141 entropy 0.03264415264129639
epoch: 31, step: 83
	action: tensor([[ 0.6130,  0.3869, -0.3206,  0.0104,  0.2135, -0.1565, -0.2144]],
       dtype=torch.float64)
	q_value: tensor([[-3.2496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8987387331092351, distance: 0.3641483710551916 entropy 0.03264415264129639
epoch: 31, step: 84
	action: tensor([[ 0.2587,  0.1735, -0.4870,  0.3442,  0.1365, -0.2212, -0.2747]],
       dtype=torch.float64)
	q_value: tensor([[-3.9740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6653164966663547, distance: 0.6620241714611331 entropy 0.03264415264129639
epoch: 31, step: 85
	action: tensor([[ 0.1828,  0.3327, -0.3642,  0.0824, -0.1870, -0.0159, -0.2503]],
       dtype=torch.float64)
	q_value: tensor([[-3.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6788011072369067, distance: 0.6485503723729148 entropy 0.03264415264129639
epoch: 31, step: 86
	action: tensor([[ 0.0793, -0.0858, -0.1528, -0.4364,  0.1445, -0.4000, -0.1747]],
       dtype=torch.float64)
	q_value: tensor([[-3.3198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04793603007021807, distance: 1.1165797780574043 entropy 0.03264415264129639
epoch: 31, step: 87
	action: tensor([[ 0.1868,  0.2009,  0.1108, -0.0757, -0.1433,  0.1553, -0.4773]],
       dtype=torch.float64)
	q_value: tensor([[-3.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5791304234206618, distance: 0.7423871734481324 entropy 0.03264415264129639
epoch: 31, step: 88
	action: tensor([[ 2.3712e-01,  4.7542e-02, -8.1953e-02, -1.8517e-02,  5.2453e-01,
          3.9533e-04, -3.6837e-01]], dtype=torch.float64)
	q_value: tensor([[-3.2140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5245007354111311, distance: 0.7890992272124661 entropy 0.03264415264129639
epoch: 31, step: 89
	action: tensor([[ 0.1555,  0.0833, -0.3301,  0.0459, -0.3866, -0.0008,  0.1057]],
       dtype=torch.float64)
	q_value: tensor([[-2.5457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5159124089219492, distance: 0.7961935745054064 entropy 0.03264415264129639
epoch: 31, step: 90
	action: tensor([[-0.0588,  0.0796,  0.2123,  0.0589,  0.0087,  0.1110, -0.1834]],
       dtype=torch.float64)
	q_value: tensor([[-3.2937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36077393969772253, distance: 0.9149217023506445 entropy 0.03264415264129639
epoch: 31, step: 91
	action: tensor([[ 0.1279,  0.3259, -0.1610, -0.0648,  0.0049,  0.4935,  0.1290]],
       dtype=torch.float64)
	q_value: tensor([[-2.6927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.662187159663699, distance: 0.6651119791383414 entropy 0.03264415264129639
epoch: 31, step: 92
	action: tensor([[-0.2482, -0.1447, -0.0946,  0.0757, -0.4249,  0.5815,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[-3.5457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02002003487287196, distance: 1.132831434983719 entropy 0.03264415264129639
epoch: 31, step: 93
	action: tensor([[ 0.3273,  0.2927, -0.0804, -0.1657,  0.2950, -0.0999, -0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-3.5329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6531259250973469, distance: 0.6739731796285164 entropy 0.03264415264129639
epoch: 31, step: 94
	action: tensor([[ 0.2605, -0.0723, -0.3205, -0.4132,  0.0966,  0.2302, -0.2257]],
       dtype=torch.float64)
	q_value: tensor([[-3.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3450257605537661, distance: 0.9261232835760134 entropy 0.03264415264129639
epoch: 31, step: 95
	action: tensor([[ 0.3703,  0.5164,  0.1786,  0.4784, -0.0981,  0.3064,  0.2276]],
       dtype=torch.float64)
	q_value: tensor([[-3.0557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.86955526119783, distance: 0.41330435075110034 entropy 0.03264415264129639
epoch: 31, step: 96
	action: tensor([[-0.0643, -0.0385, -0.4039, -0.3133, -0.0722,  0.1328,  0.4768]],
       dtype=torch.float64)
	q_value: tensor([[-3.9973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11590587827646481, distance: 1.0759843255291077 entropy 0.03264415264129639
epoch: 31, step: 97
	action: tensor([[ 0.0400,  0.1707,  0.2839, -0.4564, -0.1573,  0.3473, -0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-3.9318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2644548488535331, distance: 0.9814345931123569 entropy 0.03264415264129639
epoch: 31, step: 98
	action: tensor([[ 0.2473,  0.1843, -0.1010, -0.0884,  0.2665,  0.1909, -0.7332]],
       dtype=torch.float64)
	q_value: tensor([[-3.6250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.614361419674573, distance: 0.710635552309654 entropy 0.03264415264129639
epoch: 31, step: 99
	action: tensor([[ 0.3653,  0.1636, -0.3234, -0.5416, -0.2277,  0.5300,  0.0382]],
       dtype=torch.float64)
	q_value: tensor([[-3.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6017810391761922, distance: 0.7221337795212465 entropy 0.03264415264129639
epoch: 31, step: 100
	action: tensor([[-0.2032,  0.1072, -0.2387,  0.1532,  0.0976, -0.0576, -0.3222]],
       dtype=torch.float64)
	q_value: tensor([[-4.6265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19409096838150364, distance: 1.0273057162803005 entropy 0.03264415264129639
epoch: 31, step: 101
	action: tensor([[ 0.1637, -0.1977, -0.0028,  0.5397,  0.0194,  0.1369, -0.2179]],
       dtype=torch.float64)
	q_value: tensor([[-2.7327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5806532136779765, distance: 0.7410429037910674 entropy 0.03264415264129639
epoch: 31, step: 102
	action: tensor([[ 0.5832,  0.1652, -0.5597, -0.3948, -0.0028, -0.0868,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-2.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6703521396660057, distance: 0.6570248925156105 entropy 0.03264415264129639
epoch: 31, step: 103
	action: tensor([[ 0.3838, -0.1628,  0.0444,  0.0400, -0.3411, -0.2571,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[-4.4556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4500172277856953, distance: 0.8486551208940225 entropy 0.03264415264129639
epoch: 31, step: 104
	action: tensor([[ 0.2521,  0.7626, -0.3527, -0.0481,  0.1865, -0.2855, -0.2741]],
       dtype=torch.float64)
	q_value: tensor([[-3.1690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8240677604278014, distance: 0.47998692682371225 entropy 0.03264415264129639
epoch: 31, step: 105
	action: tensor([[0.3814, 0.1238, 0.0967, 0.3546, 0.1750, 0.5879, 0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-4.5168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9015014415875754, distance: 0.3591464933917474 entropy 0.03264415264129639
epoch: 31, step: 106
	action: tensor([[ 0.2973, -0.0615, -0.4174, -0.4980,  0.0299,  0.3124, -0.4969]],
       dtype=torch.float64)
	q_value: tensor([[-3.3081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4142341246208585, distance: 0.8758278018182939 entropy 0.03264415264129639
epoch: 31, step: 107
	action: tensor([[ 0.2394, -0.0010, -0.2658,  0.1142, -0.3013, -0.0519,  0.0504]],
       dtype=torch.float64)
	q_value: tensor([[-3.5362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5517849626999989, distance: 0.7661254767732811 entropy 0.03264415264129639
epoch: 31, step: 108
	action: tensor([[ 0.2921,  0.0683, -0.5204, -0.0964, -0.1444,  0.0543, -0.5821]],
       dtype=torch.float64)
	q_value: tensor([[-3.0059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5896334211166566, distance: 0.7330653382500653 entropy 0.03264415264129639
epoch: 31, step: 109
	action: tensor([[-0.1374, -0.0351, -0.5124,  0.0826,  0.0048,  0.0613, -0.5854]],
       dtype=torch.float64)
	q_value: tensor([[-3.3689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14412114127202325, distance: 1.0586754512918144 entropy 0.03264415264129639
epoch: 31, step: 110
	action: tensor([[-0.2630,  0.2092, -0.6345, -0.0388,  0.1007, -0.1816, -0.1529]],
       dtype=torch.float64)
	q_value: tensor([[-3.0073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16395340079475818, distance: 1.046337849777556 entropy 0.03264415264129639
epoch: 31, step: 111
	action: tensor([[ 0.1411,  0.0621,  0.0266, -0.0577,  0.1387,  0.0573, -0.2830]],
       dtype=torch.float64)
	q_value: tensor([[-3.5779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45315684131647516, distance: 0.8462293516932968 entropy 0.03264415264129639
epoch: 31, step: 112
	action: tensor([[ 0.4973,  0.1405, -0.3943, -0.1981, -0.2637,  0.0277, -0.4456]],
       dtype=torch.float64)
	q_value: tensor([[-2.5439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7000436930853908, distance: 0.6267375163215205 entropy 0.03264415264129639
epoch: 31, step: 113
	action: tensor([[ 0.2807,  0.7668, -0.2556,  0.1373,  0.2793,  0.0446, -0.4519]],
       dtype=torch.float64)
	q_value: tensor([[-3.7795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7790392018812478, distance: 0.5379158091579895 entropy 0.03264415264129639
epoch: 31, step: 114
	action: tensor([[ 0.3344,  0.2359,  0.1266, -0.2501, -0.0324,  0.2702, -0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-4.1112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6682857815795676, distance: 0.6590809164435789 entropy 0.03264415264129639
epoch: 31, step: 115
	action: tensor([[-0.0433,  0.3609, -0.0988,  0.0503,  0.1167, -0.3084,  0.2902]],
       dtype=torch.float64)
	q_value: tensor([[-3.2614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4452045492621187, distance: 0.8523601534279055 entropy 0.03264415264129639
epoch: 31, step: 116
	action: tensor([[ 0.5848, -0.1874, -0.7874,  0.1633, -0.0140,  0.0750, -0.2239]],
       dtype=torch.float64)
	q_value: tensor([[-3.4337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5845528861010209, distance: 0.7375892295305178 entropy 0.03264415264129639
epoch: 31, step: 117
	action: tensor([[ 0.1339, -0.5670, -0.3569, -0.2431,  0.2853, -0.1690, -0.1482]],
       dtype=torch.float64)
	q_value: tensor([[-3.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22667380903408008, distance: 1.2674219948408272 entropy 0.03264415264129639
epoch: 31, step: 118
	action: tensor([[ 0.1464, -0.6144, -0.2982, -0.2188,  0.0745, -0.1021, -0.2810]],
       dtype=torch.float64)
	q_value: tensor([[-2.7510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2439360288543393, distance: 1.2763086618105357 entropy 0.03264415264129639
epoch: 31, step: 119
	action: tensor([[-0.4861, -0.0936,  0.2299,  0.2159,  0.1113, -0.3688, -0.5253]],
       dtype=torch.float64)
	q_value: tensor([[-2.7379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31744845479093997, distance: 1.3134801366938067 entropy 0.03264415264129639
epoch: 31, step: 120
	action: tensor([[ 0.3760,  0.5557, -0.0380,  0.0129, -0.1013, -0.0010, -0.3197]],
       dtype=torch.float64)
	q_value: tensor([[-3.0116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8372650174694085, distance: 0.4616333264669511 entropy 0.03264415264129639
epoch: 31, step: 121
	action: tensor([[ 0.4385,  0.1459,  0.1159, -0.0957, -0.1764, -0.1673,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-3.8192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6480551703917481, distance: 0.6788815218326577 entropy 0.03264415264129639
epoch: 31, step: 122
	action: tensor([[-0.0402, -0.1175, -0.5745, -0.2960,  0.1114,  0.6205, -0.4419]],
       dtype=torch.float64)
	q_value: tensor([[-3.3380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23119091556304494, distance: 1.003381169815598 entropy 0.03264415264129639
epoch: 31, step: 123
	action: tensor([[-0.1714,  0.6926,  0.0374, -0.1852,  0.1814,  0.3900, -0.1052]],
       dtype=torch.float64)
	q_value: tensor([[-3.8286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5003897810213321, distance: 0.8088581200174916 entropy 0.03264415264129639
epoch: 31, step: 124
	action: tensor([[ 0.1122,  0.0622, -0.4129,  0.1354, -0.2196,  0.0657,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-4.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4758823715183551, distance: 0.828459108009716 entropy 0.03264415264129639
epoch: 31, step: 125
	action: tensor([[ 0.2591, -0.3212,  0.1272,  0.1571,  0.0124,  0.0620, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-2.9086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3624400738914373, distance: 0.9137285587628043 entropy 0.03264415264129639
epoch: 31, step: 126
	action: tensor([[ 0.5361, -0.2455, -0.4434, -0.1456, -0.4013, -0.2699,  0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-2.5933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36112202953529726, distance: 0.9146725586324447 entropy 0.03264415264129639
epoch: 31, step: 127
	action: tensor([[ 0.5745,  0.1035,  0.2267, -0.3028,  0.1680,  0.1177, -0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-3.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6458178624980973, distance: 0.6810359231032457 entropy 0.03264415264129639
LOSS epoch 31 actor 7.3930032181791505 critic 48.87856133736132 
epoch: 32, step: 0
	action: tensor([[ 0.0683,  0.6823, -0.2248,  0.6346,  0.2978,  0.3131, -0.1898]],
       dtype=torch.float64)
	q_value: tensor([[-2.8704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 32, step: 1
	action: tensor([[ 0.1352, -0.2511, -0.4212, -0.3221,  0.3666,  0.1081, -0.5426]],
       dtype=torch.float64)
	q_value: tensor([[-10.7313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17014541328854904, distance: 1.042455902339311 entropy 0.03264415264129639
epoch: 32, step: 2
	action: tensor([[ 0.3009,  0.1490, -0.4908,  0.0491, -0.3759,  0.3010, -0.5539]],
       dtype=torch.float64)
	q_value: tensor([[-2.3998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6550382120636448, distance: 0.6721128339928978 entropy 0.03264415264129639
epoch: 32, step: 3
	action: tensor([[-0.1005,  0.3042, -0.8369, -0.2917, -0.2239,  0.0351,  0.3524]],
       dtype=torch.float64)
	q_value: tensor([[-3.5260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47170502343356724, distance: 0.8317540683782642 entropy 0.03264415264129639
epoch: 32, step: 4
	action: tensor([[ 0.4837,  0.0916, -0.5523, -0.3263, -0.0646,  0.4296, -0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-4.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7130649417952961, distance: 0.6129830989860271 entropy 0.03264415264129639
epoch: 32, step: 5
	action: tensor([[-0.0535, -0.1489, -0.0083, -0.2480, -0.7289,  0.7248, -0.3486]],
       dtype=torch.float64)
	q_value: tensor([[-3.7147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07664999101677117, distance: 1.0996130166238545 entropy 0.03264415264129639
epoch: 32, step: 6
	action: tensor([[ 0.2637,  0.4020, -0.3751, -0.1150,  0.0829,  0.1359,  0.2652]],
       dtype=torch.float64)
	q_value: tensor([[-4.3449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7375307520144729, distance: 0.5862676177033066 entropy 0.03264415264129639
epoch: 32, step: 7
	action: tensor([[ 0.0271,  0.4784, -0.4087, -0.3552,  0.0713, -0.4718, -0.3309]],
       dtype=torch.float64)
	q_value: tensor([[-3.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5579200929353483, distance: 0.7608640793700607 entropy 0.03264415264129639
epoch: 32, step: 8
	action: tensor([[ 0.0024,  0.0505, -0.1540,  0.1054, -0.2576, -0.2340,  0.1189]],
       dtype=torch.float64)
	q_value: tensor([[-3.8313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3787158710903875, distance: 0.901990208241732 entropy 0.03264415264129639
epoch: 32, step: 9
	action: tensor([[ 0.3309, -0.0085, -0.2654, -0.3259, -0.2515, -0.3249,  0.0227]],
       dtype=torch.float64)
	q_value: tensor([[-2.6227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41248972173276244, distance: 0.877130934061263 entropy 0.03264415264129639
epoch: 32, step: 10
	action: tensor([[ 0.2421, -0.0836, -0.4251,  0.0314, -0.1818,  0.0703, -0.3409]],
       dtype=torch.float64)
	q_value: tensor([[-3.2632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48672790326339577, distance: 0.8198426751565204 entropy 0.03264415264129639
epoch: 32, step: 11
	action: tensor([[ 0.0063,  0.2390, -0.0611,  0.4234,  0.2672, -0.0765, -0.5061]],
       dtype=torch.float64)
	q_value: tensor([[-2.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5937281627098892, distance: 0.7293988131938027 entropy 0.03264415264129639
epoch: 32, step: 12
	action: tensor([[-0.1583, -0.0870, -0.1433,  0.1031, -0.1085, -0.1397, -0.5187]],
       dtype=torch.float64)
	q_value: tensor([[-2.6246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11141026438257773, distance: 1.0787165438828643 entropy 0.03264415264129639
epoch: 32, step: 13
	action: tensor([[ 0.7922,  0.0553,  0.2562,  0.4113, -0.1291,  0.5460, -0.4286]],
       dtype=torch.float64)
	q_value: tensor([[-2.4196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.983137647779576, distance: 0.14859896055759095 entropy 0.03264415264129639
epoch: 32, step: 14
	action: tensor([[ 0.3673,  0.2745, -0.4216, -0.5233, -0.4071, -0.0973, -0.4281]],
       dtype=torch.float64)
	q_value: tensor([[-4.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6695374445055182, distance: 0.6578362805930261 entropy 0.03264415264129639
epoch: 32, step: 15
	action: tensor([[ 0.1268, -0.0368, -0.1032, -0.1731,  0.2482,  0.1436,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[-4.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36839422059901195, distance: 0.9094519132121361 entropy 0.03264415264129639
epoch: 32, step: 16
	action: tensor([[ 0.4271, -0.2383, -0.5774, -0.0042,  0.1665,  0.4488, -0.0720]],
       dtype=torch.float64)
	q_value: tensor([[-2.1871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5442741451914271, distance: 0.7725178575497031 entropy 0.03264415264129639
epoch: 32, step: 17
	action: tensor([[ 0.3635,  0.5559, -0.2858, -0.2355, -0.1157,  0.3081,  0.4881]],
       dtype=torch.float64)
	q_value: tensor([[-2.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8575293123613988, distance: 0.43193605108614863 entropy 0.03264415264129639
epoch: 32, step: 18
	action: tensor([[ 0.3863,  0.5325, -0.1203,  0.1513, -0.1841,  0.3577,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[-4.4337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8682134616527282, distance: 0.41542460778209295 entropy 0.03264415264129639
epoch: 32, step: 19
	action: tensor([[ 0.4192, -0.1109, -0.3261, -0.5497, -0.2540,  0.1721,  0.2540]],
       dtype=torch.float64)
	q_value: tensor([[-3.5437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3420244122747348, distance: 0.9282427879062932 entropy 0.03264415264129639
epoch: 32, step: 20
	action: tensor([[ 0.4080,  0.3466, -0.0593,  0.1803, -0.1973, -0.2272, -0.6922]],
       dtype=torch.float64)
	q_value: tensor([[-3.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8482330006278982, distance: 0.4458054434747327 entropy 0.03264415264129639
epoch: 32, step: 21
	action: tensor([[ 0.4044, -0.0604, -0.1122,  0.1493, -0.0624, -0.4098,  0.1829]],
       dtype=torch.float64)
	q_value: tensor([[-3.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5747562690255837, distance: 0.7462350664915082 entropy 0.03264415264129639
epoch: 32, step: 22
	action: tensor([[ 0.5510, -0.3343, -0.3107,  0.1452,  0.1542,  0.1227, -0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-2.9386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.52032820391692, distance: 0.7925538590953881 entropy 0.03264415264129639
epoch: 32, step: 23
	action: tensor([[ 0.3600, -0.0832, -0.5116, -0.1885, -0.1892,  0.0889, -0.3738]],
       dtype=torch.float64)
	q_value: tensor([[-2.5350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5065627358873637, distance: 0.8038456488529131 entropy 0.03264415264129639
epoch: 32, step: 24
	action: tensor([[ 0.0239,  0.1029,  0.1155, -0.1088, -0.2331,  0.1899, -0.1443]],
       dtype=torch.float64)
	q_value: tensor([[-2.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.401941483169453, distance: 0.8849699682107907 entropy 0.03264415264129639
epoch: 32, step: 25
	action: tensor([[ 0.2401,  0.0671, -0.4087,  0.4486, -0.2933,  0.5464, -0.5480]],
       dtype=torch.float64)
	q_value: tensor([[-2.6591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6355976852566187, distance: 0.6907919322092676 entropy 0.03264415264129639
epoch: 32, step: 26
	action: tensor([[-0.1205, -0.1080, -0.4127, -0.1356,  0.5450,  0.3381, -0.4688]],
       dtype=torch.float64)
	q_value: tensor([[-3.3668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1935472536603462, distance: 1.0276521989732421 entropy 0.03264415264129639
epoch: 32, step: 27
	action: tensor([[ 0.1999,  0.3428, -0.3636, -0.2133, -0.0789, -0.3298, -0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-2.5712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6492577115430574, distance: 0.6777207127289109 entropy 0.03264415264129639
epoch: 32, step: 28
	action: tensor([[-0.4402, -0.1346, -0.3545, -0.3387, -0.1302, -0.1975,  0.0677]],
       dtype=torch.float64)
	q_value: tensor([[-3.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3298045886231531, distance: 1.3196252197000145 entropy 0.03264415264129639
epoch: 32, step: 29
	action: tensor([[ 0.1203, -0.0344, -0.3300,  0.0357, -0.3739, -0.4459, -0.3971]],
       dtype=torch.float64)
	q_value: tensor([[-3.2608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3917830870558632, distance: 0.892454203770547 entropy 0.03264415264129639
epoch: 32, step: 30
	action: tensor([[-0.3494,  0.2434, -0.2287, -0.0290,  0.2850, -0.0816,  0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-3.1091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06855997889164267, distance: 1.104419689031081 entropy 0.03264415264129639
epoch: 32, step: 31
	action: tensor([[ 0.3180,  0.0326, -0.5291, -0.0117,  0.2798,  0.2548, -0.2342]],
       dtype=torch.float64)
	q_value: tensor([[-2.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6113358943362655, distance: 0.713417749680178 entropy 0.03264415264129639
epoch: 32, step: 32
	action: tensor([[ 0.1840,  0.0872, -0.4501,  0.1015,  0.0630, -0.3168, -0.2273]],
       dtype=torch.float64)
	q_value: tensor([[-2.5636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5283397187764444, distance: 0.7859073421284568 entropy 0.03264415264129639
epoch: 32, step: 33
	action: tensor([[-0.0602,  0.3930, -0.0871, -0.2153, -0.0934,  0.0391,  0.0556]],
       dtype=torch.float64)
	q_value: tensor([[-2.7789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4488462165728698, distance: 0.8495581093955413 entropy 0.03264415264129639
epoch: 32, step: 34
	action: tensor([[ 0.0551,  0.3236, -0.0562,  0.1480, -0.1050,  0.0902, -0.4312]],
       dtype=torch.float64)
	q_value: tensor([[-2.9980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.618033376132122, distance: 0.7072442109602425 entropy 0.03264415264129639
epoch: 32, step: 35
	action: tensor([[ 0.0841,  0.0027, -0.9502,  0.4571,  0.0142, -0.2864, -0.2527]],
       dtype=torch.float64)
	q_value: tensor([[-2.7105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35409485817922215, distance: 0.9196891526236195 entropy 0.03264415264129639
epoch: 32, step: 36
	action: tensor([[ 0.1939, -0.0217, -0.1406,  0.1050,  0.4710, -0.0393,  0.2554]],
       dtype=torch.float64)
	q_value: tensor([[-3.4563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4911689941773931, distance: 0.8162881218563729 entropy 0.03264415264129639
epoch: 32, step: 37
	action: tensor([[ 0.3172,  0.1183, -0.0868, -0.0282, -0.0566,  0.0376, -0.2859]],
       dtype=torch.float64)
	q_value: tensor([[-2.5940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6361395865201718, distance: 0.6902781041137669 entropy 0.03264415264129639
epoch: 32, step: 38
	action: tensor([[ 0.0799, -0.1796, -0.0618,  0.1721, -0.1148,  0.3938,  0.4468]],
       dtype=torch.float64)
	q_value: tensor([[-2.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41851418479838276, distance: 0.8726221965382552 entropy 0.03264415264129639
epoch: 32, step: 39
	action: tensor([[ 0.2331,  0.0202, -0.2166,  0.1718, -0.1282,  0.2402, -0.1018]],
       dtype=torch.float64)
	q_value: tensor([[-2.7027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6209873385028808, distance: 0.7045041447654299 entropy 0.03264415264129639
epoch: 32, step: 40
	action: tensor([[ 0.2692,  0.2521, -0.0213, -0.0848,  0.0361, -0.2354, -0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-2.3324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.637585672325973, distance: 0.6889050567968968 entropy 0.03264415264129639
epoch: 32, step: 41
	action: tensor([[-0.0251,  0.2386, -0.4501, -0.0677,  0.2084, -0.2736,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-2.7334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4235982447414953, distance: 0.8687990564792211 entropy 0.03264415264129639
epoch: 32, step: 42
	action: tensor([[ 0.0541,  0.4397, -0.1130,  0.3033, -0.1196, -0.0029, -0.4675]],
       dtype=torch.float64)
	q_value: tensor([[-2.9340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.65302086867143, distance: 0.6740752337429974 entropy 0.03264415264129639
epoch: 32, step: 43
	action: tensor([[ 0.7261,  0.1399, -0.2473,  0.0834,  0.0254,  0.2860, -0.1644]],
       dtype=torch.float64)
	q_value: tensor([[-3.0050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9018030283556315, distance: 0.3585962474337957 entropy 0.03264415264129639
epoch: 32, step: 44
	action: tensor([[ 0.5346, -0.5455, -0.5274, -0.3161, -0.3493,  0.1616, -0.0786]],
       dtype=torch.float64)
	q_value: tensor([[-3.2985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04586911025583651, distance: 1.117791161799813 entropy 0.03264415264129639
epoch: 32, step: 45
	action: tensor([[ 0.0355,  0.4795, -0.1998, -0.2947,  0.1380,  0.0185, -0.5438]],
       dtype=torch.float64)
	q_value: tensor([[-3.4191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5755524415195554, distance: 0.745536161198359 entropy 0.03264415264129639
epoch: 32, step: 46
	action: tensor([[ 0.1458,  0.0829, -0.1186, -0.1994, -0.0830,  0.0584, -0.1706]],
       dtype=torch.float64)
	q_value: tensor([[-3.1716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44958109790256884, distance: 0.8489915409879739 entropy 0.03264415264129639
epoch: 32, step: 47
	action: tensor([[-0.0183,  0.0185, -0.2294,  0.1032,  0.3186, -0.2653, -0.0425]],
       dtype=torch.float64)
	q_value: tensor([[-2.4259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28403503262096064, distance: 0.9682836074521993 entropy 0.03264415264129639
epoch: 32, step: 48
	action: tensor([[ 0.3419, -0.1036, -0.2866,  0.2473,  0.5746,  0.2989, -0.2231]],
       dtype=torch.float64)
	q_value: tensor([[-2.4454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6715450390913269, distance: 0.6558350244005642 entropy 0.03264415264129639
epoch: 32, step: 49
	action: tensor([[ 0.0754,  0.2308, -0.1742, -0.0160, -0.0649,  0.0769,  0.1040]],
       dtype=torch.float64)
	q_value: tensor([[-2.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5407001594023215, distance: 0.7755411398395331 entropy 0.03264415264129639
epoch: 32, step: 50
	action: tensor([[ 0.1881,  0.1587, -0.0348,  0.3157, -0.3104,  0.4412,  0.2051]],
       dtype=torch.float64)
	q_value: tensor([[-2.5663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7217377258771454, distance: 0.603648128234349 entropy 0.03264415264129639
epoch: 32, step: 51
	action: tensor([[ 0.2124,  0.0665, -0.4883, -0.4574,  0.0233,  0.0964, -0.2820]],
       dtype=torch.float64)
	q_value: tensor([[-3.0724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47491407636116134, distance: 0.8292240343662173 entropy 0.03264415264129639
epoch: 32, step: 52
	action: tensor([[ 0.2738, -0.0246,  0.0479, -0.0616,  0.0796, -0.0041, -0.6593]],
       dtype=torch.float64)
	q_value: tensor([[-3.1563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5076158532401285, distance: 0.8029873878037965 entropy 0.03264415264129639
epoch: 32, step: 53
	action: tensor([[ 0.1304,  0.1378, -0.0417, -0.0457, -0.0657,  0.1405, -0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-2.5583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5481020353535506, distance: 0.7692666163759925 entropy 0.03264415264129639
epoch: 32, step: 54
	action: tensor([[ 0.0096,  0.2582, -0.5408,  0.1071, -0.2902,  0.1146, -0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-2.4156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4873127936124415, distance: 0.8193754232291254 entropy 0.03264415264129639
epoch: 32, step: 55
	action: tensor([[-0.4351,  0.3822,  0.0185, -0.0104, -0.0752, -0.0206,  0.2805]],
       dtype=torch.float64)
	q_value: tensor([[-3.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06953621961504064, distance: 1.1038407670879684 entropy 0.03264415264129639
epoch: 32, step: 56
	action: tensor([[-0.1622, -0.1315,  0.1202,  0.0066, -0.0583,  0.3847,  0.1254]],
       dtype=torch.float64)
	q_value: tensor([[-3.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13178484981261407, distance: 1.066277814684421 entropy 0.03264415264129639
epoch: 32, step: 57
	action: tensor([[-0.0336,  0.4970,  0.0590, -0.8016,  0.0837,  0.3322, -0.4350]],
       dtype=torch.float64)
	q_value: tensor([[-2.4950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33353980047005927, distance: 0.9342084746734003 entropy 0.03264415264129639
epoch: 32, step: 58
	action: tensor([[ 0.2108, -0.0882,  0.1096, -0.0278, -0.1343,  0.6833, -0.4255]],
       dtype=torch.float64)
	q_value: tensor([[-4.0491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5570429156073856, distance: 0.7616185605922303 entropy 0.03264415264129639
epoch: 32, step: 59
	action: tensor([[ 0.7415,  0.4314, -1.1274,  0.4122, -0.0550, -0.1889, -0.0872]],
       dtype=torch.float64)
	q_value: tensor([[-2.9829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9345098780774062, distance: 0.2928495713958497 entropy 0.03264415264129639
epoch: 32, step: 60
	action: tensor([[ 0.2097, -0.0153, -0.6328, -0.1640, -0.0860,  0.0918, -0.8421]],
       dtype=torch.float64)
	q_value: tensor([[-5.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4776518774780022, distance: 0.8270594195860401 entropy 0.03264415264129639
epoch: 32, step: 61
	action: tensor([[ 0.2758,  0.2860, -0.2369, -0.3899,  0.1812,  0.2255, -0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-3.3147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6730001166196616, distance: 0.6543807153465954 entropy 0.03264415264129639
epoch: 32, step: 62
	action: tensor([[ 0.1120,  0.1790, -0.0468, -0.2426, -0.3540,  0.4077, -0.1160]],
       dtype=torch.float64)
	q_value: tensor([[-2.9671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.502859053658326, distance: 0.8068567946676602 entropy 0.03264415264129639
epoch: 32, step: 63
	action: tensor([[ 0.0910,  0.1982, -0.0173, -0.4838,  0.2541,  0.0999, -0.5003]],
       dtype=torch.float64)
	q_value: tensor([[-3.3258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37360543917492817, distance: 0.9056923141115428 entropy 0.03264415264129639
epoch: 32, step: 64
	action: tensor([[ 0.0194, -0.2898, -0.0758,  0.6369, -0.1550,  0.0035,  0.2499]],
       dtype=torch.float64)
	q_value: tensor([[-2.6710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42930473471710395, distance: 0.8644877189777594 entropy 0.03264415264129639
epoch: 32, step: 65
	action: tensor([[-0.0649, -0.4598,  0.0360,  0.0200,  0.0103,  0.2403, -0.1498]],
       dtype=torch.float64)
	q_value: tensor([[-2.5724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04403719764745184, distance: 1.1692696561613016 entropy 0.03264415264129639
epoch: 32, step: 66
	action: tensor([[ 0.6012,  0.2211, -0.3087,  0.0035,  0.3975, -0.5238,  0.1826]],
       dtype=torch.float64)
	q_value: tensor([[-2.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7559304173850341, distance: 0.5653449569723954 entropy 0.03264415264129639
epoch: 32, step: 67
	action: tensor([[ 0.2351, -0.0391, -0.0941, -0.2285,  0.5578, -0.2491,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[-3.9749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28942206232723555, distance: 0.9646339725520888 entropy 0.03264415264129639
epoch: 32, step: 68
	action: tensor([[-0.1297, -0.3744,  0.0136,  0.0953, -0.1413,  0.0694, -0.3957]],
       dtype=torch.float64)
	q_value: tensor([[-2.5911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07331684153889984, distance: 1.185552157655344 entropy 0.03264415264129639
epoch: 32, step: 69
	action: tensor([[ 0.1033, -0.0969, -0.4923,  0.1600,  0.1060,  0.0460, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[-2.1230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35697719525467175, distance: 0.9176348121858199 entropy 0.03264415264129639
epoch: 32, step: 70
	action: tensor([[ 0.1487,  0.1152, -0.0957, -0.0535,  0.2747,  0.1585,  0.0333]],
       dtype=torch.float64)
	q_value: tensor([[-2.2905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5302826412572219, distance: 0.7842869674018171 entropy 0.03264415264129639
epoch: 32, step: 71
	action: tensor([[ 0.1942,  0.5192, -0.0217,  0.1964,  0.3259,  0.5429, -0.5816]],
       dtype=torch.float64)
	q_value: tensor([[-2.2658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7997960147102201, distance: 0.5120272238061171 entropy 0.03264415264129639
epoch: 32, step: 72
	action: tensor([[ 0.4007,  0.3341, -0.2943, -0.1988,  0.1242,  0.1981, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-3.3597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7981269905925245, distance: 0.5141570817135562 entropy 0.03264415264129639
epoch: 32, step: 73
	action: tensor([[ 4.4929e-01,  3.7010e-01, -4.7983e-01, -8.5761e-02,  5.5377e-02,
          2.4179e-01,  3.3408e-04]], dtype=torch.float64)
	q_value: tensor([[-3.0774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.859185399689731, distance: 0.4294182885060853 entropy 0.03264415264129639
epoch: 32, step: 74
	action: tensor([[ 0.2521, -0.4295, -0.4921, -0.2667,  0.3856,  0.0837, -0.1121]],
       dtype=torch.float64)
	q_value: tensor([[-3.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08097267110443329, distance: 1.0970360677737656 entropy 0.03264415264129639
epoch: 32, step: 75
	action: tensor([[-0.1044, -0.1691, -0.0863, -0.0210,  0.1524,  0.1509, -0.3500]],
       dtype=torch.float64)
	q_value: tensor([[-2.4744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11679315331961837, distance: 1.0754442621648581 entropy 0.03264415264129639
epoch: 32, step: 76
	action: tensor([[ 0.0957, -0.2829, -0.6723, -0.0711,  0.4071,  0.3747,  0.0948]],
       dtype=torch.float64)
	q_value: tensor([[-2.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24813377640514256, distance: 0.9922634191119897 entropy 0.03264415264129639
epoch: 32, step: 77
	action: tensor([[-0.0037,  0.2028, -0.3239,  0.2959, -0.2542, -0.0924, -0.3286]],
       dtype=torch.float64)
	q_value: tensor([[-2.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5005420606100914, distance: 0.8087348419456227 entropy 0.03264415264129639
epoch: 32, step: 78
	action: tensor([[-0.2906, -0.0015,  0.2954,  0.2129, -0.4606,  0.3146, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-2.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2041811752371231, distance: 1.0208543818604847 entropy 0.03264415264129639
epoch: 32, step: 79
	action: tensor([[ 0.6458, -0.3376, -0.4383, -0.2790,  0.0521,  0.0051, -0.7095]],
       dtype=torch.float64)
	q_value: tensor([[-2.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2898856584573912, distance: 0.9643192473702548 entropy 0.03264415264129639
epoch: 32, step: 80
	action: tensor([[ 0.5431,  0.0096, -0.5555,  0.3238, -0.2804, -0.2480, -0.4557]],
       dtype=torch.float64)
	q_value: tensor([[-3.2392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7632537608087195, distance: 0.5567987321041175 entropy 0.03264415264129639
epoch: 32, step: 81
	action: tensor([[ 0.1692,  0.5533, -0.4545, -0.0844,  0.2011, -0.0730,  0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-3.5210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7530659583349726, distance: 0.5686527917746732 entropy 0.03264415264129639
epoch: 32, step: 82
	action: tensor([[ 0.0795,  0.1643,  0.0469, -0.1137, -0.1147,  0.0279,  0.1535]],
       dtype=torch.float64)
	q_value: tensor([[-3.6692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4615688633381253, distance: 0.8396954048118612 entropy 0.03264415264129639
epoch: 32, step: 83
	action: tensor([[ 0.0333,  0.2190, -0.2772, -0.0948, -0.3999,  0.1444, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[-2.6094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4802440118480428, distance: 0.8250047405615831 entropy 0.03264415264129639
epoch: 32, step: 84
	action: tensor([[-0.2017, -0.1653, -0.3026,  0.0604, -0.1058,  0.1538, -0.2988]],
       dtype=torch.float64)
	q_value: tensor([[-3.1181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01787272241515192, distance: 1.1340718746163507 entropy 0.03264415264129639
epoch: 32, step: 85
	action: tensor([[ 0.2604, -0.1757, -0.3889, -0.3349, -0.3042, -0.3670, -0.2340]],
       dtype=torch.float64)
	q_value: tensor([[-2.3092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26275923648135, distance: 0.9825651659134457 entropy 0.03264415264129639
epoch: 32, step: 86
	action: tensor([[-0.1789, -0.1139, -0.0171, -0.0894, -0.2070,  0.2327, -0.1020]],
       dtype=torch.float64)
	q_value: tensor([[-3.2391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045960933918203994, distance: 1.1177373735046638 entropy 0.03264415264129639
epoch: 32, step: 87
	action: tensor([[ 0.5785,  0.0161, -0.1758, -0.1076,  0.0694,  0.7179,  0.1788]],
       dtype=torch.float64)
	q_value: tensor([[-2.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.815946372905801, distance: 0.4909405258690885 entropy 0.03264415264129639
epoch: 32, step: 88
	action: tensor([[ 0.1822,  0.1483, -0.4038,  0.1032,  0.0723,  0.3653, -0.4105]],
       dtype=torch.float64)
	q_value: tensor([[-3.5465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.637609624644272, distance: 0.6888822912153691 entropy 0.03264415264129639
epoch: 32, step: 89
	action: tensor([[ 0.1431,  0.2943, -0.0692, -0.1259,  0.5626, -0.1019,  0.0458]],
       dtype=torch.float64)
	q_value: tensor([[-2.5688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.559328585618206, distance: 0.7596510334523575 entropy 0.03264415264129639
epoch: 32, step: 90
	action: tensor([[-0.0550, -0.0061, -0.3077,  0.0111, -0.1069,  0.2237, -0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-2.7002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29405920008933273, distance: 0.9614812836814259 entropy 0.03264415264129639
epoch: 32, step: 91
	action: tensor([[ 0.6689,  0.0354, -0.4565,  0.2766,  0.5360, -0.2591, -0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-2.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7842203800633021, distance: 0.531571765378367 entropy 0.03264415264129639
epoch: 32, step: 92
	action: tensor([[ 0.2966,  0.1845, -0.2518, -0.1178, -0.0030, -0.2927, -0.3792]],
       dtype=torch.float64)
	q_value: tensor([[-3.5543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6170011338426509, distance: 0.7081992090880601 entropy 0.03264415264129639
epoch: 32, step: 93
	action: tensor([[ 0.3372,  0.3281, -0.5629,  0.0190, -0.0106,  0.3658, -0.2686]],
       dtype=torch.float64)
	q_value: tensor([[-2.8987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7801046045736006, distance: 0.536617412678777 entropy 0.03264415264129639
epoch: 32, step: 94
	action: tensor([[ 0.1758, -0.4148,  0.3566,  0.0196,  0.2363, -0.2084, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[-3.3171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03026573776412722, distance: 1.126893991728494 entropy 0.03264415264129639
epoch: 32, step: 95
	action: tensor([[ 0.3750, -0.3458, -0.2041,  0.1362, -0.0796,  0.1172, -0.2771]],
       dtype=torch.float64)
	q_value: tensor([[-2.3845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39972296691244447, distance: 0.8866098603170286 entropy 0.03264415264129639
epoch: 32, step: 96
	action: tensor([[ 0.5398, -0.3636,  0.2272, -0.3626,  0.1419, -0.2203, -0.0949]],
       dtype=torch.float64)
	q_value: tensor([[-2.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032142847280278275, distance: 1.125802802030784 entropy 0.03264415264129639
epoch: 32, step: 97
	action: tensor([[ 0.3251, -0.3940,  0.1174, -0.2573, -0.0298, -0.0323, -0.0420]],
       dtype=torch.float64)
	q_value: tensor([[-2.8965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03483594878744045, distance: 1.1242354151232004 entropy 0.03264415264129639
epoch: 32, step: 98
	action: tensor([[ 0.0058,  0.4189, -0.3040,  0.3526, -0.1664,  0.0491, -0.1652]],
       dtype=torch.float64)
	q_value: tensor([[-2.4243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5784430182592317, distance: 0.7429931955946595 entropy 0.03264415264129639
epoch: 32, step: 99
	action: tensor([[ 0.3306,  0.2137, -0.1927, -0.0062, -0.2633, -0.2021, -0.1133]],
       dtype=torch.float64)
	q_value: tensor([[-2.9352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7168431276889563, distance: 0.6089340320601275 entropy 0.03264415264129639
epoch: 32, step: 100
	action: tensor([[ 0.0683, -0.2670, -0.4330,  0.1864,  0.5495,  0.0806,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-2.9590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23970750641720673, distance: 0.9978081447670525 entropy 0.03264415264129639
epoch: 32, step: 101
	action: tensor([[-0.1436,  0.1896, -0.0324, -0.1284, -0.2307,  0.0986, -0.3552]],
       dtype=torch.float64)
	q_value: tensor([[-2.4370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2669270315765504, distance: 0.9797838937816504 entropy 0.03264415264129639
epoch: 32, step: 102
	action: tensor([[ 0.1062,  0.8059,  0.2490, -0.1666,  0.1018, -0.1833, -0.5715]],
       dtype=torch.float64)
	q_value: tensor([[-2.7717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6811772823655855, distance: 0.6461469861327278 entropy 0.03264415264129639
epoch: 32, step: 103
	action: tensor([[-0.0376, -0.2749,  0.1194,  0.1043, -0.1762,  0.1777,  0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-3.8904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16695222822819078, distance: 1.0444596019310295 entropy 0.03264415264129639
epoch: 32, step: 104
	action: tensor([[-0.1075, -0.1521, -0.1242, -0.0657, -0.0894,  0.0559, -0.1900]],
       dtype=torch.float64)
	q_value: tensor([[-2.3038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0757313985932132, distance: 1.1001598542683384 entropy 0.03264415264129639
epoch: 32, step: 105
	action: tensor([[-0.1243,  0.4355, -0.0966, -0.0278, -0.3746,  0.2138, -0.0536]],
       dtype=torch.float64)
	q_value: tensor([[-2.1735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44339284789574274, distance: 0.8537507230790882 entropy 0.03264415264129639
epoch: 32, step: 106
	action: tensor([[ 0.1500, -0.1118, -0.0686, -0.1099, -0.1796,  0.1323, -0.3333]],
       dtype=torch.float64)
	q_value: tensor([[-3.4190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3488532139589211, distance: 0.9234133392812963 entropy 0.03264415264129639
epoch: 32, step: 107
	action: tensor([[-0.3505,  0.5326, -0.0473,  0.3322, -0.2764,  0.1040, -0.1921]],
       dtype=torch.float64)
	q_value: tensor([[-2.3633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3576736457796921, distance: 0.9171377377564418 entropy 0.03264415264129639
epoch: 32, step: 108
	action: tensor([[ 0.0459,  0.1256, -0.0175,  0.3114,  0.5970,  0.3388,  0.2252]],
       dtype=torch.float64)
	q_value: tensor([[-3.3199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6424300219678827, distance: 0.6842853105730158 entropy 0.03264415264129639
epoch: 32, step: 109
	action: tensor([[ 0.1325,  0.0585, -0.5931, -0.1729, -0.3506,  0.1561, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[-2.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4342955729885033, distance: 0.8606993638352752 entropy 0.03264415264129639
epoch: 32, step: 110
	action: tensor([[ 0.4996,  0.0677, -0.3810, -0.5351, -0.2368,  0.0347,  0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-3.3285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.528362146231826, distance: 0.7858886569517689 entropy 0.03264415264129639
epoch: 32, step: 111
	action: tensor([[ 0.3687,  0.5265, -0.3676, -0.3007,  0.0516,  0.0288, -0.3118]],
       dtype=torch.float64)
	q_value: tensor([[-4.0093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8483240491751607, distance: 0.4456716988935774 entropy 0.03264415264129639
epoch: 32, step: 112
	action: tensor([[ 0.2894, -0.1168, -0.2366, -0.2568,  0.0326,  0.0643, -0.1875]],
       dtype=torch.float64)
	q_value: tensor([[-3.7366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3771639758915889, distance: 0.9031160387336278 entropy 0.03264415264129639
epoch: 32, step: 113
	action: tensor([[ 0.3053,  0.2286,  0.2624, -0.1112, -0.0137,  0.1716, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-2.3850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.705004911206309, distance: 0.6215328481256012 entropy 0.03264415264129639
epoch: 32, step: 114
	action: tensor([[-0.1650,  0.0494, -0.0191, -0.1058,  0.2153, -0.0707, -0.4400]],
       dtype=torch.float64)
	q_value: tensor([[-2.7365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11629569285788555, distance: 1.0757470879920696 entropy 0.03264415264129639
epoch: 32, step: 115
	action: tensor([[-0.3019,  0.0668, -0.2991, -0.3093, -0.2083,  0.6243, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-2.2454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021503220739486073, distance: 1.131973848168387 entropy 0.03264415264129639
epoch: 32, step: 116
	action: tensor([[ 0.5351, -0.2003, -0.3719,  0.0805,  0.8490,  0.1937, -0.4104]],
       dtype=torch.float64)
	q_value: tensor([[-3.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6034715719360393, distance: 0.7205993357545133 entropy 0.03264415264129639
epoch: 32, step: 117
	action: tensor([[ 0.3111,  0.5110,  0.0647, -0.0865,  0.0048,  0.2402, -0.4275]],
       dtype=torch.float64)
	q_value: tensor([[-2.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8178956308279095, distance: 0.48833390252419895 entropy 0.03264415264129639
epoch: 32, step: 118
	action: tensor([[-0.0835,  0.0996, -0.3450, -0.1095, -0.3664,  0.1635, -0.6359]],
       dtype=torch.float64)
	q_value: tensor([[-3.2989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2781160419781299, distance: 0.9722778427032478 entropy 0.03264415264129639
epoch: 32, step: 119
	action: tensor([[ 0.2527,  0.2295, -0.2864, -0.1106, -0.3641,  0.0926,  0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-3.2520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6634578451791174, distance: 0.6638598887601009 entropy 0.03264415264129639
epoch: 32, step: 120
	action: tensor([[ 0.0735, -0.0533, -0.3353, -0.1355, -0.1511, -0.0111, -0.4685]],
       dtype=torch.float64)
	q_value: tensor([[-3.1891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3133219131581503, distance: 0.9482727868073332 entropy 0.03264415264129639
epoch: 32, step: 121
	action: tensor([[ 0.2945,  0.1558, -0.6388, -0.1920, -0.5018,  0.1222,  0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-2.5347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6467053153444704, distance: 0.680182172626227 entropy 0.03264415264129639
epoch: 32, step: 122
	action: tensor([[ 0.1856,  0.1403,  0.1178,  0.3518,  0.1513,  0.2451, -0.3904]],
       dtype=torch.float64)
	q_value: tensor([[-4.1046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7559383229139109, distance: 0.5653358010033751 entropy 0.03264415264129639
epoch: 32, step: 123
	action: tensor([[-0.1155, -0.3377, -0.4139,  0.2568, -0.3486, -0.0948, -0.0406]],
       dtype=torch.float64)
	q_value: tensor([[-2.5508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0029784457218640092, distance: 1.1426387995282796 entropy 0.03264415264129639
epoch: 32, step: 124
	action: tensor([[-0.3631,  0.3552, -0.3933, -0.3428, -0.4591,  0.2231,  0.3101]],
       dtype=torch.float64)
	q_value: tensor([[-2.5932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09910425666720146, distance: 1.08616038948405 entropy 0.03264415264129639
epoch: 32, step: 125
	action: tensor([[-0.0289,  0.3902, -0.1376, -0.1741, -0.2535,  0.1800, -0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-4.6376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48346344497114113, distance: 0.8224456807249395 entropy 0.03264415264129639
epoch: 32, step: 126
	action: tensor([[ 0.0341,  0.6295,  0.2823, -0.2372, -0.0298,  0.0148, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[-3.2203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.582437971214334, distance: 0.7394642674521756 entropy 0.03264415264129639
epoch: 32, step: 127
	action: tensor([[ 0.0362,  0.5930, -0.6393, -0.5051, -0.3522, -0.1527, -0.2093]],
       dtype=torch.float64)
	q_value: tensor([[-3.3928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7161837887577146, distance: 0.6096425800459219 entropy 0.03264415264129639
LOSS epoch 32 actor 15.387248838583972 critic 73.47175329995619 
epoch: 33, step: 0
	action: tensor([[-0.0445,  0.2149, -0.6579,  0.2230,  0.0085, -0.0659, -0.2009]],
       dtype=torch.float64)
	q_value: tensor([[-4.5469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4091136967814175, distance: 0.8796474637118535 entropy 0.03264415264129639
epoch: 33, step: 1
	action: tensor([[ 0.2591,  0.3337,  0.1318,  0.4667,  0.4940, -0.2727, -0.3645]],
       dtype=torch.float64)
	q_value: tensor([[-2.5282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7984433812459171, distance: 0.5137540107818633 entropy 0.03264415264129639
epoch: 33, step: 2
	action: tensor([[ 0.0816, -0.1107, -0.2869, -0.3913, -0.1733,  0.1966, -0.0432]],
       dtype=torch.float64)
	q_value: tensor([[-2.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1987628749838697, distance: 1.0243237095505475 entropy 0.03264415264129639
epoch: 33, step: 3
	action: tensor([[ 0.2778, -0.3578, -0.1401, -0.3137, -0.0178,  0.2477, -0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-2.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1360485443488919, distance: 1.0636564147965355 entropy 0.03264415264129639
epoch: 33, step: 4
	action: tensor([[ 0.2511,  0.1623, -0.5922,  0.1587,  0.6952, -0.2435,  0.2740]],
       dtype=torch.float64)
	q_value: tensor([[-2.1100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6028144841391763, distance: 0.7211961416840132 entropy 0.03264415264129639
epoch: 33, step: 5
	action: tensor([[ 0.2755,  0.4374, -0.3965, -0.0381, -0.3133, -0.4528, -0.2974]],
       dtype=torch.float64)
	q_value: tensor([[-3.2698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7875054290260672, distance: 0.5275098975750658 entropy 0.03264415264129639
epoch: 33, step: 6
	action: tensor([[ 7.6307e-02, -5.3023e-02,  1.1107e-01,  1.5915e-02,  4.3478e-05,
          3.2210e-01, -5.7119e-01]], dtype=torch.float64)
	q_value: tensor([[-3.4941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45880379564907114, distance: 0.8418487362532706 entropy 0.03264415264129639
epoch: 33, step: 7
	action: tensor([[ 0.0061, -0.0372, -0.1594,  0.1218,  0.1406, -0.0788, -0.7766]],
       dtype=torch.float64)
	q_value: tensor([[-2.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33286452191646354, distance: 0.9346816396346025 entropy 0.03264415264129639
epoch: 33, step: 8
	action: tensor([[ 0.5764, -0.1605, -0.2809,  0.3517, -0.0924, -0.1479,  0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-2.2343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6982975202266626, distance: 0.6285591215095089 entropy 0.03264415264129639
epoch: 33, step: 9
	action: tensor([[-0.0581,  0.2516, -0.1778, -0.0817,  0.5919,  0.0460, -0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-2.5024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4116605597369909, distance: 0.8777496697984032 entropy 0.03264415264129639
epoch: 33, step: 10
	action: tensor([[ 0.4861, -0.0359,  0.0379, -0.4042, -0.0534,  0.3341, -0.4754]],
       dtype=torch.float64)
	q_value: tensor([[-2.3091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49412457881748917, distance: 0.8139139325127918 entropy 0.03264415264129639
epoch: 33, step: 11
	action: tensor([[ 0.5701,  0.4711, -0.1831, -0.2159, -0.1585,  0.0505, -0.4381]],
       dtype=torch.float64)
	q_value: tensor([[-2.7591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8901788322879065, distance: 0.3792274116552697 entropy 0.03264415264129639
epoch: 33, step: 12
	action: tensor([[ 0.1282,  0.5164, -0.5035, -0.0510,  0.2039,  0.1920,  0.1597]],
       dtype=torch.float64)
	q_value: tensor([[-3.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7113116862881763, distance: 0.614852998209235 entropy 0.03264415264129639
epoch: 33, step: 13
	action: tensor([[ 0.6144,  0.3088,  0.1168,  0.2819, -0.3748, -0.3958, -0.1468]],
       dtype=torch.float64)
	q_value: tensor([[-3.2430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9242420302074494, distance: 0.31497120874178086 entropy 0.03264415264129639
epoch: 33, step: 14
	action: tensor([[ 0.6906,  0.3751, -0.1274,  0.0216,  0.3402, -0.0690, -0.2707]],
       dtype=torch.float64)
	q_value: tensor([[-3.4016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9302796169178568, distance: 0.3021597243437725 entropy 0.03264415264129639
epoch: 33, step: 15
	action: tensor([[-0.2627,  0.0583,  0.0082,  0.2617,  0.0148,  0.2927, -0.6049]],
       dtype=torch.float64)
	q_value: tensor([[-3.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2775446250758198, distance: 0.9726625763484409 entropy 0.03264415264129639
epoch: 33, step: 16
	action: tensor([[ 0.3671,  0.1294, -0.5152, -0.0772,  0.1871,  0.2361, -0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-2.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7135624953713862, distance: 0.6124514033022929 entropy 0.03264415264129639
epoch: 33, step: 17
	action: tensor([[ 0.5057,  0.5473, -0.3785, -0.2162,  0.4499, -0.1389, -0.0602]],
       dtype=torch.float64)
	q_value: tensor([[-2.4816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8988916572705212, distance: 0.36387329981421246 entropy 0.03264415264129639
epoch: 33, step: 18
	action: tensor([[ 0.0305,  0.2751, -0.2919,  0.2430, -0.0351,  0.1958, -0.3846]],
       dtype=torch.float64)
	q_value: tensor([[-3.5229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.574864473692837, distance: 0.7461401194665869 entropy 0.03264415264129639
epoch: 33, step: 19
	action: tensor([[ 0.2955,  0.0485, -0.0207, -0.3047, -0.4018, -0.2774, -0.0560]],
       dtype=torch.float64)
	q_value: tensor([[-2.2840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4295784193476546, distance: 0.8642804057328413 entropy 0.03264415264129639
epoch: 33, step: 20
	action: tensor([[-0.1383, -0.1073, -0.5366,  0.0348,  0.0477,  0.1476,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-2.8707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11407782052020421, distance: 1.0770961674205906 entropy 0.03264415264129639
epoch: 33, step: 21
	action: tensor([[ 0.2540,  0.2767,  0.1006, -0.1080, -0.1249,  0.2951, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[-2.2267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6928150005890745, distance: 0.6342444789092256 entropy 0.03264415264129639
epoch: 33, step: 22
	action: tensor([[ 0.2499,  0.3776, -0.1177,  0.3634, -0.5921,  0.0045,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[-2.5454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7754127235075996, distance: 0.5423120664154615 entropy 0.03264415264129639
epoch: 33, step: 23
	action: tensor([[ 0.1531, -0.2602, -0.4416, -0.3821,  0.0093,  0.1403, -0.4143]],
       dtype=torch.float64)
	q_value: tensor([[-3.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15389879188077282, distance: 1.0526108707707498 entropy 0.03264415264129639
epoch: 33, step: 24
	action: tensor([[-0.0223,  0.5038, -0.5347,  0.0267,  0.2232,  0.0371,  0.2857]],
       dtype=torch.float64)
	q_value: tensor([[-2.2883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5773627226397045, distance: 0.7439445957154054 entropy 0.03264415264129639
epoch: 33, step: 25
	action: tensor([[ 0.3019,  0.5271, -0.3847, -0.2485, -0.1182,  0.2880, -0.3738]],
       dtype=torch.float64)
	q_value: tensor([[-3.2569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8182445610654905, distance: 0.4878658298279862 entropy 0.03264415264129639
epoch: 33, step: 26
	action: tensor([[ 0.4689,  0.6227, -0.3633,  0.2138,  0.0130,  0.0689, -0.2830]],
       dtype=torch.float64)
	q_value: tensor([[-3.5036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.885637714795119, distance: 0.3869885412482835 entropy 0.03264415264129639
epoch: 33, step: 27
	action: tensor([[ 0.4447,  0.2426, -0.3756,  0.2464, -0.2100, -0.1092, -0.1601]],
       dtype=torch.float64)
	q_value: tensor([[-3.2823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.844238863462788, distance: 0.45163360259916085 entropy 0.03264415264129639
epoch: 33, step: 28
	action: tensor([[ 0.1439,  0.0557, -0.0064,  0.0990,  0.0309,  0.0603, -0.2835]],
       dtype=torch.float64)
	q_value: tensor([[-2.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5519014226615004, distance: 0.7660259389035426 entropy 0.03264415264129639
epoch: 33, step: 29
	action: tensor([[ 0.0944,  0.3115, -0.7013, -0.1807,  0.0052, -0.0133, -0.1636]],
       dtype=torch.float64)
	q_value: tensor([[-1.9171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6193162214245512, distance: 0.7060555626372291 entropy 0.03264415264129639
epoch: 33, step: 30
	action: tensor([[ 0.2363,  0.1901, -0.4036,  0.0821, -0.1998, -0.0506,  0.3725]],
       dtype=torch.float64)
	q_value: tensor([[-3.1292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6659968338059802, distance: 0.6613509551983087 entropy 0.03264415264129639
epoch: 33, step: 31
	action: tensor([[ 0.6841,  0.2023, -0.6695, -0.0608,  0.2512,  0.2113, -0.2120]],
       dtype=torch.float64)
	q_value: tensor([[-2.8259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8763816453896255, distance: 0.4023445960568243 entropy 0.03264415264129639
epoch: 33, step: 32
	action: tensor([[ 0.6704, -0.0338, -0.1320,  0.0891,  0.1806, -0.3234, -0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-3.3859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6735342445135488, distance: 0.653846057909175 entropy 0.03264415264129639
epoch: 33, step: 33
	action: tensor([[ 0.5128,  0.0835, -0.1788,  0.4221, -0.1046,  0.0032, -0.1831]],
       dtype=torch.float64)
	q_value: tensor([[-2.7687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.873346124147269, distance: 0.40725454104574754 entropy 0.03264415264129639
epoch: 33, step: 34
	action: tensor([[ 0.1479,  0.5932, -0.3324, -0.4423,  0.0472,  0.4886, -0.4343]],
       dtype=torch.float64)
	q_value: tensor([[-2.4979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7582754493913237, distance: 0.5626224713043135 entropy 0.03264415264129639
epoch: 33, step: 35
	action: tensor([[ 0.2772,  0.3344, -0.2651, -0.0091, -0.5265,  0.0519, -0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-3.8786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7514099341081238, distance: 0.5705563957091689 entropy 0.03264415264129639
epoch: 33, step: 36
	action: tensor([[-0.0677,  0.3233, -0.6451,  0.2833,  0.0437,  0.1534,  0.0780]],
       dtype=torch.float64)
	q_value: tensor([[-3.1333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42127272027814766, distance: 0.8705499009853036 entropy 0.03264415264129639
epoch: 33, step: 37
	action: tensor([[ 0.2646,  0.3806, -0.2455, -0.5741, -0.2129,  0.2089, -0.3171]],
       dtype=torch.float64)
	q_value: tensor([[-2.8142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6607609618997761, distance: 0.666514504469536 entropy 0.03264415264129639
epoch: 33, step: 38
	action: tensor([[ 0.3132,  0.0934, -0.2751,  0.2575,  0.2230,  0.0749, -0.5315]],
       dtype=torch.float64)
	q_value: tensor([[-3.6027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7296655737839477, distance: 0.5949868548369351 entropy 0.03264415264129639
epoch: 33, step: 39
	action: tensor([[-0.0993,  0.6159, -0.5044, -0.0374, -0.1525,  0.1430, -0.4531]],
       dtype=torch.float64)
	q_value: tensor([[-2.1921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5330128348555224, distance: 0.7820043438614154 entropy 0.03264415264129639
epoch: 33, step: 40
	action: tensor([[ 0.4005, -0.0552, -0.3527,  0.2506, -0.3198,  0.4497, -0.4653]],
       dtype=torch.float64)
	q_value: tensor([[-3.4422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7082387783566308, distance: 0.6181167004630888 entropy 0.03264415264129639
epoch: 33, step: 41
	action: tensor([[ 0.1908,  0.0174, -0.1004, -0.4386, -0.6123,  0.3380, -0.2753]],
       dtype=torch.float64)
	q_value: tensor([[-2.7585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33779747204950084, distance: 0.9312196046080587 entropy 0.03264415264129639
epoch: 33, step: 42
	action: tensor([[ 0.2778,  0.1731, -0.2858, -0.2473,  0.0376, -0.0793, -0.2448]],
       dtype=torch.float64)
	q_value: tensor([[-3.5312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5902643363789093, distance: 0.7325015983667598 entropy 0.03264415264129639
epoch: 33, step: 43
	action: tensor([[ 0.2616,  0.2151, -0.3348,  0.6253, -0.3795,  0.3090, -0.5423]],
       dtype=torch.float64)
	q_value: tensor([[-2.3839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6906006517660641, distance: 0.6365263556596229 entropy 0.03264415264129639
epoch: 33, step: 44
	action: tensor([[ 0.1773,  0.4289, -0.4638, -0.2030,  0.3001,  0.2812,  0.1226]],
       dtype=torch.float64)
	q_value: tensor([[-3.0793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.728621859375963, distance: 0.5961343189725207 entropy 0.03264415264129639
epoch: 33, step: 45
	action: tensor([[-0.3132,  0.2499, -0.2544, -0.1795, -0.2694,  0.0296, -0.0470]],
       dtype=torch.float64)
	q_value: tensor([[-3.1670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10930262619068609, distance: 1.0799950853411038 entropy 0.03264415264129639
epoch: 33, step: 46
	action: tensor([[-0.0668, -0.0334, -0.1350,  0.0469, -0.1826,  0.1614, -0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-2.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2700353976291897, distance: 0.9777044539516896 entropy 0.03264415264129639
epoch: 33, step: 47
	action: tensor([[ 0.5188, -0.1211, -0.0034, -0.0600,  0.2942,  0.4250, -0.3434]],
       dtype=torch.float64)
	q_value: tensor([[-1.9654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6622883427641693, distance: 0.665012363131668 entropy 0.03264415264129639
epoch: 33, step: 48
	action: tensor([[-0.0772, -0.4253, -0.3413, -0.2988,  0.2746,  0.0818, -0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-2.2473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1867918513396536, distance: 1.2466483631501706 entropy 0.03264415264129639
epoch: 33, step: 49
	action: tensor([[ 0.3415,  0.1025, -0.5777,  0.0865, -0.4648, -0.1569,  0.1559]],
       dtype=torch.float64)
	q_value: tensor([[-1.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6580637252166508, distance: 0.6691589349494069 entropy 0.03264415264129639
epoch: 33, step: 50
	action: tensor([[ 0.4667, -0.0100, -0.5166, -0.2576,  0.0556,  0.3427, -0.2238]],
       dtype=torch.float64)
	q_value: tensor([[-3.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6343332613958068, distance: 0.6919893689274454 entropy 0.03264415264129639
epoch: 33, step: 51
	action: tensor([[ 0.7178,  0.1936, -0.1141, -0.3499, -0.0827, -0.1483, -0.4231]],
       dtype=torch.float64)
	q_value: tensor([[-2.7073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7035451400847814, distance: 0.6230687652931649 entropy 0.03264415264129639
epoch: 33, step: 52
	action: tensor([[ 0.7234,  0.1419, -0.4907, -0.0950,  0.1268,  0.3962, -0.2111]],
       dtype=torch.float64)
	q_value: tensor([[-3.3439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8762159870468701, distance: 0.40261409254265 entropy 0.03264415264129639
epoch: 33, step: 53
	action: tensor([[ 0.3033,  0.6047, -0.3208, -0.0967, -0.2286,  0.2521, -0.3635]],
       dtype=torch.float64)
	q_value: tensor([[-3.2925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8300965460617818, distance: 0.4716912250823785 entropy 0.03264415264129639
epoch: 33, step: 54
	action: tensor([[-0.0005,  0.2276, -0.2874, -0.1345, -0.1875,  0.2928, -0.0550]],
       dtype=torch.float64)
	q_value: tensor([[-3.5544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4644432476342689, distance: 0.8374510721812117 entropy 0.03264415264129639
epoch: 33, step: 55
	action: tensor([[ 0.0540,  0.0797,  0.1113, -0.1352,  0.1824, -0.0757, -0.4613]],
       dtype=torch.float64)
	q_value: tensor([[-2.5701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3399913096173731, distance: 0.9296757873366895 entropy 0.03264415264129639
epoch: 33, step: 56
	action: tensor([[ 0.0480,  0.3627, -0.5899, -0.1718,  0.0511,  0.2125, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-1.9744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5903498351359799, distance: 0.7324251695258882 entropy 0.03264415264129639
epoch: 33, step: 57
	action: tensor([[ 0.3824,  0.1211, -0.7828,  0.0688, -0.2760, -0.1923, -0.2615]],
       dtype=torch.float64)
	q_value: tensor([[-3.0333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7231249490682361, distance: 0.6021415622388957 entropy 0.03264415264129639
epoch: 33, step: 58
	action: tensor([[ 0.1247, -0.2178, -0.2987,  0.8788, -0.0334,  0.2839, -0.4885]],
       dtype=torch.float64)
	q_value: tensor([[-3.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5676477575514407, distance: 0.7524463696623925 entropy 0.03264415264129639
epoch: 33, step: 59
	action: tensor([[0.1626, 0.2842, 0.1060, 0.3192, 0.5074, 0.1088, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-2.6899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7448229991497162, distance: 0.5780660417245329 entropy 0.03264415264129639
epoch: 33, step: 60
	action: tensor([[ 0.1856,  0.0010, -0.2062, -0.0240, -0.0480,  0.1379,  0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-2.2276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46302033849583024, distance: 0.8385628370248487 entropy 0.03264415264129639
epoch: 33, step: 61
	action: tensor([[ 0.0985, -0.1338,  0.0969,  0.1134, -0.1075,  0.2886, -0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-2.0806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4100052685085296, distance: 0.8789835755256755 entropy 0.03264415264129639
epoch: 33, step: 62
	action: tensor([[ 0.4042, -0.0399, -0.5124,  0.4752,  0.1069,  0.0300, -0.2245]],
       dtype=torch.float64)
	q_value: tensor([[-1.8607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6812509677670822, distance: 0.6460723139893267 entropy 0.03264415264129639
epoch: 33, step: 63
	action: tensor([[ 0.4889, -0.0770, -0.3858, -0.0269, -0.1500, -0.1726, -0.3415]],
       dtype=torch.float64)
	q_value: tensor([[-2.3591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5859916493596278, distance: 0.7363109240195218 entropy 0.03264415264129639
epoch: 33, step: 64
	action: tensor([[-0.0944,  0.3420, -0.5067, -0.5312,  0.3104,  0.2158,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[-2.5220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4567214348958778, distance: 0.843466772101138 entropy 0.03264415264129639
epoch: 33, step: 65
	action: tensor([[ 0.4493,  0.0178, -0.4047,  0.2110,  0.5170, -0.3626, -0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-3.5224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6462852711661318, distance: 0.6805863984794418 entropy 0.03264415264129639
epoch: 33, step: 66
	action: tensor([[-0.1527,  0.0355, -0.3510,  0.4744,  0.2664,  0.0523, -0.8348]],
       dtype=torch.float64)
	q_value: tensor([[-2.7521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981701670148972, distance: 0.9586776569609498 entropy 0.03264415264129639
epoch: 33, step: 67
	action: tensor([[ 0.2609, -0.0964, -0.2435,  0.2146,  0.4882,  0.3650, -0.4236]],
       dtype=torch.float64)
	q_value: tensor([[-2.5977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6359130491253743, distance: 0.6904929522613207 entropy 0.03264415264129639
epoch: 33, step: 68
	action: tensor([[ 0.1003, -0.2533, -0.5145,  0.0045,  0.2058, -0.0290,  0.6077]],
       dtype=torch.float64)
	q_value: tensor([[-2.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.182231212836084, distance: 1.0348370241323115 entropy 0.03264415264129639
epoch: 33, step: 69
	action: tensor([[ 0.0101, -0.0428, -0.4043,  0.1754, -0.0754,  0.3827,  0.2791]],
       dtype=torch.float64)
	q_value: tensor([[-2.9335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37620770356671873, distance: 0.9038090731783567 entropy 0.03264415264129639
epoch: 33, step: 70
	action: tensor([[ 0.7231,  0.0494, -0.3980, -0.0117,  0.4520,  0.3919, -0.1738]],
       dtype=torch.float64)
	q_value: tensor([[-2.3245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8461456675938128, distance: 0.44886067510889716 entropy 0.03264415264129639
epoch: 33, step: 71
	action: tensor([[ 0.1155,  0.2520,  0.1142, -0.2831,  0.2949,  0.1879, -0.5988]],
       dtype=torch.float64)
	q_value: tensor([[-2.9604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5247688168255553, distance: 0.7888767529864101 entropy 0.03264415264129639
epoch: 33, step: 72
	action: tensor([[ 0.3014,  0.0197, -0.1289,  0.0474,  0.2752,  0.1601, -0.5927]],
       dtype=torch.float64)
	q_value: tensor([[-2.3356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6450563758944189, distance: 0.6817676388377565 entropy 0.03264415264129639
epoch: 33, step: 73
	action: tensor([[ 0.0072,  0.0548, -0.2861, -0.0098,  0.4998,  0.2222, -0.2187]],
       dtype=torch.float64)
	q_value: tensor([[-2.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42725922942674166, distance: 0.8660355963043096 entropy 0.03264415264129639
epoch: 33, step: 74
	action: tensor([[ 0.4461,  0.2543, -0.5128, -0.2001,  0.1331,  0.3066, -0.9145]],
       dtype=torch.float64)
	q_value: tensor([[-2.0229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8118693887324937, distance: 0.49634817178292895 entropy 0.03264415264129639
epoch: 33, step: 75
	action: tensor([[ 0.3521,  0.5292,  0.2610, -0.1056,  0.0084,  0.2079, -0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-3.4586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8548193954442269, distance: 0.4360246011983313 entropy 0.03264415264129639
epoch: 33, step: 76
	action: tensor([[ 0.3519, -0.0255,  0.1876, -0.1119, -0.0787,  0.0948, -0.1415]],
       dtype=torch.float64)
	q_value: tensor([[-2.9146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5393963915806393, distance: 0.7766410847610524 entropy 0.03264415264129639
epoch: 33, step: 77
	action: tensor([[-0.0249, -0.0453, -0.5451,  0.0864,  0.0947,  0.2002, -0.1451]],
       dtype=torch.float64)
	q_value: tensor([[-2.1810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29786913833870077, distance: 0.9588832328096929 entropy 0.03264415264129639
epoch: 33, step: 78
	action: tensor([[ 0.2078, -0.4229,  0.0691,  0.3938, -0.2592,  0.3131, -0.3228]],
       dtype=torch.float64)
	q_value: tensor([[-2.0891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4659781287562944, distance: 0.8362501629186642 entropy 0.03264415264129639
epoch: 33, step: 79
	action: tensor([[ 0.1954,  0.1892, -0.2438, -0.3486, -0.0390,  0.3792, -0.3890]],
       dtype=torch.float64)
	q_value: tensor([[-2.1322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5852291343595621, distance: 0.7369886758623347 entropy 0.03264415264129639
epoch: 33, step: 80
	action: tensor([[-0.0033,  0.0829, -0.4419,  0.0152,  0.0650, -0.2513, -0.3726]],
       dtype=torch.float64)
	q_value: tensor([[-2.7486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3621763103389124, distance: 0.9139175475615299 entropy 0.03264415264129639
epoch: 33, step: 81
	action: tensor([[ 0.3090, -0.1347, -0.4586, -0.6186, -0.0963, -0.0517, -0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-2.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.257402660406835, distance: 0.9861282213641462 entropy 0.03264415264129639
epoch: 33, step: 82
	action: tensor([[-0.1805,  0.2838, -0.0857,  0.1543,  0.0201, -0.0410, -0.4320]],
       dtype=torch.float64)
	q_value: tensor([[-3.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36301543234466593, distance: 0.9133161739107775 entropy 0.03264415264129639
epoch: 33, step: 83
	action: tensor([[ 0.3254,  0.0048, -0.3872,  0.0201, -0.1836,  0.1906, -0.5441]],
       dtype=torch.float64)
	q_value: tensor([[-2.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6227224035629062, distance: 0.7028897360540876 entropy 0.03264415264129639
epoch: 33, step: 84
	action: tensor([[ 0.2921,  0.1181, -0.4976, -0.2348, -0.3056,  0.0510, -0.4249]],
       dtype=torch.float64)
	q_value: tensor([[-2.4025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6073814228079042, distance: 0.717037906968301 entropy 0.03264415264129639
epoch: 33, step: 85
	action: tensor([[ 0.1185,  0.1294, -0.5360,  0.3076,  0.1042, -0.0398,  0.1456]],
       dtype=torch.float64)
	q_value: tensor([[-2.9388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5418093573780955, distance: 0.7746041172539984 entropy 0.03264415264129639
epoch: 33, step: 86
	action: tensor([[-0.0408,  0.4835, -0.3657,  0.0923, -0.0853,  0.1294, -0.0808]],
       dtype=torch.float64)
	q_value: tensor([[-2.4230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5563387032677541, distance: 0.7622237300410074 entropy 0.03264415264129639
epoch: 33, step: 87
	action: tensor([[ 0.3690, -0.2500, -0.0598, -0.3162,  0.1178, -0.3966, -0.4376]],
       dtype=torch.float64)
	q_value: tensor([[-2.7171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11219486121253763, distance: 1.078240202295661 entropy 0.03264415264129639
epoch: 33, step: 88
	action: tensor([[-0.1269, -0.1477, -0.4469,  0.0854,  0.4529,  0.1480, -0.3934]],
       dtype=torch.float64)
	q_value: tensor([[-2.4804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13439335001456043, distance: 1.0646748259752121 entropy 0.03264415264129639
epoch: 33, step: 89
	action: tensor([[ 0.1563, -0.2135, -0.1342, -0.1925,  0.1342, -0.3473, -0.2425]],
       dtype=torch.float64)
	q_value: tensor([[-2.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10724878478666244, distance: 1.0812395379853312 entropy 0.03264415264129639
epoch: 33, step: 90
	action: tensor([[-0.0328,  0.1828, -0.7280,  0.3609, -0.3923, -0.1092,  0.1734]],
       dtype=torch.float64)
	q_value: tensor([[-2.1078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3768783380962222, distance: 0.9033231032698308 entropy 0.03264415264129639
epoch: 33, step: 91
	action: tensor([[-0.0773,  0.5844, -0.0614,  0.0105,  0.2749,  0.2293, -0.1235]],
       dtype=torch.float64)
	q_value: tensor([[-3.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5730339547578633, distance: 0.7477447331392174 entropy 0.03264415264129639
epoch: 33, step: 92
	action: tensor([[ 0.3022,  0.0864, -0.1575,  0.2867,  0.1757,  0.5754, -0.3033]],
       dtype=torch.float64)
	q_value: tensor([[-2.6360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7856538544645598, distance: 0.5298031454634415 entropy 0.03264415264129639
epoch: 33, step: 93
	action: tensor([[ 0.4592, -0.5500, -0.3005, -0.1592, -0.2571,  0.0039, -0.1827]],
       dtype=torch.float64)
	q_value: tensor([[-2.3459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05028559681889755, distance: 1.1152011420619232 entropy 0.03264415264129639
epoch: 33, step: 94
	action: tensor([[ 0.1051,  0.1993, -0.4331, -0.2289, -0.0856, -0.0785, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-2.3586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5173226844533769, distance: 0.7950329670841941 entropy 0.03264415264129639
epoch: 33, step: 95
	action: tensor([[ 0.2594,  0.4359, -0.2001, -0.1500,  0.4792,  0.1541, -0.2889]],
       dtype=torch.float64)
	q_value: tensor([[-2.6340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7754181099182691, distance: 0.5423055630795173 entropy 0.03264415264129639
epoch: 33, step: 96
	action: tensor([[ 0.1107,  0.3894, -0.6069,  0.1859, -0.4606,  0.1939, -0.5915]],
       dtype=torch.float64)
	q_value: tensor([[-2.3955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5861288447725654, distance: 0.7361889133747377 entropy 0.03264415264129639
epoch: 33, step: 97
	action: tensor([[ 0.2844,  0.0423, -0.3113,  0.1146, -0.4651, -0.5662, -0.0469]],
       dtype=torch.float64)
	q_value: tensor([[-3.5196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6313675721850432, distance: 0.694789843930856 entropy 0.03264415264129639
epoch: 33, step: 98
	action: tensor([[ 0.0147, -0.3783,  0.0610,  0.2866, -0.3226,  0.4988, -0.0728]],
       dtype=torch.float64)
	q_value: tensor([[-3.0968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31965972394259223, distance: 0.9438865208894649 entropy 0.03264415264129639
epoch: 33, step: 99
	action: tensor([[ 0.4079,  0.5239, -0.2824, -0.2433, -0.0187,  0.4991, -0.3360]],
       dtype=torch.float64)
	q_value: tensor([[-2.1421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8740017839272614, distance: 0.4061990388151593 entropy 0.03264415264129639
epoch: 33, step: 100
	action: tensor([[-0.2542,  0.1508, -0.1795, -0.0544, -0.3535,  0.2110,  0.3394]],
       dtype=torch.float64)
	q_value: tensor([[-3.5507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14027196842873457, distance: 1.0610533890938063 entropy 0.03264415264129639
epoch: 33, step: 101
	action: tensor([[-0.1483, -0.1746, -0.2466, -0.3369, -0.2281,  0.3806,  0.1929]],
       dtype=torch.float64)
	q_value: tensor([[-2.8611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04259825898723846, distance: 1.168463608521059 entropy 0.03264415264129639
epoch: 33, step: 102
	action: tensor([[ 0.6099,  0.2268, -0.6532, -0.4061, -0.2160, -0.0452, -0.2312]],
       dtype=torch.float64)
	q_value: tensor([[-2.6921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7411372875114866, distance: 0.5822257943451367 entropy 0.03264415264129639
epoch: 33, step: 103
	action: tensor([[ 0.6659,  0.1748, -0.4637, -0.0626,  0.1799,  0.0185, -0.5006]],
       dtype=torch.float64)
	q_value: tensor([[-3.9451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8265118376745068, distance: 0.47664124104753125 entropy 0.03264415264129639
epoch: 33, step: 104
	action: tensor([[ 0.1547,  0.1413,  0.2924,  0.0335,  0.1062,  0.2194, -0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-3.0801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.628197151223937, distance: 0.6977712152591348 entropy 0.03264415264129639
epoch: 33, step: 105
	action: tensor([[ 0.4695,  0.2163, -0.3938,  0.0106, -0.0939,  0.0167, -0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-2.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8039703837700911, distance: 0.5066610725159421 entropy 0.03264415264129639
epoch: 33, step: 106
	action: tensor([[ 0.5780,  0.1337, -0.1834, -0.1179,  0.3698,  0.4698,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-2.6605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8484727003835216, distance: 0.4454532533315391 entropy 0.03264415264129639
epoch: 33, step: 107
	action: tensor([[ 0.6651,  0.0465, -0.3311, -0.1891,  0.0527, -0.1797, -0.1947]],
       dtype=torch.float64)
	q_value: tensor([[-2.7424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.643393154379505, distance: 0.68336311100156 entropy 0.03264415264129639
epoch: 33, step: 108
	action: tensor([[ 0.0069,  0.4642, -0.3438,  0.0447,  0.4020,  0.0697, -0.2306]],
       dtype=torch.float64)
	q_value: tensor([[-2.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6050776190178805, distance: 0.719138544272079 entropy 0.03264415264129639
epoch: 33, step: 109
	action: tensor([[-0.2047, -0.3451, -0.4431, -0.3654, -0.6967,  0.1035, -0.0392]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2586485519313546, distance: 1.2838341789043823 entropy 0.03264415264129639
epoch: 33, step: 110
	action: tensor([[ 0.2518,  0.0012, -0.1373, -0.3467, -0.3510,  0.0388, -0.1481]],
       dtype=torch.float64)
	q_value: tensor([[-3.3609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3763947247225977, distance: 0.9036735760826985 entropy 0.03264415264129639
epoch: 33, step: 111
	action: tensor([[ 0.1643,  0.2283, -0.0592,  0.1603, -0.1348,  0.1381, -0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-2.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6612491758209741, distance: 0.6660347265216774 entropy 0.03264415264129639
epoch: 33, step: 112
	action: tensor([[ 0.2427,  0.2050, -0.1720,  0.0657,  0.1523,  0.0589, -0.0449]],
       dtype=torch.float64)
	q_value: tensor([[-2.2052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6787736745041699, distance: 0.6485780672522675 entropy 0.03264415264129639
epoch: 33, step: 113
	action: tensor([[ 0.4496,  0.1187, -0.2805,  0.0075,  0.2768, -0.2248, -0.1774]],
       dtype=torch.float64)
	q_value: tensor([[-2.0600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6821747492061374, distance: 0.6451354285336215 entropy 0.03264415264129639
epoch: 33, step: 114
	action: tensor([[ 0.1440,  0.3583, -0.6292,  0.0064, -0.4009,  0.1317, -0.2412]],
       dtype=torch.float64)
	q_value: tensor([[-2.4303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6328746907077281, distance: 0.6933680982690897 entropy 0.03264415264129639
epoch: 33, step: 115
	action: tensor([[ 0.4251,  0.3698, -0.3182,  0.0442, -0.1969, -0.1821, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-3.3102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.854797168321323, distance: 0.43605797756259673 entropy 0.03264415264129639
epoch: 33, step: 116
	action: tensor([[ 0.2322, -0.2057, -0.3516, -0.1086,  0.0607, -0.4386, -0.6493]],
       dtype=torch.float64)
	q_value: tensor([[-2.9582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2398818644997387, distance: 0.9976937244012362 entropy 0.03264415264129639
epoch: 33, step: 117
	action: tensor([[ 0.0696,  0.0335, -0.3171,  0.1039,  0.0195,  0.4093, -0.3592]],
       dtype=torch.float64)
	q_value: tensor([[-2.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47023198215269824, distance: 0.8329128482847007 entropy 0.03264415264129639
epoch: 33, step: 118
	action: tensor([[ 0.5350,  0.2678,  0.1280,  0.3520,  0.0345,  0.1460, -0.0628]],
       dtype=torch.float64)
	q_value: tensor([[-2.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9376165327468656, distance: 0.2858192319049373 entropy 0.03264415264129639
epoch: 33, step: 119
	action: tensor([[ 0.2796, -0.1647, -0.2063,  0.2643,  0.1695,  0.0954,  0.2024]],
       dtype=torch.float64)
	q_value: tensor([[-2.5933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5430154381772794, distance: 0.7735839624693422 entropy 0.03264415264129639
epoch: 33, step: 120
	action: tensor([[ 0.1077,  0.5492, -0.1660, -0.0384, -0.3263, -0.1251, -0.6637]],
       dtype=torch.float64)
	q_value: tensor([[-2.0365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.684457200165386, distance: 0.6428147457553138 entropy 0.03264415264129639
epoch: 33, step: 121
	action: tensor([[ 0.3358,  0.2114, -0.5879, -0.1239, -0.1965, -0.0461, -0.2917]],
       dtype=torch.float64)
	q_value: tensor([[-3.4374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7218272080638839, distance: 0.6035510613611401 entropy 0.03264415264129639
epoch: 33, step: 122
	action: tensor([[ 0.5124,  0.2191, -0.5147,  0.1435,  0.5904,  0.3540, -0.4779]],
       dtype=torch.float64)
	q_value: tensor([[-2.9840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8846266342164097, distance: 0.3886954657702619 entropy 0.03264415264129639
epoch: 33, step: 123
	action: tensor([[ 0.2455,  0.1750, -0.4050, -0.1512,  0.2968,  0.0382,  0.2754]],
       dtype=torch.float64)
	q_value: tensor([[-2.9107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6215499133760779, distance: 0.7039810969208463 entropy 0.03264415264129639
epoch: 33, step: 124
	action: tensor([[ 0.3054,  0.1130, -0.1759, -0.3613,  0.0470,  0.5522, -0.0995]],
       dtype=torch.float64)
	q_value: tensor([[-2.7451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.640080438277947, distance: 0.6865298405470011 entropy 0.03264415264129639
epoch: 33, step: 125
	action: tensor([[ 0.5044,  0.3255, -0.2817,  0.0284,  0.3860,  0.3861, -0.5767]],
       dtype=torch.float64)
	q_value: tensor([[-2.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9111716526286738, distance: 0.3410613324319745 entropy 0.03264415264129639
epoch: 33, step: 126
	action: tensor([[ 0.3967,  0.0732, -0.2441,  0.2849,  0.1949,  0.2938, -0.1518]],
       dtype=torch.float64)
	q_value: tensor([[-2.8100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8132363718485185, distance: 0.494541616799193 entropy 0.03264415264129639
epoch: 33, step: 127
	action: tensor([[ 0.1454,  0.1038, -0.0417,  0.2874, -0.0059,  0.1178, -0.1461]],
       dtype=torch.float64)
	q_value: tensor([[-2.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6539769230290854, distance: 0.673145931308602 entropy 0.03264415264129639
LOSS epoch 33 actor 5.681170791687566 critic 81.93327221345857 
epoch: 34, step: 0
	action: tensor([[ 0.3486,  0.1115, -0.3855, -0.0492, -0.2427,  0.2006, -0.5989]],
       dtype=torch.float64)
	q_value: tensor([[-1.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6856510036109745, distance: 0.6415976023722172 entropy 0.03264415264129639
epoch: 34, step: 1
	action: tensor([[ 0.4738,  0.3195, -0.4736,  0.0044,  0.1938, -0.1264, -0.3264]],
       dtype=torch.float64)
	q_value: tensor([[-2.4236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8475074100160738, distance: 0.4468698597873686 entropy 0.03264415264129639
epoch: 34, step: 2
	action: tensor([[ 0.2668,  0.0011, -0.3233,  0.0325, -0.4239,  0.0396, -0.2315]],
       dtype=torch.float64)
	q_value: tensor([[-2.5788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5670272350833988, distance: 0.7529861407362459 entropy 0.03264415264129639
epoch: 34, step: 3
	action: tensor([[ 0.0953,  0.0962, -0.3526, -0.3341, -0.2749,  0.0310, -0.2091]],
       dtype=torch.float64)
	q_value: tensor([[-2.1476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4020932756917591, distance: 0.8848576544951665 entropy 0.03264415264129639
epoch: 34, step: 4
	action: tensor([[ 0.4895,  0.1050, -0.2285,  0.1218, -0.1239, -0.1825, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-2.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7554394110212981, distance: 0.565913336898389 entropy 0.03264415264129639
epoch: 34, step: 5
	action: tensor([[ 0.3936,  0.1473, -0.1736,  0.1209, -0.4618, -0.2035, -0.4094]],
       dtype=torch.float64)
	q_value: tensor([[-2.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7612402411716074, distance: 0.5591614971564666 entropy 0.03264415264129639
epoch: 34, step: 6
	action: tensor([[ 0.2066,  0.0567,  0.1395, -0.2645,  0.1048,  0.0565, -0.4271]],
       dtype=torch.float64)
	q_value: tensor([[-2.5385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4223199537083926, distance: 0.8697618944482082 entropy 0.03264415264129639
epoch: 34, step: 7
	action: tensor([[ 0.3628,  0.2474, -0.2217,  0.2315,  0.2546, -0.1569, -0.6287]],
       dtype=torch.float64)
	q_value: tensor([[-1.8220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7845656271971869, distance: 0.5311463380449432 entropy 0.03264415264129639
epoch: 34, step: 8
	action: tensor([[ 0.2591,  0.4362, -0.6898, -0.4444,  0.2649,  0.3949, -0.2916]],
       dtype=torch.float64)
	q_value: tensor([[-2.2767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8191969194110448, distance: 0.4865859965320671 entropy 0.03264415264129639
epoch: 34, step: 9
	action: tensor([[ 0.3054, -0.4246, -0.4695, -0.2481,  0.0953,  0.8620, -0.2835]],
       dtype=torch.float64)
	q_value: tensor([[-3.4689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.343852222680636, distance: 0.9269525943236943 entropy 0.03264415264129639
epoch: 34, step: 10
	action: tensor([[ 0.4732,  0.1199, -0.7276, -0.4370, -0.0641,  0.4447, -0.5320]],
       dtype=torch.float64)
	q_value: tensor([[-2.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7273959328884001, distance: 0.5974792934776952 entropy 0.03264415264129639
epoch: 34, step: 11
	action: tensor([[ 0.6386,  0.0180, -0.6901,  0.0913,  0.0202, -0.2469, -0.0400]],
       dtype=torch.float64)
	q_value: tensor([[-3.4944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7425567482656861, distance: 0.5806272969816858 entropy 0.03264415264129639
epoch: 34, step: 12
	action: tensor([[ 0.2370,  0.2630, -0.3857,  0.0707, -0.3586,  0.3578, -0.4189]],
       dtype=torch.float64)
	q_value: tensor([[-2.8683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6882391609031373, distance: 0.6389508822052664 entropy 0.03264415264129639
epoch: 34, step: 13
	action: tensor([[ 0.4076,  0.2189, -0.0935,  0.4924, -0.0354,  0.0558,  0.2813]],
       dtype=torch.float64)
	q_value: tensor([[-2.6549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8719315311400516, distance: 0.40952253219152857 entropy 0.03264415264129639
epoch: 34, step: 14
	action: tensor([[ 0.3578, -0.4056, -0.8512,  0.1541,  0.0196, -0.1702, -0.4834]],
       dtype=torch.float64)
	q_value: tensor([[-2.2168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26490603426257686, distance: 0.9811335396056173 entropy 0.03264415264129639
epoch: 34, step: 15
	action: tensor([[ 0.1111,  0.2779, -0.1925,  0.1906,  0.0816, -0.0660, -0.4199]],
       dtype=torch.float64)
	q_value: tensor([[-2.3203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6162917565643231, distance: 0.7088547567464026 entropy 0.03264415264129639
epoch: 34, step: 16
	action: tensor([[ 0.3204,  0.0196, -0.1832, -0.1774,  0.0442,  0.0310, -0.5002]],
       dtype=torch.float64)
	q_value: tensor([[-1.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.540072003845794, distance: 0.7760712881025668 entropy 0.03264415264129639
epoch: 34, step: 17
	action: tensor([[ 0.2476, -0.1569, -0.1011,  0.1343, -0.2889,  0.1797, -0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-1.8610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5119018690974082, distance: 0.7994848999956992 entropy 0.03264415264129639
epoch: 34, step: 18
	action: tensor([[ 0.4218,  0.3360, -0.5865, -0.0078, -0.0312,  0.2435, -0.5968]],
       dtype=torch.float64)
	q_value: tensor([[-1.7522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8345120429100477, distance: 0.4655216576414128 entropy 0.03264415264129639
epoch: 34, step: 19
	action: tensor([[ 0.3320, -0.2735, -0.1311,  0.2035, -0.0465,  0.3527, -0.4487]],
       dtype=torch.float64)
	q_value: tensor([[-2.8529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5672721662855517, distance: 0.7527731297736553 entropy 0.03264415264129639
epoch: 34, step: 20
	action: tensor([[ 0.4080,  0.1315, -0.6523,  0.4531, -0.2545,  0.0394, -0.7809]],
       dtype=torch.float64)
	q_value: tensor([[-1.8104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7383074830870047, distance: 0.5853994975773732 entropy 0.03264415264129639
epoch: 34, step: 21
	action: tensor([[ 0.4935, -0.1503, -0.1918,  0.4221, -0.1858, -0.1425, -0.3052]],
       dtype=torch.float64)
	q_value: tensor([[-3.0130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7306526293325213, distance: 0.5938996419631345 entropy 0.03264415264129639
epoch: 34, step: 22
	action: tensor([[ 0.2497,  0.2408, -0.4344,  0.3149,  0.1584, -0.1964, -0.9593]],
       dtype=torch.float64)
	q_value: tensor([[-2.0992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7007332349691214, distance: 0.6260167273137084 entropy 0.03264415264129639
epoch: 34, step: 23
	action: tensor([[ 0.2622, -0.1347,  0.2009,  0.1489,  0.1525,  0.0809, -0.6662]],
       dtype=torch.float64)
	q_value: tensor([[-2.9029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5560179769761794, distance: 0.7624991890753989 entropy 0.03264415264129639
epoch: 34, step: 24
	action: tensor([[ 0.1106, -0.2845, -0.1075,  0.1175,  0.1633,  0.2304, -0.1963]],
       dtype=torch.float64)
	q_value: tensor([[-1.8260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3208224638523858, distance: 0.9430795983185527 entropy 0.03264415264129639
epoch: 34, step: 25
	action: tensor([[ 0.6328,  0.0535, -0.5964, -0.0627, -0.0693,  0.0672, -0.5901]],
       dtype=torch.float64)
	q_value: tensor([[-1.4074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7551309658387395, distance: 0.5662700956065796 entropy 0.03264415264129639
epoch: 34, step: 26
	action: tensor([[ 0.1113,  0.2506,  0.0619,  0.1270,  0.4734,  0.2933, -0.6394]],
       dtype=torch.float64)
	q_value: tensor([[-2.7817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7007299541882243, distance: 0.6260201587306438 entropy 0.03264415264129639
epoch: 34, step: 27
	action: tensor([[ 0.5421, -0.0381,  0.2057, -0.0344,  0.1010,  0.0303, -0.7862]],
       dtype=torch.float64)
	q_value: tensor([[-2.0154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6597164258172985, distance: 0.6675398336418847 entropy 0.03264415264129639
epoch: 34, step: 28
	action: tensor([[ 0.4110,  0.0793, -0.6587,  0.2201, -0.0340,  0.4935, -0.3871]],
       dtype=torch.float64)
	q_value: tensor([[-2.4773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7374777048286326, distance: 0.5863268594550212 entropy 0.03264415264129639
epoch: 34, step: 29
	action: tensor([[ 0.5742,  0.3608, -0.3775,  0.1361,  0.0616,  0.2952, -0.5406]],
       dtype=torch.float64)
	q_value: tensor([[-2.5834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9301543031709695, distance: 0.302431149761346 entropy 0.03264415264129639
epoch: 34, step: 30
	action: tensor([[ 0.2892,  0.0846, -0.7606, -0.0515,  0.0101,  0.4556, -0.5185]],
       dtype=torch.float64)
	q_value: tensor([[-2.7495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6405904814977886, distance: 0.6860432262186783 entropy 0.03264415264129639
epoch: 34, step: 31
	action: tensor([[ 0.2657,  0.4591, -0.6748,  0.2595, -0.1138,  0.0705, -0.5080]],
       dtype=torch.float64)
	q_value: tensor([[-2.7470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7312894372558445, distance: 0.5931971591388182 entropy 0.03264415264129639
epoch: 34, step: 32
	action: tensor([[ 0.2205,  0.3963, -0.8107,  0.4202,  0.0156,  0.1864, -0.6074]],
       dtype=torch.float64)
	q_value: tensor([[-2.9040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6134320118111114, distance: 0.7114913702197803 entropy 0.03264415264129639
epoch: 34, step: 33
	action: tensor([[ 0.3000,  0.2267, -0.6181,  0.0650,  0.1885, -0.1281, -0.3299]],
       dtype=torch.float64)
	q_value: tensor([[-2.9985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.714683841990418, distance: 0.6112514141930895 entropy 0.03264415264129639
epoch: 34, step: 34
	action: tensor([[ 0.5889,  0.1717, -0.2700,  0.0183, -0.0611,  0.2066, -0.3911]],
       dtype=torch.float64)
	q_value: tensor([[-2.3943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.859639707956551, distance: 0.4287250156317353 entropy 0.03264415264129639
epoch: 34, step: 35
	action: tensor([[ 0.3911,  0.5443, -0.3767,  0.0649, -0.2107,  0.0440, -0.0998]],
       dtype=torch.float64)
	q_value: tensor([[-2.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8747096537841799, distance: 0.405056399442696 entropy 0.03264415264129639
epoch: 34, step: 36
	action: tensor([[ 0.3748,  0.0300, -0.5996,  0.0740,  0.0063,  0.3871, -0.4692]],
       dtype=torch.float64)
	q_value: tensor([[-2.8652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6925612603654402, distance: 0.6345063733970774 entropy 0.03264415264129639
epoch: 34, step: 37
	action: tensor([[ 0.5647, -0.1993, -0.0180,  0.2209, -0.1452,  0.2575, -0.6018]],
       dtype=torch.float64)
	q_value: tensor([[-2.2522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7344369276052338, distance: 0.5897127743035246 entropy 0.03264415264129639
epoch: 34, step: 38
	action: tensor([[ 0.3026, -0.0978, -0.5118,  0.3007, -0.2527,  0.1274,  0.1233]],
       dtype=torch.float64)
	q_value: tensor([[-2.3812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5784020887805004, distance: 0.7430292637745329 entropy 0.03264415264129639
epoch: 34, step: 39
	action: tensor([[ 0.4277,  0.0436, -0.5779,  0.0859, -0.0108,  0.4917, -0.1581]],
       dtype=torch.float64)
	q_value: tensor([[-1.9355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7440404428237941, distance: 0.5789517464405212 entropy 0.03264415264129639
epoch: 34, step: 40
	action: tensor([[ 8.5015e-01,  2.5890e-02, -3.5078e-01,  1.9378e-01, -8.4754e-04,
          5.5843e-01, -4.6535e-01]], dtype=torch.float64)
	q_value: tensor([[-2.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9320137097004156, distance: 0.2983783894561984 entropy 0.03264415264129639
epoch: 34, step: 41
	action: tensor([[ 0.3514,  0.2577, -0.8412, -0.1022, -0.1363,  0.2665, -0.4642]],
       dtype=torch.float64)
	q_value: tensor([[-3.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7564710687797547, distance: 0.5647184471225908 entropy 0.03264415264129639
epoch: 34, step: 42
	action: tensor([[ 0.6800,  0.2231, -0.4651,  0.0326, -0.0796,  0.4964, -0.3109]],
       dtype=torch.float64)
	q_value: tensor([[-3.2305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9245880367431814, distance: 0.3142511074802551 entropy 0.03264415264129639
epoch: 34, step: 43
	action: tensor([[ 0.2858, -0.1865, -0.4007,  0.0587, -0.4812,  0.5838, -0.1987]],
       dtype=torch.float64)
	q_value: tensor([[-3.1218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46598145735451835, distance: 0.8362475567097292 entropy 0.03264415264129639
epoch: 34, step: 44
	action: tensor([[ 0.5358,  0.0821, -0.0220, -0.0827,  0.3815,  0.6352, -0.4808]],
       dtype=torch.float64)
	q_value: tensor([[-2.5914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8432572146512947, distance: 0.4530545259135725 entropy 0.03264415264129639
epoch: 34, step: 45
	action: tensor([[ 0.6214,  0.1739, -0.2418, -0.0147, -0.0928,  0.2386, -0.4597]],
       dtype=torch.float64)
	q_value: tensor([[-2.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8629484592983672, distance: 0.4236416533956527 entropy 0.03264415264129639
epoch: 34, step: 46
	action: tensor([[ 0.2738,  0.3430, -0.1966, -0.2493, -0.0059,  0.1854, -0.2235]],
       dtype=torch.float64)
	q_value: tensor([[-2.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.725278827772525, distance: 0.5997948862827636 entropy 0.03264415264129639
epoch: 34, step: 47
	action: tensor([[ 0.6071,  0.0488, -0.4704, -0.0360,  0.2583, -0.0617, -0.3657]],
       dtype=torch.float64)
	q_value: tensor([[-2.2470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7363285481091613, distance: 0.5876087424127571 entropy 0.03264415264129639
epoch: 34, step: 48
	action: tensor([[ 0.2536,  0.2198, -0.2120, -0.1710, -0.3713,  0.0830, -0.2100]],
       dtype=torch.float64)
	q_value: tensor([[-2.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6428816535888672, distance: 0.6838530280128142 entropy 0.03264415264129639
epoch: 34, step: 49
	action: tensor([[ 0.4007,  0.0193, -0.0011,  0.2709,  0.2407, -0.0205, -0.1071]],
       dtype=torch.float64)
	q_value: tensor([[-2.4482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7502705962306844, distance: 0.5718623879213429 entropy 0.03264415264129639
epoch: 34, step: 50
	action: tensor([[ 0.7076,  0.3471,  0.0392,  0.0227, -0.0913,  0.3076, -0.4182]],
       dtype=torch.float64)
	q_value: tensor([[-1.7852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9588805788001624, distance: 0.23204926101628343 entropy 0.03264415264129639
epoch: 34, step: 51
	action: tensor([[ 0.6104,  0.1840, -0.3440,  0.4389,  0.0366,  0.3837,  0.1675]],
       dtype=torch.float64)
	q_value: tensor([[-2.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9309192272536405, distance: 0.3007705339227807 entropy 0.03264415264129639
epoch: 34, step: 52
	action: tensor([[ 0.0220, -0.4703, -0.3775,  0.1920, -0.0507,  0.1666, -0.3311]],
       dtype=torch.float64)
	q_value: tensor([[-2.5602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04630025772965063, distance: 1.1175385825696238 entropy 0.03264415264129639
epoch: 34, step: 53
	action: tensor([[ 5.7568e-01,  3.2528e-01, -3.1072e-01,  4.1122e-04, -1.5169e-01,
          3.9271e-01, -4.7474e-01]], dtype=torch.float64)
	q_value: tensor([[-1.5451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8962794137632496, distance: 0.36854385531145106 entropy 0.03264415264129639
epoch: 34, step: 54
	action: tensor([[ 0.6448,  0.4470, -0.1956, -0.2263,  0.3417, -0.0183, -0.1813]],
       dtype=torch.float64)
	q_value: tensor([[-2.9414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9025067180403512, distance: 0.35730906831983095 entropy 0.03264415264129639
epoch: 34, step: 55
	action: tensor([[ 0.5030, -0.1926, -0.1518,  0.3455, -0.0064,  0.2719,  0.1095]],
       dtype=torch.float64)
	q_value: tensor([[-2.8682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7482981177197452, distance: 0.5741163629598703 entropy 0.03264415264129639
epoch: 34, step: 56
	action: tensor([[ 0.5365,  0.0007, -0.6098,  0.3770,  0.0060, -0.1255, -0.4602]],
       dtype=torch.float64)
	q_value: tensor([[-1.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7704556483539333, distance: 0.5482643387376687 entropy 0.03264415264129639
epoch: 34, step: 57
	action: tensor([[ 0.4823,  0.0991, -0.3360, -0.0814, -0.1674, -0.3071, -0.3485]],
       dtype=torch.float64)
	q_value: tensor([[-2.5155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6864667175014096, distance: 0.6407646109449066 entropy 0.03264415264129639
epoch: 34, step: 58
	action: tensor([[ 0.0817, -0.2581, -0.1971,  0.1774, -0.1900,  0.1048, -0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-2.5097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2892748973619198, distance: 0.9647338581298269 entropy 0.03264415264129639
epoch: 34, step: 59
	action: tensor([[ 0.3289, -0.1169, -0.1553,  0.2242,  0.0728,  0.2616, -0.2725]],
       dtype=torch.float64)
	q_value: tensor([[-1.5318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6436509633485259, distance: 0.6831160476114212 entropy 0.03264415264129639
epoch: 34, step: 60
	action: tensor([[ 0.5487,  0.2572, -0.2732,  0.3084, -0.3893,  0.0284,  0.1703]],
       dtype=torch.float64)
	q_value: tensor([[-1.6477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.898333929566135, distance: 0.3648755075429049 entropy 0.03264415264129639
epoch: 34, step: 61
	action: tensor([[ 0.3917,  0.1592, -0.5796, -0.3306,  0.2128,  0.3212, -0.6181]],
       dtype=torch.float64)
	q_value: tensor([[-2.6383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7318602988996161, distance: 0.592566715836719 entropy 0.03264415264129639
epoch: 34, step: 62
	action: tensor([[ 0.5956, -0.1099, -0.1792, -0.2086, -0.5105, -0.0297, -0.4438]],
       dtype=torch.float64)
	q_value: tensor([[-2.6249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5268847891107044, distance: 0.7871185523126737 entropy 0.03264415264129639
epoch: 34, step: 63
	action: tensor([[ 0.3958,  0.1609, -0.2204,  0.3742, -0.0311, -0.3030,  0.1836]],
       dtype=torch.float64)
	q_value: tensor([[-2.7408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8003697135693985, distance: 0.5112930721462298 entropy 0.03264415264129639
epoch: 34, step: 64
	action: tensor([[ 0.1759,  0.1129, -0.4370, -0.0303, -0.5284,  0.1983, -0.7699]],
       dtype=torch.float64)
	q_value: tensor([[-2.3431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5334695336892643, distance: 0.7816218624365923 entropy 0.03264415264129639
epoch: 34, step: 65
	action: tensor([[ 0.3877,  0.2231, -0.2616, -0.3306, -0.0971,  0.6698,  0.1050]],
       dtype=torch.float64)
	q_value: tensor([[-3.0703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7600653440802896, distance: 0.5605355792209799 entropy 0.03264415264129639
epoch: 34, step: 66
	action: tensor([[ 0.2123,  0.2524, -0.1681,  0.0299, -0.0850,  0.1597, -0.1719]],
       dtype=torch.float64)
	q_value: tensor([[-3.0641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6946951647754178, distance: 0.6323005134666503 entropy 0.03264415264129639
epoch: 34, step: 67
	action: tensor([[ 0.1343,  0.3565, -0.2439,  0.1477,  0.2010,  0.2471, -0.1828]],
       dtype=torch.float64)
	q_value: tensor([[-1.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7046641073100094, distance: 0.6218917688012934 entropy 0.03264415264129639
epoch: 34, step: 68
	action: tensor([[ 0.7786,  0.1197, -0.5150,  0.1988, -0.0101,  0.0947, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-1.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8991701269409793, distance: 0.363371869665955 entropy 0.03264415264129639
epoch: 34, step: 69
	action: tensor([[ 0.7183,  0.1709, -0.2253, -0.2473,  0.2449, -0.0957, -0.3478]],
       dtype=torch.float64)
	q_value: tensor([[-2.8372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7430136241936127, distance: 0.5801118582755727 entropy 0.03264415264129639
epoch: 34, step: 70
	action: tensor([[ 0.1449, -0.1144, -0.3982, -0.1136, -0.3393,  0.3165, -0.3219]],
       dtype=torch.float64)
	q_value: tensor([[-2.7350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3532336254988332, distance: 0.9203020928594337 entropy 0.03264415264129639
epoch: 34, step: 71
	action: tensor([[ 0.0953,  0.2244, -0.3187, -0.3194,  0.1037,  0.4249, -0.1411]],
       dtype=torch.float64)
	q_value: tensor([[-2.1749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5516998830708639, distance: 0.7661981858226524 entropy 0.03264415264129639
epoch: 34, step: 72
	action: tensor([[ 0.2964,  0.2021, -0.1836, -0.0868, -0.1349, -0.1788, -0.4438]],
       dtype=torch.float64)
	q_value: tensor([[-2.3509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6593074726608747, distance: 0.6679408382323007 entropy 0.03264415264129639
epoch: 34, step: 73
	action: tensor([[ 0.1685,  0.3868, -0.3662, -0.5468, -0.3804,  0.3263, -0.4309]],
       dtype=torch.float64)
	q_value: tensor([[-2.2032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6215191238628506, distance: 0.7040097331865286 entropy 0.03264415264129639
epoch: 34, step: 74
	action: tensor([[ 0.5594,  0.3773, -0.3869, -0.1165,  0.0455,  0.1642, -0.7760]],
       dtype=torch.float64)
	q_value: tensor([[-3.6457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9022360742166378, distance: 0.35780467412317607 entropy 0.03264415264129639
epoch: 34, step: 75
	action: tensor([[ 0.3841,  0.2007, -0.1155, -0.1809, -0.3564, -0.0561, -0.2691]],
       dtype=torch.float64)
	q_value: tensor([[-3.1171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.692103581610297, distance: 0.6349784871089387 entropy 0.03264415264129639
epoch: 34, step: 76
	action: tensor([[ 0.0648,  0.0556, -0.2109, -0.1172,  0.0258,  0.5789, -0.4108]],
       dtype=torch.float64)
	q_value: tensor([[-2.4548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5028254298550315, distance: 0.8068840798225472 entropy 0.03264415264129639
epoch: 34, step: 77
	action: tensor([[ 0.2969, -0.1087,  0.1840,  0.2660,  0.0719, -0.3014, -0.2696]],
       dtype=torch.float64)
	q_value: tensor([[-2.1229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5514323735102662, distance: 0.7664267545478001 entropy 0.03264415264129639
epoch: 34, step: 78
	action: tensor([[ 0.6452,  0.5494,  0.3269,  0.0074, -0.2495,  0.5473, -0.5051]],
       dtype=torch.float64)
	q_value: tensor([[-1.8754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9669778479227252, distance: 0.20795021792884957 entropy 0.03264415264129639
epoch: 34, step: 79
	action: tensor([[ 0.3862,  0.0789, -0.1312, -0.2126, -0.3396,  0.2613, -0.4597]],
       dtype=torch.float64)
	q_value: tensor([[-3.6445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6386253498875999, distance: 0.6879161975113836 entropy 0.03264415264129639
epoch: 34, step: 80
	action: tensor([[ 0.6409, -0.2364, -0.0715, -0.0303, -0.2088, -0.2660, -0.2594]],
       dtype=torch.float64)
	q_value: tensor([[-2.5853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4453536755461659, distance: 0.8522455906237961 entropy 0.03264415264129639
epoch: 34, step: 81
	action: tensor([[ 0.2153,  0.0617, -0.4470,  0.5052,  0.1756, -0.1694, -0.4335]],
       dtype=torch.float64)
	q_value: tensor([[-2.3765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6177989387512414, distance: 0.7074612182021044 entropy 0.03264415264129639
epoch: 34, step: 82
	action: tensor([[ 0.5414,  0.6211, -0.0608, -0.1743, -0.0133,  0.0653, -0.1634]],
       dtype=torch.float64)
	q_value: tensor([[-2.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9310981137292988, distance: 0.30038085488907423 entropy 0.03264415264129639
epoch: 34, step: 83
	action: tensor([[ 0.0524,  0.4374, -0.5268,  0.0938, -0.1396,  0.2406, -0.3849]],
       dtype=torch.float64)
	q_value: tensor([[-3.0034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5946057377789172, distance: 0.7286106115575158 entropy 0.03264415264129639
epoch: 34, step: 84
	action: tensor([[ 0.4722,  0.0016, -0.7615, -0.3745,  0.2232,  0.0390, -0.9532]],
       dtype=torch.float64)
	q_value: tensor([[-2.5904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6103193536798712, distance: 0.7143501030399163 entropy 0.03264415264129639
epoch: 34, step: 85
	action: tensor([[ 0.9678,  0.2494, -0.3318, -0.0930, -0.4573,  0.0648, -0.3928]],
       dtype=torch.float64)
	q_value: tensor([[-3.1276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8706734976261947, distance: 0.4115290137578463 entropy 0.03264415264129639
epoch: 34, step: 86
	action: tensor([[ 0.2578,  0.6920, -0.2512, -0.3218,  0.3068, -0.0771, -0.7640]],
       dtype=torch.float64)
	q_value: tensor([[-3.9306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8230625303727364, distance: 0.4813562325084918 entropy 0.03264415264129639
epoch: 34, step: 87
	action: tensor([[ 0.0998, -0.0709, -0.1444, -0.2548, -0.4249, -0.1036, -0.4245]],
       dtype=torch.float64)
	q_value: tensor([[-3.2574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24622626053922003, distance: 0.9935213285476391 entropy 0.03264415264129639
epoch: 34, step: 88
	action: tensor([[ 0.0154,  0.0989, -0.1214,  0.0143, -0.3408,  0.5354, -0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-2.2384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44079184727977927, distance: 0.8557431678341229 entropy 0.03264415264129639
epoch: 34, step: 89
	action: tensor([[ 0.5915, -0.0661, -0.7540, -0.4133, -0.5739,  0.1342, -0.0133]],
       dtype=torch.float64)
	q_value: tensor([[-2.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5555044063133738, distance: 0.7629400673264011 entropy 0.03264415264129639
epoch: 34, step: 90
	action: tensor([[ 0.2307,  0.2551, -0.2011, -0.1450,  0.2740,  0.1835, -0.5051]],
       dtype=torch.float64)
	q_value: tensor([[-3.8573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6856765168121775, distance: 0.6415715651634751 entropy 0.03264415264129639
epoch: 34, step: 91
	action: tensor([[ 0.3961, -0.0210, -0.2271, -0.4363,  0.0367, -0.2078, -0.3123]],
       dtype=torch.float64)
	q_value: tensor([[-1.9584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3802668051640664, distance: 0.9008636693341555 entropy 0.03264415264129639
epoch: 34, step: 92
	action: tensor([[ 0.4575, -0.0452,  0.0217,  0.0528, -0.0563, -0.0179, -0.2462]],
       dtype=torch.float64)
	q_value: tensor([[-2.2496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6484484953389811, distance: 0.6785020650797134 entropy 0.03264415264129639
epoch: 34, step: 93
	action: tensor([[ 0.1707,  0.3471, -0.7343,  0.0038, -0.2156, -0.0574, -0.3834]],
       dtype=torch.float64)
	q_value: tensor([[-1.8914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6834811724355874, distance: 0.6438081460143559 entropy 0.03264415264129639
epoch: 34, step: 94
	action: tensor([[ 0.5893,  0.0926, -0.1821,  0.2238, -0.4586,  0.1684, -0.2589]],
       dtype=torch.float64)
	q_value: tensor([[-2.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8698186716897248, distance: 0.41288684169993467 entropy 0.03264415264129639
epoch: 34, step: 95
	action: tensor([[-0.1394,  0.3056, -0.1694,  0.2812,  0.2761,  0.3718, -0.4054]],
       dtype=torch.float64)
	q_value: tensor([[-2.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5146261005849188, distance: 0.7972506879449686 entropy 0.03264415264129639
epoch: 34, step: 96
	action: tensor([[ 0.0855,  0.4011, -0.3660,  0.0682, -0.1986, -0.1252, -0.6505]],
       dtype=torch.float64)
	q_value: tensor([[-2.0456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6393827377315964, distance: 0.6871949340707039 entropy 0.03264415264129639
epoch: 34, step: 97
	action: tensor([[ 0.5917,  0.4508, -0.4663, -0.1518, -0.1271, -0.0135, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-2.6564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9255301778989413, distance: 0.3122819276116331 entropy 0.03264415264129639
epoch: 34, step: 98
	action: tensor([[ 0.7755,  0.0203, -0.4167, -0.0810,  0.0150, -0.2265, -0.4872]],
       dtype=torch.float64)
	q_value: tensor([[-3.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6927441533562955, distance: 0.6343176137884836 entropy 0.03264415264129639
epoch: 34, step: 99
	action: tensor([[-0.0865,  0.0328, -0.4182, -0.1682,  0.3368,  0.3533, -0.4742]],
       dtype=torch.float64)
	q_value: tensor([[-2.9956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31149643752525336, distance: 0.9495324017300387 entropy 0.03264415264129639
epoch: 34, step: 100
	action: tensor([[-0.1355,  0.6089, -0.2163,  0.5667,  0.2152,  0.0653, -0.4135]],
       dtype=torch.float64)
	q_value: tensor([[-2.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 34, step: 101
	action: tensor([[ 0.4503,  0.1379, -0.2110,  0.0308, -0.0574,  0.0365, -0.0478]],
       dtype=torch.float64)
	q_value: tensor([[-9.1491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.743613783230823, distance: 0.5794340735458462 entropy 0.03264415264129639
epoch: 34, step: 102
	action: tensor([[ 0.5698,  0.4937, -0.5334,  0.0621, -0.1311, -0.3001,  0.4376]],
       dtype=torch.float64)
	q_value: tensor([[-1.9877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9290478770811286, distance: 0.3048171445509655 entropy 0.03264415264129639
epoch: 34, step: 103
	action: tensor([[ 0.4939,  0.1019, -0.2902, -0.1679,  0.1045,  0.4116, -0.1762]],
       dtype=torch.float64)
	q_value: tensor([[-3.6603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.762133159785693, distance: 0.5581149359683504 entropy 0.03264415264129639
epoch: 34, step: 104
	action: tensor([[ 0.6548,  0.6157, -0.6966,  0.0235,  0.1611,  0.5783, -0.5764]],
       dtype=torch.float64)
	q_value: tensor([[-2.2411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.934776814506684, distance: 0.29225213748931 entropy 0.03264415264129639
epoch: 34, step: 105
	action: tensor([[ 0.5508, -0.0772, -0.4179,  0.2649, -0.2780,  0.1093, -0.1388]],
       dtype=torch.float64)
	q_value: tensor([[-4.0934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7522459198052899, distance: 0.5695962232506714 entropy 0.03264415264129639
epoch: 34, step: 106
	action: tensor([[ 0.4366, -0.3899, -0.9164, -0.2674,  0.3210, -0.1260,  0.0505]],
       dtype=torch.float64)
	q_value: tensor([[-2.2202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23814103010866505, distance: 0.9988355378776073 entropy 0.03264415264129639
epoch: 34, step: 107
	action: tensor([[ 0.1575,  0.1543, -0.5277, -0.0286, -0.1896,  0.4017, -0.3671]],
       dtype=torch.float64)
	q_value: tensor([[-2.8559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5635830141177609, distance: 0.7559751433547454 entropy 0.03264415264129639
epoch: 34, step: 108
	action: tensor([[ 0.0942, -0.0192, -0.4696,  0.2574,  0.2723,  0.0703, -0.5703]],
       dtype=torch.float64)
	q_value: tensor([[-2.4316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4579209707365044, distance: 0.8425350882547569 entropy 0.03264415264129639
epoch: 34, step: 109
	action: tensor([[ 0.1937, -0.3425, -1.0726,  0.1410, -0.1134,  0.0657, -0.6337]],
       dtype=torch.float64)
	q_value: tensor([[-1.8621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.173222572392295, distance: 1.0405213574581365 entropy 0.03264415264129639
epoch: 34, step: 110
	action: tensor([[ 0.5118, -0.1980,  0.0095,  0.0039, -0.2241, -0.3624, -0.5072]],
       dtype=torch.float64)
	q_value: tensor([[-2.6580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44367010750716185, distance: 0.8535380595532871 entropy 0.03264415264129639
epoch: 34, step: 111
	action: tensor([[ 0.7661, -0.0874, -0.2747,  0.2518,  0.1159,  0.1406, -0.9159]],
       dtype=torch.float64)
	q_value: tensor([[-2.4416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8081242043259825, distance: 0.5012643171440027 entropy 0.03264415264129639
epoch: 34, step: 112
	action: tensor([[ 0.1271,  0.2020, -0.6721, -0.5530,  0.0302,  0.2150, -0.4313]],
       dtype=torch.float64)
	q_value: tensor([[-3.0295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5675232735646685, distance: 0.7525546850282597 entropy 0.03264415264129639
epoch: 34, step: 113
	action: tensor([[ 0.4257,  0.1319, -0.5552,  0.2244,  0.2069,  0.0427, -0.1930]],
       dtype=torch.float64)
	q_value: tensor([[-3.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7714531142588652, distance: 0.5470718232714252 entropy 0.03264415264129639
epoch: 34, step: 114
	action: tensor([[ 0.0955,  0.1656, -0.2852, -0.0437, -0.0704,  0.2707, -0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-2.1707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5449190936747887, distance: 0.7719710259217787 entropy 0.03264415264129639
epoch: 34, step: 115
	action: tensor([[ 0.1114,  0.3366, -0.3242,  0.1343, -0.3463, -0.1734,  0.0311]],
       dtype=torch.float64)
	q_value: tensor([[-1.8989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6549668108613269, distance: 0.672182388336473 entropy 0.03264415264129639
epoch: 34, step: 116
	action: tensor([[ 0.4356, -0.0968,  0.0774,  0.4187, -0.1078, -0.1161, -0.7695]],
       dtype=torch.float64)
	q_value: tensor([[-2.4573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7649669862704489, distance: 0.554780423775389 entropy 0.03264415264129639
epoch: 34, step: 117
	action: tensor([[ 0.4618,  0.0724,  0.1820, -0.0884, -0.5618,  0.0468, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[-2.4188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6848726649175343, distance: 0.6423914199234316 entropy 0.03264415264129639
epoch: 34, step: 118
	action: tensor([[ 0.7960, -0.3148, -0.7987, -0.0359, -0.2930,  0.2605, -0.4904]],
       dtype=torch.float64)
	q_value: tensor([[-2.7135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.511609997279947, distance: 0.7997239013586983 entropy 0.03264415264129639
epoch: 34, step: 119
	action: tensor([[ 0.7399,  0.0134, -0.1790,  0.1427, -0.0246,  0.4642, -0.3928]],
       dtype=torch.float64)
	q_value: tensor([[-3.1942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8849796560912165, distance: 0.38810034085708345 entropy 0.03264415264129639
epoch: 34, step: 120
	action: tensor([[-0.0078,  0.1155, -0.5699, -0.0540,  0.1917, -0.1778, -0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-2.7001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3795377453059494, distance: 0.9013934057565366 entropy 0.03264415264129639
epoch: 34, step: 121
	action: tensor([[ 0.4565, -0.1606, -0.3248, -0.0641,  0.3721,  0.3147,  0.1123]],
       dtype=torch.float64)
	q_value: tensor([[-2.1028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5783336373399537, distance: 0.7430895811501631 entropy 0.03264415264129639
epoch: 34, step: 122
	action: tensor([[ 0.4126, -0.0331, -0.4257, -0.1534,  0.0613,  0.2140, -0.3969]],
       dtype=torch.float64)
	q_value: tensor([[-2.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5978716472836222, distance: 0.72566978519276 entropy 0.03264415264129639
epoch: 34, step: 123
	action: tensor([[ 0.6533,  0.0284, -0.2491,  0.7037, -0.2037, -0.2175, -0.4151]],
       dtype=torch.float64)
	q_value: tensor([[-1.9482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9226466450078926, distance: 0.31827041518186583 entropy 0.03264415264129639
epoch: 34, step: 124
	action: tensor([[ 0.4193,  0.0767, -0.4244, -0.3703,  0.0853,  0.2198, -0.5681]],
       dtype=torch.float64)
	q_value: tensor([[-2.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6331702223916165, distance: 0.6930889653803202 entropy 0.03264415264129639
epoch: 34, step: 125
	action: tensor([[ 0.7604, -0.1198, -0.2655,  0.0577, -0.0893,  0.5031, -0.6025]],
       dtype=torch.float64)
	q_value: tensor([[-2.4125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7968677824733892, distance: 0.5157581484006618 entropy 0.03264415264129639
epoch: 34, step: 126
	action: tensor([[ 0.6310, -0.0331, -0.8825,  0.0813, -0.1402,  0.4019, -0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-2.9349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7454118982714645, distance: 0.5773986241722685 entropy 0.03264415264129639
epoch: 34, step: 127
	action: tensor([[ 0.4326, -0.0317, -0.4403,  0.0404, -0.0245,  0.3384, -0.5260]],
       dtype=torch.float64)
	q_value: tensor([[-3.2054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.696069772839789, distance: 0.6308754690057112 entropy 0.03264415264129639
LOSS epoch 34 actor 14.620943986583272 critic 177.44536470910165 
epoch: 35, step: 0
	action: tensor([[ 1.1278,  0.0055, -0.4085,  0.1133, -0.0782,  0.3182, -0.7438]],
       dtype=torch.float64)
	q_value: tensor([[-1.8615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8344781310507842, distance: 0.4655693526415323 entropy 0.03264415264129639
epoch: 35, step: 1
	action: tensor([[ 0.9439,  0.5919, -0.2203, -0.0485, -0.1609,  0.2474, -0.6713]],
       dtype=torch.float64)
	q_value: tensor([[-3.6968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.984932880890721, distance: 0.14046619039497008 entropy 0.03264415264129639
epoch: 35, step: 2
	action: tensor([[ 0.3116, -0.0769, -0.4844,  0.0554, -0.4984, -0.2485, -0.5923]],
       dtype=torch.float64)
	q_value: tensor([[-3.9385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5429859297226032, distance: 0.7736089380382263 entropy 0.03264415264129639
epoch: 35, step: 3
	action: tensor([[ 0.5474,  0.1137,  0.0942,  0.2801, -0.3925, -0.1441, -0.7383]],
       dtype=torch.float64)
	q_value: tensor([[-2.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8621504802888493, distance: 0.424873184578709 entropy 0.03264415264129639
epoch: 35, step: 4
	action: tensor([[ 0.5411,  0.5324, -0.3160, -0.2735, -0.1093,  0.5224, -0.7020]],
       dtype=torch.float64)
	q_value: tensor([[-2.6843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9323765762783959, distance: 0.2975810492700566 entropy 0.03264415264129639
epoch: 35, step: 5
	action: tensor([[ 1.1956,  0.4591, -0.2848, -0.0908,  0.0515, -0.0834, -0.8902]],
       dtype=torch.float64)
	q_value: tensor([[-3.4513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8630747823521123, distance: 0.42344636903628524 entropy 0.03264415264129639
epoch: 35, step: 6
	action: tensor([[ 0.5961,  0.0189, -0.6639,  0.5937,  0.0269,  0.3466, -0.6344]],
       dtype=torch.float64)
	q_value: tensor([[-4.4543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.826617129653227, distance: 0.4764965795451541 entropy 0.03264415264129639
epoch: 35, step: 7
	action: tensor([[ 1.3952,  0.0733, -0.7305, -0.0641,  0.0243,  0.5303, -0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-2.6178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7181284744511107, distance: 0.6075503791959425 entropy 0.03264415264129639
epoch: 35, step: 8
	action: tensor([[ 0.9087,  0.2689, -0.5880,  0.3264, -0.4564,  0.2740, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-4.5962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9775627384918698, distance: 0.17141215626644332 entropy 0.03264415264129639
epoch: 35, step: 9
	action: tensor([[ 0.5679,  0.0444, -0.8695, -0.2745, -0.3302,  0.2103, -0.6579]],
       dtype=torch.float64)
	q_value: tensor([[-3.4336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7035344087087005, distance: 0.6230800424311707 entropy 0.03264415264129639
epoch: 35, step: 10
	action: tensor([[ 0.4159,  0.1741,  0.0306,  0.2413,  0.0056,  0.4042, -0.4462]],
       dtype=torch.float64)
	q_value: tensor([[-3.3445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.860282183124687, distance: 0.42774268260609033 entropy 0.03264415264129639
epoch: 35, step: 11
	action: tensor([[ 0.8038,  0.3517, -0.5696, -0.0061, -0.4074,  0.1694, -0.5619]],
       dtype=torch.float64)
	q_value: tensor([[-1.9327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9536465572454053, distance: 0.24637559778832277 entropy 0.03264415264129639
epoch: 35, step: 12
	action: tensor([[ 0.6456,  0.0717, -0.5702,  0.4354,  0.0832,  0.0683, -0.8117]],
       dtype=torch.float64)
	q_value: tensor([[-3.5472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8761416566594207, distance: 0.4027349561724975 entropy 0.03264415264129639
epoch: 35, step: 13
	action: tensor([[ 0.8204,  0.3794, -0.5267,  0.0482, -0.1950,  0.3463, -0.4478]],
       dtype=torch.float64)
	q_value: tensor([[-2.7333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9776857935115219, distance: 0.17094146320438125 entropy 0.03264415264129639
epoch: 35, step: 14
	action: tensor([[ 0.9730,  0.0889, -0.5455, -0.0522,  0.0205,  0.7739, -0.4704]],
       dtype=torch.float64)
	q_value: tensor([[-3.3822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.926891111919749, distance: 0.3094152983627748 entropy 0.03264415264129639
epoch: 35, step: 15
	action: tensor([[ 0.2281,  0.3677, -0.5116, -0.0638, -0.1635,  0.7868, -0.2312]],
       dtype=torch.float64)
	q_value: tensor([[-3.6124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7121598770623344, distance: 0.6139490886264273 entropy 0.03264415264129639
epoch: 35, step: 16
	action: tensor([[ 0.2836,  0.3969, -0.1573, -0.1202, -0.1015,  0.5233, -0.6280]],
       dtype=torch.float64)
	q_value: tensor([[-3.1274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7974881413883723, distance: 0.5149699922282324 entropy 0.03264415264129639
epoch: 35, step: 17
	action: tensor([[ 0.4336, -0.0503, -0.6667,  0.4467,  0.2324,  0.1217, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-2.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6987816299285923, distance: 0.6280546282530174 entropy 0.03264415264129639
epoch: 35, step: 18
	action: tensor([[ 0.9530,  0.1573, -0.8519,  0.2567,  0.2292,  0.1562, -0.6459]],
       dtype=torch.float64)
	q_value: tensor([[-1.9855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9361070684320232, distance: 0.28925648260582026 entropy 0.03264415264129639
epoch: 35, step: 19
	action: tensor([[ 0.4319,  0.1993, -1.3120,  0.1590,  0.2219,  0.1536, -0.4187]],
       dtype=torch.float64)
	q_value: tensor([[-3.5026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7292009358712794, distance: 0.5954979528265426 entropy 0.03264415264129639
epoch: 35, step: 20
	action: tensor([[ 0.6570, -0.0934, -0.5883, -0.0645, -0.0237,  0.3095, -0.3577]],
       dtype=torch.float64)
	q_value: tensor([[-3.4471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.70312755558931, distance: 0.6235074362888117 entropy 0.03264415264129639
epoch: 35, step: 21
	action: tensor([[ 0.8411,  0.6573, -0.1187,  0.1599,  0.1575,  0.0491, -0.3419]],
       dtype=torch.float64)
	q_value: tensor([[-2.2614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9755414123480862, distance: 0.1789667621416887 entropy 0.03264415264129639
epoch: 35, step: 22
	action: tensor([[ 0.1741, -0.1672, -0.6639,  0.1704,  0.3517,  0.2022, -0.4366]],
       dtype=torch.float64)
	q_value: tensor([[-3.1278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4113207032685704, distance: 0.8780031508801982 entropy 0.03264415264129639
epoch: 35, step: 23
	action: tensor([[-0.2257, -0.0832, -0.3593,  0.2285, -0.1861,  0.0595, -0.3640]],
       dtype=torch.float64)
	q_value: tensor([[-1.6623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07298586743728597, distance: 1.1017926498476287 entropy 0.03264415264129639
epoch: 35, step: 24
	action: tensor([[ 0.4511,  0.7927,  0.0875,  0.0376, -0.1879, -0.4416, -0.3921]],
       dtype=torch.float64)
	q_value: tensor([[-1.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.908520175927285, distance: 0.3461141491628922 entropy 0.03264415264129639
epoch: 35, step: 25
	action: tensor([[ 0.3638,  0.0080, -0.0769,  0.2217, -0.3194,  0.2486, -0.4631]],
       dtype=torch.float64)
	q_value: tensor([[-3.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.729357100420496, distance: 0.5953262220254648 entropy 0.03264415264129639
epoch: 35, step: 26
	action: tensor([[ 0.9106,  0.0648, -0.1798,  0.2172, -0.0899,  0.1325, -0.7709]],
       dtype=torch.float64)
	q_value: tensor([[-1.9035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9026299663146578, distance: 0.35708314684197134 entropy 0.03264415264129639
epoch: 35, step: 27
	action: tensor([[ 0.8982,  0.5568, -0.7482,  0.3132,  0.2270,  0.4657, -0.5306]],
       dtype=torch.float64)
	q_value: tensor([[-3.1020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.957627734878809, distance: 0.23555782397406955 entropy 0.03264415264129639
epoch: 35, step: 28
	action: tensor([[ 0.9073,  0.3011, -0.4555,  0.1641, -0.4280, -0.2335, -0.6654]],
       dtype=torch.float64)
	q_value: tensor([[-3.9206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9455021197100838, distance: 0.26714466070816345 entropy 0.03264415264129639
epoch: 35, step: 29
	action: tensor([[ 0.6645,  0.5902, -0.2266, -0.3805, -0.0126,  0.6015, -0.8027]],
       dtype=torch.float64)
	q_value: tensor([[-3.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9749630999728693, distance: 0.18107019565426274 entropy 0.03264415264129639
epoch: 35, step: 30
	action: tensor([[ 0.6190, -0.1380, -0.5150,  0.6444, -0.0959,  0.6408, -0.3332]],
       dtype=torch.float64)
	q_value: tensor([[-3.8559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8435419704370133, distance: 0.45264280506847215 entropy 0.03264415264129639
epoch: 35, step: 31
	action: tensor([[ 1.0266,  0.2772, -1.0186,  0.2726, -0.2411,  0.2495, -1.0112]],
       dtype=torch.float64)
	q_value: tensor([[-2.5160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9620873639530028, distance: 0.2228171854905799 entropy 0.03264415264129639
epoch: 35, step: 32
	action: tensor([[ 0.6556,  0.1811, -0.7961,  0.6894,  0.1317,  0.1716, -0.8924]],
       dtype=torch.float64)
	q_value: tensor([[-4.8298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.838277393293116, distance: 0.46019517370126883 entropy 0.03264415264129639
epoch: 35, step: 33
	action: tensor([[ 0.5260,  0.5310, -0.5919,  0.0008, -0.4965,  0.3306, -0.7457]],
       dtype=torch.float64)
	q_value: tensor([[-3.3513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8943083983381012, distance: 0.3720291182708334 entropy 0.03264415264129639
epoch: 35, step: 34
	action: tensor([[ 0.4740,  0.1172, -0.3934,  0.0697, -0.3128,  0.5046,  0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-3.8526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7959053626141472, distance: 0.5169785095394941 entropy 0.03264415264129639
epoch: 35, step: 35
	action: tensor([[ 0.4808,  0.2205, -0.1490,  0.3824, -0.4005,  0.5689, -0.7578]],
       dtype=torch.float64)
	q_value: tensor([[-2.4006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8726855375883082, distance: 0.4083152152499127 entropy 0.03264415264129639
epoch: 35, step: 36
	action: tensor([[ 1.0994,  0.4390, -0.2807,  0.0057, -0.0801, -0.1415, -0.6005]],
       dtype=torch.float64)
	q_value: tensor([[-2.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9109763039014956, distance: 0.34143615257069326 entropy 0.03264415264129639
epoch: 35, step: 37
	action: tensor([[ 0.4534,  0.0816,  0.0113, -0.0020, -0.1624,  0.1837, -0.8657]],
       dtype=torch.float64)
	q_value: tensor([[-3.9605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7571092205146127, distance: 0.563978057776919 entropy 0.03264415264129639
epoch: 35, step: 38
	action: tensor([[ 0.5179,  0.0207, -0.6136,  0.3354, -0.4473, -0.2566, -0.5031]],
       dtype=torch.float64)
	q_value: tensor([[-2.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7789611084924024, distance: 0.538010857596013 entropy 0.03264415264129639
epoch: 35, step: 39
	action: tensor([[ 0.4166,  0.5231, -0.4517,  0.1168,  0.1072,  0.5086, -0.1540]],
       dtype=torch.float64)
	q_value: tensor([[-2.5643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8482640997988653, distance: 0.445759765263805 entropy 0.03264415264129639
epoch: 35, step: 40
	action: tensor([[-0.1193,  0.0348, -0.5951,  0.2223,  0.2182, -0.2035, -0.5188]],
       dtype=torch.float64)
	q_value: tensor([[-2.5672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21291159739852472, distance: 1.015239367640334 entropy 0.03264415264129639
epoch: 35, step: 41
	action: tensor([[ 0.3967,  0.2263, -0.4246, -0.2346,  0.3843,  0.5433, -0.1690]],
       dtype=torch.float64)
	q_value: tensor([[-1.8116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7952735697793915, distance: 0.5177780673817265 entropy 0.03264415264129639
epoch: 35, step: 42
	action: tensor([[ 1.0913,  0.3247, -0.3360,  0.0996, -0.0603,  0.1865, -0.2918]],
       dtype=torch.float64)
	q_value: tensor([[-2.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9408239032967397, distance: 0.27837475702214265 entropy 0.03264415264129639
epoch: 35, step: 43
	action: tensor([[ 0.5429,  0.1623, -0.2935,  0.0927, -0.0755,  0.1184, -0.2085]],
       dtype=torch.float64)
	q_value: tensor([[-3.5519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.84779657042479, distance: 0.446445975631257 entropy 0.03264415264129639
epoch: 35, step: 44
	action: tensor([[ 1.0682,  0.2939, -0.7796, -0.2124,  0.0131,  0.1584, -0.6774]],
       dtype=torch.float64)
	q_value: tensor([[-1.8958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8504239302920494, distance: 0.4425758906158985 entropy 0.03264415264129639
epoch: 35, step: 45
	action: tensor([[ 0.7677,  0.1898, -0.4509,  0.1670,  0.2710,  0.4142, -0.5673]],
       dtype=torch.float64)
	q_value: tensor([[-4.2525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9606353810919767, distance: 0.22704383766687894 entropy 0.03264415264129639
epoch: 35, step: 46
	action: tensor([[ 0.6107, -0.0137, -0.3568, -0.1085, -0.3068,  0.0576, -0.8264]],
       dtype=torch.float64)
	q_value: tensor([[-2.6848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6850097525208554, distance: 0.6422516772386767 entropy 0.03264415264129639
epoch: 35, step: 47
	action: tensor([[ 0.8792,  0.2696, -0.5390,  0.3908,  0.0145,  0.3663,  0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-2.7013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9802576912390047, distance: 0.1607887515656191 entropy 0.03264415264129639
epoch: 35, step: 48
	action: tensor([[ 0.6880,  0.5450, -0.3276,  0.4350, -0.2855,  0.2157, -1.2524]],
       dtype=torch.float64)
	q_value: tensor([[-3.0202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9188937938870894, distance: 0.32589953196382554 entropy 0.03264415264129639
epoch: 35, step: 49
	action: tensor([[ 0.7551, -0.2007, -0.5191,  0.0142, -0.1523,  0.1654, -0.5081]],
       dtype=torch.float64)
	q_value: tensor([[-4.1746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6359853716093519, distance: 0.6904243688562565 entropy 0.03264415264129639
epoch: 35, step: 50
	action: tensor([[ 0.8997,  0.1249, -0.5613, -0.1020, -0.2381,  0.5801, -0.7642]],
       dtype=torch.float64)
	q_value: tensor([[-2.4156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8862188969373662, distance: 0.38600396270833254 entropy 0.03264415264129639
epoch: 35, step: 51
	action: tensor([[ 0.5706,  0.3336, -0.5606,  0.4901,  0.1001,  0.4666, -0.3440]],
       dtype=torch.float64)
	q_value: tensor([[-3.8393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8587479112093388, distance: 0.43008483822194615 entropy 0.03264415264129639
epoch: 35, step: 52
	action: tensor([[ 0.4607,  0.8863, -0.4037,  0.1133,  0.0250,  0.5126, -0.3532]],
       dtype=torch.float64)
	q_value: tensor([[-2.6723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 35, step: 53
	action: tensor([[ 0.9470,  0.4531, -0.4723,  0.2572, -0.1136,  0.3703, -0.4732]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9904997976556359, distance: 0.11153806886239141 entropy 0.03264415264129639
epoch: 35, step: 54
	action: tensor([[ 0.7023,  0.8255, -0.5897, -0.1747, -0.5500,  0.3030, -0.6861]],
       dtype=torch.float64)
	q_value: tensor([[-3.6876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9604843519905268, distance: 0.22747896698610529 entropy 0.03264415264129639
epoch: 35, step: 55
	action: tensor([[ 0.5662,  0.0713, -0.1862,  0.2688,  0.3604, -0.1846, -0.7954]],
       dtype=torch.float64)
	q_value: tensor([[-4.8896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8191648909655905, distance: 0.486629092874436 entropy 0.03264415264129639
epoch: 35, step: 56
	action: tensor([[ 0.6767,  0.5685, -0.7617,  0.1085, -0.1303,  0.3916, -0.6604]],
       dtype=torch.float64)
	q_value: tensor([[-2.2779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9361950703017462, distance: 0.28905721265718004 entropy 0.03264415264129639
epoch: 35, step: 57
	action: tensor([[ 0.4293,  0.2243, -0.4054,  0.4255,  0.0621,  0.2963, -0.5882]],
       dtype=torch.float64)
	q_value: tensor([[-3.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8357369468193969, distance: 0.46379561548802195 entropy 0.03264415264129639
epoch: 35, step: 58
	action: tensor([[ 1.1767,  0.3189, -0.3345,  0.5500, -0.1335,  0.3296, -0.5051]],
       dtype=torch.float64)
	q_value: tensor([[-2.1940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9762541986020188, distance: 0.1763397045700161 entropy 0.03264415264129639
epoch: 35, step: 59
	action: tensor([[ 0.8115, -0.0398, -0.5343,  0.1071,  0.1531,  0.1480, -0.5580]],
       dtype=torch.float64)
	q_value: tensor([[-4.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8047100282501153, distance: 0.5057043210946827 entropy 0.03264415264129639
epoch: 35, step: 60
	action: tensor([[ 0.5929,  0.2099, -0.2954,  0.3584, -0.4177,  0.1387, -0.5892]],
       dtype=torch.float64)
	q_value: tensor([[-2.5722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.913894649500638, distance: 0.3357930960372374 entropy 0.03264415264129639
epoch: 35, step: 61
	action: tensor([[ 0.3004,  0.6568, -1.0771,  0.2212,  0.1808, -0.0552, -0.3861]],
       dtype=torch.float64)
	q_value: tensor([[-2.6788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7641982452827213, distance: 0.5556869651180251 entropy 0.03264415264129639
epoch: 35, step: 62
	action: tensor([[ 0.3586, -0.0735, -0.6842,  0.1841, -0.3303,  0.3849, -0.5075]],
       dtype=torch.float64)
	q_value: tensor([[-3.6405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5758917729148778, distance: 0.7452380862018121 entropy 0.03264415264129639
epoch: 35, step: 63
	action: tensor([[ 0.5798,  0.0764, -0.6920, -0.2494, -0.6890,  0.1325, -0.4474]],
       dtype=torch.float64)
	q_value: tensor([[-2.3220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7082758237177684, distance: 0.6180774576137981 entropy 0.03264415264129639
epoch: 35, step: 64
	action: tensor([[ 0.1960,  0.1334,  0.0467, -0.2087,  0.1450,  0.2839, -0.5817]],
       dtype=torch.float64)
	q_value: tensor([[-3.4443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5737156970389251, distance: 0.7471475277441808 entropy 0.03264415264129639
epoch: 35, step: 65
	action: tensor([[ 0.9948,  0.8560, -0.4620,  0.4303,  0.3477,  0.0897, -0.5435]],
       dtype=torch.float64)
	q_value: tensor([[-1.7418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 35, step: 66
	action: tensor([[ 0.3860,  0.2305, -0.8800,  0.3383, -0.1277,  0.3303, -1.1306]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6855491352094648, distance: 0.6417015524861507 entropy 0.03264415264129639
epoch: 35, step: 67
	action: tensor([[ 0.5752,  0.3111, -0.6091,  0.3393,  0.0965,  0.2450, -0.4537]],
       dtype=torch.float64)
	q_value: tensor([[-3.4847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.896783554024572, distance: 0.36764709931043094 entropy 0.03264415264129639
epoch: 35, step: 68
	action: tensor([[ 0.5346,  0.5712, -0.9393,  0.6396, -0.0249,  0.3026, -0.7939]],
       dtype=torch.float64)
	q_value: tensor([[-2.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6933125669294828, distance: 0.6337306084418609 entropy 0.03264415264129639
epoch: 35, step: 69
	action: tensor([[ 0.7363,  0.1712, -0.2470,  0.2043, -0.0178,  0.2816, -0.1401]],
       dtype=torch.float64)
	q_value: tensor([[-3.8197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.947412460242288, distance: 0.26242071832457775 entropy 0.03264415264129639
epoch: 35, step: 70
	action: tensor([[ 0.9247,  0.0450, -0.5152,  0.1378,  0.1415,  0.0270, -0.2655]],
       dtype=torch.float64)
	q_value: tensor([[-2.2703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.841939550294657, distance: 0.45495485078067 entropy 0.03264415264129639
epoch: 35, step: 71
	action: tensor([[ 0.9737,  0.0081, -0.5486,  0.3686, -0.2058,  0.4583, -0.3739]],
       dtype=torch.float64)
	q_value: tensor([[-2.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9446328389081099, distance: 0.2692668074964211 entropy 0.03264415264129639
epoch: 35, step: 72
	action: tensor([[ 0.4937,  0.3703, -0.4129,  0.2275, -0.0103,  0.2728, -0.4376]],
       dtype=torch.float64)
	q_value: tensor([[-3.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8924973710485434, distance: 0.3752029425899798 entropy 0.03264415264129639
epoch: 35, step: 73
	action: tensor([[ 0.9856,  0.3157, -0.3397, -0.0735, -0.4773,  0.1755, -0.5148]],
       dtype=torch.float64)
	q_value: tensor([[-2.2941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9245229121167347, distance: 0.31438676944015864 entropy 0.03264415264129639
epoch: 35, step: 74
	action: tensor([[ 0.5038,  0.1943, -0.3682,  0.1590, -0.2528,  0.7297, -0.9217]],
       dtype=torch.float64)
	q_value: tensor([[-3.8022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8447194892658536, distance: 0.45093627053310736 entropy 0.03264415264129639
epoch: 35, step: 75
	action: tensor([[ 0.8843,  0.3027, -0.6768,  0.0856, -0.4618, -0.4894, -0.6121]],
       dtype=torch.float64)
	q_value: tensor([[-3.2961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9161561708567905, distance: 0.33135402120733115 entropy 0.03264415264129639
epoch: 35, step: 76
	action: tensor([[ 0.3049,  0.5261, -0.8110,  0.0257, -0.1057,  0.0829, -0.2192]],
       dtype=torch.float64)
	q_value: tensor([[-3.9506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8120492504729361, distance: 0.49611084891134366 entropy 0.03264415264129639
epoch: 35, step: 77
	action: tensor([[ 0.2061,  0.3631,  0.1611,  0.0913, -0.0555,  0.2529, -0.3932]],
       dtype=torch.float64)
	q_value: tensor([[-3.0569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7771261725971116, distance: 0.5402393683040491 entropy 0.03264415264129639
epoch: 35, step: 78
	action: tensor([[ 0.1763,  0.1062, -0.4763, -0.0066, -0.6885,  0.0077, -0.4669]],
       dtype=torch.float64)
	q_value: tensor([[-1.9737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5468662717482546, distance: 0.7703177194429967 entropy 0.03264415264129639
epoch: 35, step: 79
	action: tensor([[ 0.0752,  0.2647, -0.4756,  0.1305,  0.1274,  0.2267, -0.3209]],
       dtype=torch.float64)
	q_value: tensor([[-2.6515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5771944084046663, distance: 0.7440927180036435 entropy 0.03264415264129639
epoch: 35, step: 80
	action: tensor([[ 0.6108,  0.5382, -0.4548,  0.4383, -0.0303,  0.3283, -0.6848]],
       dtype=torch.float64)
	q_value: tensor([[-1.7712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8725373498089302, distance: 0.40855277556803854 entropy 0.03264415264129639
epoch: 35, step: 81
	action: tensor([[ 0.6468,  0.4206, -0.5212,  0.3593, -0.2312,  0.2531, -0.4884]],
       dtype=torch.float64)
	q_value: tensor([[-3.1349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9208710510088934, distance: 0.3219025314137006 entropy 0.03264415264129639
epoch: 35, step: 82
	action: tensor([[ 0.5834,  0.7272,  0.0934,  0.0667,  0.0628, -0.3243, -0.8012]],
       dtype=torch.float64)
	q_value: tensor([[-3.0112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9545759821853637, distance: 0.2438930731544084 entropy 0.03264415264129639
epoch: 35, step: 83
	action: tensor([[ 0.7616,  0.1243, -0.2507,  0.1490, -0.1399,  0.3392, -0.4049]],
       dtype=torch.float64)
	q_value: tensor([[-3.4240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9299960242179217, distance: 0.3027736289664931 entropy 0.03264415264129639
epoch: 35, step: 84
	action: tensor([[ 0.5489,  0.3810, -0.3057,  0.2508,  0.1134,  0.3479, -0.5296]],
       dtype=torch.float64)
	q_value: tensor([[-2.5521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9309185169039864, distance: 0.30077208031323793 entropy 0.03264415264129639
epoch: 35, step: 85
	action: tensor([[ 0.5225,  0.4897, -0.7427,  0.3475, -0.2418,  0.1932, -0.2673]],
       dtype=torch.float64)
	q_value: tensor([[-2.3420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8450354344230356, distance: 0.45047728287721256 entropy 0.03264415264129639
epoch: 35, step: 86
	action: tensor([[-0.0970, -0.1868, -0.2364,  0.0584,  0.0809, -0.1714, -0.4599]],
       dtype=torch.float64)
	q_value: tensor([[-3.0646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0642481449650909, distance: 1.1069730342450945 entropy 0.03264415264129639
epoch: 35, step: 87
	action: tensor([[ 0.3114,  0.6234, -0.4357,  0.2916,  0.2941, -0.2375, -0.1291]],
       dtype=torch.float64)
	q_value: tensor([[-1.4122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7846465709253364, distance: 0.5310465466250339 entropy 0.03264415264129639
epoch: 35, step: 88
	action: tensor([[ 0.5776,  0.3112, -0.8946, -0.3289, -0.2282,  0.1862, -0.6439]],
       dtype=torch.float64)
	q_value: tensor([[-2.5919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8678326389805296, distance: 0.4160243996365332 entropy 0.03264415264129639
epoch: 35, step: 89
	action: tensor([[ 0.3420,  0.4345, -0.6302,  0.1056, -0.6489, -0.0321, -0.1994]],
       dtype=torch.float64)
	q_value: tensor([[-3.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8153128468140369, distance: 0.4917847267023963 entropy 0.03264415264129639
epoch: 35, step: 90
	action: tensor([[ 0.4585,  0.4332, -0.5103, -0.2424, -0.2671,  0.2255, -0.2396]],
       dtype=torch.float64)
	q_value: tensor([[-3.0785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8744991451073598, distance: 0.405396537782352 entropy 0.03264415264129639
epoch: 35, step: 91
	action: tensor([[ 0.4990,  0.5218, -0.4249,  0.2617,  0.0051,  0.0434, -0.6242]],
       dtype=torch.float64)
	q_value: tensor([[-3.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8969841233122677, distance: 0.3672897213117606 entropy 0.03264415264129639
epoch: 35, step: 92
	action: tensor([[ 0.4470,  0.0041, -0.5807,  0.1169,  0.2541, -0.0963, -0.5354]],
       dtype=torch.float64)
	q_value: tensor([[-2.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6779381057920015, distance: 0.6494210547958306 entropy 0.03264415264129639
epoch: 35, step: 93
	action: tensor([[ 0.7799,  0.6656, -0.4836, -0.3021, -0.0790,  0.5056, -0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-1.9622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9891046904130015, distance: 0.11944733945886055 entropy 0.03264415264129639
epoch: 35, step: 94
	action: tensor([[ 0.7015,  0.7483,  0.0334,  0.2621,  0.0008,  0.1037, -0.2208]],
       dtype=torch.float64)
	q_value: tensor([[-4.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 35, step: 95
	action: tensor([[ 0.0648,  0.1042, -0.3774, -0.0476, -0.2457,  0.4842, -0.3831]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45970618228870763, distance: 0.8411465971965849 entropy 0.03264415264129639
epoch: 35, step: 96
	action: tensor([[ 0.4486, -0.1075, -0.2189, -0.1306, -0.2558,  0.3363, -0.7981]],
       dtype=torch.float64)
	q_value: tensor([[-2.1173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5856512144128692, distance: 0.7366135923847503 entropy 0.03264415264129639
epoch: 35, step: 97
	action: tensor([[ 0.6131, -0.2439, -0.6073,  0.4047,  0.1975, -0.1197, -0.7652]],
       dtype=torch.float64)
	q_value: tensor([[-2.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6485671268682209, distance: 0.6783875747018273 entropy 0.03264415264129639
epoch: 35, step: 98
	action: tensor([[ 0.7079,  0.4268, -0.6620,  0.0113, -0.5144,  0.3607, -0.7326]],
       dtype=torch.float64)
	q_value: tensor([[-2.4220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9425268363488025, distance: 0.2743400700479432 entropy 0.03264415264129639
epoch: 35, step: 99
	action: tensor([[ 0.5113,  0.2898, -0.8113,  0.1722, -0.2971,  0.0616, -0.3833]],
       dtype=torch.float64)
	q_value: tensor([[-4.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8488981066472364, distance: 0.44482751858299374 entropy 0.03264415264129639
epoch: 35, step: 100
	action: tensor([[ 0.7005,  0.2107, -0.4229, -0.2133,  0.0622,  0.2825, -0.2413]],
       dtype=torch.float64)
	q_value: tensor([[-2.8713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8615214168438299, distance: 0.42584151571821977 entropy 0.03264415264129639
epoch: 35, step: 101
	action: tensor([[ 0.6962,  0.1535, -0.7427, -0.4012, -0.1897,  0.3596, -1.1577]],
       dtype=torch.float64)
	q_value: tensor([[-2.5763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7880096585593042, distance: 0.5268836602889752 entropy 0.03264415264129639
epoch: 35, step: 102
	action: tensor([[ 0.7147,  0.3519, -0.4419,  0.3935, -0.1017,  0.0274,  0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-4.1039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9587268930258773, distance: 0.23248250410083773 entropy 0.03264415264129639
epoch: 35, step: 103
	action: tensor([[ 0.6473,  0.3178, -0.1994,  0.1692,  0.2608,  0.2656, -0.5391]],
       dtype=torch.float64)
	q_value: tensor([[-2.6408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9591979614955062, distance: 0.23115198323600025 entropy 0.03264415264129639
epoch: 35, step: 104
	action: tensor([[ 0.9854,  0.4031, -0.4905,  0.2581, -0.2703,  0.3644, -0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-2.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09105313473771022 entropy 0.03264415264129639
epoch: 35, step: 105
	action: tensor([[ 0.7599,  0.2893, -0.6268, -0.1139,  0.0595,  0.0632, -0.8280]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9077300496768278, distance: 0.3476056578341917 entropy 0.03264415264129639
epoch: 35, step: 106
	action: tensor([[ 0.3520,  0.5567, -0.2905,  0.5699,  0.1047, -0.0687, -0.3299]],
       dtype=torch.float64)
	q_value: tensor([[-3.3218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7953748140204786, distance: 0.5176500220429902 entropy 0.03264415264129639
epoch: 35, step: 107
	action: tensor([[ 0.4410,  0.3312, -0.5208,  0.0225, -0.4672, -0.0635, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-2.3938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8523550614190945, distance: 0.43970962786524503 entropy 0.03264415264129639
epoch: 35, step: 108
	action: tensor([[ 0.2804,  0.2706, -0.4737,  0.2563,  0.0047,  0.6063, -0.4794]],
       dtype=torch.float64)
	q_value: tensor([[-2.7065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7340264954749662, distance: 0.5901683037981619 entropy 0.03264415264129639
epoch: 35, step: 109
	action: tensor([[ 0.7351,  0.4018, -0.0811,  0.2114,  0.3255,  0.1414, -0.8065]],
       dtype=torch.float64)
	q_value: tensor([[-2.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9840553014999149, distance: 0.1444990024804939 entropy 0.03264415264129639
epoch: 35, step: 110
	action: tensor([[ 0.7153,  0.1932, -0.8631, -0.0866, -0.3200,  0.2403, -0.4806]],
       dtype=torch.float64)
	q_value: tensor([[-2.8338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8673297778722523, distance: 0.4168150785037315 entropy 0.03264415264129639
epoch: 35, step: 111
	action: tensor([[ 0.4611,  0.2628, -0.1726,  0.0902, -0.3527,  0.2052, -0.6042]],
       dtype=torch.float64)
	q_value: tensor([[-3.5351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8556767808802204, distance: 0.43473519093840673 entropy 0.03264415264129639
epoch: 35, step: 112
	action: tensor([[ 0.7591,  0.4802, -0.5921,  0.4532,  0.1871,  0.0089, -0.0336]],
       dtype=torch.float64)
	q_value: tensor([[-2.4958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9555655864728576, distance: 0.24122172537665174 entropy 0.03264415264129639
epoch: 35, step: 113
	action: tensor([[ 0.5894,  0.5561, -0.7509, -0.1383,  0.2313,  0.3013, -0.8579]],
       dtype=torch.float64)
	q_value: tensor([[-3.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9602025566665664, distance: 0.2282886289251725 entropy 0.03264415264129639
epoch: 35, step: 114
	action: tensor([[ 1.1200,  0.3064, -0.3701,  0.1605, -0.2165, -0.2872, -0.3086]],
       dtype=torch.float64)
	q_value: tensor([[-3.6324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8804416995117336, distance: 0.39568224194293966 entropy 0.03264415264129639
epoch: 35, step: 115
	action: tensor([[ 0.4254,  0.1157, -0.6950, -0.1005,  0.1573,  0.0245, -0.6409]],
       dtype=torch.float64)
	q_value: tensor([[-3.7658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7219494430443598, distance: 0.6034184402696828 entropy 0.03264415264129639
epoch: 35, step: 116
	action: tensor([[ 0.4657,  0.3287, -0.4156,  0.0332,  0.4100,  0.1896, -0.5653]],
       dtype=torch.float64)
	q_value: tensor([[-2.2863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8767958598171841, distance: 0.401669951996257 entropy 0.03264415264129639
epoch: 35, step: 117
	action: tensor([[ 0.6817, -0.0046, -0.7259, -0.1943,  0.3057,  0.1534, -0.1006]],
       dtype=torch.float64)
	q_value: tensor([[-2.0986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7122805772784138, distance: 0.6138203512616862 entropy 0.03264415264129639
epoch: 35, step: 118
	action: tensor([[ 0.5285,  0.8067, -0.5179,  0.2395,  0.0210,  0.3905, -0.5341]],
       dtype=torch.float64)
	q_value: tensor([[-2.6274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 35, step: 119
	action: tensor([[ 0.6649,  0.5055, -0.6289,  0.0044,  0.1924,  0.4208, -0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.954942430066579, distance: 0.2429073052033937 entropy 0.03264415264129639
epoch: 35, step: 120
	action: tensor([[ 0.3348,  0.4385, -0.4816,  0.2724,  0.0309,  0.6508, -0.5689]],
       dtype=torch.float64)
	q_value: tensor([[-3.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7588724210368704, distance: 0.5619273054904176 entropy 0.03264415264129639
epoch: 35, step: 121
	action: tensor([[ 0.1334,  0.3449, -0.7881,  0.4054, -0.2904,  0.3525, -0.7601]],
       dtype=torch.float64)
	q_value: tensor([[-2.7095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4899467793118939, distance: 0.8172678981241437 entropy 0.03264415264129639
epoch: 35, step: 122
	action: tensor([[ 0.7208,  0.0853, -0.7244,  0.2246,  0.1177,  0.4341, -0.7051]],
       dtype=torch.float64)
	q_value: tensor([[-2.9110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8965048782680182, distance: 0.368143072947417 entropy 0.03264415264129639
epoch: 35, step: 123
	action: tensor([[-0.0072,  0.0417, -0.4506, -0.2367,  0.2549, -0.1219, -0.2827]],
       dtype=torch.float64)
	q_value: tensor([[-2.9124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28726037978027374, distance: 0.9661001375031779 entropy 0.03264415264129639
epoch: 35, step: 124
	action: tensor([[ 0.5546,  0.1065, -0.6927, -0.1299, -0.4586,  0.1634, -0.3443]],
       dtype=torch.float64)
	q_value: tensor([[-1.7370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7435306728611135, distance: 0.5795279808522962 entropy 0.03264415264129639
epoch: 35, step: 125
	action: tensor([[ 0.7938,  0.4784, -0.6784, -0.0505, -0.1087,  0.3913, -0.7520]],
       dtype=torch.float64)
	q_value: tensor([[-2.9649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9755602287999329, distance: 0.17889790764703814 entropy 0.03264415264129639
epoch: 35, step: 126
	action: tensor([[ 0.5734,  0.1994, -0.4356,  0.3362, -0.1508, -0.5251, -0.8269]],
       dtype=torch.float64)
	q_value: tensor([[-3.9837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8748676964982939, distance: 0.4048008473785727 entropy 0.03264415264129639
epoch: 35, step: 127
	action: tensor([[ 0.6499, -0.2125, -0.6569,  0.4782, -0.1504,  0.2601, -0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-3.1892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7422411862351113, distance: 0.5809830410439969 entropy 0.03264415264129639
LOSS epoch 35 actor 84.37610715989827 critic 968.4387463268167 
epoch: 36, step: 0
	action: tensor([[ 1.6729,  0.5128, -0.5433,  0.4237, -0.1568,  0.6230, -1.0697]],
       dtype=torch.float64)
	q_value: tensor([[-2.0066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8861367900528442, distance: 0.3861432119610994 entropy 0.03264415264129639
epoch: 36, step: 1
	action: tensor([[ 1.8650,  0.4028, -1.3881,  0.5033, -0.1074,  0.3407, -1.0971]],
       dtype=torch.float64)
	q_value: tensor([[-5.9559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 2
	action: tensor([[ 1.1960, -0.1521, -0.7745,  0.3223,  0.1277,  0.2700,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7783780371992854, distance: 0.5387199909137474 entropy 0.03264415264129639
epoch: 36, step: 3
	action: tensor([[ 1.5476,  0.1499, -1.5718,  0.4065, -0.3856,  0.6259, -1.6357]],
       dtype=torch.float64)
	q_value: tensor([[-3.3185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7566108059946963, distance: 0.5645564057875866 entropy 0.03264415264129639
epoch: 36, step: 4
	action: tensor([[ 1.9681,  0.4689, -1.1722,  0.2722,  0.0290,  0.7218, -0.8873]],
       dtype=torch.float64)
	q_value: tensor([[-7.4020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 5
	action: tensor([[ 1.1872,  0.1054, -0.7758,  0.2860,  0.0708,  0.4005, -0.2812]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9247182419750839, distance: 0.3139796995128756 entropy 0.03264415264129639
epoch: 36, step: 6
	action: tensor([[ 1.1750,  0.6705, -0.6910,  0.6578, -0.6334,  0.9315, -1.0276]],
       dtype=torch.float64)
	q_value: tensor([[-3.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081767388357858, distance: 0.3467632380834821 entropy 0.03264415264129639
epoch: 36, step: 7
	action: tensor([[ 2.1394,  1.0813, -1.3616,  0.8523, -0.6584,  0.9801, -1.2344]],
       dtype=torch.float64)
	q_value: tensor([[-5.9188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 8
	action: tensor([[ 0.9548,  0.3354, -0.0810,  0.4408, -0.3821,  0.5771, -0.7897]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9892179688783049, distance: 0.1188247704002329 entropy 0.03264415264129639
epoch: 36, step: 9
	action: tensor([[ 1.7208,  0.9449, -0.7488,  0.8901, -0.0292,  0.4657, -1.1262]],
       dtype=torch.float64)
	q_value: tensor([[-3.6214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 10
	action: tensor([[ 0.6258,  0.2932, -0.4565,  0.2983, -0.4696,  0.3562,  0.2314]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9023730495703924, distance: 0.3575539292785334 entropy 0.03264415264129639
epoch: 36, step: 11
	action: tensor([[ 1.3977,  0.7722, -0.6859,  0.1648, -0.0253,  0.9875, -1.1166]],
       dtype=torch.float64)
	q_value: tensor([[-2.7446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9035606517728884, distance: 0.35537250741590987 entropy 0.03264415264129639
epoch: 36, step: 12
	action: tensor([[ 2.0033,  0.9473, -1.0784,  0.6374, -0.7810,  0.4578, -0.9192]],
       dtype=torch.float64)
	q_value: tensor([[-6.2995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 13
	action: tensor([[ 0.9042,  0.5112, -0.4240,  0.2222, -0.1284,  0.4321, -0.7911]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9766610257151633, distance: 0.17482260046178447 entropy 0.03264415264129639
epoch: 36, step: 14
	action: tensor([[ 1.7186,  0.7437, -1.6722,  0.4585, -0.0328,  0.5982, -1.0646]],
       dtype=torch.float64)
	q_value: tensor([[-3.7173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9790825193810867, distance: 0.16550509996300192 entropy 0.03264415264129639
epoch: 36, step: 15
	action: tensor([[ 1.9455,  0.7591, -1.0796,  0.5927, -0.5232,  0.8195, -1.3328]],
       dtype=torch.float64)
	q_value: tensor([[-7.5351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 16
	action: tensor([[ 1.0140,  0.5526, -0.9682,  0.8176, -0.2939,  0.2318, -0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.894513210113234, distance: 0.37166847990261537 entropy 0.03264415264129639
epoch: 36, step: 17
	action: tensor([[ 2.0150,  0.7994, -1.1983,  0.5262, -0.5373,  0.5476, -0.7745]],
       dtype=torch.float64)
	q_value: tensor([[-4.2751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 18
	action: tensor([[ 0.9266,  0.7540, -0.4810,  0.4328, -0.2992,  0.7400, -0.6326]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 19
	action: tensor([[ 1.0217, -0.0645, -0.7437,  0.4074, -0.5309,  0.2307, -0.3244]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8711103707074693, distance: 0.41083334027239926 entropy 0.03264415264129639
epoch: 36, step: 20
	action: tensor([[ 1.4363,  0.2784, -0.7893,  0.6299, -0.8485,  0.5252, -1.1451]],
       dtype=torch.float64)
	q_value: tensor([[-3.3230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9817404114947581, distance: 0.1546330016794456 entropy 0.03264415264129639
epoch: 36, step: 21
	action: tensor([[ 1.6589,  0.7383, -0.9254,  0.5953, -0.3873,  0.7919, -0.9354]],
       dtype=torch.float64)
	q_value: tensor([[-6.0976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 22
	action: tensor([[ 0.9032, -0.2280, -0.4451, -0.0707, -0.4867,  0.2861, -0.2639]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5892624911284304, distance: 0.7333965719724134 entropy 0.03264415264129639
epoch: 36, step: 23
	action: tensor([[ 1.2107,  0.7627, -1.0959,  0.0535, -0.4143,  0.7782, -0.3621]],
       dtype=torch.float64)
	q_value: tensor([[-2.7707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9868554100286219, distance: 0.13119890638370532 entropy 0.03264415264129639
epoch: 36, step: 24
	action: tensor([[ 1.8792,  0.6718, -0.8805,  0.4214, -0.6071,  0.5616, -0.9153]],
       dtype=torch.float64)
	q_value: tensor([[-5.9290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 25
	action: tensor([[ 0.8453, -0.0883, -0.6746,  0.2138, -0.1312,  0.5708, -0.6235]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8511331785791306, distance: 0.4415253576664786 entropy 0.03264415264129639
epoch: 36, step: 26
	action: tensor([[ 1.2456,  0.2706, -0.9644,  0.4969, -0.4169,  0.2498, -0.5197]],
       dtype=torch.float64)
	q_value: tensor([[-2.9539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.980274217122602, distance: 0.16072144098598679 entropy 0.03264415264129639
epoch: 36, step: 27
	action: tensor([[ 1.6689,  0.6459, -1.2319,  0.6087, -0.6716,  0.4244, -1.0826]],
       dtype=torch.float64)
	q_value: tensor([[-4.6159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9783607595949094, distance: 0.1683362697436076 entropy 0.03264415264129639
epoch: 36, step: 28
	action: tensor([[ 2.0626,  0.7521, -1.1974,  0.6016, -0.2638,  1.0337, -0.9795]],
       dtype=torch.float64)
	q_value: tensor([[-7.2399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 29
	action: tensor([[ 1.0776,  0.5867, -0.8907,  0.1088,  0.0421,  0.3572, -0.1419]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9874636013809273, distance: 0.1281277175427576 entropy 0.03264415264129639
epoch: 36, step: 30
	action: tensor([[ 1.2118,  0.3967, -1.1903,  0.1047, -0.4844,  0.2727, -0.7937]],
       dtype=torch.float64)
	q_value: tensor([[-4.2820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9386651825494415, distance: 0.28340677741032533 entropy 0.03264415264129639
epoch: 36, step: 31
	action: tensor([[ 1.8135,  0.1948, -0.9534,  0.5289, -0.4771,  0.6596, -0.8162]],
       dtype=torch.float64)
	q_value: tensor([[-5.4899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 32
	action: tensor([[ 0.9380,  0.2664, -0.4234,  0.3683,  0.0791, -0.0658, -0.5544]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9705951063052849, distance: 0.1962304953274275 entropy 0.03264415264129639
epoch: 36, step: 33
	action: tensor([[ 1.3407,  0.4537, -1.0285,  0.6194, -0.4578,  0.4750, -0.9635]],
       dtype=torch.float64)
	q_value: tensor([[-3.0516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.040482845425796395 entropy 0.03264415264129639
epoch: 36, step: 34
	action: tensor([[ 0.6656,  0.5973, -1.0855,  0.1715, -0.1605,  0.7162, -1.0247]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8651112831383322, distance: 0.42028559487501954 entropy 0.03264415264129639
epoch: 36, step: 35
	action: tensor([[ 1.0781,  0.0925, -0.8321,  0.4163, -0.6940,  0.5564, -0.9281]],
       dtype=torch.float64)
	q_value: tensor([[-4.8513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9216548019064578, distance: 0.320304385388533 entropy 0.03264415264129639
epoch: 36, step: 36
	action: tensor([[ 1.4978,  0.8847, -1.2373,  0.7163, -0.4318,  0.4410, -1.0240]],
       dtype=torch.float64)
	q_value: tensor([[-4.6989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9697384509155228, distance: 0.19906837492043783 entropy 0.03264415264129639
epoch: 36, step: 37
	action: tensor([[ 1.7446,  0.4619, -1.1559,  1.1077, -0.1912,  0.7239, -1.3289]],
       dtype=torch.float64)
	q_value: tensor([[-7.0666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 38
	action: tensor([[ 0.7280,  0.3230, -0.0255, -0.0254,  0.2372,  0.7293, -0.2514]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9849557251451512, distance: 0.14035966496773117 entropy 0.03264415264129639
epoch: 36, step: 39
	action: tensor([[ 1.6265,  0.4725, -0.8370,  0.7475, -0.1474,  0.4316, -0.8944]],
       dtype=torch.float64)
	q_value: tensor([[-2.5923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9662093178565145, distance: 0.21035613023536184 entropy 0.03264415264129639
epoch: 36, step: 40
	action: tensor([[ 1.4295,  0.6606, -1.1257,  0.6513, -0.1876,  0.6168, -1.4471]],
       dtype=torch.float64)
	q_value: tensor([[-5.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9874437929157347, distance: 0.12822890336724582 entropy 0.03264415264129639
epoch: 36, step: 41
	action: tensor([[ 2.0986,  0.6771, -1.2138,  0.7614, -0.3338,  0.8335, -0.7002]],
       dtype=torch.float64)
	q_value: tensor([[-6.8399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 42
	action: tensor([[ 1.0134,  0.7629, -0.7803,  0.1083, -0.1839,  0.2276, -0.0884]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9647081933382786, distance: 0.21497781227013235 entropy 0.03264415264129639
epoch: 36, step: 43
	action: tensor([[ 1.3820,  0.6221, -1.4365,  0.7234, -0.5139,  1.1556, -0.2819]],
       dtype=torch.float64)
	q_value: tensor([[-4.3570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9358602622145706, distance: 0.2898146154669213 entropy 0.03264415264129639
epoch: 36, step: 44
	action: tensor([[ 2.1628,  0.0113, -1.4366,  0.8000, -0.4410,  0.6575, -0.9890]],
       dtype=torch.float64)
	q_value: tensor([[-6.8145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 45
	action: tensor([[ 0.6531,  0.3142, -0.7669,  0.3965, -0.2378,  0.6530, -0.8082]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8446895246222367, distance: 0.4509797772577321 entropy 0.03264415264129639
epoch: 36, step: 46
	action: tensor([[ 1.6496,  0.0219, -0.5507,  0.6199, -0.0830,  0.8454, -0.5497]],
       dtype=torch.float64)
	q_value: tensor([[-3.5792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081480948279825, distance: 0.34681731977407876 entropy 0.03264415264129639
epoch: 36, step: 47
	action: tensor([[ 2.0995,  1.3042, -1.3190,  0.4566, -0.7477,  1.0818, -1.3550]],
       dtype=torch.float64)
	q_value: tensor([[-4.9751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 48
	action: tensor([[ 0.6772,  0.4418, -1.0644,  0.0429, -0.1005,  0.1380, -0.6094]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9460572319423723, distance: 0.2657806183316554 entropy 0.03264415264129639
epoch: 36, step: 49
	action: tensor([[ 1.0115,  0.6489, -0.6864,  0.5018, -0.1196,  0.4525, -0.7902]],
       dtype=torch.float64)
	q_value: tensor([[-3.7425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9189663325847078, distance: 0.32575376250374094 entropy 0.03264415264129639
epoch: 36, step: 50
	action: tensor([[ 1.1201,  0.8870, -0.7484,  0.6576, -0.3623,  0.1819, -0.4238]],
       dtype=torch.float64)
	q_value: tensor([[-4.4338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 51
	action: tensor([[ 0.4154, -0.2726, -0.3309,  0.4001, -0.2895,  0.4362, -0.4550]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6398302580177996, distance: 0.686768402708915 entropy 0.03264415264129639
epoch: 36, step: 52
	action: tensor([[ 1.1005,  0.5395, -0.7684,  0.5102, -0.0433,  0.5096, -0.2628]],
       dtype=torch.float64)
	q_value: tensor([[-1.8178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.958773227864408, distance: 0.23235197038218533 entropy 0.03264415264129639
epoch: 36, step: 53
	action: tensor([[ 1.6715,  0.5519, -0.9246,  0.1061, -0.2457,  0.6923, -0.5751]],
       dtype=torch.float64)
	q_value: tensor([[-4.1456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8816396395613579, distance: 0.39369493979092607 entropy 0.03264415264129639
epoch: 36, step: 54
	action: tensor([[ 1.8591,  0.7010, -0.8598,  0.8083, -0.6192,  0.8141, -1.0688]],
       dtype=torch.float64)
	q_value: tensor([[-6.1280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 55
	action: tensor([[ 0.8316,  0.3754, -0.4560,  0.1921, -0.1963, -0.0115, -0.2851]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9697071447563677, distance: 0.19917131834949808 entropy 0.03264415264129639
epoch: 36, step: 56
	action: tensor([[ 1.3382,  0.1956, -1.1927,  0.5786,  0.0196,  0.5590, -0.6723]],
       dtype=torch.float64)
	q_value: tensor([[-2.8627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9657648637132052, distance: 0.21173503440308136 entropy 0.03264415264129639
epoch: 36, step: 57
	action: tensor([[ 1.6364,  0.9672, -1.2690,  0.3342,  0.2930,  0.4269, -1.3611]],
       dtype=torch.float64)
	q_value: tensor([[-4.9841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 58
	action: tensor([[ 0.5805,  0.5691, -0.8346,  0.1302, -0.3629,  0.5963, -0.9836]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8564914929215542, distance: 0.4335064030150723 entropy 0.03264415264129639
epoch: 36, step: 59
	action: tensor([[ 1.7201,  0.8076, -0.6984,  0.4809, -0.4348,  0.4660, -1.0753]],
       dtype=torch.float64)
	q_value: tensor([[-4.3488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 60
	action: tensor([[ 1.2266,  0.2946, -0.3270,  0.4639, -0.2818,  0.4730, -0.2204]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.981355221299376, distance: 0.15625549886599077 entropy 0.03264415264129639
epoch: 36, step: 61
	action: tensor([[ 1.6207,  0.7915, -1.0348,  0.6221, -0.2081,  0.5817, -0.9744]],
       dtype=torch.float64)
	q_value: tensor([[-3.8688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9563658914796707, distance: 0.2390395414208416 entropy 0.03264415264129639
epoch: 36, step: 62
	action: tensor([[ 1.7154,  0.8786, -1.4562,  0.7094, -0.5410,  0.8300, -1.1061]],
       dtype=torch.float64)
	q_value: tensor([[-6.7906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 63
	action: tensor([[ 0.6376,  0.0990, -0.8139,  0.4931, -0.4222,  0.4025, -0.7219]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.793521071055641, distance: 0.5199894862399775 entropy 0.03264415264129639
epoch: 36, step: 64
	action: tensor([[ 1.4650,  0.4966, -0.6558,  0.4301, -0.2037,  0.4477, -0.5667]],
       dtype=torch.float64)
	q_value: tensor([[-3.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9373070161874129, distance: 0.28652740291123124 entropy 0.03264415264129639
epoch: 36, step: 65
	action: tensor([[ 1.7362,  0.6708, -1.3607,  0.3166, -0.5710,  0.2897, -1.6217]],
       dtype=torch.float64)
	q_value: tensor([[-5.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 66
	action: tensor([[ 0.5278,  0.0197, -0.2438,  0.5887, -0.0302,  0.1162, -0.8701]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8706372654600085, distance: 0.4115866567898252 entropy 0.03264415264129639
epoch: 36, step: 67
	action: tensor([[ 1.2881,  0.3788, -0.4992,  0.5455, -0.6083,  0.4530, -0.9378]],
       dtype=torch.float64)
	q_value: tensor([[-2.3005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.986814176762874, distance: 0.13140452419137053 entropy 0.03264415264129639
epoch: 36, step: 68
	action: tensor([[ 1.5459,  0.3659, -1.1011,  0.9810,  0.0518,  0.7795, -0.9229]],
       dtype=torch.float64)
	q_value: tensor([[-5.0742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9808859165076584, distance: 0.15820981861682645 entropy 0.03264415264129639
epoch: 36, step: 69
	action: tensor([[ 2.1698,  0.8800, -1.3088,  0.7519, -0.4424,  0.8449, -1.2201]],
       dtype=torch.float64)
	q_value: tensor([[-6.0206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 70
	action: tensor([[ 1.0889,  0.0541, -0.8662,  0.3468, -0.2022,  0.3602, -0.1146]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.918639304664421, distance: 0.3264104235788996 entropy 0.03264415264129639
epoch: 36, step: 71
	action: tensor([[ 0.9904,  0.5300, -0.9043,  0.7593, -0.0120,  1.0832, -0.4585]],
       dtype=torch.float64)
	q_value: tensor([[-3.5438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8098848475318849, distance: 0.4989592283045754 entropy 0.03264415264129639
epoch: 36, step: 72
	action: tensor([[ 1.9291,  0.2222, -1.2849,  0.7588, -0.2869,  0.6536, -0.8156]],
       dtype=torch.float64)
	q_value: tensor([[-4.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 73
	action: tensor([[ 0.8824,  0.0919, -0.6475,  0.0922, -0.3479,  0.5278, -0.5486]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9024897091962503, distance: 0.3573402353349877 entropy 0.03264415264129639
epoch: 36, step: 74
	action: tensor([[ 1.5002,  0.4836, -0.9240,  0.5050,  0.1689,  0.5521, -1.1470]],
       dtype=torch.float64)
	q_value: tensor([[-3.3992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9652953738574868, distance: 0.21318192446896603 entropy 0.03264415264129639
epoch: 36, step: 75
	action: tensor([[ 2.2945,  0.5795, -1.0442,  0.7047, -0.5781,  0.6886, -1.0021]],
       dtype=torch.float64)
	q_value: tensor([[-5.8540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 76
	action: tensor([[ 1.0765,  0.5156, -0.5269,  0.5675, -0.2374,  0.0948, -0.6497]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9733154323710006, distance: 0.1869333454649511 entropy 0.03264415264129639
epoch: 36, step: 77
	action: tensor([[ 1.2094,  0.7297, -1.5430,  0.3442, -0.4958,  0.4750, -0.7858]],
       dtype=torch.float64)
	q_value: tensor([[-4.0765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06065353489735585 entropy 0.03264415264129639
epoch: 36, step: 78
	action: tensor([[ 0.9990,  0.2111, -0.5796,  0.6594, -0.1785,  0.7804, -0.5116]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9630101877412195, distance: 0.22008870589427748 entropy 0.03264415264129639
epoch: 36, step: 79
	action: tensor([[ 1.9044,  0.8720, -1.2748,  0.5645, -0.0675,  1.2269, -1.4404]],
       dtype=torch.float64)
	q_value: tensor([[-3.7784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 80
	action: tensor([[ 0.3630,  0.3482, -0.3159,  0.0119, -0.3444,  0.0189, -0.4839]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8076430066982243, distance: 0.501892473916735 entropy 0.03264415264129639
epoch: 36, step: 81
	action: tensor([[ 1.2400,  0.5002, -0.6250,  0.5739, -0.3709,  0.4146, -0.4250]],
       dtype=torch.float64)
	q_value: tensor([[-2.2168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9766429660890438, distance: 0.17489022597598655 entropy 0.03264415264129639
epoch: 36, step: 82
	action: tensor([[ 1.6290,  0.4487, -1.0371,  0.8981, -0.7160,  0.9556, -1.1294]],
       dtype=torch.float64)
	q_value: tensor([[-4.5842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9755446028730106, distance: 0.17895508901187238 entropy 0.03264415264129639
epoch: 36, step: 83
	action: tensor([[ 2.6603,  0.4398, -0.9917,  0.7178, -0.4159,  0.9511, -1.4558]],
       dtype=torch.float64)
	q_value: tensor([[-7.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 84
	action: tensor([[ 0.7001, -0.0542, -1.0154,  0.1066, -0.3344,  0.6496, -0.7982]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7089496543856049, distance: 0.617363220748312 entropy 0.03264415264129639
epoch: 36, step: 85
	action: tensor([[ 1.3489,  0.7891, -1.1749,  0.2894,  0.0167,  0.3944, -0.8191]],
       dtype=torch.float64)
	q_value: tensor([[-3.6767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9496721740908294, distance: 0.2567206349403309 entropy 0.03264415264129639
epoch: 36, step: 86
	action: tensor([[ 1.7415,  0.6668, -1.1601,  0.6011, -0.6111,  0.6153, -0.9345]],
       dtype=torch.float64)
	q_value: tensor([[-5.9777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 87
	action: tensor([[ 0.6285,  0.4551, -0.8520, -0.0165, -0.4994,  0.5019, -0.1718]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9119013544887453, distance: 0.3396575783760516 entropy 0.03264415264129639
epoch: 36, step: 88
	action: tensor([[ 1.0738,  0.6927, -0.9100,  0.5607, -0.0869,  0.8403, -1.1506]],
       dtype=torch.float64)
	q_value: tensor([[-3.9417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8838714328583269, distance: 0.3899655359658881 entropy 0.03264415264129639
epoch: 36, step: 89
	action: tensor([[ 2.3072,  0.5300, -0.9870,  0.5515,  0.3713,  0.4488, -1.0801]],
       dtype=torch.float64)
	q_value: tensor([[-5.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 90
	action: tensor([[ 0.8135,  0.4102, -0.3408,  0.5486,  0.2077,  0.2152, -0.4905]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9606253469709145, distance: 0.22707277279005236 entropy 0.03264415264129639
epoch: 36, step: 91
	action: tensor([[ 1.7387,  0.1500, -1.0298,  0.7108,  0.0755,  0.4690, -0.8973]],
       dtype=torch.float64)
	q_value: tensor([[-2.8347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9464672989270317, distance: 0.26476847360736033 entropy 0.03264415264129639
epoch: 36, step: 92
	action: tensor([[ 2.2259,  0.6593, -1.3051,  1.0439, -0.3359,  0.9883, -1.1795]],
       dtype=torch.float64)
	q_value: tensor([[-5.5374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 93
	action: tensor([[ 1.1221,  0.2908, -0.5872,  0.9145,  0.0953,  0.9187, -0.4942]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.900150607640606, distance: 0.36160081980094316 entropy 0.03264415264129639
epoch: 36, step: 94
	action: tensor([[ 2.1750,  0.5421, -1.5700,  0.6878, -0.6128,  1.4188, -1.3529]],
       dtype=torch.float64)
	q_value: tensor([[-4.1766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 95
	action: tensor([[ 1.5307,  0.3946, -0.5041,  0.0083,  0.0081, -0.0406, -0.9163]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6457603430093337, distance: 0.6810912212728628 entropy 0.03264415264129639
epoch: 36, step: 96
	action: tensor([[ 1.7247,  0.5981, -1.4110,  0.2297, -0.1304,  0.8547, -0.6011]],
       dtype=torch.float64)
	q_value: tensor([[-5.1752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 97
	action: tensor([[ 0.8186,  0.1757, -0.4010, -0.3027, -0.1402,  0.8631, -0.5755]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8994896669260565, distance: 0.3627956318067742 entropy 0.03264415264129639
epoch: 36, step: 98
	action: tensor([[ 1.3457,  0.5139, -0.8685,  0.1697, -0.2717,  0.1461, -1.1245]],
       dtype=torch.float64)
	q_value: tensor([[-3.6311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9203068241740187, distance: 0.32304815149359256 entropy 0.03264415264129639
epoch: 36, step: 99
	action: tensor([[ 1.5499,  0.5058, -1.1438,  0.8073, -0.4824,  0.6940, -0.8837]],
       dtype=torch.float64)
	q_value: tensor([[-5.6263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08635185976536405 entropy 0.03264415264129639
epoch: 36, step: 100
	action: tensor([[ 0.6276,  0.3898, -0.7311,  0.5947,  0.0723,  0.3837, -0.5906]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8309512015546303, distance: 0.4705033688883085 entropy 0.03264415264129639
epoch: 36, step: 101
	action: tensor([[ 1.5254,  0.4292, -0.8034,  0.6477,  0.0551,  0.5527, -0.7046]],
       dtype=torch.float64)
	q_value: tensor([[-3.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9630237425319697, distance: 0.22004837682089468 entropy 0.03264415264129639
epoch: 36, step: 102
	action: tensor([[ 1.7206,  0.9064, -1.2284,  1.1893, -0.7344,  0.9899, -0.9613]],
       dtype=torch.float64)
	q_value: tensor([[-5.3971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 103
	action: tensor([[ 0.6088,  0.5255, -0.8330,  0.5864, -0.0946,  0.2600, -0.4999]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7939501678598196, distance: 0.5194488938622347 entropy 0.03264415264129639
epoch: 36, step: 104
	action: tensor([[ 1.1710,  0.3302, -1.5116,  0.9364, -0.5460,  0.6729, -1.0763]],
       dtype=torch.float64)
	q_value: tensor([[-3.2696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8074265387930886, distance: 0.5021747955063434 entropy 0.03264415264129639
epoch: 36, step: 105
	action: tensor([[ 1.8146,  0.6569, -1.8763,  0.2987, -0.7791,  1.0703, -0.9959]],
       dtype=torch.float64)
	q_value: tensor([[-6.1823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 106
	action: tensor([[ 0.9316,  0.6524, -0.7092,  0.3117,  0.3903,  0.3453, -0.2864]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.945474284842198, distance: 0.26721287425207874 entropy 0.03264415264129639
epoch: 36, step: 107
	action: tensor([[ 1.4582,  0.4569, -1.3363,  0.4745,  0.1084,  0.7205, -0.6550]],
       dtype=torch.float64)
	q_value: tensor([[-3.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09417280158771661 entropy 0.03264415264129639
epoch: 36, step: 108
	action: tensor([[ 0.6797,  0.1102, -0.5557,  0.5880, -0.7323,  0.3550, -0.8406]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8613635689481728, distance: 0.4260841490607939 entropy 0.03264415264129639
epoch: 36, step: 109
	action: tensor([[ 1.3349,  0.2073, -1.3874,  0.5882,  0.1204,  0.5988, -1.0374]],
       dtype=torch.float64)
	q_value: tensor([[-3.4779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9492364611664041, distance: 0.25782951893901185 entropy 0.03264415264129639
epoch: 36, step: 110
	action: tensor([[ 1.7363,  0.5653, -1.7520,  0.9268, -0.6506,  0.7649, -0.8465]],
       dtype=torch.float64)
	q_value: tensor([[-5.5425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 111
	action: tensor([[ 0.9638,  0.8220, -0.5564,  0.6416,  0.2032,  0.5416, -0.6575]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 112
	action: tensor([[ 0.7028,  0.6322, -0.4515,  0.6661, -0.0257,  0.4668, -0.7594]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 113
	action: tensor([[ 1.2406,  0.0570, -0.7313, -0.1109, -0.2357,  0.5778, -0.5224]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7588862743001021, distance: 0.5619111633329632 entropy 0.03264415264129639
epoch: 36, step: 114
	action: tensor([[ 1.6249,  0.5858, -1.2098,  0.8478, -0.3842,  0.7753, -1.0712]],
       dtype=torch.float64)
	q_value: tensor([[-4.2372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09662481661069762 entropy 0.03264415264129639
epoch: 36, step: 115
	action: tensor([[ 1.1907,  0.3888, -0.7700,  0.5389, -0.5929,  0.5859, -0.8321]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09316360946674071 entropy 0.03264415264129639
epoch: 36, step: 116
	action: tensor([[ 1.2323,  0.2596, -0.5691,  0.6300, -0.0407,  0.5687, -0.4039]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07622934884734774 entropy 0.03264415264129639
epoch: 36, step: 117
	action: tensor([[ 0.5888,  0.0648,  0.1992,  0.5291,  0.1161,  0.5542, -0.2625]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.983677160314602, distance: 0.14620241649531648 entropy 0.03264415264129639
epoch: 36, step: 118
	action: tensor([[ 1.3579,  0.1314, -1.4680,  0.7641, -0.5415,  0.5365, -0.9738]],
       dtype=torch.float64)
	q_value: tensor([[-2.1463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8383534887950264, distance: 0.460086892911842 entropy 0.03264415264129639
epoch: 36, step: 119
	action: tensor([[ 1.8182,  0.5892, -1.3923,  0.5831, -0.3611,  0.7526, -1.4838]],
       dtype=torch.float64)
	q_value: tensor([[-6.0404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 120
	action: tensor([[ 1.0677e+00,  2.7662e-04, -1.9768e-01,  4.6497e-01, -3.1485e-02,
          5.4946e-03, -3.2213e-01]], dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9031621104878437, distance: 0.3561060492519412 entropy 0.03264415264129639
epoch: 36, step: 121
	action: tensor([[ 1.4067,  0.6273, -1.2026,  0.6590, -0.5277,  0.7088, -0.6565]],
       dtype=torch.float64)
	q_value: tensor([[-2.8780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06734692433331069 entropy 0.03264415264129639
epoch: 36, step: 122
	action: tensor([[ 0.8932,  0.7846, -0.8354, -0.0015, -0.2778,  0.2527, -0.7529]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.969346608451542, distance: 0.20035305067503664 entropy 0.03264415264129639
epoch: 36, step: 123
	action: tensor([[ 1.3736,  0.3250, -0.2424,  0.2944,  0.1676,  0.6447, -0.3945]],
       dtype=torch.float64)
	q_value: tensor([[-4.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8976587539673976, distance: 0.3660850918861431 entropy 0.03264415264129639
epoch: 36, step: 124
	action: tensor([[ 1.9935,  1.3197, -1.2784,  0.4959, -0.7147,  1.2497, -0.8707]],
       dtype=torch.float64)
	q_value: tensor([[-4.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 125
	action: tensor([[ 0.2011,  0.4394, -0.1721,  0.4837, -0.1631,  0.2792, -0.8555]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7227114090630034, distance: 0.6025910731523357 entropy 0.03264415264129639
epoch: 36, step: 126
	action: tensor([[ 1.0333,  0.7618, -0.5522,  0.5783, -0.8997,  0.7850, -0.9501]],
       dtype=torch.float64)
	q_value: tensor([[-2.3591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8679306706015532, distance: 0.4158700835054536 entropy 0.03264415264129639
epoch: 36, step: 127
	action: tensor([[ 1.5339,  0.9675, -1.0897,  0.6702, -0.7286,  0.5798, -1.4170]],
       dtype=torch.float64)
	q_value: tensor([[-5.6928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.94619069692566, distance: 0.2654516180256908 entropy 0.03264415264129639
LOSS epoch 36 actor 654.5625531807847 critic 2682.7738662276633 
epoch: 37, step: 0
	action: tensor([[ 5.7327,  1.7930, -3.4888,  2.3490, -1.6850,  3.3395, -2.2301]],
       dtype=torch.float64)
	q_value: tensor([[-9.4162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 1
	action: tensor([[ 1.4772,  0.6223, -0.8448,  0.6944, -0.4911,  1.1797, -1.2741]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9543192661321734, distance: 0.24458128895506623 entropy 0.03264415264129639
epoch: 37, step: 2
	action: tensor([[ 6.1800,  2.5523, -3.6950,  2.7247, -1.7937,  3.1206, -3.0784]],
       dtype=torch.float64)
	q_value: tensor([[-8.5183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 3
	action: tensor([[ 1.4405,  0.7171, -1.0643,  0.7163, -0.3621,  1.0833, -1.0940]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 4
	action: tensor([[ 1.7862,  0.1795, -1.5088,  0.4801, -0.9086,  1.1082, -0.8533]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6604764252004427, distance: 0.6667939653595844 entropy 0.03264415264129639
epoch: 37, step: 5
	action: tensor([[ 6.1800,  2.2143, -3.7461,  2.6445, -1.8711,  2.8565, -2.7087]],
       dtype=torch.float64)
	q_value: tensor([[-8.9367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 6
	action: tensor([[ 2.4462,  0.4049, -0.9685,  0.7570, -0.4060,  1.2109, -0.8242]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 7
	action: tensor([[ 1.7701,  0.4166, -1.4016,  0.6842, -0.8962,  0.3923, -0.7517]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8929070365575259, distance: 0.37448735813874523 entropy 0.03264415264129639
epoch: 37, step: 8
	action: tensor([[ 5.0481,  2.5494, -3.2199,  3.1419, -1.8513,  2.2180, -2.4114]],
       dtype=torch.float64)
	q_value: tensor([[-8.2588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 9
	action: tensor([[ 1.6651,  0.7618, -1.7206,  1.0849, -0.4232,  1.0159, -0.6791]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8795156681611229, distance: 0.3972116522156931 entropy 0.03264415264129639
epoch: 37, step: 10
	action: tensor([[ 6.1800,  2.3119, -4.1630,  3.5193, -1.8350,  2.6971, -3.0506]],
       dtype=torch.float64)
	q_value: tensor([[-9.4102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 11
	action: tensor([[ 1.4802,  0.9408, -1.4386,  0.8714, -0.8258,  1.2202, -0.7907]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9472550556105495, distance: 0.2628131627468813 entropy 0.03264415264129639
epoch: 37, step: 12
	action: tensor([[ 5.7110,  2.3589, -3.9612,  2.9315, -1.8549,  2.5304, -3.0898]],
       dtype=torch.float64)
	q_value: tensor([[-9.7324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 13
	action: tensor([[ 1.9524,  0.8425, -1.1566,  0.7046, -0.5611,  0.6071, -0.8175]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9823267840381368, distance: 0.15212986696829328 entropy 0.03264415264129639
epoch: 37, step: 14
	action: tensor([[ 5.4800,  2.0187, -3.0761,  2.6197, -1.9413,  2.8369, -2.6190]],
       dtype=torch.float64)
	q_value: tensor([[-8.5385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 15
	action: tensor([[ 1.9379,  0.5002, -0.9417,  0.7407, -0.8949,  0.7910, -0.6205]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9829582948356874, distance: 0.14938714142233359 entropy 0.03264415264129639
epoch: 37, step: 16
	action: tensor([[ 5.6669,  2.1773, -3.6205,  2.7526, -2.1683,  3.0624, -2.7694]],
       dtype=torch.float64)
	q_value: tensor([[-8.0126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 17
	action: tensor([[ 1.6456,  0.2607, -0.9022,  0.6412, -0.4894,  0.8631, -0.8347]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.968049152401085, distance: 0.20454924701781474 entropy 0.03264415264129639
epoch: 37, step: 18
	action: tensor([[ 5.2605,  2.1195, -3.8570,  2.8836, -1.9604,  3.1561, -1.9314]],
       dtype=torch.float64)
	q_value: tensor([[-7.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 19
	action: tensor([[ 1.9157,  0.5740, -1.3041,  1.0729, -0.7059,  0.9730, -1.1606]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9285449296836645, distance: 0.3058955919673889 entropy 0.03264415264129639
epoch: 37, step: 20
	action: tensor([[ 6.1800,  1.6601, -4.3578,  2.8509, -1.8504,  3.5461, -3.3047]],
       dtype=torch.float64)
	q_value: tensor([[-9.2901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 21
	action: tensor([[ 2.1093,  0.2470, -1.1357,  1.1019, -0.6434,  1.2334, -0.8979]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8773982546623811, distance: 0.40068678536602975 entropy 0.03264415264129639
epoch: 37, step: 22
	action: tensor([[ 6.1800,  2.3243, -4.4822,  3.3048, -2.6394,  3.4043, -2.7857]],
       dtype=torch.float64)
	q_value: tensor([[-8.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 23
	action: tensor([[ 1.9300,  0.9495, -1.0069,  0.8816, -0.1889,  1.1040, -0.9869]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 24
	action: tensor([[ 1.5301,  0.8613, -1.3797,  1.0359, -0.3275,  0.5498, -1.2005]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9616768575223017, distance: 0.22402023587026001 entropy 0.03264415264129639
epoch: 37, step: 25
	action: tensor([[ 5.8174,  1.8010, -3.9637,  3.1006, -2.1709,  2.8386, -2.9248]],
       dtype=torch.float64)
	q_value: tensor([[-8.9003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 26
	action: tensor([[ 1.4284,  0.5846, -1.1169,  0.9839, -0.2955,  0.6184, -0.9301]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.945708908744416, distance: 0.2666373465752735 entropy 0.03264415264129639
epoch: 37, step: 27
	action: tensor([[ 5.4657,  1.5430, -3.5530,  2.6771, -1.7436,  3.3388, -2.4368]],
       dtype=torch.float64)
	q_value: tensor([[-7.4877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 28
	action: tensor([[ 1.7515,  0.4559, -1.0648,  0.6824, -0.6564,  0.7484, -0.7767]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9735176220076331, distance: 0.1862237995293957 entropy 0.03264415264129639
epoch: 37, step: 29
	action: tensor([[ 5.3251,  2.0194, -3.6888,  2.4100, -2.0244,  2.5195, -2.9966]],
       dtype=torch.float64)
	q_value: tensor([[-7.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 30
	action: tensor([[ 2.0979,  1.0711, -1.3491,  0.9804, -0.9575,  1.1497, -1.4164]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 31
	action: tensor([[ 2.0922,  0.5620, -0.6144,  0.9540, -0.5445,  0.8685, -1.0945]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9349423461018103, distance: 0.29188104479101 entropy 0.03264415264129639
epoch: 37, step: 32
	action: tensor([[ 6.1681,  2.0539, -4.3142,  3.2843, -1.8117,  3.3077, -2.6535]],
       dtype=torch.float64)
	q_value: tensor([[-7.9514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 33
	action: tensor([[ 1.7496,  0.5395, -1.1779,  0.6093, -0.8775,  1.3851, -0.9322]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9291099898780877, distance: 0.30468369406330537 entropy 0.03264415264129639
epoch: 37, step: 34
	action: tensor([[ 6.1527,  2.1935, -3.9511,  3.3730, -2.2400,  3.3843, -2.7032]],
       dtype=torch.float64)
	q_value: tensor([[-9.4077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 35
	action: tensor([[ 1.3325,  0.8069, -1.0401,  0.9775, -0.6687,  1.1004, -0.8637]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 36
	action: tensor([[ 2.0858,  0.6246, -1.4911,  0.9072, -0.5036,  0.5309, -0.6787]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9725229432768008, distance: 0.1896888477332209 entropy 0.03264415264129639
epoch: 37, step: 37
	action: tensor([[ 5.6869,  2.5525, -3.8618,  3.1035, -1.5171,  2.6932, -2.9240]],
       dtype=torch.float64)
	q_value: tensor([[-8.3628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 38
	action: tensor([[ 1.9300,  0.4336, -1.0503,  1.3481, -1.0588,  0.9460, -0.5414]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9098420201943577, distance: 0.3436044495645899 entropy 0.03264415264129639
epoch: 37, step: 39
	action: tensor([[ 6.1800,  2.1361, -3.6278,  3.2231, -1.7991,  3.0026, -2.5859]],
       dtype=torch.float64)
	q_value: tensor([[-8.3993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 40
	action: tensor([[ 1.7754,  0.7851, -1.3262,  0.8380, -0.1505,  1.0850, -0.6639]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9536973921483314, distance: 0.2462404631082013 entropy 0.03264415264129639
epoch: 37, step: 41
	action: tensor([[ 6.1800,  2.1510, -3.8872,  3.0862, -1.9714,  2.9454, -2.9995]],
       dtype=torch.float64)
	q_value: tensor([[-8.6155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 42
	action: tensor([[ 1.9115,  0.6503, -1.1134,  0.9218, -0.8884,  1.0196, -1.2053]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9761881528103302, distance: 0.17658476701594317 entropy 0.03264415264129639
epoch: 37, step: 43
	action: tensor([[ 6.1800,  2.1499, -4.1836,  3.1571, -1.8562,  3.0771, -2.8561]],
       dtype=torch.float64)
	q_value: tensor([[-9.4075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 44
	action: tensor([[ 1.8629,  0.3717, -0.8890,  0.9834, -0.5796,  1.0364, -0.8084]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9608607484646433, distance: 0.22639297756476465 entropy 0.03264415264129639
epoch: 37, step: 45
	action: tensor([[ 6.1300,  2.5099, -4.0622,  2.8358, -2.0782,  3.7258, -2.9318]],
       dtype=torch.float64)
	q_value: tensor([[-7.9217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 46
	action: tensor([[ 1.5696,  1.1452, -1.3106,  0.8152, -0.6316,  1.0665, -0.7698]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 47
	action: tensor([[ 1.8987,  0.8672, -1.1622,  0.8786, -0.4766,  0.8176, -1.0605]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9734125511596345, distance: 0.18659286236168712 entropy 0.03264415264129639
epoch: 37, step: 48
	action: tensor([[ 6.1800,  1.9086, -4.2519,  2.6504, -2.2661,  2.8844, -2.9675]],
       dtype=torch.float64)
	q_value: tensor([[-8.9992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 49
	action: tensor([[ 1.7255,  0.7064, -1.3153,  0.6356, -0.5801,  1.0335, -1.1743]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9857245086586284, distance: 0.13672635175302844 entropy 0.03264415264129639
epoch: 37, step: 50
	action: tensor([[ 5.6211,  2.4849, -4.0918,  2.8072, -2.1547,  3.1590, -2.7372]],
       dtype=torch.float64)
	q_value: tensor([[-9.4540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 51
	action: tensor([[ 1.9388,  0.4676, -1.3656,  0.5916, -0.9624,  0.9116, -0.4402]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.894626591434797, distance: 0.37146868430228636 entropy 0.03264415264129639
epoch: 37, step: 52
	action: tensor([[ 5.7745,  2.1258, -3.6196,  2.3670, -1.8739,  3.1013, -2.8097]],
       dtype=torch.float64)
	q_value: tensor([[-8.5732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 53
	action: tensor([[ 1.8754,  0.3280, -1.3135,  0.6817, -0.7908,  1.2717, -0.7200]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8483105835168151, distance: 0.4456914816264633 entropy 0.03264415264129639
epoch: 37, step: 54
	action: tensor([[ 6.1800,  2.1912, -3.8587,  3.3021, -2.2545,  3.1204, -2.6425]],
       dtype=torch.float64)
	q_value: tensor([[-8.8215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 55
	action: tensor([[ 1.7334,  0.4285, -1.4844,  0.6302, -0.8084,  0.3753, -1.2559]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9032264570831963, distance: 0.3559877173689934 entropy 0.03264415264129639
epoch: 37, step: 56
	action: tensor([[ 5.5613,  2.4175, -3.4966,  2.9219, -2.0689,  2.7697, -2.4981]],
       dtype=torch.float64)
	q_value: tensor([[-8.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 57
	action: tensor([[ 2.1563e+00,  6.0820e-01, -9.9818e-01,  4.9949e-01,  1.3653e-03,
          1.2755e+00, -8.9238e-01]], dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9603057846018482, distance: 0.22799236535766718 entropy 0.03264415264129639
epoch: 37, step: 58
	action: tensor([[ 5.9571,  1.9843, -4.1692,  2.9955, -2.0141,  2.8453, -2.7750]],
       dtype=torch.float64)
	q_value: tensor([[-8.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 59
	action: tensor([[ 1.9475,  0.5171, -0.8882,  0.9467, -0.7542,  1.0199, -0.6430]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9722365488854776, distance: 0.19067485208560028 entropy 0.03264415264129639
epoch: 37, step: 60
	action: tensor([[ 6.1800,  1.9127, -3.7439,  2.5076, -2.7678,  3.3776, -2.9186]],
       dtype=torch.float64)
	q_value: tensor([[-8.1077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 61
	action: tensor([[ 2.4191,  0.5683, -1.3687,  0.8484, -0.3603,  0.7547, -1.2019]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 62
	action: tensor([[ 2.4649,  0.4336, -1.4380,  0.7059, -0.8415,  1.0456, -0.9057]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 63
	action: tensor([[ 2.1365,  0.4558, -1.3581,  0.4163, -0.3481,  1.3111, -0.9587]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9242427262985194, distance: 0.31496976170492164 entropy 0.03264415264129639
epoch: 37, step: 64
	action: tensor([[ 6.1800,  1.9538, -4.3124,  3.0371, -2.1475,  2.9101, -2.9643]],
       dtype=torch.float64)
	q_value: tensor([[-8.9239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 65
	action: tensor([[ 2.2094,  0.2431, -1.4843,  0.6993, -1.1247,  0.9225, -0.9662]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7149708607435468, distance: 0.6109438873357493 entropy 0.03264415264129639
epoch: 37, step: 66
	action: tensor([[ 6.1800,  2.1564, -3.9792,  3.2358, -2.2022,  2.7236, -2.5317]],
       dtype=torch.float64)
	q_value: tensor([[-9.1241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 67
	action: tensor([[ 2.1925,  1.1747, -1.1025,  0.6666, -0.9129,  0.9918, -1.2452]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 68
	action: tensor([[ 1.7772,  0.9786, -1.3787,  0.8365, -0.4204,  1.3119, -0.9357]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 69
	action: tensor([[ 1.8527,  0.7559, -1.3425,  0.7270, -0.3652,  1.0649, -0.8199]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9792490086666501, distance: 0.16484512864076611 entropy 0.03264415264129639
epoch: 37, step: 70
	action: tensor([[ 5.6928,  2.2003, -4.0194,  2.5912, -2.0627,  3.5579, -2.8077]],
       dtype=torch.float64)
	q_value: tensor([[-8.9482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 71
	action: tensor([[ 2.1894,  0.9970, -1.1338,  0.9797, -0.5791,  1.0624, -0.7921]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 72
	action: tensor([[ 1.7930,  0.4318, -1.4430,  0.8787, -0.4917,  1.2250, -0.7781]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8787612148519888, distance: 0.398453348836274 entropy 0.03264415264129639
epoch: 37, step: 73
	action: tensor([[ 6.1800,  2.5293, -4.1334,  2.9796, -2.4523,  3.2078, -3.2429]],
       dtype=torch.float64)
	q_value: tensor([[-8.8670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 74
	action: tensor([[ 1.5398,  0.8895, -1.0740,  0.7537, -1.0917,  0.8438, -1.2259]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 75
	action: tensor([[ 1.6906,  0.6249, -1.4862,  0.7501, -0.1769,  0.8914, -0.8300]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9765352836315668, distance: 0.17529290881204754 entropy 0.03264415264129639
epoch: 37, step: 76
	action: tensor([[ 5.6535,  1.9116, -4.1545,  2.9363, -1.9966,  2.8903, -2.7489]],
       dtype=torch.float64)
	q_value: tensor([[-8.5685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 77
	action: tensor([[ 1.7342,  1.1640, -1.0242,  0.9026, -0.4021,  0.9184, -0.8231]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 78
	action: tensor([[ 1.7304,  0.7759, -0.5452,  0.7044, -0.9276,  1.0505, -0.8894]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.938000622335443, distance: 0.2849379912893997 entropy 0.03264415264129639
epoch: 37, step: 79
	action: tensor([[ 5.7315,  2.5702, -3.9811,  2.4012, -2.0036,  3.2807, -2.7849]],
       dtype=torch.float64)
	q_value: tensor([[-8.5286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 80
	action: tensor([[ 1.3204,  0.3706, -1.3734,  1.2477, -1.0469,  0.7339, -0.8809]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8076254692508898, distance: 0.5019153525032276 entropy 0.03264415264129639
epoch: 37, step: 81
	action: tensor([[ 6.1800,  1.9464, -3.5424,  2.4163, -1.9342,  2.8895, -2.5753]],
       dtype=torch.float64)
	q_value: tensor([[-8.1331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 82
	action: tensor([[ 2.1213,  0.2934, -1.2833,  0.6349, -0.4519,  0.8508, -0.9514]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9095364043973168, distance: 0.34418632876946975 entropy 0.03264415264129639
epoch: 37, step: 83
	action: tensor([[ 5.7266,  1.9144, -4.1280,  2.7495, -2.3888,  3.1326, -2.6863]],
       dtype=torch.float64)
	q_value: tensor([[-8.0959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 84
	action: tensor([[ 1.7573,  0.3178, -1.4742,  1.3022, -0.8951,  0.7219, -0.6853]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8187108071255602, distance: 0.48723968189654276 entropy 0.03264415264129639
epoch: 37, step: 85
	action: tensor([[ 5.9011,  2.8330, -3.9555,  3.1719, -2.0517,  2.9166, -3.3734]],
       dtype=torch.float64)
	q_value: tensor([[-8.5203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 86
	action: tensor([[ 2.2279,  0.7958, -1.0913,  0.9692, -0.6193,  1.3538, -1.0918]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9285665906104041, distance: 0.3058492237796226 entropy 0.03264415264129639
epoch: 37, step: 87
	action: tensor([[ 6.1800,  2.6481, -4.7956,  3.2300, -2.0463,  3.7398, -3.1677]],
       dtype=torch.float64)
	q_value: tensor([[-9.6148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 88
	action: tensor([[ 1.6865,  0.6546, -0.9471,  1.4844, -0.8173,  1.3186, -1.0061]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8082775170694867, distance: 0.5010640168279157 entropy 0.03264415264129639
epoch: 37, step: 89
	action: tensor([[ 6.1800,  2.5669, -4.9107,  3.2775, -2.0975,  3.4334, -3.3363]],
       dtype=torch.float64)
	q_value: tensor([[-9.3487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 90
	action: tensor([[ 1.7199,  0.8838, -1.2378,  1.2033, -0.1837,  0.8605, -0.8316]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 91
	action: tensor([[ 2.2672,  1.0064, -0.9104,  1.0843, -0.6475,  0.6327, -0.9484]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 92
	action: tensor([[ 1.5860,  0.6381, -1.1954,  1.2279, -1.0013,  0.9636, -1.2512]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9289074932949458, distance: 0.30511854666238586 entropy 0.03264415264129639
epoch: 37, step: 93
	action: tensor([[ 6.1800,  2.8694, -3.6139,  2.8570, -1.8803,  3.7256, -2.8481]],
       dtype=torch.float64)
	q_value: tensor([[-9.6235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 94
	action: tensor([[ 1.3392,  0.8826, -1.5211,  0.8306, -0.5849,  1.2275, -0.8099]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 95
	action: tensor([[ 1.5531,  0.4577, -1.5248,  1.0886, -0.9266,  1.2917, -0.9498]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7757449539258464, distance: 0.5419107987016353 entropy 0.03264415264129639
epoch: 37, step: 96
	action: tensor([[ 6.1800,  2.1762, -4.3695,  3.4071, -1.9549,  3.7393, -3.3960]],
       dtype=torch.float64)
	q_value: tensor([[-9.6634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 97
	action: tensor([[ 1.6845,  0.7076, -1.3060,  0.7453, -0.5260,  0.9710, -0.8834]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9827994504226334, distance: 0.1500817395862216 entropy 0.03264415264129639
epoch: 37, step: 98
	action: tensor([[ 5.8491,  2.0465, -4.0078,  2.4541, -1.7064,  3.3003, -2.8762]],
       dtype=torch.float64)
	q_value: tensor([[-8.9448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 99
	action: tensor([[ 1.9160,  0.6563, -1.5536,  1.0246, -0.5498,  0.8002, -1.0472]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 100
	action: tensor([[ 1.6516,  0.7307, -1.0501,  0.2087, -0.5530,  1.1740, -0.7797]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9661621738168562, distance: 0.21050282128328182 entropy 0.03264415264129639
epoch: 37, step: 101
	action: tensor([[ 5.5487,  1.9301, -3.7467,  2.9079, -2.0486,  1.9630, -3.0456]],
       dtype=torch.float64)
	q_value: tensor([[-8.7646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 102
	action: tensor([[ 1.9328,  0.4509, -1.1990,  0.6940, -0.3209,  0.7238, -1.1611]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9815120238662407, distance: 0.15559705728930914 entropy 0.03264415264129639
epoch: 37, step: 103
	action: tensor([[ 5.9576,  1.8874, -3.8152,  2.7052, -2.0413,  2.7071, -2.5200]],
       dtype=torch.float64)
	q_value: tensor([[-8.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 104
	action: tensor([[ 2.1791,  0.3322, -1.3936,  1.1566, -0.9335,  1.0765, -0.8222]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8128536767459722, distance: 0.495048037144319 entropy 0.03264415264129639
epoch: 37, step: 105
	action: tensor([[ 6.1800,  2.1299, -4.5754,  3.4196, -2.1572,  3.2851, -3.4142]],
       dtype=torch.float64)
	q_value: tensor([[-9.0154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 106
	action: tensor([[ 1.4920,  0.3529, -0.8530,  0.3881, -0.6008,  0.9611, -0.8143]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.976400014515483, distance: 0.17579744595093674 entropy 0.03264415264129639
epoch: 37, step: 107
	action: tensor([[ 5.2255,  1.8153, -3.2529,  2.6017, -1.7815,  2.8822, -2.9143]],
       dtype=torch.float64)
	q_value: tensor([[-7.3809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 108
	action: tensor([[ 1.8866,  0.7452, -0.8333,  0.8645, -0.8043,  1.2648, -0.3809]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9473393880252009, distance: 0.26260297639461555 entropy 0.03264415264129639
epoch: 37, step: 109
	action: tensor([[ 6.1800,  2.2912, -4.0003,  3.1463, -1.9639,  2.6922, -3.0728]],
       dtype=torch.float64)
	q_value: tensor([[-8.5258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 110
	action: tensor([[ 1.7474,  0.6329, -1.2722,  0.9736, -0.5608,  1.0556, -1.4300]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9525727839065449, distance: 0.2492128947269157 entropy 0.03264415264129639
epoch: 37, step: 111
	action: tensor([[ 6.1800,  2.4086, -4.2023,  3.5690, -2.3590,  3.0301, -2.9423]],
       dtype=torch.float64)
	q_value: tensor([[-9.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 112
	action: tensor([[ 2.1013,  0.6453, -1.0601,  1.1614, -0.6967,  0.8248, -0.6308]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9476728486326617, distance: 0.26177022103159103 entropy 0.03264415264129639
epoch: 37, step: 113
	action: tensor([[ 6.1800,  2.0741, -3.9360,  2.9471, -2.3995,  3.2484, -2.9822]],
       dtype=torch.float64)
	q_value: tensor([[-8.2956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 114
	action: tensor([[ 1.9740,  0.5478, -1.2260,  0.7108, -0.6747,  0.8041, -0.7449]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9721537571532125, distance: 0.19095894050649698 entropy 0.03264415264129639
epoch: 37, step: 115
	action: tensor([[ 5.2791,  1.5964, -3.3568,  2.9289, -1.7965,  2.8746, -2.5943]],
       dtype=torch.float64)
	q_value: tensor([[-8.3795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 116
	action: tensor([[ 2.1747,  0.8362, -1.1435,  1.1193, -0.4928,  1.0496, -0.7650]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9071780525761662, distance: 0.34864386798500613 entropy 0.03264415264129639
epoch: 37, step: 117
	action: tensor([[ 6.1800,  2.2898, -3.8710,  3.0558, -2.2401,  3.4657, -3.3602]],
       dtype=torch.float64)
	q_value: tensor([[-8.9141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 118
	action: tensor([[ 1.5763,  0.5643, -0.9149,  0.9625, -0.5050,  0.8210, -0.5370]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9539266351078104, distance: 0.24563014181247644 entropy 0.03264415264129639
epoch: 37, step: 119
	action: tensor([[ 6.1800,  2.1727, -4.0840,  3.2451, -1.9749,  2.2932, -3.0234]],
       dtype=torch.float64)
	q_value: tensor([[-7.5428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 120
	action: tensor([[ 1.6553,  0.4812, -1.1331,  0.8423, -0.4794,  0.7980, -0.8067]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9851738714639674, distance: 0.13933832121732748 entropy 0.03264415264129639
epoch: 37, step: 121
	action: tensor([[ 5.8354,  2.2807, -3.8187,  2.9055, -2.2295,  2.6590, -2.3167]],
       dtype=torch.float64)
	q_value: tensor([[-8.0235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 122
	action: tensor([[ 2.1797,  0.8801, -1.4293,  0.7464, -0.9889,  1.0533, -0.6000]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9687503829078281, distance: 0.20229215614442322 entropy 0.03264415264129639
epoch: 37, step: 123
	action: tensor([[ 6.0733,  2.1177, -3.9354,  2.5757, -1.8322,  2.5182, -2.8984]],
       dtype=torch.float64)
	q_value: tensor([[-9.6667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 124
	action: tensor([[ 1.6818,  0.4745, -1.3952,  0.8308, -0.6035,  0.9951, -0.8230]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9225920515681083, distance: 0.3183827077389907 entropy 0.03264415264129639
epoch: 37, step: 125
	action: tensor([[ 6.1646,  2.0902, -4.1696,  2.7276, -2.4936,  3.1800, -2.5943]],
       dtype=torch.float64)
	q_value: tensor([[-8.7502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 37, step: 126
	action: tensor([[ 2.0854,  0.2868, -1.4657,  0.5238, -0.3855,  0.9439, -0.9251]],
       dtype=torch.float64)
	q_value: tensor([[-8.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8551662000358684, distance: 0.43550350629275936 entropy 0.03264415264129639
epoch: 37, step: 127
	action: tensor([[ 5.7974,  2.0629, -3.6940,  2.7286, -1.8236,  3.0358, -2.8559]],
       dtype=torch.float64)
	q_value: tensor([[-8.3515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 37 actor 503.60657829840005 critic 1589.6839394605831 
epoch: 38, step: 0
	action: tensor([[ 1.6657,  0.7819, -1.1744,  0.4303, -0.9178,  1.3471, -0.3973]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 1
	action: tensor([[ 1.8197,  0.2618, -1.1170,  0.9595, -0.7803,  0.9197, -0.4932]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9298967744982919, distance: 0.3029881850249705 entropy 0.03264415264129639
epoch: 38, step: 2
	action: tensor([[ 6.1800,  3.3438, -6.1986,  5.4427, -5.6826,  6.1800, -3.3367]],
       dtype=torch.float64)
	q_value: tensor([[-13.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 3
	action: tensor([[ 1.9079,  0.7183, -0.7310,  1.0354, -0.8634,  0.9798, -0.5636]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9358025714215785, distance: 0.2899449237408181 entropy 0.03264415264129639
epoch: 38, step: 4
	action: tensor([[ 5.7081,  3.2873, -6.1366,  5.3970, -6.0298,  6.1800, -3.6244]],
       dtype=torch.float64)
	q_value: tensor([[-13.7760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 5
	action: tensor([[ 1.8174,  0.9263, -1.3429,  0.6476, -1.6845,  1.1196, -0.3592]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9200779188820394, distance: 0.32351176965410916 entropy 0.03264415264129639
epoch: 38, step: 6
	action: tensor([[ 5.8661,  3.2601, -6.2800,  5.4902, -5.0400,  5.7119, -2.7234]],
       dtype=torch.float64)
	q_value: tensor([[-17.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 7
	action: tensor([[ 2.0145,  0.4763, -1.2484,  0.8618, -0.7928,  1.1255, -0.2766]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9266314950149488, distance: 0.3099641936890234 entropy 0.03264415264129639
epoch: 38, step: 8
	action: tensor([[ 5.9523,  3.3998, -6.2800,  5.6524, -5.4990,  5.9451, -3.8125]],
       dtype=torch.float64)
	q_value: tensor([[-13.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 9
	action: tensor([[ 1.8688,  0.5008, -1.0607,  0.7952, -0.5740,  1.8932, -1.0238]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8586317121564107, distance: 0.4302617034903895 entropy 0.03264415264129639
epoch: 38, step: 10
	action: tensor([[ 5.9582,  4.3400, -6.2800,  6.1800, -6.2800,  6.1800, -4.1729]],
       dtype=torch.float64)
	q_value: tensor([[-16.1902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 11
	action: tensor([[ 2.1430,  0.6243, -1.0574,  1.1631, -0.6350,  0.8697, -0.3248]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9384409986202132, distance: 0.28392424283011647 entropy 0.03264415264129639
epoch: 38, step: 12
	action: tensor([[ 6.1800,  3.4575, -6.2800,  5.4305, -5.8032,  6.1800, -3.5072]],
       dtype=torch.float64)
	q_value: tensor([[-13.2841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 13
	action: tensor([[ 2.5049,  0.4416, -0.7694,  0.6203, -1.1275,  0.9991, -0.7125]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 14
	action: tensor([[ 1.8512,  0.4780, -1.5467,  1.0194, -0.6624,  1.1052, -0.2966]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.843943449605506, distance: 0.4520616799345926 entropy 0.03264415264129639
epoch: 38, step: 15
	action: tensor([[ 6.1800,  3.6455, -5.9256,  5.3728, -5.8564,  6.1346, -3.9080]],
       dtype=torch.float64)
	q_value: tensor([[-14.4708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 16
	action: tensor([[ 2.3529,  0.7290, -1.5040,  0.9110, -0.7829,  0.9774, -0.4825]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 17
	action: tensor([[ 1.5890,  0.4259, -0.9362,  1.0238, -0.8892,  0.8613, -0.4495]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9734792517711945, distance: 0.18635866024391065 entropy 0.03264415264129639
epoch: 38, step: 18
	action: tensor([[ 6.1800,  2.9327, -6.2800,  5.1992, -5.5083,  5.8366, -3.3034]],
       dtype=torch.float64)
	q_value: tensor([[-13.0630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 19
	action: tensor([[ 2.0551,  0.6645, -1.8690,  0.9607, -0.4349,  0.7474, -0.1161]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8906702802031543, distance: 0.378377942041029 entropy 0.03264415264129639
epoch: 38, step: 20
	action: tensor([[ 6.1741,  3.0287, -6.2800,  5.7000, -5.6566,  5.9286, -3.2319]],
       dtype=torch.float64)
	q_value: tensor([[-14.3325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 21
	action: tensor([[ 2.1284,  0.5741, -1.7106,  0.5140, -1.2397,  0.9178, -0.9594]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.77669026617889, distance: 0.5407674221718978 entropy 0.03264415264129639
epoch: 38, step: 22
	action: tensor([[ 6.1476,  3.7202, -6.2800,  4.8743, -5.6891,  5.6231, -3.2909]],
       dtype=torch.float64)
	q_value: tensor([[-16.8056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 23
	action: tensor([[ 1.9620,  0.2296, -1.0490,  1.0099, -0.7264,  1.2165, -0.7176]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.900961743562631, distance: 0.3601290756225781 entropy 0.03264415264129639
epoch: 38, step: 24
	action: tensor([[ 6.1800,  3.5578, -6.2800,  5.7182, -6.2800,  6.1800, -3.8251]],
       dtype=torch.float64)
	q_value: tensor([[-13.8569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 25
	action: tensor([[ 1.8808,  0.6889, -1.8533,  0.7595, -0.8806,  1.2257, -0.7856]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 26
	action: tensor([[ 1.8018,  0.7556, -1.1874,  0.9911, -1.1813,  1.2123, -0.8359]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9513452133395252, distance: 0.252417510757623 entropy 0.03264415264129639
epoch: 38, step: 27
	action: tensor([[ 6.1800,  3.7011, -6.2800,  5.7946, -5.9854,  6.0052, -3.5876]],
       dtype=torch.float64)
	q_value: tensor([[-16.3902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 28
	action: tensor([[ 2.1436,  0.6326, -1.6957,  0.9022, -0.6652,  0.8695, -0.8157]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8856099066900149, distance: 0.3870355880693056 entropy 0.03264415264129639
epoch: 38, step: 29
	action: tensor([[ 6.1800,  3.6402, -6.1913,  6.1568, -6.1739,  6.1800, -3.8798]],
       dtype=torch.float64)
	q_value: tensor([[-15.5724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 30
	action: tensor([[ 1.9654,  0.7645, -1.4198,  0.9334, -0.8278,  1.2611, -1.0603]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 31
	action: tensor([[ 1.7053,  0.9066, -1.1706,  0.8317, -0.9075,  1.2603, -0.3782]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9661813778419531, distance: 0.21044307933701498 entropy 0.03264415264129639
epoch: 38, step: 32
	action: tensor([[ 5.9822,  3.6630, -6.0879,  5.6785, -5.4971,  6.1800, -3.3720]],
       dtype=torch.float64)
	q_value: tensor([[-15.4217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 33
	action: tensor([[ 2.1023,  0.1146, -1.0699,  0.8003, -1.1685,  0.9001, -0.6910]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.831396163696687, distance: 0.46988374267762717 entropy 0.03264415264129639
epoch: 38, step: 34
	action: tensor([[ 6.0167,  2.9411, -6.2800,  5.3381, -6.0212,  6.1374, -3.0568]],
       dtype=torch.float64)
	q_value: tensor([[-13.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 35
	action: tensor([[ 1.4887,  0.6566, -0.6904,  0.5971, -0.7673,  1.0399, -0.8433]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9686642144280999, distance: 0.20257086688399176 entropy 0.03264415264129639
epoch: 38, step: 36
	action: tensor([[ 6.1800,  3.1915, -6.2032,  4.7938, -5.1545,  5.4816, -3.2616]],
       dtype=torch.float64)
	q_value: tensor([[-13.2829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 37
	action: tensor([[ 1.8608,  0.5977, -1.4560,  1.0476, -0.6305,  1.0212, -0.9777]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.90159256548403, distance: 0.35898032649020567 entropy 0.03264415264129639
epoch: 38, step: 38
	action: tensor([[ 6.1785,  3.7995, -6.2800,  5.6550, -6.2123,  6.1800, -4.0558]],
       dtype=torch.float64)
	q_value: tensor([[-15.5785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 39
	action: tensor([[ 2.0384,  0.4464, -1.3074,  0.7746, -0.9732,  0.6452, -0.4998]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.938225678216126, distance: 0.2844203629389626 entropy 0.03264415264129639
epoch: 38, step: 40
	action: tensor([[ 5.9244,  3.2659, -6.2800,  5.4132, -5.6614,  5.8266, -2.8599]],
       dtype=torch.float64)
	q_value: tensor([[-13.6890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 41
	action: tensor([[ 2.1982,  0.6952, -1.0715,  0.7829, -0.9004,  1.0183, -1.0867]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 42
	action: tensor([[ 2.2697,  0.8344, -1.1862,  0.6350, -1.3353,  1.1320, -1.1181]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.976021348603822, distance: 0.17720218400378424 entropy 0.03264415264129639
epoch: 38, step: 43
	action: tensor([[ 6.1800,  3.1316, -5.8590,  5.3898, -5.5684,  5.9815, -3.4411]],
       dtype=torch.float64)
	q_value: tensor([[-17.1719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 44
	action: tensor([[ 2.1738,  0.8259, -0.8586,  1.2175, -0.5909,  0.8576, -0.8623]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 45
	action: tensor([[ 1.9747,  1.0523, -1.3143,  1.0866, -1.2944,  1.0319, -0.5062]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 46
	action: tensor([[ 2.1834,  0.4530, -0.8676,  0.9174, -0.9615,  1.2747, -0.5286]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9526292132603076, distance: 0.24906459266221867 entropy 0.03264415264129639
epoch: 38, step: 47
	action: tensor([[ 6.1800,  3.5526, -6.2800,  5.9438, -6.2800,  5.8885, -3.5205]],
       dtype=torch.float64)
	q_value: tensor([[-14.1336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 48
	action: tensor([[ 1.7655,  0.7979, -0.7548,  1.2095, -0.6845,  1.2075, -0.6966]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8469678861214907, distance: 0.4476596820707881 entropy 0.03264415264129639
epoch: 38, step: 49
	action: tensor([[ 6.1800,  3.6843, -6.2605,  6.1324, -5.8798,  6.1800, -3.3944]],
       dtype=torch.float64)
	q_value: tensor([[-14.5347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 50
	action: tensor([[ 1.8885,  0.6174, -1.3832,  0.6572, -0.8512,  1.1424, -0.8633]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9207691704643916, distance: 0.322109693622287 entropy 0.03264415264129639
epoch: 38, step: 51
	action: tensor([[ 6.1800,  3.7004, -6.1520,  5.4928, -5.7668,  6.1800, -3.0067]],
       dtype=torch.float64)
	q_value: tensor([[-15.6543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 52
	action: tensor([[ 2.2436,  0.2923, -1.5152,  0.8433, -1.1348,  1.4200, -0.9170]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.678091410845381, distance: 0.6492664705653701 entropy 0.03264415264129639
epoch: 38, step: 53
	action: tensor([[ 5.7410,  4.2467, -6.2674,  6.1800, -6.0870,  6.1079, -3.6743]],
       dtype=torch.float64)
	q_value: tensor([[-16.5293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 54
	action: tensor([[ 1.9052,  0.6944, -1.4186,  0.8350, -1.0634,  0.8815, -0.8888]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9450215409318938, distance: 0.2683199565426994 entropy 0.03264415264129639
epoch: 38, step: 55
	action: tensor([[ 5.9949,  3.3172, -5.8403,  5.6044, -5.2013,  6.0600, -3.2603]],
       dtype=torch.float64)
	q_value: tensor([[-15.8816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 56
	action: tensor([[ 2.0087,  0.5563, -1.3264,  0.9993, -0.6081,  1.2522, -0.7348]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8981321984094537, distance: 0.3652373307177835 entropy 0.03264415264129639
epoch: 38, step: 57
	action: tensor([[ 6.1800,  3.6418, -6.1299,  6.0644, -6.2800,  6.0782, -3.8221]],
       dtype=torch.float64)
	q_value: tensor([[-15.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 58
	action: tensor([[ 1.7576,  0.1630, -1.1882,  1.1260, -1.1078,  1.5015, -0.6082]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7495560797819111, distance: 0.5726798992404162 entropy 0.03264415264129639
epoch: 38, step: 59
	action: tensor([[ 6.1800,  3.8063, -5.8761,  6.1800, -6.2092,  6.1800, -4.1300]],
       dtype=torch.float64)
	q_value: tensor([[-15.3261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 60
	action: tensor([[ 2.2354,  0.2836, -1.6260,  1.0062, -0.9833,  1.2417, -0.7301]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6723750410363043, distance: 0.6550058564265011 entropy 0.03264415264129639
epoch: 38, step: 61
	action: tensor([[ 6.1800,  3.4432, -6.2800,  6.1800, -6.2414,  6.1800, -4.1822]],
       dtype=torch.float64)
	q_value: tensor([[-15.8353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 62
	action: tensor([[ 2.1482,  0.6239, -0.7864,  0.8477, -1.0290,  0.8233, -0.5292]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9856240819012977, distance: 0.13720643749375747 entropy 0.03264415264129639
epoch: 38, step: 63
	action: tensor([[ 6.1800,  2.7960, -6.2800,  5.2160, -5.4370,  5.7062, -3.3065]],
       dtype=torch.float64)
	q_value: tensor([[-13.5086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 64
	action: tensor([[ 2.1758,  0.6210, -1.3902,  0.9949, -1.2046,  1.0021, -0.4168]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9104400622536618, distance: 0.34246294320330745 entropy 0.03264415264129639
epoch: 38, step: 65
	action: tensor([[ 6.0068,  3.5697, -6.2800,  5.2325, -5.6397,  6.1800, -3.2273]],
       dtype=torch.float64)
	q_value: tensor([[-15.3690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 66
	action: tensor([[ 1.8308,  0.2944, -0.7128,  0.5874, -0.8471,  1.1064, -0.6776]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9626447407887624, distance: 0.2211732351206676 entropy 0.03264415264129639
epoch: 38, step: 67
	action: tensor([[ 6.1800,  3.6187, -6.2800,  5.8904, -5.8813,  5.6555, -3.2538]],
       dtype=torch.float64)
	q_value: tensor([[-12.9478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 68
	action: tensor([[ 2.0195,  0.5465, -1.2864,  1.1673, -0.2428,  1.2708, -0.6533]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.861058758180063, distance: 0.4265522934266786 entropy 0.03264415264129639
epoch: 38, step: 69
	action: tensor([[ 6.1800,  3.7128, -6.1604,  6.1800, -5.5828,  6.1800, -3.6010]],
       dtype=torch.float64)
	q_value: tensor([[-14.4198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 70
	action: tensor([[ 2.1816,  0.8837, -1.1416,  1.2555, -0.4101,  0.8596, -0.6684]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8706878050755118, distance: 0.4115062493079699 entropy 0.03264415264129639
epoch: 38, step: 71
	action: tensor([[ 6.1433,  3.6881, -6.2659,  5.6906, -6.2800,  6.1800, -3.4744]],
       dtype=torch.float64)
	q_value: tensor([[-14.3922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 72
	action: tensor([[ 1.7662,  0.6652, -1.2285,  0.7774, -1.0450,  1.0860, -0.7156]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9545121108102058, distance: 0.2440644837267387 entropy 0.03264415264129639
epoch: 38, step: 73
	action: tensor([[ 6.1800,  3.8782, -5.7972,  5.2421, -5.4664,  5.9509, -3.3350]],
       dtype=torch.float64)
	q_value: tensor([[-15.4207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 74
	action: tensor([[ 2.4299,  0.4814, -1.7603,  1.2404, -0.8477,  0.7059, -0.2442]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 75
	action: tensor([[ 1.6479,  1.0617, -1.5024,  0.8340, -1.1003,  0.7314, -0.2403]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 76
	action: tensor([[ 2.0363,  0.1615, -1.4506,  0.5601, -1.1397,  1.1309, -1.1780]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6265874576232913, distance: 0.6992800589744776 entropy 0.03264415264129639
epoch: 38, step: 77
	action: tensor([[ 6.1800,  3.5471, -6.0853,  5.9345, -6.2800,  6.1800, -3.1946]],
       dtype=torch.float64)
	q_value: tensor([[-15.8553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 78
	action: tensor([[ 2.0891,  0.1205, -1.6084,  0.5728, -1.1356,  1.2291, -0.5200]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5526887256407684, distance: 0.7653526945834852 entropy 0.03264415264129639
epoch: 38, step: 79
	action: tensor([[ 6.1800,  3.6242, -6.2742,  5.4410, -5.8734,  6.1800, -3.5598]],
       dtype=torch.float64)
	q_value: tensor([[-15.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 80
	action: tensor([[ 2.3091,  0.1826, -1.7534,  0.9862, -0.6673,  0.9704, -0.6623]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6605277031457408, distance: 0.6667436108067544 entropy 0.03264415264129639
epoch: 38, step: 81
	action: tensor([[ 6.1800,  4.2851, -6.2800,  5.3296, -5.8650,  6.0800, -3.5182]],
       dtype=torch.float64)
	q_value: tensor([[-14.6356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 82
	action: tensor([[ 1.9284,  0.5623, -1.0230,  0.5957, -0.7948,  0.4433, -0.2446]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.954896740903942, distance: 0.2430304301229684 entropy 0.03264415264129639
epoch: 38, step: 83
	action: tensor([[ 6.1800,  2.3769, -5.9731,  4.6835, -4.7491,  4.8872, -3.2849]],
       dtype=torch.float64)
	q_value: tensor([[-12.1639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 84
	action: tensor([[ 2.4163,  0.8489, -1.3830,  1.1837, -0.6192,  0.8068, -0.7992]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 85
	action: tensor([[ 2.3300,  0.4352, -1.2438,  0.9838, -0.5707,  0.7573, -1.0046]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9594266518795831, distance: 0.23050328383699162 entropy 0.03264415264129639
epoch: 38, step: 86
	action: tensor([[ 6.1557,  3.9359, -6.2800,  5.6556, -5.4577,  6.1800, -3.2772]],
       dtype=torch.float64)
	q_value: tensor([[-14.0894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 87
	action: tensor([[ 1.7737,  0.8163, -1.0161,  0.9472, -0.6720,  0.8995, -0.6589]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9587461395425851, distance: 0.23242829204261117 entropy 0.03264415264129639
epoch: 38, step: 88
	action: tensor([[ 6.1800,  3.4044, -6.0913,  5.3094, -5.7458,  6.1783, -3.4177]],
       dtype=torch.float64)
	q_value: tensor([[-14.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 89
	action: tensor([[ 2.1996,  0.8638, -1.5072,  0.2932, -0.6118,  1.0832, -0.4552]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9654408443186463, distance: 0.2127346607709443 entropy 0.03264415264129639
epoch: 38, step: 90
	action: tensor([[ 6.1800,  2.9416, -6.0867,  4.6711, -5.1933,  5.9413, -2.9451]],
       dtype=torch.float64)
	q_value: tensor([[-15.2338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 91
	action: tensor([[ 2.0251,  0.0798, -0.9556,  1.0401, -0.7926,  1.2566, -0.3503]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8884162475394811, distance: 0.3822585203476683 entropy 0.03264415264129639
epoch: 38, step: 92
	action: tensor([[ 6.1800,  3.5601, -6.2800,  5.5795, -6.2076,  6.1800, -3.9017]],
       dtype=torch.float64)
	q_value: tensor([[-12.9385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 93
	action: tensor([[ 2.0098,  0.9609, -1.2613,  0.8487, -0.4372,  1.2266, -0.2010]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9377782668836144, distance: 0.2854484868237463 entropy 0.03264415264129639
epoch: 38, step: 94
	action: tensor([[ 6.1800,  3.3547, -6.2800,  5.5114, -5.8002,  5.8376, -3.4288]],
       dtype=torch.float64)
	q_value: tensor([[-14.5777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 95
	action: tensor([[ 2.6977,  0.9574, -1.1385,  0.7701, -1.0720,  0.7514, -0.6599]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 96
	action: tensor([[ 1.9475,  0.1764, -1.2739,  1.1074, -0.9934,  1.2965, -0.1887]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7755845019462206, distance: 0.5421046295974372 entropy 0.03264415264129639
epoch: 38, step: 97
	action: tensor([[ 6.1800,  3.6785, -6.2800,  6.1800, -6.2800,  5.9626, -3.8802]],
       dtype=torch.float64)
	q_value: tensor([[-14.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 98
	action: tensor([[ 1.9651,  0.8167, -1.2228,  1.4940, -0.8173,  1.1893, -0.5087]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8068300064308175, distance: 0.5029519843392587 entropy 0.03264415264129639
epoch: 38, step: 99
	action: tensor([[ 6.1800,  3.6442, -6.0546,  6.1800, -6.2800,  6.1800, -3.2587]],
       dtype=torch.float64)
	q_value: tensor([[-15.6481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 100
	action: tensor([[ 2.0481,  0.8067, -1.2977,  0.3555, -0.4955,  1.1035, -0.4067]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9792578248438596, distance: 0.16481010722331255 entropy 0.03264415264129639
epoch: 38, step: 101
	action: tensor([[ 6.1800,  3.3088, -6.2800,  4.8347, -5.3047,  5.5605, -2.8888]],
       dtype=torch.float64)
	q_value: tensor([[-14.2917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 102
	action: tensor([[ 1.9573,  0.3760, -1.6897,  1.0015, -0.6181,  1.3169, -0.7933]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7299861673694443, distance: 0.5946339483308178 entropy 0.03264415264129639
epoch: 38, step: 103
	action: tensor([[ 6.1800,  3.8596, -6.2800,  5.8787, -6.2010,  6.1800, -3.8863]],
       dtype=torch.float64)
	q_value: tensor([[-15.8405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 104
	action: tensor([[ 1.8757,  0.4532, -1.5116,  1.0345, -1.1096,  1.2538, -1.0657]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7740442147400192, distance: 0.5439618296743656 entropy 0.03264415264129639
epoch: 38, step: 105
	action: tensor([[ 6.1800,  3.7765, -6.2800,  6.1800, -6.2000,  6.1139, -4.3421]],
       dtype=torch.float64)
	q_value: tensor([[-16.8381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 106
	action: tensor([[ 1.8582,  0.5964, -0.8811,  0.4249, -0.8659,  0.8246, -0.6584]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9752575057690062, distance: 0.18000245655467167 entropy 0.03264415264129639
epoch: 38, step: 107
	action: tensor([[ 6.1800,  2.7262, -6.2800,  4.9378, -4.8838,  5.2576, -3.3522]],
       dtype=torch.float64)
	q_value: tensor([[-13.3911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 108
	action: tensor([[ 2.2078,  0.6449, -0.9887,  0.8933, -1.3279,  0.8096, -0.7852]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9887547036345677, distance: 0.12135066093733073 entropy 0.03264415264129639
epoch: 38, step: 109
	action: tensor([[ 6.0356,  3.0753, -6.2800,  4.8100, -5.6916,  6.1341, -3.0818]],
       dtype=torch.float64)
	q_value: tensor([[-15.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 110
	action: tensor([[ 2.1717,  0.7553, -1.7641,  0.7502, -1.5130,  0.6738, -0.9811]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 111
	action: tensor([[ 1.7655,  0.3158, -0.6128,  0.7643, -0.4771,  0.9677, -0.6848]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9626563270224818, distance: 0.22113893254075007 entropy 0.03264415264129639
epoch: 38, step: 112
	action: tensor([[ 6.1800,  3.3240, -6.1131,  5.7723, -5.9729,  5.9020, -4.1203]],
       dtype=torch.float64)
	q_value: tensor([[-11.9370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 113
	action: tensor([[ 1.7104,  0.5027, -1.1625,  0.7923, -0.9171,  0.8034, -0.8158]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9652602482934735, distance: 0.21328978100546775 entropy 0.03264415264129639
epoch: 38, step: 114
	action: tensor([[ 6.1800,  2.8983, -6.2800,  5.2684, -5.3710,  5.3390, -3.3342]],
       dtype=torch.float64)
	q_value: tensor([[-14.2839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 115
	action: tensor([[ 2.0834,  0.1757, -1.2352,  0.7926, -0.4705,  0.7017, -0.9716]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9067345242501631, distance: 0.34947583259769666 entropy 0.03264415264129639
epoch: 38, step: 116
	action: tensor([[ 6.1800,  3.4123, -6.2800,  5.8329, -5.6928,  5.9881, -3.6567]],
       dtype=torch.float64)
	q_value: tensor([[-13.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 117
	action: tensor([[ 2.1335,  0.4748, -1.5964,  0.9022, -0.9037,  1.3330, -0.8672]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7713288803473326, distance: 0.547220492194801 entropy 0.03264415264129639
epoch: 38, step: 118
	action: tensor([[ 6.1800,  3.6312, -6.2180,  5.9512, -6.2800,  6.1800, -3.9911]],
       dtype=torch.float64)
	q_value: tensor([[-16.4447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 119
	action: tensor([[ 1.6101,  0.4955, -0.9594,  0.9295, -0.6119,  1.3775, -0.4435]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9254638548930855, distance: 0.31242095618830323 entropy 0.03264415264129639
epoch: 38, step: 120
	action: tensor([[ 6.1800,  3.8310, -6.2800,  5.1547, -5.8719,  5.9470, -3.8081]],
       dtype=torch.float64)
	q_value: tensor([[-13.8950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 121
	action: tensor([[ 2.6977,  0.9132, -1.3649,  0.8679, -0.6986,  1.4133, -0.7501]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 122
	action: tensor([[ 2.5704,  0.6994, -0.6490,  0.4166, -0.8325,  0.9257, -0.6299]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 123
	action: tensor([[ 1.9175,  0.6797, -1.2156,  1.0147, -1.0836,  1.0542, -0.7397]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 124
	action: tensor([[ 2.0244,  0.3791, -1.7292,  0.9885, -0.9906,  1.1822, -0.6899]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6787408106364047, distance: 0.648611243609571 entropy 0.03264415264129639
epoch: 38, step: 125
	action: tensor([[ 6.1800,  3.3900, -6.2800,  5.9403, -6.0524,  6.1800, -3.6583]],
       dtype=torch.float64)
	q_value: tensor([[-16.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 126
	action: tensor([[ 2.4278,  0.7270, -1.5978,  0.9549, -0.9745,  1.3033, -0.4881]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 38, step: 127
	action: tensor([[ 1.9033,  0.3800, -0.7936,  1.0662, -0.9188,  1.3975, -0.1881]],
       dtype=torch.float64)
	q_value: tensor([[-12.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9221347846360155, distance: 0.3193217037797187 entropy 0.03264415264129639
LOSS epoch 38 actor 398.65913749898965 critic 1230.9421253813264 
epoch: 39, step: 0
	action: tensor([[ 6.1800,  1.8349, -5.6414,  5.7556, -5.8129,  6.1800, -1.7181]],
       dtype=torch.float64)
	q_value: tensor([[-25.8269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 1
	action: tensor([[ 1.0827,  0.2941, -0.4707,  0.6071, -0.4040,  0.9007,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9704182799327044, distance: 0.1968196271584686 entropy 0.03264415264129639
epoch: 39, step: 2
	action: tensor([[ 6.1400,  0.6414, -4.0534,  4.6228, -3.5647,  4.6962, -0.7538]],
       dtype=torch.float64)
	q_value: tensor([[-15.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 3
	action: tensor([[ 1.1295, -0.2003, -0.6820,  0.7735, -0.6184,  0.4121,  0.2179]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9146935794169198, distance: 0.33423163469218437 entropy 0.03264415264129639
epoch: 39, step: 4
	action: tensor([[ 6.0887,  1.2806, -4.0521,  3.9247, -3.8272,  4.3932, -1.2813]],
       dtype=torch.float64)
	q_value: tensor([[-13.3247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 5
	action: tensor([[ 0.7615,  0.2260, -0.4282,  0.3970, -0.1093,  0.6521, -0.1947]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9501639078908471, distance: 0.2554633971897214 entropy 0.03264415264129639
epoch: 39, step: 6
	action: tensor([[ 6.1800,  0.9174, -2.7928,  3.3152, -3.1278,  3.8201, -0.6957]],
       dtype=torch.float64)
	q_value: tensor([[-11.1733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 7
	action: tensor([[ 0.7281,  0.1973, -0.8639,  0.7836, -0.6660,  0.4183,  0.0647]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7944289694774765, distance: 0.5188450165844005 entropy 0.03264415264129639
epoch: 39, step: 8
	action: tensor([[ 5.7524,  0.6824, -2.8703,  3.3333, -2.9479,  3.4922, -0.2318]],
       dtype=torch.float64)
	q_value: tensor([[-13.4821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 9
	action: tensor([[ 1.3179, -0.0917, -0.8297,  0.9265, -0.2869,  1.0589, -0.1380]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.943832445878117, distance: 0.271206097495546 entropy 0.03264415264129639
epoch: 39, step: 10
	action: tensor([[ 5.8197,  1.8350, -4.9177,  5.4276, -4.5510,  5.5001, -0.7875]],
       dtype=torch.float64)
	q_value: tensor([[-18.2426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 11
	action: tensor([[ 0.8778,  0.4528, -0.4637,  0.5336, -0.3526,  0.2338, -0.2808]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9593895815982656, distance: 0.23060856071326644 entropy 0.03264415264129639
epoch: 39, step: 12
	action: tensor([[ 5.9671,  1.1715, -2.7807,  2.7912, -3.1995,  2.8658, -0.1871]],
       dtype=torch.float64)
	q_value: tensor([[-12.7161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 13
	action: tensor([[ 1.0255,  0.3181, -0.6176,  0.4280, -0.1572,  1.0159, -0.4750]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9661549966624107, distance: 0.21052514439510683 entropy 0.03264415264129639
epoch: 39, step: 14
	action: tensor([[ 6.1800,  0.7813, -4.2018,  4.9456, -4.4921,  5.0307, -1.0912]],
       dtype=torch.float64)
	q_value: tensor([[-16.6396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 15
	action: tensor([[ 0.5891, -0.0842, -0.8566,  0.7511, -0.5662,  0.6173,  0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6354779261790534, distance: 0.6909054356164508 entropy 0.03264415264129639
epoch: 39, step: 16
	action: tensor([[ 6.0760,  0.8695, -3.2969,  3.6522, -3.1100,  3.7098, -0.5080]],
       dtype=torch.float64)
	q_value: tensor([[-12.3037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 17
	action: tensor([[ 0.7571, -0.1624, -0.8720,  0.9408, -0.5509,  0.5859,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6917272721252912, distance: 0.6353664024067324 entropy 0.03264415264129639
epoch: 39, step: 18
	action: tensor([[ 6.0010,  1.1572, -3.5105,  3.8151, -3.8654,  4.6388, -0.7627]],
       dtype=torch.float64)
	q_value: tensor([[-13.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 19
	action: tensor([[ 1.4024,  0.2704, -0.5275,  0.9015, -0.6472,  0.5328, -0.4406]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9821092350712035, distance: 0.15306332665759756 entropy 0.03264415264129639
epoch: 39, step: 20
	action: tensor([[ 6.1800,  1.5233, -4.5912,  4.6616, -4.8376,  4.7999, -1.1737]],
       dtype=torch.float64)
	q_value: tensor([[-18.9566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 21
	action: tensor([[ 0.8707,  0.3311, -0.4636,  0.8544, -0.8244,  0.2900, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9092512748027489, distance: 0.34472831751090116 entropy 0.03264415264129639
epoch: 39, step: 22
	action: tensor([[ 5.5581,  1.1606, -3.4347,  3.3949, -2.8107,  3.5856, -0.4621]],
       dtype=torch.float64)
	q_value: tensor([[-13.6940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 23
	action: tensor([[ 0.7910, -0.0956, -0.8320,  0.4258, -0.5625, -0.0942,  0.1510]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8038614564942534, distance: 0.5068018204980095 entropy 0.03264415264129639
epoch: 39, step: 24
	action: tensor([[ 5.0415,  0.6311, -2.8201,  3.1812, -2.8173,  3.0168, -0.7084]],
       dtype=torch.float64)
	q_value: tensor([[-10.0888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 25
	action: tensor([[ 1.1115,  0.3080, -0.8002,  0.4670, -0.4713,  0.4644, -0.3620]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9915691360777709, distance: 0.10507338731483551 entropy 0.03264415264129639
epoch: 39, step: 26
	action: tensor([[ 6.1515,  1.2692, -3.4161,  3.7834, -4.0855,  4.1617, -0.6912]],
       dtype=torch.float64)
	q_value: tensor([[-16.4155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 27
	action: tensor([[ 1.0963,  0.2647, -0.2700,  0.4626, -0.4606,  0.7154, -0.0555]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05109924986872064 entropy 0.03264415264129639
epoch: 39, step: 28
	action: tensor([[ 0.9599,  0.5207, -0.7434,  0.6826, -0.9323,  0.8281,  0.1200]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8801898175940747, distance: 0.3960988285110388 entropy 0.03264415264129639
epoch: 39, step: 29
	action: tensor([[ 6.0900,  0.9345, -3.6398,  3.6052, -3.7091,  3.8230, -0.8052]],
       dtype=torch.float64)
	q_value: tensor([[-18.5649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 30
	action: tensor([[ 0.8956,  0.4020, -0.8024,  0.3188, -0.4631,  0.7687, -0.1045]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9453071598687028, distance: 0.26762207362564044 entropy 0.03264415264129639
epoch: 39, step: 31
	action: tensor([[ 6.0547,  0.7850, -3.3667,  3.6362, -3.1266,  4.0458, -0.4206]],
       dtype=torch.float64)
	q_value: tensor([[-15.8506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 32
	action: tensor([[ 1.0866, -0.0110, -0.8651,  0.0877, -0.2936,  0.7545, -0.4325]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8632698240765182, distance: 0.42314467463911015 entropy 0.03264415264129639
epoch: 39, step: 33
	action: tensor([[ 6.1800,  0.9331, -3.8701,  3.7771, -4.0699,  3.9637, -1.1505]],
       dtype=torch.float64)
	q_value: tensor([[-15.5245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 34
	action: tensor([[ 1.1620,  0.2967, -0.6685,  0.6775, -0.3666,  0.2957, -0.0394]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06384929924010731 entropy 0.03264415264129639
epoch: 39, step: 35
	action: tensor([[ 1.0362, -0.1040, -0.4869,  0.5188, -0.2836,  0.1935, -0.1291]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9007441141446718, distance: 0.3605245373133476 entropy 0.03264415264129639
epoch: 39, step: 36
	action: tensor([[ 6.1800,  1.0365, -3.1039,  3.3160, -3.3965,  4.1870, -1.0688]],
       dtype=torch.float64)
	q_value: tensor([[-11.0146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 37
	action: tensor([[ 0.9383,  0.4775, -0.4253,  0.6377, -0.7491,  0.4074, -0.2557]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9350478868358716, distance: 0.2916441945387556 entropy 0.03264415264129639
epoch: 39, step: 38
	action: tensor([[ 5.9421,  1.0046, -2.9782,  3.4933, -3.3045,  3.8571, -0.7703]],
       dtype=torch.float64)
	q_value: tensor([[-15.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 39
	action: tensor([[ 0.9044,  0.1269, -0.7902,  0.6043, -0.3794,  0.5620, -0.4082]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9158897962543484, distance: 0.3318799651143973 entropy 0.03264415264129639
epoch: 39, step: 40
	action: tensor([[ 6.1800,  1.3524, -3.7386,  3.7205, -3.8908,  4.2333, -0.5765]],
       dtype=torch.float64)
	q_value: tensor([[-14.6136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 41
	action: tensor([[ 0.6064,  0.1046, -0.6780,  0.3322, -0.5147,  0.6065, -0.5293]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7832341530121344, distance: 0.5327851626267709 entropy 0.03264415264129639
epoch: 39, step: 42
	action: tensor([[ 5.9145,  1.0545, -2.6019,  2.7586, -2.9029,  3.0208, -0.4628]],
       dtype=torch.float64)
	q_value: tensor([[-12.9154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 43
	action: tensor([[ 1.0624,  0.0830, -1.0577,  0.5915, -0.7109,  0.1698, -0.4967]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9092629724299667, distance: 0.3447060988334599 entropy 0.03264415264129639
epoch: 39, step: 44
	action: tensor([[ 6.1800,  1.0258, -3.0690,  3.6678, -3.9253,  3.6399, -0.6991]],
       dtype=torch.float64)
	q_value: tensor([[-16.6449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 45
	action: tensor([[ 1.0167,  0.0745, -0.8327,  0.5017, -0.5034,  0.6418, -0.2329]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9254871861360234, distance: 0.31237205548570435 entropy 0.03264415264129639
epoch: 39, step: 46
	action: tensor([[ 6.1800,  1.1591, -3.1902,  3.8194, -3.6731,  3.8691, -0.4380]],
       dtype=torch.float64)
	q_value: tensor([[-15.3689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 47
	action: tensor([[ 1.4080,  0.6216, -0.4299,  0.5167,  0.0198,  0.3308,  0.2047]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 48
	action: tensor([[ 0.9101,  0.0898, -0.5958,  0.4190, -0.6347,  0.2755, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.939826316893645, distance: 0.2807113614663236 entropy 0.03264415264129639
epoch: 39, step: 49
	action: tensor([[ 5.3081,  0.6743, -3.3042,  2.7483, -2.6856,  3.2178, -0.6507]],
       dtype=torch.float64)
	q_value: tensor([[-11.9765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 50
	action: tensor([[ 0.9833,  0.4755, -0.4140,  1.0004, -0.2504,  0.3629, -0.1592]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8348677629859209, distance: 0.46502106386463293 entropy 0.03264415264129639
epoch: 39, step: 51
	action: tensor([[ 5.7998,  1.1994, -3.6247,  4.2150, -4.2335,  3.8141, -0.3230]],
       dtype=torch.float64)
	q_value: tensor([[-14.0635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 52
	action: tensor([[ 1.0957,  0.1041, -0.7796,  0.3060, -0.8330,  1.1489, -0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8984812019608546, distance: 0.3646111343649524 entropy 0.03264415264129639
epoch: 39, step: 53
	action: tensor([[ 5.9627,  1.3232, -3.8853,  4.4011, -3.9170,  4.3038, -0.7922]],
       dtype=torch.float64)
	q_value: tensor([[-19.3123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 54
	action: tensor([[ 1.4355,  0.1171, -0.4228,  0.4219, -0.3052,  0.6234, -0.0790]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9145681913106808, distance: 0.3344771805890703 entropy 0.03264415264129639
epoch: 39, step: 55
	action: tensor([[ 6.1800,  1.0696, -4.3060,  4.7387, -4.3266,  4.5843, -0.8294]],
       dtype=torch.float64)
	q_value: tensor([[-15.7781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 56
	action: tensor([[ 0.3113, -0.2027, -0.4859,  0.3646, -0.3792,  0.5129,  0.2923]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5311805512158552, distance: 0.7835369887891036 entropy 0.03264415264129639
epoch: 39, step: 57
	action: tensor([[ 4.5055,  0.5676, -2.2794,  2.4488, -2.3220,  2.5085, -0.8219]],
       dtype=torch.float64)
	q_value: tensor([[-7.3357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 58
	action: tensor([[ 1.2038,  0.3403, -0.9849,  0.0990, -0.5239,  0.5373, -0.3489]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.950931551304238, distance: 0.25348826403941793 entropy 0.03264415264129639
epoch: 39, step: 59
	action: tensor([[ 6.1800,  1.0496, -3.8706,  4.1301, -3.1282,  4.2591, -0.4624]],
       dtype=torch.float64)
	q_value: tensor([[-18.0917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 60
	action: tensor([[ 0.3911,  0.3804, -0.4371,  0.0955, -0.0524,  0.3612, -0.2300]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8236156643452084, distance: 0.48060324644024505 entropy 0.03264415264129639
epoch: 39, step: 61
	action: tensor([[ 3.8434,  0.8701, -1.6874,  2.1559, -1.6063,  2.2636, -0.3398]],
       dtype=torch.float64)
	q_value: tensor([[-8.0493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 62
	action: tensor([[ 1.2017,  0.2978, -0.7913, -0.1310, -0.5428,  0.3479,  0.0394]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8470031470992218, distance: 0.4476081052250598 entropy 0.03264415264129639
epoch: 39, step: 63
	action: tensor([[ 5.9009,  1.0738, -2.9892,  2.9781, -2.8240,  3.0261, -0.6330]],
       dtype=torch.float64)
	q_value: tensor([[-15.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 64
	action: tensor([[ 0.9356,  0.3270, -0.5310,  0.8489, -0.1437,  0.1594, -0.0763]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9400108218687508, distance: 0.2802806714801896 entropy 0.03264415264129639
epoch: 39, step: 65
	action: tensor([[ 6.1800,  1.1181, -3.7884,  3.0794, -3.0745,  4.0551, -0.1666]],
       dtype=torch.float64)
	q_value: tensor([[-12.1558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 66
	action: tensor([[ 0.8253,  0.1379, -0.7719,  0.7831, -0.6480,  0.5125,  0.1405]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8446841340823227, distance: 0.4509876035276281 entropy 0.03264415264129639
epoch: 39, step: 67
	action: tensor([[ 6.0812,  1.0225, -3.4800,  3.4088, -3.5652,  3.9629, -0.7963]],
       dtype=torch.float64)
	q_value: tensor([[-13.6014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 68
	action: tensor([[ 1.0961,  0.1548, -0.2722,  0.2909, -0.3607,  0.5334, -0.2288]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9624876635423668, distance: 0.22163775924215703 entropy 0.03264415264129639
epoch: 39, step: 69
	action: tensor([[ 5.9956,  0.9756, -3.3866,  4.1619, -3.4259,  3.8267, -0.7828]],
       dtype=torch.float64)
	q_value: tensor([[-13.0097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 70
	action: tensor([[ 0.4907, -0.4233, -0.9055,  0.6980,  0.1099,  0.7226, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5036881305543274, distance: 0.8061837204989447 entropy 0.03264415264129639
epoch: 39, step: 71
	action: tensor([[ 6.1623,  0.6322, -3.8336,  4.0794, -3.2748,  3.9781, -0.7573]],
       dtype=torch.float64)
	q_value: tensor([[-9.6202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 72
	action: tensor([[ 1.1178,  0.1694, -0.1764,  0.4359, -0.4367,  0.9754, -0.3937]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.046338680918496446 entropy 0.03264415264129639
epoch: 39, step: 73
	action: tensor([[ 0.8886, -0.6422,  0.1626,  0.5778, -0.5854,  0.7292, -0.7144]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.717939567006751, distance: 0.6077539321552486 entropy 0.03264415264129639
epoch: 39, step: 74
	action: tensor([[ 6.0744,  1.3774, -4.1185,  4.6927, -4.3250,  4.8907, -0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-12.2492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 75
	action: tensor([[ 0.9017, -0.2566, -0.6280,  0.4386, -0.4998,  0.2333, -0.4179]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7693985790170746, distance: 0.5495252883683929 entropy 0.03264415264129639
epoch: 39, step: 76
	action: tensor([[ 6.1800,  1.1218, -3.5003,  3.6220, -3.7663,  3.7328, -0.5968]],
       dtype=torch.float64)
	q_value: tensor([[-11.4639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 77
	action: tensor([[ 0.9807,  0.2400, -0.4460,  0.5953, -0.5333,  0.4733, -0.3071]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9801806493574385, distance: 0.1611021750484922 entropy 0.03264415264129639
epoch: 39, step: 78
	action: tensor([[ 6.1800,  1.2071, -3.5028,  3.3889, -3.3597,  3.9175, -0.5914]],
       dtype=torch.float64)
	q_value: tensor([[-14.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 79
	action: tensor([[ 0.9588,  0.4825, -0.4924,  0.6879, -0.3763,  0.4831,  0.1461]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.900060490742453, distance: 0.36176396047847825 entropy 0.03264415264129639
epoch: 39, step: 80
	action: tensor([[ 6.1800,  1.1790, -3.0837,  3.4129, -3.4851,  4.0850, -0.7933]],
       dtype=torch.float64)
	q_value: tensor([[-13.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 81
	action: tensor([[ 1.0739,  0.2109, -1.1121,  0.3729, -0.2957,  0.5097, -0.1898]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9464162824208501, distance: 0.26489460535081416 entropy 0.03264415264129639
epoch: 39, step: 82
	action: tensor([[ 6.1428,  0.8602, -3.6675,  4.0718, -3.6693,  4.0299, -0.2247]],
       dtype=torch.float64)
	q_value: tensor([[-16.1121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 83
	action: tensor([[ 1.0295,  0.1245, -0.3984,  0.8316, -1.2731,  0.7317, -0.3977]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9540774738040109, distance: 0.24522773030074746 entropy 0.03264415264129639
epoch: 39, step: 84
	action: tensor([[ 6.1800,  1.5903, -3.2687,  4.1489, -3.7268,  4.4949, -0.6409]],
       dtype=torch.float64)
	q_value: tensor([[-18.2939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 85
	action: tensor([[ 1.2799,  0.2732, -0.4998,  0.5559, -0.4451,  0.2950, -0.3377]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9670300998669424, distance: 0.207785629853433 entropy 0.03264415264129639
epoch: 39, step: 86
	action: tensor([[ 6.1800,  1.2648, -3.7145,  3.8716, -3.9774,  4.0268, -0.6390]],
       dtype=torch.float64)
	q_value: tensor([[-15.7070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 87
	action: tensor([[ 0.7360, -0.0455, -0.3771,  0.8395, -0.5534,  0.7200,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8861585325205034, distance: 0.3861063426899127 entropy 0.03264415264129639
epoch: 39, step: 88
	action: tensor([[ 6.1800,  0.8366, -3.5062,  3.5965, -3.7427,  4.2127, -0.9500]],
       dtype=torch.float64)
	q_value: tensor([[-11.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 89
	action: tensor([[ 0.6134, -0.0632, -0.1816,  0.3521, -0.0461,  0.6808, -0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8898043571504096, distance: 0.3798734181871697 entropy 0.03264415264129639
epoch: 39, step: 90
	action: tensor([[ 5.7656,  0.5924, -2.9657,  3.4651, -3.6058,  3.9616, -0.4411]],
       dtype=torch.float64)
	q_value: tensor([[-8.3578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 91
	action: tensor([[ 1.2885,  0.2937, -0.7508,  0.7757, -0.5562,  0.3033, -0.0623]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.04378648449412264 entropy 0.03264415264129639
epoch: 39, step: 92
	action: tensor([[ 0.5268,  0.1601, -0.9792,  0.7820, -0.5994,  0.3269,  0.1551]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6332486141999013, distance: 0.6930149045983859 entropy 0.03264415264129639
epoch: 39, step: 93
	action: tensor([[ 5.6246,  1.1538, -3.1275,  2.7137, -2.8852,  3.0805, -0.7829]],
       dtype=torch.float64)
	q_value: tensor([[-12.2404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 94
	action: tensor([[ 0.8221, -0.0079, -0.5196,  0.5473, -0.0321,  0.6219, -0.0154]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9397212172913944, distance: 0.2809564003254182 entropy 0.03264415264129639
epoch: 39, step: 95
	action: tensor([[ 6.1800,  1.4376, -3.7018,  3.7848, -2.9813,  3.9212, -0.0642]],
       dtype=torch.float64)
	q_value: tensor([[-10.5522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 96
	action: tensor([[ 1.2081, -0.1408, -0.2242,  0.3842, -0.4158,  0.2172,  0.1761]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8056921969035833, distance: 0.5044310529441417 entropy 0.03264415264129639
epoch: 39, step: 97
	action: tensor([[ 6.1800,  0.6100, -3.3683,  3.7376, -2.9995,  3.8407, -0.8073]],
       dtype=torch.float64)
	q_value: tensor([[-11.3281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 98
	action: tensor([[ 1.1073,  0.1575, -0.5406,  0.2527, -0.3933,  0.6172, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9663629080464856, distance: 0.20987751552449618 entropy 0.03264415264129639
epoch: 39, step: 99
	action: tensor([[ 5.9573,  0.7145, -3.6966,  3.6610, -3.2955,  3.9859, -1.1436]],
       dtype=torch.float64)
	q_value: tensor([[-13.7842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 100
	action: tensor([[ 0.7607,  0.2031, -0.4551,  0.3495, -0.2647,  0.6834, -0.0886]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9420622303636698, distance: 0.2754467037666245 entropy 0.03264415264129639
epoch: 39, step: 101
	action: tensor([[ 6.1492,  0.6232, -2.7974,  3.6378, -2.9762,  3.2250, -0.7611]],
       dtype=torch.float64)
	q_value: tensor([[-11.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 102
	action: tensor([[ 1.4719, -0.0597, -0.5181,  1.1128, -0.3435,  0.6520, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9800749841319361, distance: 0.16153105562085737 entropy 0.03264415264129639
epoch: 39, step: 103
	action: tensor([[ 6.1800,  1.4735, -4.9899,  5.2722, -4.7949,  5.6143, -1.0772]],
       dtype=torch.float64)
	q_value: tensor([[-17.0060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 104
	action: tensor([[ 7.5457e-01,  6.8219e-04, -4.9145e-01,  1.3160e-01, -8.1947e-01,
          5.3929e-01, -2.3378e-01]], dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8111601649797149, distance: 0.4972828702016065 entropy 0.03264415264129639
epoch: 39, step: 105
	action: tensor([[ 5.2568,  1.2880, -2.8763,  2.9936, -2.7277,  3.2582, -0.3837]],
       dtype=torch.float64)
	q_value: tensor([[-12.7508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 106
	action: tensor([[ 0.8570,  0.3415, -0.1060,  0.6175, -0.3996,  0.5737, -0.2025]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9500164103177108, distance: 0.2558411594759492 entropy 0.03264415264129639
epoch: 39, step: 107
	action: tensor([[ 6.1800,  1.0113, -3.3980,  3.3641, -3.3086,  4.3316, -0.8641]],
       dtype=torch.float64)
	q_value: tensor([[-12.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 108
	action: tensor([[ 1.3185,  0.5208, -0.8391,  0.3227, -0.3147,  0.2147, -0.0689]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9618383406352737, distance: 0.22354775799414295 entropy 0.03264415264129639
epoch: 39, step: 109
	action: tensor([[ 6.1507,  0.9708, -3.2240,  3.5972, -3.8523,  3.4962, -0.3110]],
       dtype=torch.float64)
	q_value: tensor([[-16.7162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 110
	action: tensor([[ 1.2215,  0.2372, -0.5196,  0.1564, -0.7316,  0.6205, -0.2486]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9322477915510556, distance: 0.2978642770567611 entropy 0.03264415264129639
epoch: 39, step: 111
	action: tensor([[ 6.1574,  1.3014, -3.5577,  3.5705, -3.2620,  3.8382, -0.5841]],
       dtype=torch.float64)
	q_value: tensor([[-16.5812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 112
	action: tensor([[ 0.8873,  0.4563, -0.5012,  0.5698, -0.0297,  0.4508, -0.2022]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9245892809458061, distance: 0.3142485150953829 entropy 0.03264415264129639
epoch: 39, step: 113
	action: tensor([[ 6.1800,  0.9793, -2.9175,  3.5205, -3.0083,  3.8225, -0.8280]],
       dtype=torch.float64)
	q_value: tensor([[-12.5774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 114
	action: tensor([[ 0.9951,  0.3566, -0.4553,  0.0250, -0.1664,  0.4701,  0.3580]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9768434971325246, distance: 0.17413785049074545 entropy 0.03264415264129639
epoch: 39, step: 115
	action: tensor([[ 5.4165,  0.9837, -2.1903,  2.9490, -2.8145,  3.2649, -0.2940]],
       dtype=torch.float64)
	q_value: tensor([[-11.8152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 116
	action: tensor([[ 0.8182,  0.4016, -0.1262, -0.1668, -0.2828,  0.4788, -0.2204]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9583178841663309, distance: 0.23363159361994001 entropy 0.03264415264129639
epoch: 39, step: 117
	action: tensor([[ 4.5085,  0.7799, -2.2998,  2.6629, -2.3287,  2.4689, -0.8932]],
       dtype=torch.float64)
	q_value: tensor([[-10.8505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 118
	action: tensor([[ 1.4789,  0.3215, -0.5300,  0.0464, -0.3092,  0.4240, -0.0565]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8080062486789745, distance: 0.5014183695932989 entropy 0.03264415264129639
epoch: 39, step: 119
	action: tensor([[ 6.1662,  1.0136, -3.5326,  3.7689, -3.1875,  3.9559, -0.8434]],
       dtype=torch.float64)
	q_value: tensor([[-16.2252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 120
	action: tensor([[ 1.2510, -0.0351, -0.5844,  0.3299, -0.5469,  0.4142, -0.0616]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8639902676350965, distance: 0.4220284086519991 entropy 0.03264415264129639
epoch: 39, step: 121
	action: tensor([[ 6.1800,  1.0415, -3.6592,  3.4081, -3.6444,  4.0962, -0.5116]],
       dtype=torch.float64)
	q_value: tensor([[-14.1692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 122
	action: tensor([[ 1.0526,  0.1858, -0.9075,  0.3426, -0.4805,  0.4779,  0.3570]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9498437147646508, distance: 0.256282749739405 entropy 0.03264415264129639
epoch: 39, step: 123
	action: tensor([[ 6.1139,  1.0234, -2.8133,  3.0928, -3.3505,  3.9564, -0.4062]],
       dtype=torch.float64)
	q_value: tensor([[-14.4731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 124
	action: tensor([[ 0.7822,  0.3630, -0.6486,  0.3572, -0.4945,  0.6621, -0.0959]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.90878761480647, distance: 0.3456078510117979 entropy 0.03264415264129639
epoch: 39, step: 125
	action: tensor([[ 5.3696,  0.7765, -3.3913,  2.6858, -2.8520,  3.2812, -0.5160]],
       dtype=torch.float64)
	q_value: tensor([[-14.0005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 39, step: 126
	action: tensor([[ 1.0110,  0.2030, -0.2593,  0.3188, -0.4713,  0.5028,  0.0539]],
       dtype=torch.float64)
	q_value: tensor([[-21.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9860919378306147, distance: 0.13495531863759377 entropy 0.03264415264129639
epoch: 39, step: 127
	action: tensor([[ 6.1800,  1.1159, -3.3358,  3.3717, -3.0766,  3.6548, -0.6874]],
       dtype=torch.float64)
	q_value: tensor([[-12.1071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 39 actor 661.8621209316335 critic 1410.8061481154305 
epoch: 40, step: 0
	action: tensor([[ 0.7365, -0.0894, -0.2506,  0.4320, -0.0131,  0.4608,  0.3601]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9060025704598175, distance: 0.3508445075215937 entropy 0.03264415264129639
epoch: 40, step: 1
	action: tensor([[ 6.1800,  0.6994, -2.4492,  4.3409, -2.8535,  3.5762,  0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-14.5042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 2
	action: tensor([[ 0.9967,  0.4085, -0.4737,  0.8464, -0.8173,  0.4635,  0.0650]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9035722948833963, distance: 0.35535105472910133 entropy 0.03264415264129639
epoch: 40, step: 3
	action: tensor([[ 6.1800,  0.2855, -2.0763,  5.1741, -3.2585,  4.4510,  0.4239]],
       dtype=torch.float64)
	q_value: tensor([[-27.6886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 4
	action: tensor([[ 1.5467,  0.4408, -0.6715,  0.6090, -0.0511,  0.4310, -0.3013]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9452760010296727, distance: 0.26769829572131215 entropy 0.03264415264129639
epoch: 40, step: 5
	action: tensor([[ 6.1565,  0.5189, -3.2313,  5.8469, -4.4395,  5.3434,  0.2949]],
       dtype=torch.float64)
	q_value: tensor([[-32.1625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5830566530057955 entropy 0.03264415264129639
epoch: 40, step: 6
	action: tensor([[ 1.2341, -0.3388, -0.4866,  0.8255, -0.2850,  0.6200, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9273972405323307, distance: 0.30834240611896124 entropy 0.03264415264129639
epoch: 40, step: 7
	action: tensor([[ 6.1116,  0.2222, -3.3343,  6.1800, -4.3555,  5.7007,  0.2969]],
       dtype=torch.float64)
	q_value: tensor([[-24.2448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8423508959479884 entropy 0.03264415264129639
epoch: 40, step: 8
	action: tensor([[ 1.3342,  0.0360, -0.4457,  0.1475, -0.5571,  0.6330,  0.5071]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8747036831816817, distance: 0.40506605063295964 entropy 0.03264415264129639
epoch: 40, step: 9
	action: tensor([[ 6.1271,  0.3860, -2.7712,  4.6917, -3.4881,  4.1023,  0.2534]],
       dtype=torch.float64)
	q_value: tensor([[-25.3660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 10
	action: tensor([[ 1.2677,  0.5260, -0.4167,  0.3415, -0.3847,  0.5379, -0.2079]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9673208812919707, distance: 0.20686730719269167 entropy 0.03264415264129639
epoch: 40, step: 11
	action: tensor([[ 6.1800,  0.5586, -3.0240,  4.3789, -3.1561,  4.5593,  0.6122]],
       dtype=torch.float64)
	q_value: tensor([[-28.9318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 12
	action: tensor([[ 1.0420,  0.1548, -0.3557,  0.3628, -0.1405,  0.3815,  0.6296]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9813242434252392, distance: 0.15638525246466628 entropy 0.03264415264129639
epoch: 40, step: 13
	action: tensor([[ 6.0895, -0.1065, -2.0704,  4.4189, -2.9305,  4.1739,  0.7911]],
       dtype=torch.float64)
	q_value: tensor([[-19.2094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 14
	action: tensor([[ 0.8226, -0.0074, -0.4401,  0.6558, -0.5903,  0.6919, -0.0495]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9191448471181164, distance: 0.3253947522014728 entropy 0.03264415264129639
epoch: 40, step: 15
	action: tensor([[ 6.1800,  0.5913, -2.9721,  4.6476, -3.8180,  4.6430,  0.2892]],
       dtype=torch.float64)
	q_value: tensor([[-23.2842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 16
	action: tensor([[ 0.6470, -0.0883, -0.5172,  0.6981, -0.5492,  0.3418, -0.0908]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8279347838434493, distance: 0.4746825158285315 entropy 0.03264415264129639
epoch: 40, step: 17
	action: tensor([[ 6.1609,  0.1709, -2.4590,  4.5376, -2.8310,  3.3237,  0.8650]],
       dtype=torch.float64)
	q_value: tensor([[-18.7665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 18
	action: tensor([[ 0.6123,  0.2216, -0.4451,  0.6890, -0.1961,  0.6224, -0.4205]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8411220505162367, distance: 0.4561298626948603 entropy 0.03264415264129639
epoch: 40, step: 19
	action: tensor([[ 6.1800,  0.4404, -2.7191,  4.2410, -2.9735,  4.1864,  0.3832]],
       dtype=torch.float64)
	q_value: tensor([[-21.3855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 20
	action: tensor([[ 0.9840,  0.5379, -0.4078,  0.7610,  0.1684,  0.8145, -0.2410]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8092221461788257, distance: 0.49982810513779796 entropy 0.03264415264129639
epoch: 40, step: 21
	action: tensor([[ 6.1800,  0.3110, -3.3039,  5.6225, -3.8855,  5.2759,  0.7341]],
       dtype=torch.float64)
	q_value: tensor([[-26.1814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 22
	action: tensor([[ 1.4208, -0.0962, -0.2712,  0.3037, -0.3657,  0.7824, -0.0895]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8275360269447627, distance: 0.47523223004258536 entropy 0.03264415264129639
epoch: 40, step: 23
	action: tensor([[ 6.1800,  0.5117, -2.9371,  5.8698, -4.1155,  5.1509,  0.3046]],
       dtype=torch.float64)
	q_value: tensor([[-26.8779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 24
	action: tensor([[ 0.9348, -0.1273, -1.0670,  1.0886, -0.6363,  0.6179, -0.0728]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7015986894425059, distance: 0.6251108779011745 entropy 0.03264415264129639
epoch: 40, step: 25
	action: tensor([[ 6.1800,  0.6463, -3.6359,  5.7759, -3.7504,  4.8744,  0.5301]],
       dtype=torch.float64)
	q_value: tensor([[-29.2244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 26
	action: tensor([[ 0.8268,  0.0628, -0.7994,  0.6995, -0.1530,  0.8253, -0.0218]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8685010275743148, distance: 0.4149711198874785 entropy 0.03264415264129639
epoch: 40, step: 27
	action: tensor([[ 6.0900,  0.6180, -2.8616,  5.8717, -3.5621,  4.7734,  0.6941]],
       dtype=torch.float64)
	q_value: tensor([[-24.6919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 28
	action: tensor([[ 1.0454, -0.0378, -0.0929,  0.8197, -0.4397,  0.7572,  0.2771]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07489183028739517 entropy 0.03264415264129639
epoch: 40, step: 29
	action: tensor([[ 0.6898,  0.0025, -0.3391,  0.6859, -0.3447,  0.8596, -0.2403]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8937721030587076, distance: 0.37297179016431015 entropy 0.03264415264129639
epoch: 40, step: 30
	action: tensor([[ 6.1800,  0.3444, -2.7091,  5.2206, -3.4354,  4.4294,  0.5705]],
       dtype=torch.float64)
	q_value: tensor([[-22.2839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 31
	action: tensor([[ 0.7992, -0.1538, -0.6033,  0.4507, -0.0425,  0.5707,  0.2143]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8558354650978786, distance: 0.43449612827393724 entropy 0.03264415264129639
epoch: 40, step: 32
	action: tensor([[ 6.1800,  0.4084, -2.2651,  5.0833, -3.1260,  4.4434,  0.4569]],
       dtype=torch.float64)
	q_value: tensor([[-17.4689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 33
	action: tensor([[ 0.7943,  0.4357, -0.3740,  0.9045, -0.3568,  0.1974,  0.3975]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8570428331508135, distance: 0.43267286524414394 entropy 0.03264415264129639
epoch: 40, step: 34
	action: tensor([[ 5.6039,  0.1878, -2.3250,  4.4831, -3.1642,  3.6426,  0.5935]],
       dtype=torch.float64)
	q_value: tensor([[-20.3162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 35
	action: tensor([[ 0.6043, -0.1350, -0.5044, -0.2376, -0.4349,  0.4971, -0.3627]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5852509970059099, distance: 0.7369692522040614 entropy 0.03264415264129639
epoch: 40, step: 36
	action: tensor([[ 4.6623,  0.5907, -2.0325,  3.6416, -1.9354,  3.1317,  0.1821]],
       dtype=torch.float64)
	q_value: tensor([[-18.0495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 37
	action: tensor([[ 1.1331, -0.0099, -0.1000,  0.5973, -0.6302,  0.6126,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9798590286042246, distance: 0.1624040665351353 entropy 0.03264415264129639
epoch: 40, step: 38
	action: tensor([[ 6.1350,  0.2793, -3.2078,  5.0384, -3.5817,  4.9805,  0.5525]],
       dtype=torch.float64)
	q_value: tensor([[-24.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 39
	action: tensor([[ 1.2160, -0.1548, -0.3073,  0.4172, -0.6808,  0.5047, -0.0354]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8598346366254253, distance: 0.4284272112071808 entropy 0.03264415264129639
epoch: 40, step: 40
	action: tensor([[ 6.1800,  0.2591, -2.5960,  5.2682, -3.1012,  4.5544,  0.6075]],
       dtype=torch.float64)
	q_value: tensor([[-24.3413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 41
	action: tensor([[ 0.6210,  0.2386, -0.1005,  0.1008, -0.4809,  0.6324, -0.1311]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9049814328611205, distance: 0.352745053002451 entropy 0.03264415264129639
epoch: 40, step: 42
	action: tensor([[ 4.6718,  0.7162, -1.7538,  3.5391, -2.3604,  3.2111,  0.1044]],
       dtype=torch.float64)
	q_value: tensor([[-18.7071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 43
	action: tensor([[ 1.1614,  0.4003, -0.4612,  0.7213, -0.7060,  0.7036, -0.1872]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9580230286070376, distance: 0.23445648189268356 entropy 0.03264415264129639
epoch: 40, step: 44
	action: tensor([[ 6.0954,  0.4211, -3.0719,  5.6531, -3.3046,  4.9145,  0.2166]],
       dtype=torch.float64)
	q_value: tensor([[-31.3067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 45
	action: tensor([[ 0.8998,  0.1030, -0.4210,  0.5845, -0.7589,  0.9348,  0.1899]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9356405336212286, distance: 0.290310611508665 entropy 0.03264415264129639
epoch: 40, step: 46
	action: tensor([[ 6.1800,  0.4199, -2.8788,  4.8696, -3.5463,  4.3513,  0.4810]],
       dtype=torch.float64)
	q_value: tensor([[-27.0826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 47
	action: tensor([[ 0.8934,  0.0894, -0.6128,  0.6184, -0.1462,  0.4019,  0.4557]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9566378504244409, distance: 0.23829344443268524 entropy 0.03264415264129639
epoch: 40, step: 48
	action: tensor([[ 6.1800,  0.5971, -2.7627,  4.6131, -3.5030,  3.9765,  0.6229]],
       dtype=torch.float64)
	q_value: tensor([[-19.5240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 49
	action: tensor([[ 0.7624,  0.1980, -0.2912,  0.5557, -0.1000,  0.6108,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9565559896969644, distance: 0.23851826812256835 entropy 0.03264415264129639
epoch: 40, step: 50
	action: tensor([[ 5.8662,  0.3145, -2.3699,  4.3563, -2.6174,  4.0870,  0.4409]],
       dtype=torch.float64)
	q_value: tensor([[-18.6338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 51
	action: tensor([[ 0.9778, -0.1150, -0.3952,  0.5366, -0.7157,  0.6477, -0.1780]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9290623150639308, distance: 0.30478612949166495 entropy 0.03264415264129639
epoch: 40, step: 52
	action: tensor([[ 6.1800,  0.6121, -2.7014,  5.2559, -3.3998,  4.2754,  0.6284]],
       dtype=torch.float64)
	q_value: tensor([[-24.6351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 53
	action: tensor([[ 1.2288, -0.0636, -0.6394,  0.6734, -0.9447,  0.8808, -0.2439]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9255376655491991, distance: 0.3122662278518617 entropy 0.03264415264129639
epoch: 40, step: 54
	action: tensor([[ 6.1800,  0.2915, -3.1383,  5.5782, -4.0530,  4.8065,  0.6898]],
       dtype=torch.float64)
	q_value: tensor([[-33.4054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 55
	action: tensor([[ 1.1119,  0.2866,  0.0664,  0.4882, -0.5896,  0.7400, -0.2429]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9817963390775015, distance: 0.15439600613131546 entropy 0.03264415264129639
epoch: 40, step: 56
	action: tensor([[ 6.1800,  0.8460, -2.6553,  4.6988, -3.3270,  4.5527,  1.1320]],
       dtype=torch.float64)
	q_value: tensor([[-26.0086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 57
	action: tensor([[ 0.7206,  0.3850, -0.2938,  0.5575, -0.1184,  0.7458,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.890589847211374, distance: 0.37851710124902754 entropy 0.03264415264129639
epoch: 40, step: 58
	action: tensor([[ 6.1800,  0.1815, -2.3836,  4.5905, -3.6225,  4.2396,  0.4046]],
       dtype=torch.float64)
	q_value: tensor([[-20.9085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 59
	action: tensor([[ 0.7588,  0.5235, -0.3875,  0.6953, -0.6370,  0.6076,  0.1328]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8208147040840015, distance: 0.4844041747698261 entropy 0.03264415264129639
epoch: 40, step: 60
	action: tensor([[ 5.7985,  0.0355, -2.2152,  4.5649, -3.1880,  3.4762,  0.7691]],
       dtype=torch.float64)
	q_value: tensor([[-25.1031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 61
	action: tensor([[ 1.3676, -0.0428, -0.5332,  0.9905, -0.2862,  0.7129, -0.1084]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9889338716958471, distance: 0.12038005679745706 entropy 0.03264415264129639
epoch: 40, step: 62
	action: tensor([[ 6.1800,  0.6027, -3.5247,  5.7943, -4.8762,  5.7386,  0.9352]],
       dtype=torch.float64)
	q_value: tensor([[-29.3707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7290549246721911 entropy 0.03264415264129639
epoch: 40, step: 63
	action: tensor([[ 0.9217, -0.0522, -0.5182,  1.0189,  0.0408,  0.5617,  0.2075]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9417584609327311, distance: 0.27616784734435906 entropy 0.03264415264129639
epoch: 40, step: 64
	action: tensor([[ 6.1800,  0.5796, -3.2404,  5.6697, -3.8019,  4.7511,  0.8316]],
       dtype=torch.float64)
	q_value: tensor([[-21.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 65
	action: tensor([[ 1.0644,  0.2726, -0.5596,  0.3346, -0.5059,  0.2830,  0.1806]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9804441435562614, distance: 0.16002768163133907 entropy 0.03264415264129639
epoch: 40, step: 66
	action: tensor([[ 5.5715,  0.1582, -2.5214,  3.8014, -3.1695,  3.4256,  0.3174]],
       dtype=torch.float64)
	q_value: tensor([[-23.0189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 67
	action: tensor([[ 0.7169, -0.3620, -0.3528,  0.5133, -0.7109,  0.5808,  0.0689]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7424651406887299, distance: 0.5807305918409562 entropy 0.03264415264129639
epoch: 40, step: 68
	action: tensor([[ 6.1800,  0.2184, -3.0081,  4.4005, -2.9806,  4.0675,  0.2130]],
       dtype=torch.float64)
	q_value: tensor([[-19.1025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 69
	action: tensor([[ 0.9020,  0.2262, -0.6238,  0.6045, -0.0359,  0.7346,  0.2307]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9478911332359622, distance: 0.26122365846515133 entropy 0.03264415264129639
epoch: 40, step: 70
	action: tensor([[ 6.1800,  0.2934, -2.5196,  5.5360, -3.7350,  4.2280,  0.2259]],
       dtype=torch.float64)
	q_value: tensor([[-22.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 71
	action: tensor([[ 0.5903, -0.2813, -0.1086, -0.1201, -0.2907,  0.9152, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6314835484328594, distance: 0.6946805406631164 entropy 0.03264415264129639
epoch: 40, step: 72
	action: tensor([[ 5.5082,  0.1917, -2.5170,  4.1818, -2.3440,  3.5935,  1.0888]],
       dtype=torch.float64)
	q_value: tensor([[-16.3745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 73
	action: tensor([[ 0.9430, -0.2169, -0.3385,  0.5811, -0.0887,  0.6668,  0.0937]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9385982939379452, distance: 0.28356126976178114 entropy 0.03264415264129639
epoch: 40, step: 74
	action: tensor([[ 6.1800,  0.5165, -3.0045,  5.4020, -3.6643,  4.7309,  0.4550]],
       dtype=torch.float64)
	q_value: tensor([[-18.8845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 75
	action: tensor([[ 0.7683,  0.2309, -0.4377,  0.4535, -0.7314,  0.2315, -0.2408]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9509591708807573, distance: 0.2534169124460763 entropy 0.03264415264129639
epoch: 40, step: 76
	action: tensor([[ 5.2983,  0.5825, -2.3348,  3.7761, -2.5166,  2.9921,  0.4162]],
       dtype=torch.float64)
	q_value: tensor([[-21.7705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 77
	action: tensor([[ 0.7629, -0.1073, -0.0549,  1.1445, -0.4829,  0.5044, -0.2195]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9553581512759053, distance: 0.24178412295925758 entropy 0.03264415264129639
epoch: 40, step: 78
	action: tensor([[ 6.0079,  0.5477, -2.5914,  5.7095, -3.6726,  4.9834,  0.5832]],
       dtype=torch.float64)
	q_value: tensor([[-21.1423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 79
	action: tensor([[ 0.9433, -0.2358, -0.0103,  0.5186,  0.1726,  0.5103,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9139922981970083, distance: 0.33560263714265476 entropy 0.03264415264129639
epoch: 40, step: 80
	action: tensor([[ 6.1594,  0.1502, -2.7439,  5.2464, -3.7968,  4.6656,  0.4623]],
       dtype=torch.float64)
	q_value: tensor([[-15.8121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 81
	action: tensor([[ 0.8368,  0.0645, -0.4755,  0.8132, -0.2405,  0.5440,  0.4360]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.942208654748965, distance: 0.2750984194840154 entropy 0.03264415264129639
epoch: 40, step: 82
	action: tensor([[ 6.1800,  0.5490, -2.5015,  4.7297, -3.1267,  3.9849,  0.5670]],
       dtype=torch.float64)
	q_value: tensor([[-20.0972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 83
	action: tensor([[ 1.1721,  0.1666, -0.4129,  0.7427, -0.9732,  0.6488,  0.1083]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06962213392118137 entropy 0.03264415264129639
epoch: 40, step: 84
	action: tensor([[ 1.0632,  0.4323, -0.0244,  0.8756, -0.5439,  0.5335, -0.1396]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8772461837158854, distance: 0.40093520734200716 entropy 0.03264415264129639
epoch: 40, step: 85
	action: tensor([[ 6.1800,  0.8573, -2.6043,  5.4736, -3.3691,  4.7387,  0.1814]],
       dtype=torch.float64)
	q_value: tensor([[-25.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 86
	action: tensor([[ 0.5512, -0.0765, -0.6119,  0.4987, -0.4324,  0.1100,  0.1271]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7510639726404909, distance: 0.5709532778175308 entropy 0.03264415264129639
epoch: 40, step: 87
	action: tensor([[ 4.5374,  0.7094, -1.8231,  3.7508, -2.2220,  3.0223,  0.5466]],
       dtype=torch.float64)
	q_value: tensor([[-14.8833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 88
	action: tensor([[ 0.8536,  0.2480, -0.2032,  0.6067, -0.0453,  0.5789, -0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.972743628067307, distance: 0.18892555934850278 entropy 0.03264415264129639
epoch: 40, step: 89
	action: tensor([[ 6.1800,  0.2647, -3.1247,  4.5007, -2.9784,  4.1314,  0.3383]],
       dtype=torch.float64)
	q_value: tensor([[-19.5724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 90
	action: tensor([[ 0.9188,  0.2926, -0.4026,  0.4641, -0.6120,  1.0081, -0.6026]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9466886536753732, distance: 0.2642205050927119 entropy 0.03264415264129639
epoch: 40, step: 91
	action: tensor([[ 6.1800,  0.4663, -2.9445,  5.1255, -3.5651,  4.9295,  0.5559]],
       dtype=torch.float64)
	q_value: tensor([[-30.6863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 92
	action: tensor([[ 1.1037,  0.4579, -0.9691,  0.7109, -0.4231,  0.1153,  0.4259]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9717923223411601, distance: 0.19219423622380818 entropy 0.03264415264129639
epoch: 40, step: 93
	action: tensor([[ 6.1800,  0.5195, -2.1022,  4.5328, -2.5462,  3.7235,  0.4263]],
       dtype=torch.float64)
	q_value: tensor([[-26.7164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 94
	action: tensor([[ 0.6509,  0.3656, -0.7132,  0.6077, -0.8889,  0.3217,  0.3793]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8362088734306857, distance: 0.46312889548153885 entropy 0.03264415264129639
epoch: 40, step: 95
	action: tensor([[ 5.0205,  0.4189, -2.1444,  3.3611, -2.4558,  3.1714,  0.2603]],
       dtype=torch.float64)
	q_value: tensor([[-23.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 96
	action: tensor([[ 0.9217, -0.0939,  0.0453,  0.6547, -0.4644,  0.5199, -0.9934]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9832654348001193, distance: 0.14803483015049212 entropy 0.03264415264129639
epoch: 40, step: 97
	action: tensor([[ 6.1800,  0.8563, -3.4313,  5.6537, -3.8821,  4.9380,  0.8553]],
       dtype=torch.float64)
	q_value: tensor([[-24.0802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 98
	action: tensor([[ 1.0007,  0.6971, -0.5890,  0.7468, -0.4883,  0.5307, -0.2877]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8124390142518857, distance: 0.4955961758061359 entropy 0.03264415264129639
epoch: 40, step: 99
	action: tensor([[ 6.1800,  0.7688, -2.8829,  4.6119, -3.7049,  4.4175, -0.0877]],
       dtype=torch.float64)
	q_value: tensor([[-30.4017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 100
	action: tensor([[ 0.7324,  0.1270, -0.4380,  0.3432, -0.7942,  0.7899,  0.1414]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8766737209240562, distance: 0.4018690011940098 entropy 0.03264415264129639
epoch: 40, step: 101
	action: tensor([[ 5.8312,  0.0178, -2.2710,  3.6718, -2.8750,  3.4842,  0.7775]],
       dtype=torch.float64)
	q_value: tensor([[-24.5506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 102
	action: tensor([[ 1.0543,  0.1514, -0.4807,  0.6310, -0.5480,  0.6253,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9892602418828819, distance: 0.11859160407159913 entropy 0.03264415264129639
epoch: 40, step: 103
	action: tensor([[ 6.1800,  0.3828, -2.4535,  5.4004, -3.6661,  4.3489,  0.6816]],
       dtype=torch.float64)
	q_value: tensor([[-25.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 104
	action: tensor([[ 0.8469,  0.0565, -0.5009,  0.8161, -0.1577,  0.7331,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9261743755820956, distance: 0.31092830384687836 entropy 0.03264415264129639
epoch: 40, step: 105
	action: tensor([[ 6.1800,  0.0093, -3.0039,  5.7012, -3.4378,  4.9025,  0.6545]],
       dtype=torch.float64)
	q_value: tensor([[-22.1403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 106
	action: tensor([[ 0.8744,  0.1599, -0.5053,  0.2396, -0.2795,  0.6644,  0.2631]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9564602511347462, distance: 0.23878093757936147 entropy 0.03264415264129639
epoch: 40, step: 107
	action: tensor([[ 5.8615,  0.3072, -2.2623,  3.9771, -3.1007,  3.2185,  0.1934]],
       dtype=torch.float64)
	q_value: tensor([[-20.8937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 108
	action: tensor([[ 0.9993,  0.1564, -0.5595,  0.5473, -0.6004,  0.7080, -0.5585]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9683673399849949, distance: 0.2035281804826471 entropy 0.03264415264129639
epoch: 40, step: 109
	action: tensor([[ 6.1800,  0.1921, -2.9430,  5.1276, -4.1743,  4.8099,  0.4084]],
       dtype=torch.float64)
	q_value: tensor([[-29.4597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 110
	action: tensor([[ 0.9905,  0.5622, -0.3573,  0.7207, -0.3013,  0.7171, -0.2223]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8314442956890992, distance: 0.4698166681211868 entropy 0.03264415264129639
epoch: 40, step: 111
	action: tensor([[ 6.1294,  0.4418, -3.0329,  5.1374, -4.2893,  4.2781,  0.0983]],
       dtype=torch.float64)
	q_value: tensor([[-27.5822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 112
	action: tensor([[ 0.9769, -0.0271, -1.0574,  0.6851, -0.3938,  1.0262,  0.4105]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8038580330327381, distance: 0.5068062434149094 entropy 0.03264415264129639
epoch: 40, step: 113
	action: tensor([[ 5.9012,  0.0236, -3.2762,  5.2307, -4.2830,  4.9207,  0.9056]],
       dtype=torch.float64)
	q_value: tensor([[-29.4482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 114
	action: tensor([[ 0.7628, -0.4070, -0.2159,  0.7776, -0.1758,  0.4311, -0.1964]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8651760688825466, distance: 0.4201846532030118 entropy 0.03264415264129639
epoch: 40, step: 115
	action: tensor([[ 6.1767,  0.6968, -3.2427,  4.8571, -3.6148,  4.8103,  0.5054]],
       dtype=torch.float64)
	q_value: tensor([[-16.8808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 116
	action: tensor([[ 0.8840, -0.1382, -0.4288,  1.1059, -0.2567,  0.9122,  0.2604]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9023811842111475, distance: 0.35753903260633757 entropy 0.03264415264129639
epoch: 40, step: 117
	action: tensor([[ 6.1800,  0.5267, -3.1480,  5.7127, -4.6239,  5.7897,  0.3388]],
       dtype=torch.float64)
	q_value: tensor([[-24.0086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6257269346762355 entropy 0.03264415264129639
epoch: 40, step: 118
	action: tensor([[ 0.5478,  0.3559, -0.6178,  1.0510, -0.2769,  0.3213, -0.2161]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6658065991141536, distance: 0.6615392677633408 entropy 0.03264415264129639
epoch: 40, step: 119
	action: tensor([[ 6.1800,  0.4818, -2.2185,  4.5456, -2.4020,  4.4324,  0.3652]],
       dtype=torch.float64)
	q_value: tensor([[-21.8054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 120
	action: tensor([[ 1.0007,  0.3788, -0.1117,  0.5619, -0.2795,  0.8025,  0.3216]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.944439436036186, distance: 0.2697366850793693 entropy 0.03264415264129639
epoch: 40, step: 121
	action: tensor([[ 6.1800,  0.5280, -2.5578,  4.7044, -3.7754,  4.1082,  0.8945]],
       dtype=torch.float64)
	q_value: tensor([[-23.3410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 122
	action: tensor([[ 1.1709, -0.0831, -0.1335,  0.6911, -0.4781,  0.6003,  0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9769885616377927, distance: 0.17359154805307822 entropy 0.03264415264129639
epoch: 40, step: 123
	action: tensor([[ 6.1800,  0.2502, -2.7046,  5.2858, -4.1247,  4.7833,  0.0456]],
       dtype=torch.float64)
	q_value: tensor([[-23.1434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 124
	action: tensor([[ 1.0450, -0.5424, -0.0790,  0.2869, -0.3933,  0.4421, -0.1378]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5441514244027259, distance: 0.7726218648363925 entropy 0.03264415264129639
epoch: 40, step: 125
	action: tensor([[ 6.1800,  0.2698, -2.8305,  5.2181, -3.2581,  3.9638,  0.3955]],
       dtype=torch.float64)
	q_value: tensor([[-17.5759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 40, step: 126
	action: tensor([[ 1.1173,  0.2954, -0.1505,  0.3844, -0.2390,  0.7543, -0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9879553712739564, distance: 0.12558952041315755 entropy 0.03264415264129639
epoch: 40, step: 127
	action: tensor([[ 6.1800,  0.2289, -2.9878,  5.8111, -3.9229,  4.4739,  0.7411]],
       dtype=torch.float64)
	q_value: tensor([[-24.8117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 40 actor 605.1423285832504 critic 732.2470630294332 
epoch: 41, step: 0
	action: tensor([[ 1.9180, -0.3204, -0.6719,  1.6377, -0.6875,  1.4370,  0.2536]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7811310373054736, distance: 0.5353635302555306 entropy 0.03264415264129639
epoch: 41, step: 1
	action: tensor([[ 6.1800,  1.1550, -5.1841,  6.0882, -6.2800,  6.1800,  3.9480]],
       dtype=torch.float64)
	q_value: tensor([[-66.3066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 41, step: 2
	action: tensor([[ 1.2941,  0.1746, -0.6675,  1.4110, -0.6667,  1.4947,  0.2072]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.750796213307717, distance: 0.5712602582553196 entropy 0.03264415264129639
epoch: 41, step: 3
	action: tensor([[ 5.9413,  0.5698, -4.1271,  6.0749, -6.1329,  5.8047,  2.9818]],
       dtype=torch.float64)
	q_value: tensor([[-64.6453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1269947323768787 entropy 0.03264415264129639
epoch: 41, step: 4
	action: tensor([[ 1.5400,  0.2384, -0.9035,  1.3322, -0.9238,  1.3396,  0.9513]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8569981699853331, distance: 0.4327404485317863 entropy 0.03264415264129639
epoch: 41, step: 5
	action: tensor([[ 5.7977,  0.3685, -4.1074,  6.1317, -6.1514,  6.1800,  2.5994]],
       dtype=torch.float64)
	q_value: tensor([[-67.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2552256970167261 entropy 0.03264415264129639
epoch: 41, step: 6
	action: tensor([[ 1.3504,  0.3775, -0.5206,  1.2378, -0.5278,  0.9458,  0.4604]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8376711704616294, distance: 0.46105689573706343 entropy 0.03264415264129639
epoch: 41, step: 7
	action: tensor([[ 6.1800,  0.3935, -3.4218,  6.1800, -6.0914,  5.8404,  2.8059]],
       dtype=torch.float64)
	q_value: tensor([[-54.6560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.906227447018898 entropy 0.03264415264129639
epoch: 41, step: 8
	action: tensor([[ 1.7521,  0.0985, -0.6769,  1.2848, -1.1360,  1.3173,  0.7537]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9137518544063006, distance: 0.33607141639357707 entropy 0.03264415264129639
epoch: 41, step: 9
	action: tensor([[ 6.1800,  0.1607, -4.9502,  6.1800, -6.2800,  6.1800,  2.6271]],
       dtype=torch.float64)
	q_value: tensor([[-67.7042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0949577618719664 entropy 0.03264415264129639
epoch: 41, step: 10
	action: tensor([[ 1.8188, -0.2616, -0.5587,  1.5261, -0.6023,  1.2044,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8677154913416133, distance: 0.41620873214156984 entropy 0.03264415264129639
epoch: 41, step: 11
	action: tensor([[ 6.1800,  0.7429, -4.8525,  6.1800, -6.2800,  6.1800,  3.0172]],
       dtype=torch.float64)
	q_value: tensor([[-62.4084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8114377899330438 entropy 0.03264415264129639
epoch: 41, step: 12
	action: tensor([[ 1.6061,  0.2220, -0.9592,  1.6002, -1.1937,  1.1782, -0.0280]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.812764646476448, distance: 0.49516577661908606 entropy 0.03264415264129639
epoch: 41, step: 13
	action: tensor([[ 6.1800,  0.4495, -4.2842,  5.8326, -6.2800,  6.1800,  3.1492]],
       dtype=torch.float64)
	q_value: tensor([[-76.9303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0998795446733327 entropy 0.03264415264129639
epoch: 41, step: 14
	action: tensor([[ 2.2402,  0.0354, -0.1184,  1.0454, -1.0356,  1.2514,  0.5382]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9247063358317029, distance: 0.31400452717115684 entropy 0.03264415264129639
epoch: 41, step: 15
	action: tensor([[ 6.1234,  0.2014, -4.1584,  6.1800, -6.1576,  5.9493,  3.2127]],
       dtype=torch.float64)
	q_value: tensor([[-60.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0237284934363502 entropy 0.03264415264129639
epoch: 41, step: 16
	action: tensor([[ 1.4398,  0.1434, -0.4805,  1.4522, -0.7919,  1.0111,  0.6020]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8758660073456549, distance: 0.40318285461100933 entropy 0.03264415264129639
epoch: 41, step: 17
	action: tensor([[ 6.1800,  0.5369, -4.3387,  6.0933, -6.2800,  6.1800,  2.6914]],
       dtype=torch.float64)
	q_value: tensor([[-58.1390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9900896173776493 entropy 0.03264415264129639
epoch: 41, step: 18
	action: tensor([[ 1.8601, -0.0380, -0.5877,  1.4228, -0.9948,  1.3411,  0.4139]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8879361738655592, distance: 0.3830799450069646 entropy 0.03264415264129639
epoch: 41, step: 19
	action: tensor([[ 6.1800,  0.6726, -4.3381,  6.1800, -6.2035,  6.1800,  2.9832]],
       dtype=torch.float64)
	q_value: tensor([[-67.0764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8108797387915158 entropy 0.03264415264129639
epoch: 41, step: 20
	action: tensor([[ 1.7228,  0.4132, -0.4120,  1.3031, -0.7693,  1.3048,  0.5329]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8511240570355749, distance: 0.4415388842906503 entropy 0.03264415264129639
epoch: 41, step: 21
	action: tensor([[ 5.8850,  0.8662, -3.9908,  6.1315, -5.8607,  6.1800,  3.4761]],
       dtype=torch.float64)
	q_value: tensor([[-65.2466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 41, step: 22
	action: tensor([[ 1.6982,  0.1038, -0.2841,  0.9684, -0.5212,  1.1518, -0.0534]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9222287844347308, distance: 0.3191289011259139 entropy 0.03264415264129639
epoch: 41, step: 23
	action: tensor([[ 6.1800,  0.0696, -4.4220,  6.1800, -6.2800,  6.0551,  3.0525]],
       dtype=torch.float64)
	q_value: tensor([[-58.2538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1201848064675115 entropy 0.03264415264129639
epoch: 41, step: 24
	action: tensor([[ 1.9542,  0.1338, -0.4143,  1.9551, -0.3077,  0.9795,  0.0866]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.619158242207777, distance: 0.7062020497497643 entropy 0.03264415264129639
epoch: 41, step: 25
	action: tensor([[ 6.1800,  0.5253, -5.1649,  6.1800, -6.0693,  6.0329,  3.6459]],
       dtype=torch.float64)
	q_value: tensor([[-62.3064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9572930671348192 entropy 0.03264415264129639
epoch: 41, step: 26
	action: tensor([[ 1.6178,  0.4963, -0.5948,  1.5575, -0.8885,  0.7347,  0.1022]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8089594817780411, distance: 0.5001720703542318 entropy 0.03264415264129639
epoch: 41, step: 27
	action: tensor([[ 6.1606,  0.6701, -3.8764,  6.0734, -6.1363,  6.1800,  2.7403]],
       dtype=torch.float64)
	q_value: tensor([[-66.1363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.938958089265575 entropy 0.03264415264129639
epoch: 41, step: 28
	action: tensor([[ 1.5794, -0.2822, -0.3993,  1.5318, -0.7371,  1.1285,  0.4124]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8979237803859341, distance: 0.3656107713212147 entropy 0.03264415264129639
epoch: 41, step: 29
	action: tensor([[ 6.1800, -0.4150, -4.9120,  6.1800, -6.1498,  6.1800,  3.1320]],
       dtype=torch.float64)
	q_value: tensor([[-58.7480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.452315667529986 entropy 0.03264415264129639
epoch: 41, step: 30
	action: tensor([[ 1.7506, -0.0730, -0.4981,  1.9092, -0.7091,  1.2345,  0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7188621642292756, distance: 0.6067591606688773 entropy 0.03264415264129639
epoch: 41, step: 31
	action: tensor([[ 6.1800,  0.4931, -5.5510,  6.1476, -6.1987,  5.9891,  3.8654]],
       dtype=torch.float64)
	q_value: tensor([[-66.9001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9625857908206248 entropy 0.03264415264129639
epoch: 41, step: 32
	action: tensor([[ 1.4920, -0.1388, -0.6865,  1.6823, -0.8780,  1.1344,  0.5411]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8153666714849797, distance: 0.4917130593462504 entropy 0.03264415264129639
epoch: 41, step: 33
	action: tensor([[ 6.1800,  0.5591, -4.9692,  6.1800, -6.0388,  6.1800,  3.9470]],
       dtype=torch.float64)
	q_value: tensor([[-62.7880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9120737924339575 entropy 0.03264415264129639
epoch: 41, step: 34
	action: tensor([[ 2.2053,  0.2643, -0.3662,  1.5695, -0.9624,  1.2480,  0.6902]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8477364172075388, distance: 0.4465341881931073 entropy 0.03264415264129639
epoch: 41, step: 35
	action: tensor([[ 6.0003,  0.4661, -4.7644,  6.1405, -6.2800,  6.1800,  3.0566]],
       dtype=torch.float64)
	q_value: tensor([[-66.0393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0806177310025558 entropy 0.03264415264129639
epoch: 41, step: 36
	action: tensor([[ 1.1943, -0.0621, -0.7634,  1.1562, -1.0675,  1.3258, -0.0940]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7998015749446509, distance: 0.5120201135301586 entropy 0.03264415264129639
epoch: 41, step: 37
	action: tensor([[ 6.1624,  0.2694, -3.9706,  5.9246, -6.2390,  6.0674,  2.7123]],
       dtype=torch.float64)
	q_value: tensor([[-64.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.080950708542755 entropy 0.03264415264129639
epoch: 41, step: 38
	action: tensor([[ 2.2046,  0.1878, -0.5289,  0.9311, -1.1427,  1.1373,  0.3280]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9671846884793243, distance: 0.2072979269333043 entropy 0.03264415264129639
epoch: 41, step: 39
	action: tensor([[ 6.1483,  0.4045, -3.4243,  6.1800, -6.1520,  6.1800,  2.9568]],
       dtype=torch.float64)
	q_value: tensor([[-64.6889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8608098721850418 entropy 0.03264415264129639
epoch: 41, step: 40
	action: tensor([[ 1.5857,  0.2661, -0.7496,  1.9841, -0.7777,  1.2596,  1.0028]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6370783862194057, distance: 0.6893870324396302 entropy 0.03264415264129639
epoch: 41, step: 41
	action: tensor([[ 5.9530,  0.7803, -4.8427,  6.1800, -6.2800,  6.1800,  3.1847]],
       dtype=torch.float64)
	q_value: tensor([[-69.2273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0105153929075148 entropy 0.03264415264129639
epoch: 41, step: 42
	action: tensor([[ 1.6479, -0.2330, -0.2705,  1.4251, -1.1140,  1.3295, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9082206506546039, distance: 0.3466803134236819 entropy 0.03264415264129639
epoch: 41, step: 43
	action: tensor([[ 5.9424,  0.2970, -4.5605,  6.1386, -6.2800,  6.1800,  3.2053]],
       dtype=torch.float64)
	q_value: tensor([[-66.5430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.241527202537411 entropy 0.03264415264129639
epoch: 41, step: 44
	action: tensor([[ 1.6128,  0.3910, -0.7618,  1.6506, -0.7892,  1.2583,  0.1969]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7600361373656415, distance: 0.5605696945605632 entropy 0.03264415264129639
epoch: 41, step: 45
	action: tensor([[ 6.1800,  0.3740, -4.7547,  6.1800, -5.8966,  6.1800,  3.1945]],
       dtype=torch.float64)
	q_value: tensor([[-72.1276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.109876811106465 entropy 0.03264415264129639
epoch: 41, step: 46
	action: tensor([[ 1.8849, -0.0421, -0.3979,  1.8046, -0.6187,  0.9162,  0.4850]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7993424212315593, distance: 0.5126069345625137 entropy 0.03264415264129639
epoch: 41, step: 47
	action: tensor([[ 6.1800,  0.0636, -4.8413,  6.1800, -6.1353,  6.1800,  2.7689]],
       dtype=torch.float64)
	q_value: tensor([[-59.6322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0946085027392338 entropy 0.03264415264129639
epoch: 41, step: 48
	action: tensor([[ 1.4338, -0.0430, -0.0538,  1.9360, -0.6495,  1.0464,  0.1759]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6724972012879031, distance: 0.6548837302908 entropy 0.03264415264129639
epoch: 41, step: 49
	action: tensor([[ 6.1415,  0.6235, -4.9002,  6.1800, -6.2800,  6.1800,  3.0483]],
       dtype=torch.float64)
	q_value: tensor([[-58.4696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9953312362770266 entropy 0.03264415264129639
epoch: 41, step: 50
	action: tensor([[ 1.4871,  0.4670, -0.6051,  1.4976, -0.9296,  1.6371,  0.3919]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7771959837602777, distance: 0.5401547516109292 entropy 0.03264415264129639
epoch: 41, step: 51
	action: tensor([[ 6.1800,  0.1920, -4.6494,  6.1800, -5.7693,  6.1800,  3.1200]],
       dtype=torch.float64)
	q_value: tensor([[-73.9034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1692206989190894 entropy 0.03264415264129639
epoch: 41, step: 52
	action: tensor([[ 2.0225,  0.4451, -0.4961,  1.6336, -0.5808,  1.2690, -0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7098360943388976, distance: 0.616422365056412 entropy 0.03264415264129639
epoch: 41, step: 53
	action: tensor([[ 6.1800,  0.3909, -4.6159,  5.9623, -6.1821,  6.1800,  3.6719]],
       dtype=torch.float64)
	q_value: tensor([[-69.7916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0367314898377957 entropy 0.03264415264129639
epoch: 41, step: 54
	action: tensor([[ 1.7968,  0.2233, -0.2689,  1.2174, -1.0898,  1.4223,  0.1584]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9211555270916574, distance: 0.3213233753365908 entropy 0.03264415264129639
epoch: 41, step: 55
	action: tensor([[ 6.1800,  0.0489, -4.3978,  5.9899, -6.2800,  5.9408,  2.8487]],
       dtype=torch.float64)
	q_value: tensor([[-68.8048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2152830077348524 entropy 0.03264415264129639
epoch: 41, step: 56
	action: tensor([[ 1.3624,  0.3244, -0.2341,  1.8681, -0.6410,  1.3692,  0.3538]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5802915455682381, distance: 0.7413623933315285 entropy 0.03264415264129639
epoch: 41, step: 57
	action: tensor([[ 6.1059,  0.1686, -4.3650,  6.1800, -6.2481,  6.1800,  3.3141]],
       dtype=torch.float64)
	q_value: tensor([[-64.1243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2435158288910324 entropy 0.03264415264129639
epoch: 41, step: 58
	action: tensor([[ 1.9371, -0.3489, -0.5793,  1.1119, -0.9801,  1.3454,  0.6534]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8589275884111897, distance: 0.42981121030209085 entropy 0.03264415264129639
epoch: 41, step: 59
	action: tensor([[ 5.7053,  0.1704, -4.3286,  6.1800, -6.2800,  6.1800,  3.3299]],
       dtype=torch.float64)
	q_value: tensor([[-61.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3571987331311384 entropy 0.03264415264129639
epoch: 41, step: 60
	action: tensor([[ 1.5856, -0.1668, -0.4980,  1.3331, -0.1725,  1.0949,  0.5948]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9037062980288554, distance: 0.3551040576966538 entropy 0.03264415264129639
epoch: 41, step: 61
	action: tensor([[ 6.1800,  0.3855, -4.4661,  6.1800, -6.1431,  6.1800,  3.2585]],
       dtype=torch.float64)
	q_value: tensor([[-53.2506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0353339205630632 entropy 0.03264415264129639
epoch: 41, step: 62
	action: tensor([[ 1.6040,  0.1840, -0.9653,  1.1187, -0.8754,  1.2780,  0.4688]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8941973867153942, distance: 0.37222444465703675 entropy 0.03264415264129639
epoch: 41, step: 63
	action: tensor([[ 6.1800,  0.5994, -4.3950,  6.0522, -6.2800,  6.1800,  3.0165]],
       dtype=torch.float64)
	q_value: tensor([[-68.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9435602951228055 entropy 0.03264415264129639
epoch: 41, step: 64
	action: tensor([[ 1.8173, -0.3553, -0.5073,  1.2602, -0.6644,  0.6104,  0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9383744293378621, distance: 0.28407771775406143 entropy 0.03264415264129639
epoch: 41, step: 65
	action: tensor([[ 6.0416,  0.5350, -4.0120,  6.0778, -6.2800,  5.9988,  2.9082]],
       dtype=torch.float64)
	q_value: tensor([[-52.1906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0090883492434952 entropy 0.03264415264129639
epoch: 41, step: 66
	action: tensor([[ 1.9019, -0.0511, -0.5683,  1.6167, -0.9919,  1.1940,  0.2100]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8637831900686967, distance: 0.42234955984956013 entropy 0.03264415264129639
epoch: 41, step: 67
	action: tensor([[ 5.7095,  0.1339, -4.8399,  6.1800, -6.2800,  6.1800,  3.2713]],
       dtype=torch.float64)
	q_value: tensor([[-67.4823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4942248841652048 entropy 0.03264415264129639
epoch: 41, step: 68
	action: tensor([[ 1.9683,  0.1529, -1.0334,  1.3837, -0.9383,  1.4463,  0.3828]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7876717975378502, distance: 0.5273033553075691 entropy 0.03264415264129639
epoch: 41, step: 69
	action: tensor([[ 6.1800,  0.4909, -4.6781,  6.1800, -6.2800,  6.1800,  2.9566]],
       dtype=torch.float64)
	q_value: tensor([[-73.5746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9247673935884152 entropy 0.03264415264129639
epoch: 41, step: 70
	action: tensor([[ 1.2555,  0.3287, -0.4455,  1.4468, -0.6640,  1.3715,  0.3923]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7255863388057566, distance: 0.5994591000250875 entropy 0.03264415264129639
epoch: 41, step: 71
	action: tensor([[ 6.1800,  0.0552, -4.0782,  5.9516, -6.2800,  6.0423,  3.0888]],
       dtype=torch.float64)
	q_value: tensor([[-61.0593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.124299350280908 entropy 0.03264415264129639
epoch: 41, step: 72
	action: tensor([[ 1.8795, -0.2773, -0.8377,  1.7296, -0.5930,  1.4471,  0.4407]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7094559913437448, distance: 0.6168259771196195 entropy 0.03264415264129639
epoch: 41, step: 73
	action: tensor([[ 6.0810,  0.3685, -5.7486,  6.1800, -5.7847,  5.9962,  3.2367]],
       dtype=torch.float64)
	q_value: tensor([[-67.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0757150718524497 entropy 0.03264415264129639
epoch: 41, step: 74
	action: tensor([[ 1.7651,  0.4461, -0.1164,  1.2889, -0.6823,  1.6430,  0.2853]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.752739545294999, distance: 0.5690285082738438 entropy 0.03264415264129639
epoch: 41, step: 75
	action: tensor([[ 6.1800,  0.5970, -4.3365,  5.7281, -6.2800,  5.9781,  3.0433]],
       dtype=torch.float64)
	q_value: tensor([[-68.0013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0609030720590087 entropy 0.03264415264129639
epoch: 41, step: 76
	action: tensor([[ 2.0117, -0.2357, -0.7194,  1.5237, -1.1286,  0.9902,  0.4545]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8955784015582341, distance: 0.36978718938670846 entropy 0.03264415264129639
epoch: 41, step: 77
	action: tensor([[ 5.7467,  0.4470, -4.6354,  6.1800, -6.2800,  6.1800,  3.3599]],
       dtype=torch.float64)
	q_value: tensor([[-64.2064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.298814538242715 entropy 0.03264415264129639
epoch: 41, step: 78
	action: tensor([[ 1.9461,  0.0694, -0.6220,  1.3015, -1.1411,  1.3630,  0.1192]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9042977291807485, distance: 0.35401186228701786 entropy 0.03264415264129639
epoch: 41, step: 79
	action: tensor([[ 6.1800,  0.5816, -4.6119,  6.0833, -6.1977,  5.9711,  3.1833]],
       dtype=torch.float64)
	q_value: tensor([[-71.0695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.918653095669849 entropy 0.03264415264129639
epoch: 41, step: 80
	action: tensor([[ 1.9952, -0.0059, -0.4395,  1.0667, -0.4787,  0.8965,  0.2588]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9484140472912992, distance: 0.2599096599896157 entropy 0.03264415264129639
epoch: 41, step: 81
	action: tensor([[ 6.1170,  0.4750, -3.7645,  5.7340, -6.2800,  6.1800,  2.6942]],
       dtype=torch.float64)
	q_value: tensor([[-53.8515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9346062051435887 entropy 0.03264415264129639
epoch: 41, step: 82
	action: tensor([[ 1.4856, -0.2026, -0.1389,  1.7908, -0.7878,  1.6105,  0.5495]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7507673070393798, distance: 0.571293388818278 entropy 0.03264415264129639
epoch: 41, step: 83
	action: tensor([[ 6.1800,  0.4111, -5.0840,  6.1800, -6.2800,  6.1800,  3.5613]],
       dtype=torch.float64)
	q_value: tensor([[-64.2620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0966695709344423 entropy 0.03264415264129639
epoch: 41, step: 84
	action: tensor([[ 2.1115,  0.6172, -0.6795,  1.3167, -0.6664,  0.8308,  0.2998]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8575559698078328, distance: 0.4318956397882467 entropy 0.03264415264129639
epoch: 41, step: 85
	action: tensor([[ 6.0992,  0.7786, -3.7223,  6.0950, -6.2800,  5.9736,  2.7195]],
       dtype=torch.float64)
	q_value: tensor([[-63.9183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 41, step: 86
	action: tensor([[ 1.5573,  0.2426, -0.3877,  0.9473, -0.7419,  1.3082,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9417880112495803, distance: 0.2760977780867549 entropy 0.03264415264129639
epoch: 41, step: 87
	action: tensor([[ 6.1075,  0.0635, -3.7749,  6.1800, -6.1962,  6.1800,  3.5067]],
       dtype=torch.float64)
	q_value: tensor([[-63.2442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2174953191401683 entropy 0.03264415264129639
epoch: 41, step: 88
	action: tensor([[ 2.3254, -0.3430, -0.8495,  1.8120, -0.9469,  1.2647,  0.1488]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.723893514599735, distance: 0.6013052520491728 entropy 0.03264415264129639
epoch: 41, step: 89
	action: tensor([[ 6.1800,  0.2859, -5.1790,  5.9048, -5.9944,  6.1800,  3.5275]],
       dtype=torch.float64)
	q_value: tensor([[-70.0135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0967338778450286 entropy 0.03264415264129639
epoch: 41, step: 90
	action: tensor([[ 1.8587,  0.4348, -0.6280,  1.2096, -0.6122,  1.2634,  0.4027]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8837034193167742, distance: 0.39024753294260156 entropy 0.03264415264129639
epoch: 41, step: 91
	action: tensor([[ 6.1800,  0.2249, -4.2467,  6.1800, -6.2587,  6.0978,  2.8082]],
       dtype=torch.float64)
	q_value: tensor([[-65.2554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9896678363610673 entropy 0.03264415264129639
epoch: 41, step: 92
	action: tensor([[ 1.9017,  0.2350, -0.7502,  1.2655, -0.5037,  1.3541,  0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8734891274882406, distance: 0.40702456305234563 entropy 0.03264415264129639
epoch: 41, step: 93
	action: tensor([[ 6.1800,  0.3462, -4.5787,  6.1281, -6.2800,  6.1800,  3.0637]],
       dtype=torch.float64)
	q_value: tensor([[-66.4964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9623021759146123 entropy 0.03264415264129639
epoch: 41, step: 94
	action: tensor([[ 2.2054,  0.5045, -1.0606,  1.6994, -0.5892,  1.3583,  0.1396]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6671293291491541, distance: 0.6602287909266996 entropy 0.03264415264129639
epoch: 41, step: 95
	action: tensor([[ 6.0747,  0.6983, -4.9294,  5.9694, -6.2800,  6.1800,  3.2713]],
       dtype=torch.float64)
	q_value: tensor([[-76.4903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0672286818559866 entropy 0.03264415264129639
epoch: 41, step: 96
	action: tensor([[ 1.8817, -0.1127, -1.0066,  1.0939, -0.8743,  1.5184,  0.3617]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7823942732739251, distance: 0.5338163279609923 entropy 0.03264415264129639
epoch: 41, step: 97
	action: tensor([[ 6.1800,  0.1720, -4.1592,  6.1800, -6.1735,  5.9625,  2.9754]],
       dtype=torch.float64)
	q_value: tensor([[-69.8143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0117543315796678 entropy 0.03264415264129639
epoch: 41, step: 98
	action: tensor([[ 1.9593,  0.2352, -0.0991,  1.2537, -1.0726,  1.3893,  0.4159]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8943296629705131, distance: 0.3719916911702931 entropy 0.03264415264129639
epoch: 41, step: 99
	action: tensor([[ 6.1800,  0.0635, -4.3015,  5.9736, -6.2779,  6.1800,  2.9478]],
       dtype=torch.float64)
	q_value: tensor([[-65.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1366197968423724 entropy 0.03264415264129639
epoch: 41, step: 100
	action: tensor([[ 2.3531, -0.0366, -1.1389,  1.5281, -0.3757,  1.4409,  0.1459]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7308315581461293, distance: 0.5937023439309334 entropy 0.03264415264129639
epoch: 41, step: 101
	action: tensor([[ 6.1800,  0.5455, -5.2521,  6.1800, -6.2800,  6.1800,  3.4940]],
       dtype=torch.float64)
	q_value: tensor([[-70.1234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8640583640681496 entropy 0.03264415264129639
epoch: 41, step: 102
	action: tensor([[ 1.6023, -0.3741, -0.7229,  1.7200, -0.9090,  0.7003,  0.5953]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9037088145466662, distance: 0.35509941756180546 entropy 0.03264415264129639
epoch: 41, step: 103
	action: tensor([[ 6.1800,  0.4189, -4.6855,  6.1800, -5.8182,  6.1800,  3.5454]],
       dtype=torch.float64)
	q_value: tensor([[-58.5742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0474943044684624 entropy 0.03264415264129639
epoch: 41, step: 104
	action: tensor([[ 1.8347, -0.0035, -0.6877,  1.0167, -0.3155,  1.1778,  0.3511]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9461037717246252, distance: 0.2656659408534878 entropy 0.03264415264129639
epoch: 41, step: 105
	action: tensor([[ 6.1800, -0.0536, -4.3290,  6.1800, -6.2800,  5.9019,  3.1748]],
       dtype=torch.float64)
	q_value: tensor([[-57.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1356715083072995 entropy 0.03264415264129639
epoch: 41, step: 106
	action: tensor([[ 1.4561, -0.3149, -0.5488,  1.5637, -0.8843,  0.9578,  0.6484]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9068717508497993, distance: 0.3492186364951408 entropy 0.03264415264129639
epoch: 41, step: 107
	action: tensor([[ 6.1396,  0.4865, -4.1652,  6.1608, -6.2800,  6.1800,  2.4690]],
       dtype=torch.float64)
	q_value: tensor([[-56.3012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0469701546022074 entropy 0.03264415264129639
epoch: 41, step: 108
	action: tensor([[ 1.9283, -0.4432, -0.6862,  1.3052, -0.8288,  1.5414,  0.3942]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8015812361691232, distance: 0.5097392352767083 entropy 0.03264415264129639
epoch: 41, step: 109
	action: tensor([[ 6.1800,  0.3787, -4.5867,  6.0967, -6.1598,  6.1800,  3.5871]],
       dtype=torch.float64)
	q_value: tensor([[-64.9722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9567942543393455 entropy 0.03264415264129639
epoch: 41, step: 110
	action: tensor([[ 1.7280, -0.1798, -0.1574,  1.6561, -0.9299,  1.0809,  0.0408]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8720086341346502, distance: 0.40939923811393236 entropy 0.03264415264129639
epoch: 41, step: 111
	action: tensor([[ 6.1800,  0.2326, -4.5799,  5.9970, -6.2800,  6.1800,  3.4231]],
       dtype=torch.float64)
	q_value: tensor([[-62.2899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1575624989090418 entropy 0.03264415264129639
epoch: 41, step: 112
	action: tensor([[ 2.0989, -0.2337, -0.4062,  1.5936, -0.9115,  1.2210, -0.1174]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8758388292148773, distance: 0.4032269890032225 entropy 0.03264415264129639
epoch: 41, step: 113
	action: tensor([[ 5.9684,  0.5141, -4.8157,  6.0457, -6.2554,  6.1800,  2.9161]],
       dtype=torch.float64)
	q_value: tensor([[-66.0451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1237914971759677 entropy 0.03264415264129639
epoch: 41, step: 114
	action: tensor([[ 1.6951, -0.3787, -0.7083,  1.3853, -1.2183,  0.8522,  0.4673]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8894009456884744, distance: 0.38056811577478444 entropy 0.03264415264129639
epoch: 41, step: 115
	action: tensor([[ 5.9257,  0.8092, -4.3031,  6.1133, -6.2800,  6.1800,  2.9284]],
       dtype=torch.float64)
	q_value: tensor([[-61.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0099971197515956 entropy 0.03264415264129639
epoch: 41, step: 116
	action: tensor([[ 1.5945,  0.0799, -1.1287,  1.6009, -0.8273,  1.3720,  0.6982]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7035245283807452, distance: 0.6230904250591232 entropy 0.03264415264129639
epoch: 41, step: 117
	action: tensor([[ 6.1800,  0.4015, -4.5844,  6.0279, -6.2800,  5.8753,  3.0212]],
       dtype=torch.float64)
	q_value: tensor([[-71.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.076970764085401 entropy 0.03264415264129639
epoch: 41, step: 118
	action: tensor([[ 1.5438, -0.0202, -0.7334,  1.3904, -0.9035,  0.6675,  0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9559781251127455, distance: 0.24009933702121317 entropy 0.03264415264129639
epoch: 41, step: 119
	action: tensor([[ 6.1244,  0.2839, -3.9781,  6.1326, -6.1843,  6.0862,  2.9625]],
       dtype=torch.float64)
	q_value: tensor([[-59.9699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1296615919110327 entropy 0.03264415264129639
epoch: 41, step: 120
	action: tensor([[ 1.7597,  0.0220, -0.5611,  1.2742, -0.8013,  1.6086, -0.2342]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8540326816107024, distance: 0.43720438391747635 entropy 0.03264415264129639
epoch: 41, step: 121
	action: tensor([[ 6.1141,  0.1796, -5.1921,  6.1800, -5.6408,  6.1800,  3.2902]],
       dtype=torch.float64)
	q_value: tensor([[-72.1748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.073393918777432 entropy 0.03264415264129639
epoch: 41, step: 122
	action: tensor([[ 1.7612, -0.5403, -0.7709,  1.8575, -1.2825,  1.1439,  0.3798]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7655885734929511, distance: 0.554046329615468 entropy 0.03264415264129639
epoch: 41, step: 123
	action: tensor([[ 6.0538,  0.7408, -5.1299,  5.8985, -6.2800,  6.1486,  3.1820]],
       dtype=torch.float64)
	q_value: tensor([[-68.7273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0832095723871613 entropy 0.03264415264129639
epoch: 41, step: 124
	action: tensor([[ 1.6226,  0.5366, -0.8665,  1.3815, -0.8545,  1.2021,  0.4490]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8505040648626353, distance: 0.4424573209128103 entropy 0.03264415264129639
epoch: 41, step: 125
	action: tensor([[ 6.1800,  0.1189, -4.4818,  5.8898, -6.2800,  6.1800,  3.0287]],
       dtype=torch.float64)
	q_value: tensor([[-70.9282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2331140779446825 entropy 0.03264415264129639
epoch: 41, step: 126
	action: tensor([[ 1.8333,  0.2792, -0.3434,  1.3369, -0.4300,  1.0670,  0.0408]],
       dtype=torch.float64)
	q_value: tensor([[-44.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8154978373884576, distance: 0.49153836865804607 entropy 0.03264415264129639
epoch: 41, step: 127
	action: tensor([[ 6.0912,  0.2175, -4.2308,  6.1800, -5.9389,  5.6749,  3.3382]],
       dtype=torch.float64)
	q_value: tensor([[-60.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.062670974740943 entropy 0.03264415264129639
LOSS epoch 41 actor 575.6535510977437 critic 136.5268104247435 
epoch: 42, step: 0
	action: tensor([[ 1.9629, -0.1817, -0.5640,  2.2217, -1.5350,  1.5504, -0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7189281141105595, distance: 0.6066879891000982 entropy 0.03264415264129639
epoch: 42, step: 1
	action: tensor([[ 6.1800e+00, -1.5736e+00, -2.8604e+00,  6.1439e+00, -6.2800e+00,
          6.1800e+00, -6.1062e-03]], dtype=torch.float64)
	q_value: tensor([[-88.0009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 2
	action: tensor([[ 1.8303, -0.3144, -0.0608,  1.6295, -1.1011,  1.3717, -0.2816]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8725342645821617, distance: 0.40855772003738006 entropy 0.03264415264129639
epoch: 42, step: 3
	action: tensor([[ 6.1800, -1.3752, -3.1127,  6.1800, -6.0533,  6.1800, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-72.4343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 4
	action: tensor([[ 1.9986, -0.3152, -0.1705,  1.4914, -0.9506,  1.1274, -0.1299]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9029877545397593, distance: 0.3564264883150148 entropy 0.03264415264129639
epoch: 42, step: 5
	action: tensor([[ 6.1800, -2.2758, -2.8091,  6.1800, -6.0810,  6.1800, -0.0591]],
       dtype=torch.float64)
	q_value: tensor([[-66.4156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 6
	action: tensor([[ 2.0708, -0.1293, -0.6851,  1.9982, -0.7323,  1.3952, -0.1020]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6356627398152707, distance: 0.6907302679877317 entropy 0.03264415264129639
epoch: 42, step: 7
	action: tensor([[ 6.1766, -2.0604, -3.5327,  6.0718, -6.0374,  5.8396, -0.5059]],
       dtype=torch.float64)
	q_value: tensor([[-77.7405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 8
	action: tensor([[ 2.1814, -0.0563, -0.1634,  1.5439, -1.1060,  1.5770, -0.1496]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8591868241458105, distance: 0.4294161165401197 entropy 0.03264415264129639
epoch: 42, step: 9
	action: tensor([[ 5.9247, -1.7420, -2.5524,  6.0772, -6.2633,  6.1800, -0.3437]],
       dtype=torch.float64)
	q_value: tensor([[-76.5859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 10
	action: tensor([[ 2.4203,  0.1033, -0.6263,  1.7535, -1.3924,  1.0375,  0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 11
	action: tensor([[ 2.0023, -0.6849, -0.4766,  1.4830, -1.0882,  1.6460, -0.1622]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.736783667101118, distance: 0.587101392453637 entropy 0.03264415264129639
epoch: 42, step: 12
	action: tensor([[ 6.1270, -1.7623, -3.2950,  6.1800, -6.2599,  6.1800, -0.0589]],
       dtype=torch.float64)
	q_value: tensor([[-74.6661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 13
	action: tensor([[ 2.1210, -0.1843, -0.8138,  1.5848, -0.7505,  1.5490, -0.2651]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.734499079906145, distance: 0.5896437621659818 entropy 0.03264415264129639
epoch: 42, step: 14
	action: tensor([[ 6.1800, -1.6542, -3.4126,  6.1800, -6.2800,  6.1800, -0.3951]],
       dtype=torch.float64)
	q_value: tensor([[-79.0706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 15
	action: tensor([[ 2.3600,  0.0093, -0.0598,  1.6438, -1.2673,  1.3911,  0.8685]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8868605597158339, distance: 0.38491399892020195 entropy 0.03264415264129639
epoch: 42, step: 16
	action: tensor([[ 6.1800, -1.4395, -2.6226,  6.1800, -6.2800,  6.1800, -0.3184]],
       dtype=torch.float64)
	q_value: tensor([[-70.3250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 17
	action: tensor([[ 1.1543, -0.0035, -0.7171,  2.1446, -1.3334,  1.1115,  0.1471]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5489440423161804, distance: 0.768549607336507 entropy 0.03264415264129639
epoch: 42, step: 18
	action: tensor([[ 6.1800, -1.5904, -2.4821,  6.1296, -6.2298,  6.1800, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-73.4232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 19
	action: tensor([[ 1.7896, -0.0745, -0.5157,  1.4899, -1.1830,  1.5310,  0.2597]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8564447192701803, distance: 0.4335770435152533 entropy 0.03264415264129639
epoch: 42, step: 20
	action: tensor([[ 6.1800, -1.5429, -2.7214,  6.1800, -5.8268,  6.1800, -0.5736]],
       dtype=torch.float64)
	q_value: tensor([[-76.4910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 21
	action: tensor([[ 2.0808, -0.0432, -0.3953,  1.7552, -1.3923,  1.6570, -0.5390]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8272405533231677, distance: 0.47563915125084755 entropy 0.03264415264129639
epoch: 42, step: 22
	action: tensor([[ 5.9787, -2.1531, -3.0488,  6.1800, -5.8819,  6.1800, -0.5872]],
       dtype=torch.float64)
	q_value: tensor([[-87.8616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 23
	action: tensor([[ 1.9889, -0.3292, -0.8150,  1.3537, -1.2645,  0.9699,  0.1037]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8419897642443368, distance: 0.4548825781320789 entropy 0.03264415264129639
epoch: 42, step: 24
	action: tensor([[ 6.0818, -1.7643, -2.5248,  6.1800, -6.0989,  6.1800, -0.3222]],
       dtype=torch.float64)
	q_value: tensor([[-71.0317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 25
	action: tensor([[ 2.4003, -0.7188, -0.9423,  1.5476, -1.2905,  1.2401,  0.2410]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 26
	action: tensor([[ 1.9587,  0.0533, -0.7130,  1.4745, -1.1785,  1.4236, -0.2003]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8422245427323963, distance: 0.4545445103363342 entropy 0.03264415264129639
epoch: 42, step: 27
	action: tensor([[ 5.9358, -1.4146, -2.7116,  6.1800, -6.2800,  6.0995, -0.5541]],
       dtype=torch.float64)
	q_value: tensor([[-81.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 28
	action: tensor([[ 1.8022, -0.4322, -0.5534,  2.5147, -0.9263,  1.5149, -0.5151]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5679234287429245, distance: 0.7522064485496672 entropy 0.03264415264129639
epoch: 42, step: 29
	action: tensor([[ 6.0296, -2.0377, -3.4717,  6.1800, -6.2800,  6.1800, -0.2818]],
       dtype=torch.float64)
	q_value: tensor([[-82.7294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 30
	action: tensor([[ 1.9285, -0.0783, -0.4921,  1.9144, -0.6234,  1.3378,  0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6958564528861215, distance: 0.6310968269160238 entropy 0.03264415264129639
epoch: 42, step: 31
	action: tensor([[ 6.1800, -1.8824, -3.0384,  5.9180, -6.2800,  6.1800,  0.3871]],
       dtype=torch.float64)
	q_value: tensor([[-70.2166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 32
	action: tensor([[ 1.8758, -0.3167, -0.5292,  1.9225, -0.6993,  1.7809, -0.0485]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6151200187324661, distance: 0.7099362539742389 entropy 0.03264415264129639
epoch: 42, step: 33
	action: tensor([[ 6.1800, -2.3207, -2.8998,  6.1005, -6.2800,  6.1800, -0.1689]],
       dtype=torch.float64)
	q_value: tensor([[-78.6636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 34
	action: tensor([[ 1.7676, -0.2423, -0.6355,  1.6358, -1.1457,  1.3148,  0.5266]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8377022936430474, distance: 0.4610126945766036 entropy 0.03264415264129639
epoch: 42, step: 35
	action: tensor([[ 6.1800, -1.5426, -2.8820,  6.1800, -6.2800,  6.0137, -0.2355]],
       dtype=torch.float64)
	q_value: tensor([[-72.2598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 36
	action: tensor([[ 2.0337, -0.5792, -0.5351,  1.7833, -0.7599,  1.3864, -0.0602]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7666200323865502, distance: 0.5528260263588453 entropy 0.03264415264129639
epoch: 42, step: 37
	action: tensor([[ 6.1104, -1.8640, -3.0273,  6.1800, -5.8678,  5.9272, -0.2181]],
       dtype=torch.float64)
	q_value: tensor([[-70.5711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 38
	action: tensor([[ 2.1370,  0.2354, -0.3149,  2.2724, -1.3023,  1.2602,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.707456085349464, distance: 0.6189452403001213 entropy 0.03264415264129639
epoch: 42, step: 39
	action: tensor([[ 6.1800, -1.8484, -3.0683,  6.1149, -6.2800,  6.0359,  0.0398]],
       dtype=torch.float64)
	q_value: tensor([[-82.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 40
	action: tensor([[ 2.4231, -0.2124, -0.4173,  1.9218, -0.8561,  1.4218, -0.5250]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 41
	action: tensor([[ 2.5042, -0.8919, -0.2199,  1.9885, -0.8743,  1.4829, -0.1358]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 42
	action: tensor([[ 2.0775, -0.3068, -0.5245,  1.8179, -1.3614,  0.8119, -0.5280]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9093070947427921, distance: 0.34462227925001787 entropy 0.03264415264129639
epoch: 42, step: 43
	action: tensor([[ 6.1057, -1.4356, -3.0336,  6.1800, -6.0057,  6.1800, -0.2604]],
       dtype=torch.float64)
	q_value: tensor([[-76.3832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 44
	action: tensor([[ 2.0303, -0.6083, -0.5117,  1.7532, -1.1749,  0.6608,  0.6358]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9169183446680249, distance: 0.3298445128334231 entropy 0.03264415264129639
epoch: 42, step: 45
	action: tensor([[ 6.1534, -1.8353, -2.6834,  5.9775, -6.2800,  6.1800,  0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-61.7744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 46
	action: tensor([[ 1.6405, -0.3762, -0.6219,  1.7531, -1.3575,  1.1695, -0.1065]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8320501240517932, distance: 0.46897159295846247 entropy 0.03264415264129639
epoch: 42, step: 47
	action: tensor([[ 6.1800, -2.1266, -3.2719,  6.1348, -5.9578,  6.1800, -0.1298]],
       dtype=torch.float64)
	q_value: tensor([[-76.8689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 48
	action: tensor([[ 2.0322, -0.1421, -0.1195,  1.6539, -1.2363,  1.1048, -0.2547]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9017679208629873, distance: 0.35866034457377793 entropy 0.03264415264129639
epoch: 42, step: 49
	action: tensor([[ 6.1800, -2.2334, -2.7858,  6.1800, -5.9840,  6.0320,  0.1773]],
       dtype=torch.float64)
	q_value: tensor([[-72.4821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 50
	action: tensor([[ 2.1591, -0.0671, -0.6813,  1.7952, -1.2587,  1.6422, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7361720995646204, distance: 0.5877830443692812 entropy 0.03264415264129639
epoch: 42, step: 51
	action: tensor([[ 5.7817, -1.5934, -2.8709,  6.1800, -6.2800,  5.9792, -0.3377]],
       dtype=torch.float64)
	q_value: tensor([[-84.8630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 52
	action: tensor([[ 2.2162, -0.5082, -0.6404,  1.3326, -1.0706,  1.4102,  0.2377]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7947543043842389, distance: 0.5184342942363187 entropy 0.03264415264129639
epoch: 42, step: 53
	action: tensor([[ 6.1800, -1.5436, -2.7687,  6.1800, -6.0576,  6.1800, -0.5956]],
       dtype=torch.float64)
	q_value: tensor([[-70.2739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 54
	action: tensor([[ 1.9409, -0.2784, -0.6111,  2.0596, -1.1172,  1.0264, -0.4815]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7662868412390804, distance: 0.5532205140338073 entropy 0.03264415264129639
epoch: 42, step: 55
	action: tensor([[ 6.1800, -2.2434, -3.5914,  6.1800, -6.2800,  6.1800, -0.0950]],
       dtype=torch.float64)
	q_value: tensor([[-78.9240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 56
	action: tensor([[ 2.1501, -0.4614, -0.1913,  1.6547, -1.2586,  1.3266, -0.4722]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8697609345868011, distance: 0.41297839187750773 entropy 0.03264415264129639
epoch: 42, step: 57
	action: tensor([[ 6.1800, -1.5782, -3.3405,  6.1800, -6.2800,  6.1800, -0.5171]],
       dtype=torch.float64)
	q_value: tensor([[-75.1732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 58
	action: tensor([[ 2.4179, -0.7653, -0.5157,  1.7924, -0.8147,  1.1905, -0.1150]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 59
	action: tensor([[ 2.1044, -0.3794, -0.3784,  2.0445, -1.2689,  0.8968,  0.2215]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.852111994950854, distance: 0.4400714239253071 entropy 0.03264415264129639
epoch: 42, step: 60
	action: tensor([[ 6.1800, -1.7840, -3.2793,  6.1800, -6.2800,  6.1692, -0.0900]],
       dtype=torch.float64)
	q_value: tensor([[-70.4181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 61
	action: tensor([[ 2.0384, -0.4019, -0.2110,  1.6601, -0.9418,  1.0350, -0.1420]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8906529636929009, distance: 0.3784079061079767 entropy 0.03264415264129639
epoch: 42, step: 62
	action: tensor([[ 5.8686, -1.9820, -2.7872,  6.1800, -6.0469,  6.1800, -0.4644]],
       dtype=torch.float64)
	q_value: tensor([[-66.1418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 63
	action: tensor([[ 2.2557,  0.0378, -0.3122,  2.2220, -1.0889,  1.6051, -0.4472]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6689975433028134, distance: 0.6583734392605445 entropy 0.03264415264129639
epoch: 42, step: 64
	action: tensor([[ 6.1800, -1.7793, -3.5414,  6.1800, -6.2800,  5.8902, -0.1565]],
       dtype=torch.float64)
	q_value: tensor([[-86.5597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 65
	action: tensor([[ 2.4529, -0.2443, -0.9168,  2.0528, -0.8031,  1.2287, -0.2685]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 66
	action: tensor([[ 1.5342, -0.0366, -0.9477,  1.7994, -1.1554,  1.2146, -0.3683]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6991214387628991, distance: 0.6277002694946768 entropy 0.03264415264129639
epoch: 42, step: 67
	action: tensor([[ 6.1666, -1.3298, -2.7661,  6.1800, -6.2800,  6.1800,  0.4523]],
       dtype=torch.float64)
	q_value: tensor([[-83.1545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 68
	action: tensor([[ 1.9556, -0.5567, -0.9077,  2.0029, -0.7950,  1.6012, -0.2930]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.530221293936529, distance: 0.7843381815333959 entropy 0.03264415264129639
epoch: 42, step: 69
	action: tensor([[ 6.1800, -1.8162, -3.1756,  6.1800, -6.0311,  5.9812, -0.1960]],
       dtype=torch.float64)
	q_value: tensor([[-81.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 70
	action: tensor([[ 2.4776, -0.2726, -0.6018,  1.4780, -0.8577,  1.5524, -0.2652]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 71
	action: tensor([[ 1.9119, -0.5003, -0.6313,  1.4169, -0.9442,  1.5552,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7767206245867134, distance: 0.5407306629235343 entropy 0.03264415264129639
epoch: 42, step: 72
	action: tensor([[ 6.1800, -1.9844, -3.2535,  6.1458, -6.2800,  6.0537,  0.1354]],
       dtype=torch.float64)
	q_value: tensor([[-73.0831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 73
	action: tensor([[ 1.9624, -0.1172, -0.6193,  1.9370, -0.8095,  1.0935,  0.4687]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7485481086633046, distance: 0.573831185223475 entropy 0.03264415264129639
epoch: 42, step: 74
	action: tensor([[ 5.9635, -1.7706, -3.0613,  6.1800, -5.9539,  6.1712, -0.3640]],
       dtype=torch.float64)
	q_value: tensor([[-69.6718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 75
	action: tensor([[ 2.2100, -0.6954, -0.3638,  1.8974, -0.7982,  1.4422, -0.3834]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7430339972272442, distance: 0.5800888631408958 entropy 0.03264415264129639
epoch: 42, step: 76
	action: tensor([[ 6.1223, -1.9577, -3.6220,  6.1800, -6.2800,  6.1800,  0.0391]],
       dtype=torch.float64)
	q_value: tensor([[-72.6545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 77
	action: tensor([[ 2.2189, -0.1806, -0.8011,  1.7905, -1.0086,  1.3026, -0.0955]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7340385037610896, distance: 0.5901549810616393 entropy 0.03264415264129639
epoch: 42, step: 78
	action: tensor([[ 6.1800, -1.6126, -3.0439,  6.1800, -6.2800,  6.1800,  0.0952]],
       dtype=torch.float64)
	q_value: tensor([[-78.4458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 79
	action: tensor([[ 1.8464, -0.3660, -0.7456,  1.8060, -1.0184,  1.9214,  0.2528]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5960611531114797, distance: 0.7273015346461082 entropy 0.03264415264129639
epoch: 42, step: 80
	action: tensor([[ 6.1011, -1.7883, -3.3804,  5.9159, -6.2800,  6.1755, -0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-82.3864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 81
	action: tensor([[ 2.2848,  0.1607, -0.3968,  1.6035, -0.9311,  0.9509, -0.3219]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8713151653446152, distance: 0.41050682093447044 entropy 0.03264415264129639
epoch: 42, step: 82
	action: tensor([[ 6.1800, -1.5954, -2.3446,  6.1458, -6.2800,  6.1800,  0.1687]],
       dtype=torch.float64)
	q_value: tensor([[-72.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 83
	action: tensor([[ 1.9977,  0.2751, -0.6171,  1.7565, -1.5679,  1.7137, -0.1052]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8379425227490209, distance: 0.460671378355399 entropy 0.03264415264129639
epoch: 42, step: 84
	action: tensor([[ 5.8053, -2.0208, -2.8868,  5.9593, -6.2800,  6.1800,  0.3218]],
       dtype=torch.float64)
	q_value: tensor([[-92.1828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 85
	action: tensor([[ 1.6617,  0.1387, -0.5861,  1.8926, -1.2216,  1.1370, -0.2713]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8025875676360475, distance: 0.5084449557251735 entropy 0.03264415264129639
epoch: 42, step: 86
	action: tensor([[ 6.1800, -2.0985, -2.8720,  6.1349, -6.1541,  6.1800,  0.3908]],
       dtype=torch.float64)
	q_value: tensor([[-81.6210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 87
	action: tensor([[ 2.2433, -0.4549, -0.4102,  1.7892, -0.7767,  1.4595, -0.0580]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7671127736902319, distance: 0.5522421198797313 entropy 0.03264415264129639
epoch: 42, step: 88
	action: tensor([[ 6.1800, -1.9808, -3.0842,  6.1800, -6.2800,  6.1800,  0.0420]],
       dtype=torch.float64)
	q_value: tensor([[-71.6323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 89
	action: tensor([[ 2.0585, -0.1667, -0.6097,  1.6099, -1.0130,  1.4815, -0.1909]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8003839449144211, distance: 0.5112748471616023 entropy 0.03264415264129639
epoch: 42, step: 90
	action: tensor([[ 6.1800, -1.7431, -2.8908,  6.1800, -6.2800,  6.1800, -0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-78.5027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 91
	action: tensor([[ 2.2691, -0.0085, -0.5659,  1.8926, -1.1617,  1.4652,  0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7553596482825984, distance: 0.5660056148854795 entropy 0.03264415264129639
epoch: 42, step: 92
	action: tensor([[ 6.1800e+00, -1.9197e+00, -2.4862e+00,  6.0203e+00, -5.8318e+00,
          6.1800e+00, -6.7178e-04]], dtype=torch.float64)
	q_value: tensor([[-81.0704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 93
	action: tensor([[ 2.1236, -0.1485, -0.4179,  1.7491, -0.9396,  1.4138, -0.3328]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.80080250017176, distance: 0.5107385449335894 entropy 0.03264415264129639
epoch: 42, step: 94
	action: tensor([[ 6.1800, -2.0018, -3.2730,  6.1800, -6.1752,  5.9609, -0.0856]],
       dtype=torch.float64)
	q_value: tensor([[-77.2849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 95
	action: tensor([[ 2.3578,  0.2815, -0.3997,  1.5774, -1.0292,  1.8310,  0.0750]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7879189441983032, distance: 0.5269963795811792 entropy 0.03264415264129639
epoch: 42, step: 96
	action: tensor([[ 6.1800, -2.0528, -3.0284,  5.5386, -6.2800,  6.1800, -0.0311]],
       dtype=torch.float64)
	q_value: tensor([[-83.2678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 97
	action: tensor([[ 1.9956, -0.0887, -0.4379,  1.5658, -0.9368,  1.3941,  0.2415]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8567165413755574, distance: 0.4331663596757869 entropy 0.03264415264129639
epoch: 42, step: 98
	action: tensor([[ 6.1800, -1.6294, -2.9326,  6.1800, -6.1770,  6.1800, -0.2025]],
       dtype=torch.float64)
	q_value: tensor([[-71.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 99
	action: tensor([[ 2.2079, -0.0230, -0.0942,  1.9885, -1.2900,  1.7149, -0.0895]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.796295921675192, distance: 0.5164836231198433 entropy 0.03264415264129639
epoch: 42, step: 100
	action: tensor([[ 6.1493, -1.6142, -3.2329,  6.1800, -6.0648,  5.5968, -0.1494]],
       dtype=torch.float64)
	q_value: tensor([[-83.3469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 101
	action: tensor([[ 1.9670, -0.4558, -0.8444,  1.7716, -0.6950,  1.2935,  0.7706]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7422221364086512, distance: 0.5810045096049825 entropy 0.03264415264129639
epoch: 42, step: 102
	action: tensor([[ 6.1800, -1.9089, -3.2392,  6.1800, -6.2800,  6.1800,  0.3010]],
       dtype=torch.float64)
	q_value: tensor([[-67.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 103
	action: tensor([[ 2.2242, -0.4628, -0.4139,  1.6529, -1.5330,  1.3185,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.846943667361292, distance: 0.4476951038306311 entropy 0.03264415264129639
epoch: 42, step: 104
	action: tensor([[ 6.1100, -1.0550, -2.8928,  6.1337, -6.1481,  6.1645, -0.0882]],
       dtype=torch.float64)
	q_value: tensor([[-76.3484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4476979953105285 entropy 0.03264415264129639
epoch: 42, step: 105
	action: tensor([[ 2.1883, -0.1743, -0.5499,  2.1832, -1.1583,  1.3948, -0.0496]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6649871057657241, distance: 0.6623498688860331 entropy 0.03264415264129639
epoch: 42, step: 106
	action: tensor([[ 6.1800, -2.3459, -2.8169,  6.1800, -5.6520,  6.0659, -0.4543]],
       dtype=torch.float64)
	q_value: tensor([[-81.5562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 107
	action: tensor([[ 2.2624, -0.5622,  0.0982,  1.5809, -0.8062,  1.4149,  0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8052126035034451, distance: 0.5050531913893317 entropy 0.03264415264129639
epoch: 42, step: 108
	action: tensor([[ 6.1800, -1.4870, -3.2629,  6.1800, -6.2548,  6.1168,  0.0690]],
       dtype=torch.float64)
	q_value: tensor([[-63.7285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 109
	action: tensor([[ 2.0799, -0.4049, -0.2244,  1.8916, -1.0850,  0.9643, -0.4721]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8751973445178518, distance: 0.4042672929146233 entropy 0.03264415264129639
epoch: 42, step: 110
	action: tensor([[ 6.1800, -1.9195, -2.6479,  6.1800, -5.8913,  6.1800,  0.2557]],
       dtype=torch.float64)
	q_value: tensor([[-71.3877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 111
	action: tensor([[ 2.4088, -0.0293, -0.5408,  2.0227, -1.5722,  1.2634,  0.0633]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 112
	action: tensor([[ 1.8952, -0.3103, -0.3632,  1.2934, -1.1608,  1.0383, -0.5974]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9160760069005535, distance: 0.33151238887346485 entropy 0.03264415264129639
epoch: 42, step: 113
	action: tensor([[ 6.1800, -1.5587, -2.7053,  6.1800, -6.2800,  6.1800, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[-71.9306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 114
	action: tensor([[ 2.2015, -0.1334,  0.0794,  1.4521, -1.4240,  1.4383,  0.2227]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8974797913288796, distance: 0.3664050358969764 entropy 0.03264415264129639
epoch: 42, step: 115
	action: tensor([[ 5.9193, -2.2683, -2.6262,  5.7797, -6.2800,  6.1800, -0.3045]],
       dtype=torch.float64)
	q_value: tensor([[-72.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 116
	action: tensor([[ 2.2055, -0.6572, -0.4099,  2.0366, -1.3388,  1.5918, -0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7125303975328869, distance: 0.6135538102409147 entropy 0.03264415264129639
epoch: 42, step: 117
	action: tensor([[ 6.0147, -2.0292, -3.3449,  6.1800, -6.2800,  6.1800, -0.0954]],
       dtype=torch.float64)
	q_value: tensor([[-79.8913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 118
	action: tensor([[ 2.0135, -0.3723, -0.8071,  1.6046, -0.7879,  1.0177, -0.1190]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8451894925275303, distance: 0.4502533060826796 entropy 0.03264415264129639
epoch: 42, step: 119
	action: tensor([[ 6.1164, -1.7680, -2.7210,  6.1800, -6.2220,  6.1800,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-69.7291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 120
	action: tensor([[ 2.1702, -0.0320, -0.5783,  1.7053, -0.8512,  1.5169,  0.4128]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.768548786653538, distance: 0.5505368887344373 entropy 0.03264415264129639
epoch: 42, step: 121
	action: tensor([[ 5.9951, -1.5058, -2.3564,  6.1800, -5.6985,  5.9305,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-74.3566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 122
	action: tensor([[ 2.3640, -0.3485, -0.6776,  1.6371, -1.1272,  1.1762,  0.1294]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8370431718639193, distance: 0.46194787606284077 entropy 0.03264415264129639
epoch: 42, step: 123
	action: tensor([[ 6.1800, -1.4497, -2.7039,  6.0428, -6.0156,  6.1554, -0.2005]],
       dtype=torch.float64)
	q_value: tensor([[-72.5103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 124
	action: tensor([[ 2.1069, -0.8862, -0.4583,  1.6143, -1.1150,  1.1125,  0.1460]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8119565169095023, distance: 0.49623322260131025 entropy 0.03264415264129639
epoch: 42, step: 125
	action: tensor([[ 6.1800, -1.6382, -3.0542,  6.1632, -6.2567,  5.9406,  0.2927]],
       dtype=torch.float64)
	q_value: tensor([[-65.3571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 42, step: 126
	action: tensor([[ 1.7467, -0.4600, -0.5384,  1.5719, -0.6514,  0.8504,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[-42.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9154227249900536, distance: 0.3328001685215744 entropy 0.03264415264129639
epoch: 42, step: 127
	action: tensor([[ 6.1800, -1.9321, -2.6702,  6.0978, -6.2443,  6.1800, -0.3093]],
       dtype=torch.float64)
	q_value: tensor([[-61.3266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 42 actor 597.8908174827055 critic 358.4245182551867 
epoch: 43, step: 0
	action: tensor([[ 1.8290, -0.2945, -0.4216,  1.9690, -0.6218,  0.8616, -0.1764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7781196985398089, distance: 0.5390338850441891 entropy 0.03264415264129639
epoch: 43, step: 1
	action: tensor([[ 6.1800, -1.9021, -2.4892,  5.5601, -4.8621,  5.5930, -1.8255]],
       dtype=torch.float64)
	q_value: tensor([[-47.3518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 2
	action: tensor([[ 2.0378,  0.2262, -0.7391,  1.4669, -0.7599,  1.3546, -0.3951]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8231441058600244, distance: 0.4812452571558297 entropy 0.03264415264129639
epoch: 43, step: 3
	action: tensor([[ 6.1800, -1.5138, -2.1470,  6.1800, -4.2815,  5.0094, -1.5868]],
       dtype=torch.float64)
	q_value: tensor([[-58.0859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 4
	action: tensor([[ 1.8032, -0.4017, -0.5315,  1.8822, -1.3823,  1.6434,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7310567621990262, distance: 0.5934539267106933 entropy 0.03264415264129639
epoch: 43, step: 5
	action: tensor([[ 6.1800, -1.1762, -2.4668,  6.1177, -4.6499,  5.1004, -1.6089]],
       dtype=torch.float64)
	q_value: tensor([[-58.2515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 6
	action: tensor([[ 2.1349, -0.3428, -0.3139,  1.5451, -1.0495,  1.8301, -0.4301]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7699497476306816, distance: 0.5488681755710132 entropy 0.03264415264129639
epoch: 43, step: 7
	action: tensor([[ 6.1800, -1.3426, -2.1896,  6.1800, -5.2310,  5.2779, -2.0639]],
       dtype=torch.float64)
	q_value: tensor([[-57.6340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 8
	action: tensor([[ 1.7909, -0.2661, -0.6902,  1.6243, -0.9086,  1.1037, -0.4505]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8418864404781358, distance: 0.4550312789425633 entropy 0.03264415264129639
epoch: 43, step: 9
	action: tensor([[ 6.1800, -1.4751, -2.1713,  6.0839, -4.4942,  5.5013, -2.0755]],
       dtype=torch.float64)
	q_value: tensor([[-53.9228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 10
	action: tensor([[ 2.5627, -0.3917, -0.3435,  1.3991, -0.9585,  1.2220, -0.5926]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 11
	action: tensor([[ 1.9441, -0.3261, -0.5576,  1.2894, -1.0802,  1.2291, -0.4981]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8756850608813053, distance: 0.40347660148854547 entropy 0.03264415264129639
epoch: 43, step: 12
	action: tensor([[ 6.1800, -1.2444, -2.0440,  6.1800, -4.6016,  4.7841, -1.6285]],
       dtype=torch.float64)
	q_value: tensor([[-53.5227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 13
	action: tensor([[ 1.7314, -0.5136, -0.4706,  1.5224, -0.7038,  0.7536, -0.9672]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9242590930182205, distance: 0.3149357365875496 entropy 0.03264415264129639
epoch: 43, step: 14
	action: tensor([[ 6.1800, -1.5150, -2.1586,  6.1800, -4.8781,  5.3118, -2.0110]],
       dtype=torch.float64)
	q_value: tensor([[-49.5180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 15
	action: tensor([[ 1.9182, -0.0966, -0.7341,  1.9308, -0.4363,  1.0391, -0.1817]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6949811237574218, distance: 0.6320043269199466 entropy 0.03264415264129639
epoch: 43, step: 16
	action: tensor([[ 5.9642, -1.6602, -1.9170,  6.1800, -4.9289,  5.8158, -2.1746]],
       dtype=torch.float64)
	q_value: tensor([[-51.0941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 17
	action: tensor([[ 2.0415, -0.0857, -0.7027,  1.8650, -1.5900,  1.1663, -0.8039]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8467173414141842, distance: 0.44802598719471765 entropy 0.03264415264129639
epoch: 43, step: 18
	action: tensor([[ 6.1800, -1.1596, -2.4197,  6.1800, -4.8499,  5.3343, -2.0568]],
       dtype=torch.float64)
	q_value: tensor([[-64.9880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4196748037885913 entropy 0.03264415264129639
epoch: 43, step: 19
	action: tensor([[ 2.0004, -0.9567, -0.4611,  1.6661, -0.9994,  1.0764, -0.8489]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8025331088776804, distance: 0.5085150814292834 entropy 0.03264415264129639
epoch: 43, step: 20
	action: tensor([[ 6.0683, -1.8070, -2.5132,  6.1800, -5.0185,  5.6330, -2.4537]],
       dtype=torch.float64)
	q_value: tensor([[-51.1177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 21
	action: tensor([[ 1.6977,  0.2008, -0.8641,  1.4633, -1.2296,  1.2187,  0.0495]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8678501751369396, distance: 0.41599679936179246 entropy 0.03264415264129639
epoch: 43, step: 22
	action: tensor([[ 5.9837, -1.2391, -2.0657,  6.1231, -4.1266,  4.6350, -1.5542]],
       dtype=torch.float64)
	q_value: tensor([[-57.8308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 23
	action: tensor([[ 2.1206, -0.7637, -0.9285,  2.1107, -0.9549,  0.6709, -0.5893]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7552254454087024, distance: 0.5661608410473847 entropy 0.03264415264129639
epoch: 43, step: 24
	action: tensor([[ 6.0497, -1.1837, -2.8312,  6.1800, -5.4179,  5.6375, -2.3645]],
       dtype=torch.float64)
	q_value: tensor([[-52.7292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3876086362711735 entropy 0.03264415264129639
epoch: 43, step: 25
	action: tensor([[ 2.0435, -0.7651, -0.5027,  1.5420, -1.2479,  1.7210, -0.4001]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6663485829826321, distance: 0.6610026187794016 entropy 0.03264415264129639
epoch: 43, step: 26
	action: tensor([[ 6.0229, -1.7671, -2.4706,  6.0625, -4.4253,  5.3425, -2.3167]],
       dtype=torch.float64)
	q_value: tensor([[-56.3384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 27
	action: tensor([[ 2.2069, -0.5834, -0.4573,  1.8576, -1.3005,  1.1240, -0.7168]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8413050647266936, distance: 0.45586707512485114 entropy 0.03264415264129639
epoch: 43, step: 28
	action: tensor([[ 5.9257, -1.4954, -2.1384,  6.0341, -4.7040,  5.5277, -2.2858]],
       dtype=torch.float64)
	q_value: tensor([[-55.9974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 29
	action: tensor([[ 1.6898, -0.1788, -0.5889,  1.5833, -0.8636,  1.2453, -0.4167]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8469238597774041, distance: 0.4477240718298268 entropy 0.03264415264129639
epoch: 43, step: 30
	action: tensor([[ 6.1800, -1.3983, -2.2557,  6.1012, -4.6636,  5.5323, -1.9503]],
       dtype=torch.float64)
	q_value: tensor([[-54.2284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 31
	action: tensor([[ 1.9199, -0.2847, -0.1348,  1.7341, -0.9962,  1.3360, -1.0360]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8708479637116839, distance: 0.41125133638470845 entropy 0.03264415264129639
epoch: 43, step: 32
	action: tensor([[ 6.0669, -1.4609, -2.8760,  6.1800, -4.8630,  6.1800, -2.1057]],
       dtype=torch.float64)
	q_value: tensor([[-57.0177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 33
	action: tensor([[ 1.9345, -0.5495, -0.5368,  1.5433, -1.4546,  1.5410, -0.1602]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7397094309355389, distance: 0.5838293309945587 entropy 0.03264415264129639
epoch: 43, step: 34
	action: tensor([[ 6.1800, -1.1603, -1.8854,  6.1182, -4.3883,  5.0268, -2.0893]],
       dtype=torch.float64)
	q_value: tensor([[-56.4436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 35
	action: tensor([[ 1.4017, -0.3155, -0.8025,  1.8838, -0.7366,  1.2505, -0.6776]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6366893936603812, distance: 0.6897563885162371 entropy 0.03264415264129639
epoch: 43, step: 36
	action: tensor([[ 6.1800, -1.8583, -2.6069,  6.1800, -4.8239,  5.4341, -2.1018]],
       dtype=torch.float64)
	q_value: tensor([[-54.6899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 37
	action: tensor([[ 2.6274,  0.0105, -0.7147,  1.2018, -0.8930,  1.0091, -0.3571]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 38
	action: tensor([[ 1.9291, -0.5781, -0.4633,  1.3797, -0.9467,  1.2679, -0.6137]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8445066800713882, distance: 0.45124516482870897 entropy 0.03264415264129639
epoch: 43, step: 39
	action: tensor([[ 6.1800, -1.4143, -2.2290,  6.1611, -4.2920,  5.4305, -1.8988]],
       dtype=torch.float64)
	q_value: tensor([[-51.5744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 40
	action: tensor([[ 2.2296, -0.2984, -0.4859,  1.7416, -0.9961,  1.2389, -0.3484]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.834276669474014, distance: 0.46585259561680303 entropy 0.03264415264129639
epoch: 43, step: 41
	action: tensor([[ 6.1800, -1.4881, -2.5199,  5.8889, -4.8680,  5.4326, -2.1327]],
       dtype=torch.float64)
	q_value: tensor([[-53.6115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 42
	action: tensor([[ 2.0710, -0.5430, -0.3357,  1.7613, -1.3346,  1.4135, -0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8264415309362374, distance: 0.47673781160340384 entropy 0.03264415264129639
epoch: 43, step: 43
	action: tensor([[ 6.1800, -1.6595, -2.7868,  6.1800, -4.6772,  5.6069, -2.3225]],
       dtype=torch.float64)
	q_value: tensor([[-53.2965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 44
	action: tensor([[ 1.9936, -0.6552, -0.9576,  1.8970, -1.3791,  1.1116,  0.1548]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6659216645763025, distance: 0.6614253713268609 entropy 0.03264415264129639
epoch: 43, step: 45
	action: tensor([[ 6.0824, -1.1169, -1.9309,  6.1344, -4.6930,  5.3715, -2.0272]],
       dtype=torch.float64)
	q_value: tensor([[-54.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4734545107187433 entropy 0.03264415264129639
epoch: 43, step: 46
	action: tensor([[ 2.0512, -0.3313, -0.3602,  1.4866, -1.0911,  1.1419, -0.4698]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.907154626767388, distance: 0.34868785946317654 entropy 0.03264415264129639
epoch: 43, step: 47
	action: tensor([[ 6.1800, -1.4162, -2.0417,  6.1800, -4.3817,  5.5416, -1.8030]],
       dtype=torch.float64)
	q_value: tensor([[-51.9106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 48
	action: tensor([[ 1.9530, -0.1670, -0.3165,  1.3344, -0.6535,  0.5836, -0.5322]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9232525058650829, distance: 0.3170215576033065 entropy 0.03264415264129639
epoch: 43, step: 49
	action: tensor([[ 6.1800, -0.9533, -2.3779,  6.1800, -4.0550,  4.9583, -2.2931]],
       dtype=torch.float64)
	q_value: tensor([[-44.6351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 50
	action: tensor([[ 2.2020, -0.1991, -0.4161,  1.5258, -1.3200,  1.5328, -0.6145]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8488097978367192, distance: 0.4449574853571105 entropy 0.03264415264129639
epoch: 43, step: 51
	action: tensor([[ 5.9542, -1.6367, -2.2768,  5.9381, -4.3992,  5.0217, -1.9118]],
       dtype=torch.float64)
	q_value: tensor([[-59.9043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 52
	action: tensor([[ 2.2346e+00,  7.4901e-04, -6.5018e-01,  1.8900e+00, -8.9097e-01,
          9.8506e-01, -5.6982e-01]], dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7824418894584677, distance: 0.533757920289885 entropy 0.03264415264129639
epoch: 43, step: 53
	action: tensor([[ 6.1800, -1.6709, -2.1392,  6.0934, -4.8514,  5.5601, -2.2959]],
       dtype=torch.float64)
	q_value: tensor([[-56.6786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 54
	action: tensor([[ 2.2824, -0.1374, -0.8264,  1.8262, -1.1673,  1.2610, -0.6542]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7444060888573416, distance: 0.5785380735384533 entropy 0.03264415264129639
epoch: 43, step: 55
	action: tensor([[ 6.0589, -1.8986, -2.1985,  6.1800, -5.0711,  5.2161, -2.3203]],
       dtype=torch.float64)
	q_value: tensor([[-61.8975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 56
	action: tensor([[ 1.8289,  0.1522, -0.8279,  1.6063, -1.2472,  1.2760, -0.3907]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8299356977185645, distance: 0.4719144483365505 entropy 0.03264415264129639
epoch: 43, step: 57
	action: tensor([[ 6.1800, -1.4142, -2.3893,  5.9761, -4.5591,  4.7080, -1.5205]],
       dtype=torch.float64)
	q_value: tensor([[-61.7697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 58
	action: tensor([[ 2.3282, -0.4782, -0.1014,  1.7593, -0.5184,  1.1848, -0.3876]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7874061391248182, distance: 0.5276331249321143 entropy 0.03264415264129639
epoch: 43, step: 59
	action: tensor([[ 6.1800, -2.1420, -2.3297,  6.1800, -5.2356,  5.7064, -1.7423]],
       dtype=torch.float64)
	q_value: tensor([[-46.0162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 60
	action: tensor([[ 2.0201,  0.0079, -0.7676,  2.0850, -1.3751,  0.9521, -0.6196]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7792197940670789, distance: 0.5376959438026031 entropy 0.03264415264129639
epoch: 43, step: 61
	action: tensor([[ 6.1800, -1.7028, -2.4028,  6.1800, -4.7043,  5.3580, -1.7839]],
       dtype=torch.float64)
	q_value: tensor([[-62.3961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 62
	action: tensor([[ 2.2407, -0.2842, -0.4384,  1.8654, -0.8210,  1.1315, -0.4939]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8044829906503506, distance: 0.5059981931862871 entropy 0.03264415264129639
epoch: 43, step: 63
	action: tensor([[ 6.1800, -1.2203, -2.5237,  5.8001, -5.1471,  5.6732, -2.3986]],
       dtype=torch.float64)
	q_value: tensor([[-52.7910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2787641507518401 entropy 0.03264415264129639
epoch: 43, step: 64
	action: tensor([[ 2.1300, -0.6590, -0.3841,  1.2953, -0.9260,  1.4468, -0.6009]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8060506717723621, distance: 0.5039655303809238 entropy 0.03264415264129639
epoch: 43, step: 65
	action: tensor([[ 6.1800, -0.9997, -2.3219,  6.1800, -4.5739,  5.0099, -2.5245]],
       dtype=torch.float64)
	q_value: tensor([[-51.4326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 66
	action: tensor([[ 2.0478, -0.3605, -0.4051,  1.6840, -0.7313,  1.2755, -0.7839]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8304998460309477, distance: 0.47113106554650097 entropy 0.03264415264129639
epoch: 43, step: 67
	action: tensor([[ 6.1800, -1.5942, -2.9228,  5.9717, -5.1150,  5.6168, -2.1303]],
       dtype=torch.float64)
	q_value: tensor([[-53.9582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 68
	action: tensor([[ 2.3275, -0.5949, -0.6916,  1.8078, -0.8248,  1.4939, -0.5208]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6889368294206355, distance: 0.6382355492782296 entropy 0.03264415264129639
epoch: 43, step: 69
	action: tensor([[ 6.1553, -2.0114, -2.7403,  6.1800, -5.4494,  6.1800, -1.8706]],
       dtype=torch.float64)
	q_value: tensor([[-55.7159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 70
	action: tensor([[ 2.3515, -0.4954, -0.5599,  1.7873, -0.7459,  0.9830, -0.1544]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8498490334447428, distance: 0.4434255969632987 entropy 0.03264415264129639
epoch: 43, step: 71
	action: tensor([[ 6.0501, -1.2096, -2.1079,  6.0001, -5.0516,  5.5183, -2.0366]],
       dtype=torch.float64)
	q_value: tensor([[-47.4204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3816305576525092 entropy 0.03264415264129639
epoch: 43, step: 72
	action: tensor([[ 1.9311, -0.3640, -0.2671,  1.5689, -1.1321,  1.2095, -0.4949]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8985593155492962, distance: 0.3644708324433428 entropy 0.03264415264129639
epoch: 43, step: 73
	action: tensor([[ 6.1213, -1.3722, -2.0351,  6.1418, -5.3390,  5.2213, -2.2390]],
       dtype=torch.float64)
	q_value: tensor([[-52.4533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 74
	action: tensor([[ 1.9516, -0.4411, -0.3846,  2.0954, -0.9914,  0.9482, -0.1705]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7908107088720896, distance: 0.523391201215011 entropy 0.03264415264129639
epoch: 43, step: 75
	action: tensor([[ 6.1800, -1.5660, -2.0132,  6.1800, -4.7003,  5.5620, -2.3214]],
       dtype=torch.float64)
	q_value: tensor([[-50.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 76
	action: tensor([[ 1.4548, -0.7117, -0.7949,  1.2392, -0.6585,  1.5875, -0.6347]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7074261625638866, distance: 0.6189768938249658 entropy 0.03264415264129639
epoch: 43, step: 77
	action: tensor([[ 6.1800, -1.8115, -2.1902,  6.1800, -4.9221,  5.5879, -2.0820]],
       dtype=torch.float64)
	q_value: tensor([[-51.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 78
	action: tensor([[ 2.5068,  0.1308, -0.6724,  1.7879, -0.6340,  1.4657, -0.3319]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 79
	action: tensor([[ 2.2671, -0.4867, -0.6741,  1.6811, -1.1657,  0.9484, -0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8621567090491116, distance: 0.42486358547705916 entropy 0.03264415264129639
epoch: 43, step: 80
	action: tensor([[ 6.1437, -1.1549, -2.2081,  6.1800, -4.1264,  4.8948, -1.9600]],
       dtype=torch.float64)
	q_value: tensor([[-50.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 81
	action: tensor([[ 2.1563, -0.5303, -0.5017,  1.4179, -1.1273,  1.3087, -0.3057]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8309484839957578, distance: 0.47050715068231663 entropy 0.03264415264129639
epoch: 43, step: 82
	action: tensor([[ 6.0199, -1.5183, -2.3313,  6.1800, -4.3007,  5.3316, -1.7629]],
       dtype=torch.float64)
	q_value: tensor([[-51.9434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 83
	action: tensor([[ 1.9500, -0.5957, -0.8997,  1.6812, -0.9583,  1.2421, -0.3548]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7115056887562154, distance: 0.6146463687058008 entropy 0.03264415264129639
epoch: 43, step: 84
	action: tensor([[ 5.9617, -1.5975, -2.7074,  6.1800, -5.0182,  5.2970, -2.4856]],
       dtype=torch.float64)
	q_value: tensor([[-54.3172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 85
	action: tensor([[ 1.9609,  0.1028, -0.6338,  1.2469, -0.9340,  1.4740, -0.5446]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.881736400552433, distance: 0.39353398175888005 entropy 0.03264415264129639
epoch: 43, step: 86
	action: tensor([[ 6.1800, -1.1463, -2.8878,  6.1136, -4.2535,  5.1143, -1.9623]],
       dtype=torch.float64)
	q_value: tensor([[-58.7298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 87
	action: tensor([[ 2.2417, -0.3565, -0.6349,  1.8258, -1.1796,  1.0512, -0.4976]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8378231675520688, distance: 0.46084098917472993 entropy 0.03264415264129639
epoch: 43, step: 88
	action: tensor([[ 6.1800, -1.6610, -2.3328,  6.1520, -4.5670,  5.4680, -1.4978]],
       dtype=torch.float64)
	q_value: tensor([[-55.7045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 89
	action: tensor([[ 1.6175, -0.4119, -0.3188,  1.9667, -1.5660,  0.8338, -0.1825]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9104156302826082, distance: 0.342509652001743 entropy 0.03264415264129639
epoch: 43, step: 90
	action: tensor([[ 6.1800, -1.3830, -2.3125,  6.1800, -4.0552,  5.1960, -1.3737]],
       dtype=torch.float64)
	q_value: tensor([[-53.0056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 91
	action: tensor([[ 1.7879e+00,  1.1330e-03, -8.8601e-01,  1.9985e+00, -7.8190e-01,
          1.0894e+00, -3.7681e-01]], dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6520637031129652, distance: 0.6750043346511956 entropy 0.03264415264129639
epoch: 43, step: 92
	action: tensor([[ 6.1068, -1.7165, -2.5550,  6.1800, -5.2490,  5.0859, -2.1867]],
       dtype=torch.float64)
	q_value: tensor([[-57.7570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 93
	action: tensor([[ 2.0225,  0.2735, -0.4597,  1.7107, -0.9000,  1.7565, -0.6056]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7512458686708396, distance: 0.5707446436672619 entropy 0.03264415264129639
epoch: 43, step: 94
	action: tensor([[ 6.1800, -1.4043, -2.0614,  6.1573, -5.2507,  5.3200, -2.3366]],
       dtype=torch.float64)
	q_value: tensor([[-63.6093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 95
	action: tensor([[ 1.9790, -0.6873, -0.4117,  1.6778, -0.9083,  0.8521, -0.2719]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8897712577352609, distance: 0.3799304651066688 entropy 0.03264415264129639
epoch: 43, step: 96
	action: tensor([[ 6.0541, -0.9171, -2.8510,  6.1800, -4.8450,  5.4229, -2.0004]],
       dtype=torch.float64)
	q_value: tensor([[-45.6728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2861968309196639 entropy 0.03264415264129639
epoch: 43, step: 97
	action: tensor([[ 2.4103, -0.2688, -0.3806,  1.7646, -1.6404,  1.2816, -0.3365]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 98
	action: tensor([[ 2.3566, -0.2596, -0.4835,  2.0271, -1.0290,  0.9097, -0.6579]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8084147624750884, distance: 0.5008846402501665 entropy 0.03264415264129639
epoch: 43, step: 99
	action: tensor([[ 6.1800, -1.6017, -2.7170,  6.1800, -4.5941,  5.3405, -2.0468]],
       dtype=torch.float64)
	q_value: tensor([[-55.1074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 100
	action: tensor([[ 1.9631, -0.1432, -1.0343,  1.7735, -0.7963,  1.6653, -0.6678]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5812030489053368, distance: 0.7405569275608537 entropy 0.03264415264129639
epoch: 43, step: 101
	action: tensor([[ 6.1800, -1.4814, -2.9771,  6.0945, -5.5900,  6.0056, -2.8350]],
       dtype=torch.float64)
	q_value: tensor([[-64.2823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 102
	action: tensor([[ 2.5517, -0.4428, -0.6887,  1.4839, -0.9472,  1.3598, -0.6803]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 103
	action: tensor([[ 2.3614, -0.1387, -0.3184,  2.3961, -1.2054,  1.6176, -0.5563]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6898087693693473, distance: 0.6373404037656666 entropy 0.03264415264129639
epoch: 43, step: 104
	action: tensor([[ 6.1800, -1.8381, -2.7128,  6.1800, -5.1494,  5.9392, -1.9119]],
       dtype=torch.float64)
	q_value: tensor([[-62.4673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 105
	action: tensor([[ 1.7793, -0.0513, -0.7719,  1.4584, -0.8744,  1.1099, -0.3039]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8745860125825315, distance: 0.4052562125645901 entropy 0.03264415264129639
epoch: 43, step: 106
	action: tensor([[ 6.1800, -1.5573, -2.7058,  6.0498, -4.1661,  4.6442, -2.0087]],
       dtype=torch.float64)
	q_value: tensor([[-54.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 107
	action: tensor([[ 2.0575, -0.6914, -0.2340,  1.6724, -1.4850,  0.8342, -0.7728]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.868824365770759, distance: 0.41446062694318453 entropy 0.03264415264129639
epoch: 43, step: 108
	action: tensor([[ 6.1800, -1.3635, -2.5513,  6.1800, -4.3584,  4.6180, -2.5574]],
       dtype=torch.float64)
	q_value: tensor([[-52.7668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 109
	action: tensor([[ 1.7799,  0.0245, -0.5665,  1.4274, -0.8460,  1.5102, -0.7521]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8467448185208304, distance: 0.44798582932663356 entropy 0.03264415264129639
epoch: 43, step: 110
	action: tensor([[ 6.1800, -1.2617, -2.8469,  6.1800, -4.7731,  5.4621, -1.9723]],
       dtype=torch.float64)
	q_value: tensor([[-59.7124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 111
	action: tensor([[ 2.0409, -0.2841, -0.5318,  1.9075, -0.9970,  2.4318, -0.2961]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5795606746793315, distance: 0.7420076084847727 entropy 0.03264415264129639
epoch: 43, step: 112
	action: tensor([[ 5.7833, -1.5403, -1.9290,  6.1800, -5.1694,  5.8234, -2.2050]],
       dtype=torch.float64)
	q_value: tensor([[-63.4545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 113
	action: tensor([[ 1.7765, -0.5230, -0.7275,  1.6459, -0.8518,  0.6923, -0.7911]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8911125682817655, distance: 0.3776118116139109 entropy 0.03264415264129639
epoch: 43, step: 114
	action: tensor([[ 6.1800, -1.2721, -2.3905,  6.1546, -4.7898,  5.6729, -2.2177]],
       dtype=torch.float64)
	q_value: tensor([[-51.2570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 115
	action: tensor([[ 2.1842, -0.5917, -0.3089,  0.8786, -1.4211,  1.0320, -0.8241]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6552504776285528, distance: 0.6719060163555705 entropy 0.03264415264129639
epoch: 43, step: 116
	action: tensor([[ 6.1800, -1.4053, -2.2251,  6.1800, -3.9479,  4.8571, -1.5314]],
       dtype=torch.float64)
	q_value: tensor([[-51.3062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 117
	action: tensor([[ 2.1921, -0.2685, -0.7658,  1.9242, -0.6265,  1.2597, -0.2380]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6763169499022059, distance: 0.651053491341063 entropy 0.03264415264129639
epoch: 43, step: 118
	action: tensor([[ 5.8781, -1.6056, -2.3923,  5.7822, -4.9801,  5.8183, -1.8425]],
       dtype=torch.float64)
	q_value: tensor([[-53.6300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 119
	action: tensor([[ 1.7990, -0.4028, -0.3247,  1.2435, -1.1263,  1.4104, -0.6364]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8666493160765463, distance: 0.4178826277380494 entropy 0.03264415264129639
epoch: 43, step: 120
	action: tensor([[ 6.1800, -1.5326, -2.1416,  6.1800, -4.8926,  5.1600, -1.8164]],
       dtype=torch.float64)
	q_value: tensor([[-53.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 121
	action: tensor([[ 1.9032, -0.3914, -0.6984,  1.6982, -1.5484,  1.7084, -0.5159]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6722180255680268, distance: 0.6551627946100438 entropy 0.03264415264129639
epoch: 43, step: 122
	action: tensor([[ 6.1800, -1.7011, -2.1329,  5.8000, -4.9127,  5.6958, -1.9291]],
       dtype=torch.float64)
	q_value: tensor([[-64.1940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 123
	action: tensor([[ 1.8938, -0.5426, -0.5888,  1.2488, -0.5735,  1.6764, -0.2289]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7951822689719208, distance: 0.5178935099412915 entropy 0.03264415264129639
epoch: 43, step: 124
	action: tensor([[ 6.1800, -1.7205, -2.0948,  6.1796, -4.4183,  5.2988, -1.4918]],
       dtype=torch.float64)
	q_value: tensor([[-50.7160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 125
	action: tensor([[ 2.0818, -0.0065, -1.0605,  1.6658, -1.0581,  1.6367, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6396392709814447, distance: 0.6869504647524016 entropy 0.03264415264129639
epoch: 43, step: 126
	action: tensor([[ 6.1800, -1.9797, -2.3902,  6.1800, -4.9200,  5.5000, -2.1634]],
       dtype=torch.float64)
	q_value: tensor([[-61.9077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 43, step: 127
	action: tensor([[ 1.9950, -0.3091, -1.2955,  1.9414, -0.8824,  1.2421, -0.6484]],
       dtype=torch.float64)
	q_value: tensor([[-31.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.497229782939102, distance: 0.8114120722082707 entropy 0.03264415264129639
LOSS epoch 43 actor 264.0236392774447 critic 187.75556509149027 
epoch: 44, step: 0
	action: tensor([[ 5.1825, -0.4503, -1.7404,  4.8380, -3.0707,  2.5817, -1.9584]],
       dtype=torch.float64)
	q_value: tensor([[-43.0987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 1
	action: tensor([[ 2.2837, -0.5910, -0.4892,  1.4408, -0.9986,  0.3286, -0.7561]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8823856051735431, distance: 0.39245234859149647 entropy 0.03264415264129639
epoch: 44, step: 2
	action: tensor([[ 5.4394, -0.3981, -1.4148,  3.8687, -2.8548,  2.7930, -1.5848]],
       dtype=torch.float64)
	q_value: tensor([[-31.8352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 3
	action: tensor([[ 2.1806, -0.5008, -0.6743,  2.0389, -0.5908,  0.7845, -1.1276]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7855355397454395, distance: 0.5299493455795394 entropy 0.03264415264129639
epoch: 44, step: 4
	action: tensor([[ 6.1800, -0.7861, -1.8383,  5.0301, -3.3081,  2.9877, -2.0672]],
       dtype=torch.float64)
	q_value: tensor([[-37.6411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 5
	action: tensor([[ 2.0791, -0.1743, -0.7224,  1.5110, -1.1388,  1.1159, -0.4300]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8676501269067542, distance: 0.4163115479928011 entropy 0.03264415264129639
epoch: 44, step: 6
	action: tensor([[ 4.6954, -0.5814, -1.3503,  3.6206, -2.5172,  2.4901, -1.0561]],
       dtype=torch.float64)
	q_value: tensor([[-38.4191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 7
	action: tensor([[ 2.7412,  0.1833, -0.8014,  1.7816, -1.5301,  1.3379, -0.3876]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 8
	action: tensor([[ 2.0958,  0.0086, -0.3214,  1.5142, -1.1484,  1.1819, -0.8641]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9299446382436315, distance: 0.3028847331121242 entropy 0.03264415264129639
epoch: 44, step: 9
	action: tensor([[ 5.1040, -0.3320, -1.4102,  4.3179, -2.5434,  2.3538, -1.4801]],
       dtype=torch.float64)
	q_value: tensor([[-40.1627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 10
	action: tensor([[ 1.8132, -0.3081, -0.3110,  2.0643, -1.2379,  1.1708, -0.8150]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8201446071932965, distance: 0.48530908949053453 entropy 0.03264415264129639
epoch: 44, step: 11
	action: tensor([[ 5.8032, -0.5608, -1.2661,  4.7582, -2.7438,  2.9350, -2.3297]],
       dtype=torch.float64)
	q_value: tensor([[-39.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 12
	action: tensor([[ 1.8268, -0.1023, -0.3196,  1.0901, -1.1841,  1.1282, -0.8232]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9447051390050839, distance: 0.2690909417346367 entropy 0.03264415264129639
epoch: 44, step: 13
	action: tensor([[ 4.7254, -0.3773, -1.1979,  3.5750, -2.1184,  2.3844, -1.4924]],
       dtype=torch.float64)
	q_value: tensor([[-37.9533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 14
	action: tensor([[ 2.3080, -0.1491, -0.3058,  1.7302, -0.7139,  0.9752, -0.1564]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8375468283526494, distance: 0.46123344420610424 entropy 0.03264415264129639
epoch: 44, step: 15
	action: tensor([[ 5.1549, -0.4670, -1.7289,  4.8590, -2.8714,  2.5205, -1.5535]],
       dtype=torch.float64)
	q_value: tensor([[-32.3961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 16
	action: tensor([[ 1.9257, -0.2385,  0.0288,  1.6844, -1.3849,  0.7438, -0.8048]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8864128670575724, distance: 0.3856747992067068 entropy 0.03264415264129639
epoch: 44, step: 17
	action: tensor([[ 4.8379,  0.0106, -1.5173,  3.7283, -2.6201,  2.0127, -1.8551]],
       dtype=torch.float64)
	q_value: tensor([[-36.0089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 18
	action: tensor([[ 1.8376, -0.0330, -0.8764,  1.4069, -0.9180,  1.2397, -0.6016]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.835685551039415, distance: 0.4638681676312384 entropy 0.03264415264129639
epoch: 44, step: 19
	action: tensor([[ 5.4871, -1.0005, -1.2979,  4.5498, -2.7680,  2.5746, -1.4599]],
       dtype=torch.float64)
	q_value: tensor([[-40.6188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 20
	action: tensor([[ 2.0033, -0.3469, -0.8285,  1.7522, -1.2072,  1.2187, -0.5316]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7473143303835315, distance: 0.575237247682469 entropy 0.03264415264129639
epoch: 44, step: 21
	action: tensor([[ 5.0971, -0.2953, -1.6665,  4.0414, -3.0306,  2.4944, -1.8282]],
       dtype=torch.float64)
	q_value: tensor([[-40.1934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 22
	action: tensor([[ 1.7516, -0.2011, -0.6584,  1.7014, -1.2825,  0.7453, -0.8984]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9157746620819356, distance: 0.3321070342224611 entropy 0.03264415264129639
epoch: 44, step: 23
	action: tensor([[ 5.0155, -0.3871, -1.6909,  4.5537, -2.4953,  2.7651, -1.1680]],
       dtype=torch.float64)
	q_value: tensor([[-39.7796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 24
	action: tensor([[ 1.8472, -0.1827, -0.8664,  1.7465, -1.2516,  0.8706, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8407612513344933, distance: 0.45664748630351787 entropy 0.03264415264129639
epoch: 44, step: 25
	action: tensor([[ 4.5907, -0.3810, -1.5136,  3.8105, -2.6847,  2.2435, -1.3149]],
       dtype=torch.float64)
	q_value: tensor([[-36.6069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 26
	action: tensor([[ 1.8274, -0.4610, -0.2130,  1.9035, -1.0587,  0.6839, -0.7795]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8828265812921953, distance: 0.3917159394317149 entropy 0.03264415264129639
epoch: 44, step: 27
	action: tensor([[ 5.5763, -0.2610, -1.8840,  4.6101, -3.0925,  3.2168, -2.0242]],
       dtype=torch.float64)
	q_value: tensor([[-34.6270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 28
	action: tensor([[ 1.6988, -0.2515, -0.6849,  1.6251, -0.9407,  0.7478, -0.6559]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9121686532449862, distance: 0.3391419119968076 entropy 0.03264415264129639
epoch: 44, step: 29
	action: tensor([[ 5.1438, -0.7460, -1.0295,  4.1389, -2.8668,  3.1042, -0.9277]],
       dtype=torch.float64)
	q_value: tensor([[-36.2171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 30
	action: tensor([[ 2.2484, -0.3225, -0.3937,  1.3365, -1.2359,  1.2943, -0.8988]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8785957137116389, distance: 0.3987252172566553 entropy 0.03264415264129639
epoch: 44, step: 31
	action: tensor([[ 4.8887, -0.0815, -1.4986,  4.1005, -2.8436,  2.2122, -1.5082]],
       dtype=torch.float64)
	q_value: tensor([[-39.3182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 32
	action: tensor([[ 2.3356, -0.1508, -0.3595,  1.6482, -1.0934,  0.7589, -0.5295]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9263115360714234, distance: 0.31063933300821567 entropy 0.03264415264129639
epoch: 44, step: 33
	action: tensor([[ 4.7597, -0.2311, -1.5792,  3.7076, -2.2953,  2.1647, -1.8109]],
       dtype=torch.float64)
	q_value: tensor([[-35.0567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 34
	action: tensor([[ 1.7354, -0.7842, -0.6671,  1.4496, -1.0875,  1.1976, -0.6797]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7476520464611508, distance: 0.574852714953654 entropy 0.03264415264129639
epoch: 44, step: 35
	action: tensor([[ 5.4508, -0.6091, -1.2388,  4.0519, -3.1556,  2.0597, -1.6696]],
       dtype=torch.float64)
	q_value: tensor([[-35.9048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 36
	action: tensor([[ 2.0261,  0.0336, -0.3985,  1.1662, -1.3706,  0.8875, -0.6941]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9590842806989879, distance: 0.23147377186056842 entropy 0.03264415264129639
epoch: 44, step: 37
	action: tensor([[ 4.2310, -0.3688, -1.8388,  3.7272, -2.3169,  2.6073, -1.4173]],
       dtype=torch.float64)
	q_value: tensor([[-38.3385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 38
	action: tensor([[ 1.6528, -0.5294, -0.3307,  1.7333, -0.7613,  0.8735, -0.6660]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8862284622507368, distance: 0.38598773713849194 entropy 0.03264415264129639
epoch: 44, step: 39
	action: tensor([[ 5.7098, -0.4948, -1.9280,  4.6932, -3.0286,  2.9022, -1.4521]],
       dtype=torch.float64)
	q_value: tensor([[-32.7776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 40
	action: tensor([[ 2.0540, -0.2618, -0.5098,  1.6846, -1.0955,  1.0782, -0.7682]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8911125281642327, distance: 0.3776118811759009 entropy 0.03264415264129639
epoch: 44, step: 41
	action: tensor([[ 5.1628, -0.6603, -1.4675,  3.9839, -3.1538,  2.7562, -1.5459]],
       dtype=torch.float64)
	q_value: tensor([[-38.6049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 42
	action: tensor([[ 1.9375, -0.5978, -0.2788,  1.5474, -1.2464,  0.9748, -0.9155]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8814233816250787, distance: 0.3940544385301976 entropy 0.03264415264129639
epoch: 44, step: 43
	action: tensor([[ 5.0055, -0.5161, -1.6556,  4.1809, -2.5625,  2.6157, -1.5678]],
       dtype=torch.float64)
	q_value: tensor([[-36.1540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 44
	action: tensor([[ 1.5819, -0.5070, -1.0939,  1.8720, -1.4463,  0.8182, -0.4824]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.681142687118546, distance: 0.6461820416777245 entropy 0.03264415264129639
epoch: 44, step: 45
	action: tensor([[ 4.6972, -0.4314, -0.9646,  4.1804, -2.7358,  2.4829, -1.8550]],
       dtype=torch.float64)
	q_value: tensor([[-39.6054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 46
	action: tensor([[ 2.2626e+00, -3.6068e-04, -6.8577e-01,  1.6922e+00, -7.2995e-01,
          1.1999e+00, -1.0762e+00]], dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8031930371704865, distance: 0.5076646494035293 entropy 0.03264415264129639
epoch: 44, step: 47
	action: tensor([[ 5.5125, -0.7424, -1.9015,  5.1323, -3.2156,  2.5919, -1.7098]],
       dtype=torch.float64)
	q_value: tensor([[-42.0797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 48
	action: tensor([[ 2.0654, -0.2105, -1.3174,  1.7698, -1.1176,  0.7790, -0.7060]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6548245260248391, distance: 0.6723209713860285 entropy 0.03264415264129639
epoch: 44, step: 49
	action: tensor([[ 5.5593, -0.3305, -1.1200,  4.0341, -2.6874,  2.0061, -1.5677]],
       dtype=torch.float64)
	q_value: tensor([[-42.2344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 50
	action: tensor([[ 2.1143,  0.3044, -0.4735,  1.8848, -1.2787,  0.8038, -0.4601]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8238275953141971, distance: 0.48031443018824727 entropy 0.03264415264129639
epoch: 44, step: 51
	action: tensor([[ 4.8350, -0.8329, -1.2016,  4.5826, -2.6678,  2.7571, -1.6684]],
       dtype=torch.float64)
	q_value: tensor([[-40.1030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 52
	action: tensor([[ 2.3642, -0.2403, -0.8592,  1.7063, -1.0652,  1.3201, -0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7380005327136889, distance: 0.5857427170476972 entropy 0.03264415264129639
epoch: 44, step: 53
	action: tensor([[ 4.6193, -0.3503, -1.7391,  3.9096, -2.4815,  2.4950, -1.4707]],
       dtype=torch.float64)
	q_value: tensor([[-39.5187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 54
	action: tensor([[ 2.3462, -0.0850, -0.6246,  1.2650, -0.7878,  0.7288, -0.0842]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9640108806924352, distance: 0.21709124177282083 entropy 0.03264415264129639
epoch: 44, step: 55
	action: tensor([[ 4.1339, -0.4416, -1.1863,  3.7026, -2.7293,  2.0676, -1.4897]],
       dtype=torch.float64)
	q_value: tensor([[-31.7268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 56
	action: tensor([[ 1.9289, -0.0770, -0.4972,  1.9858, -1.0359,  1.3825, -0.6218]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.735374317829169, distance: 0.5886710638337783 entropy 0.03264415264129639
epoch: 44, step: 57
	action: tensor([[ 5.2922, -0.5027, -1.8179,  4.6878, -2.6893,  3.0734, -1.5148]],
       dtype=torch.float64)
	q_value: tensor([[-41.2151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 58
	action: tensor([[ 2.0071, -0.5811, -0.7573,  1.7194, -0.7424,  1.2688, -0.5585]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7569590187348673, distance: 0.5641524106612933 entropy 0.03264415264129639
epoch: 44, step: 59
	action: tensor([[ 5.6760, -0.8529, -1.3678,  4.7244, -3.0982,  2.8175, -1.8359]],
       dtype=torch.float64)
	q_value: tensor([[-36.3060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 60
	action: tensor([[ 2.1671, -0.0606, -0.8662,  1.9614, -0.9052,  0.8668, -0.7809]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7593052834464847, distance: 0.561422704421964 entropy 0.03264415264129639
epoch: 44, step: 61
	action: tensor([[ 5.4372, -0.5857, -1.0778,  4.4651, -2.9575,  2.5772, -1.3744]],
       dtype=torch.float64)
	q_value: tensor([[-40.7532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 62
	action: tensor([[ 2.5531, -0.0301, -0.5749,  1.6059, -1.0644,  1.0844, -0.3366]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 63
	action: tensor([[ 2.0405, -0.5833, -0.5342,  1.9108, -1.0828,  1.1138, -0.7087]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8130643870563113, distance: 0.49476926836669993 entropy 0.03264415264129639
epoch: 44, step: 64
	action: tensor([[ 5.7475, -0.3978, -1.3925,  4.4814, -3.1717,  2.5575, -1.8190]],
       dtype=torch.float64)
	q_value: tensor([[-37.2166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 65
	action: tensor([[ 1.9328,  0.1090, -0.2696,  1.4914, -1.3907,  1.2794, -0.7673]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9333069828505682, distance: 0.2955268031179182 entropy 0.03264415264129639
epoch: 44, step: 66
	action: tensor([[ 5.0921, -0.8305, -1.3707,  4.1143, -2.1823,  2.3333, -1.3981]],
       dtype=torch.float64)
	q_value: tensor([[-41.6605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 67
	action: tensor([[ 1.9151,  0.1738, -0.8810,  1.2781, -1.1082,  0.9645, -0.6598]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9256049649636973, distance: 0.31212508209646733 entropy 0.03264415264129639
epoch: 44, step: 68
	action: tensor([[ 4.6163, -0.3040, -1.5150,  3.2669, -2.3286,  2.4659, -1.3385]],
       dtype=torch.float64)
	q_value: tensor([[-41.2681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 69
	action: tensor([[ 2.0660, -0.1780, -0.4342,  1.7874, -0.6186,  1.1479, -0.6717]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7937338404965875, distance: 0.5197215015364602 entropy 0.03264415264129639
epoch: 44, step: 70
	action: tensor([[ 5.7367, -0.5566, -1.5920,  5.0906, -3.4915,  2.4727, -1.8384]],
       dtype=torch.float64)
	q_value: tensor([[-36.4489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 71
	action: tensor([[ 1.9610,  0.1178, -0.3665,  1.9403, -1.0517,  1.2109, -0.9145]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7803455189465449, distance: 0.5363233767903084 entropy 0.03264415264129639
epoch: 44, step: 72
	action: tensor([[ 5.2648, -0.5643, -1.2766,  4.1432, -3.1497,  2.9866, -1.5547]],
       dtype=torch.float64)
	q_value: tensor([[-42.3235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 73
	action: tensor([[ 2.0460, -0.2878, -0.4643,  1.4601, -1.3250,  0.6898, -1.2161]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9475811640640938, distance: 0.26199944988675505 entropy 0.03264415264129639
epoch: 44, step: 74
	action: tensor([[ 5.5152, -0.2185, -1.5996,  4.0324, -2.8309,  1.9785, -1.3913]],
       dtype=torch.float64)
	q_value: tensor([[-39.4137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 75
	action: tensor([[ 2.1202, -0.2638, -0.9103,  1.6841, -1.0170,  0.8618, -0.3416]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8305722683816193, distance: 0.47103040468018326 entropy 0.03264415264129639
epoch: 44, step: 76
	action: tensor([[ 4.5149, -0.7111, -1.0650,  3.9392, -3.0016,  2.1952, -1.8566]],
       dtype=torch.float64)
	q_value: tensor([[-36.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 77
	action: tensor([[ 2.0838, -0.4062, -1.2703,  1.6825, -1.2807,  0.8938, -0.4899]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6157813215633432, distance: 0.7093260836383177 entropy 0.03264415264129639
epoch: 44, step: 78
	action: tensor([[ 4.6571, -0.5356, -1.4344,  4.0249, -2.8882,  2.6631, -1.4989]],
       dtype=torch.float64)
	q_value: tensor([[-40.6830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 79
	action: tensor([[ 2.2089,  0.0158,  0.0873,  1.7053, -0.8384,  1.0542, -0.8691]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.780874215433715, distance: 0.5356775372122815 entropy 0.03264415264129639
epoch: 44, step: 80
	action: tensor([[ 5.7438, -0.8124, -1.4149,  4.0282, -2.8949,  2.7530, -1.5041]],
       dtype=torch.float64)
	q_value: tensor([[-36.0466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 81
	action: tensor([[ 2.1441,  0.1596, -0.5510,  1.6518, -1.1303,  0.7850, -0.6845]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9072037542038306, distance: 0.34859559634503345 entropy 0.03264415264129639
epoch: 44, step: 82
	action: tensor([[ 4.6494,  0.1855, -2.0919,  3.9664, -2.3551,  2.7594, -1.4212]],
       dtype=torch.float64)
	q_value: tensor([[-39.3977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 83
	action: tensor([[ 2.1517, -0.1179, -0.6748,  1.5954, -0.9496,  1.0318, -0.8265]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8795876705377529, distance: 0.3970929460870686 entropy 0.03264415264129639
epoch: 44, step: 84
	action: tensor([[ 5.2999, -0.3979, -1.7097,  4.1920, -3.0094,  2.5640, -1.2800]],
       dtype=torch.float64)
	q_value: tensor([[-39.6147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 85
	action: tensor([[ 1.9450, -0.1962, -0.7992,  1.8366, -1.2060,  1.0933, -0.4260]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.78421904021226, distance: 0.5315734157332023 entropy 0.03264415264129639
epoch: 44, step: 86
	action: tensor([[ 5.0990, -0.2670, -0.9682,  4.3622, -2.7984,  2.5159, -1.1375]],
       dtype=torch.float64)
	q_value: tensor([[-39.8279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 87
	action: tensor([[ 2.3559,  0.0048, -0.5864,  1.5675, -1.2585,  0.8091, -0.4103]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9428612660080156, distance: 0.2735407290427182 entropy 0.03264415264129639
epoch: 44, step: 88
	action: tensor([[ 4.9958, -0.1153, -1.6792,  3.7346, -2.5417,  2.4706, -1.9294]],
       dtype=torch.float64)
	q_value: tensor([[-37.6361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 89
	action: tensor([[ 2.3751, -0.3196, -0.6938,  1.0171, -0.8648,  0.8959, -0.3883]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8714271522003397, distance: 0.4103281620654868 entropy 0.03264415264129639
epoch: 44, step: 90
	action: tensor([[ 4.6194, -0.2327, -1.3800,  4.0152, -2.3870,  2.9075, -1.5649]],
       dtype=torch.float64)
	q_value: tensor([[-33.0946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 91
	action: tensor([[ 2.2158, -0.4559, -0.4234,  1.7086, -1.1865,  1.4468, -0.8188]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8084674761023247, distance: 0.500815727678936 entropy 0.03264415264129639
epoch: 44, step: 92
	action: tensor([[ 5.4112, -0.7007, -1.6278,  4.8168, -3.0937,  2.6191, -2.2128]],
       dtype=torch.float64)
	q_value: tensor([[-39.9252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 93
	action: tensor([[ 1.7923, -0.2080, -1.0506,  1.6541, -1.3752,  1.1477, -0.5677]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7067655950343932, distance: 0.6196752568659355 entropy 0.03264415264129639
epoch: 44, step: 94
	action: tensor([[ 5.2820, -0.5538, -1.5602,  3.8912, -2.8121,  2.4602, -1.1660]],
       dtype=torch.float64)
	q_value: tensor([[-42.8105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 95
	action: tensor([[ 2.0744,  0.0127, -0.4469,  1.5195, -1.1030,  0.9241, -0.9699]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9375657178220337, distance: 0.2859356163172945 entropy 0.03264415264129639
epoch: 44, step: 96
	action: tensor([[ 4.9575, -0.8644, -1.4271,  4.2006, -3.0482,  2.3711, -1.4096]],
       dtype=torch.float64)
	q_value: tensor([[-39.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 97
	action: tensor([[ 1.8597, -0.3953, -0.4453,  1.8355, -1.2826,  0.9864, -0.3794]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8840490927650838, distance: 0.3896671263655593 entropy 0.03264415264129639
epoch: 44, step: 98
	action: tensor([[ 5.0919, -0.6453, -1.5592,  3.9158, -2.4884,  2.6544, -1.3957]],
       dtype=torch.float64)
	q_value: tensor([[-36.1642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 99
	action: tensor([[ 2.3129, -0.3407, -0.5943,  1.8532, -1.4567,  1.3948, -1.2614]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7987596622342684, distance: 0.513350763242522 entropy 0.03264415264129639
epoch: 44, step: 100
	action: tensor([[ 5.6265, -0.5895, -1.6979,  4.8911, -3.2518,  2.7815, -2.1321]],
       dtype=torch.float64)
	q_value: tensor([[-45.7846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 101
	action: tensor([[ 1.5788, -0.4202, -0.8201,  1.4482, -0.7666,  0.8413, -0.9356]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8695871016425313, distance: 0.41325390564464354 entropy 0.03264415264129639
epoch: 44, step: 102
	action: tensor([[ 5.5909, -0.6558, -1.3968,  4.3264, -2.5756,  3.0553, -1.6356]],
       dtype=torch.float64)
	q_value: tensor([[-36.6047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 103
	action: tensor([[ 2.2236, -0.0050, -0.9655,  1.7062, -0.3505,  1.3752, -0.8574]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6966877988997835, distance: 0.6302337166290821 entropy 0.03264415264129639
epoch: 44, step: 104
	action: tensor([[ 6.0968, -0.2235, -1.8420,  5.3615, -3.4504,  2.9414, -1.1139]],
       dtype=torch.float64)
	q_value: tensor([[-41.6298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 105
	action: tensor([[ 1.8495, -0.3650, -0.9382,  1.3674, -0.6563,  0.9541, -0.3925]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8567603011510347, distance: 0.4331002085990526 entropy 0.03264415264129639
epoch: 44, step: 106
	action: tensor([[ 4.9439, -0.5188, -1.7202,  3.7896, -2.6907,  2.4699, -2.0373]],
       dtype=torch.float64)
	q_value: tensor([[-34.5180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 107
	action: tensor([[ 1.3915, -0.0415, -1.1187,  1.9859, -1.4758,  1.1465, -0.5473]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5817920646627838, distance: 0.7400359672489646 entropy 0.03264415264129639
epoch: 44, step: 108
	action: tensor([[ 4.9114, -0.6712, -1.5511,  4.0444, -2.2887,  2.4484, -1.7277]],
       dtype=torch.float64)
	q_value: tensor([[-43.4261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 109
	action: tensor([[ 1.8545, -0.0364, -0.4025,  1.6818, -1.0883,  1.0453, -0.3206]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8886891370203726, distance: 0.38179080805609333 entropy 0.03264415264129639
epoch: 44, step: 110
	action: tensor([[ 4.8311, -0.4269, -1.5743,  4.3045, -2.9360,  2.0491, -1.3227]],
       dtype=torch.float64)
	q_value: tensor([[-36.5095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 111
	action: tensor([[ 2.0971,  0.2136, -0.7306,  1.4922, -0.7849,  0.8135, -0.7767]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9102405637393245, distance: 0.3428441563078012 entropy 0.03264415264129639
epoch: 44, step: 112
	action: tensor([[ 4.4059, -0.2662, -1.1953,  4.0328, -2.9776,  2.3801, -1.6646]],
       dtype=torch.float64)
	q_value: tensor([[-39.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 113
	action: tensor([[ 2.0472, -0.1512, -0.0794,  1.9686, -1.0445,  1.1030, -0.8574]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8081117980077062, distance: 0.5012805222732526 entropy 0.03264415264129639
epoch: 44, step: 114
	action: tensor([[ 5.2668, -0.3084, -1.4860,  4.4535, -2.8579,  3.2173, -1.9582]],
       dtype=torch.float64)
	q_value: tensor([[-37.9502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 115
	action: tensor([[ 2.4648, -0.5191, -0.6991,  1.6743, -0.9189,  1.1889, -0.8308]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 116
	action: tensor([[ 1.5494, -0.1176, -0.1631,  1.6094, -1.1342,  1.0683, -0.7026]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9047339412568278, distance: 0.3532041457242685 entropy 0.03264415264129639
epoch: 44, step: 117
	action: tensor([[ 4.7435, -0.5066, -1.3751,  4.4145, -2.4786,  2.5755, -1.8361]],
       dtype=torch.float64)
	q_value: tensor([[-36.3674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 118
	action: tensor([[ 1.9827, -1.2816, -0.4275,  1.2283, -0.9812,  1.2652, -0.6279]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6019029869015312, distance: 0.7220232005139506 entropy 0.03264415264129639
epoch: 44, step: 119
	action: tensor([[ 5.6221, -0.4010, -1.5713,  4.5429, -3.1918,  3.0228, -1.9280]],
       dtype=torch.float64)
	q_value: tensor([[-31.5909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 120
	action: tensor([[ 2.0879, -0.2642, -0.3772,  1.3602, -0.9520,  1.0689, -1.0013]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9276199230424711, distance: 0.307869178987875 entropy 0.03264415264129639
epoch: 44, step: 121
	action: tensor([[ 5.2811, -0.3445, -1.2127,  4.3689, -2.9808,  2.4786, -1.4954]],
       dtype=torch.float64)
	q_value: tensor([[-37.4944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 122
	action: tensor([[ 1.9778, -0.2423, -0.3571,  1.7752, -1.2764,  0.6583, -0.6398]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9263266125457325, distance: 0.31060755336966367 entropy 0.03264415264129639
epoch: 44, step: 123
	action: tensor([[ 4.9719, -0.8483, -1.8936,  3.8070, -2.4308,  2.4000, -1.3699]],
       dtype=torch.float64)
	q_value: tensor([[-36.1304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 124
	action: tensor([[ 1.8459, -0.3240, -0.6199,  1.8828, -1.0200,  1.0846, -0.6060]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8036043800198288, distance: 0.5071338413224767 entropy 0.03264415264129639
epoch: 44, step: 125
	action: tensor([[ 5.4240, -0.5801, -1.9263,  4.7306, -3.1294,  2.4178, -2.0733]],
       dtype=torch.float64)
	q_value: tensor([[-38.1389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 44, step: 126
	action: tensor([[ 2.2075, -0.2202, -0.7017,  2.1824, -1.0994,  0.8175, -0.9183]],
       dtype=torch.float64)
	q_value: tensor([[-23.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7454715001041663, distance: 0.5773310325794647 entropy 0.03264415264129639
epoch: 44, step: 127
	action: tensor([[ 5.6755, -0.3469, -1.6318,  5.0189, -2.8903,  2.5528, -1.9908]],
       dtype=torch.float64)
	q_value: tensor([[-40.9696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 44 actor 185.31140773043722 critic 386.6761031330806 
epoch: 45, step: 0
	action: tensor([[ 1.9429, -0.2593, -0.4581,  1.2044, -0.8636,  0.8317, -0.9694]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9338288240461181, distance: 0.2943683541253906 entropy 0.03264415264129639
epoch: 45, step: 1
	action: tensor([[ 2.6219,  0.2227, -0.6895,  2.4365, -1.7279,  1.0761, -1.0835]],
       dtype=torch.float64)
	q_value: tensor([[-30.3399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 2
	action: tensor([[ 1.6385,  0.1026, -0.7650,  1.5854, -1.4396,  0.4240, -0.6234]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9628897782797018, distance: 0.22044663189626704 entropy 0.03264415264129639
epoch: 45, step: 3
	action: tensor([[ 2.8057, -0.2116, -0.8873,  2.2594, -1.2141,  0.8810, -0.6266]],
       dtype=torch.float64)
	q_value: tensor([[-33.7742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 4
	action: tensor([[ 1.3157,  0.1917, -0.4038,  1.9934, -0.5357,  0.1711, -0.7389]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6918479683480556, distance: 0.6352420095793522 entropy 0.03264415264129639
epoch: 45, step: 5
	action: tensor([[ 3.1546,  0.2303, -1.3471,  2.1926, -2.1845,  1.1230, -0.1725]],
       dtype=torch.float64)
	q_value: tensor([[-27.1851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 6
	action: tensor([[ 1.9927, -0.1038, -0.3225,  1.7436, -1.0927,  0.9300, -0.5996]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8916019290079535, distance: 0.3767623266284442 entropy 0.03264415264129639
epoch: 45, step: 7
	action: tensor([[ 3.2224, -0.3058, -1.0180,  2.7344, -1.3435,  1.1503, -1.1466]],
       dtype=torch.float64)
	q_value: tensor([[-31.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 8
	action: tensor([[ 1.9633,  0.0633, -0.8067,  1.0372, -0.8806,  0.8621, -0.7347]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9491896505618226, distance: 0.2579483677711253 entropy 0.03264415264129639
epoch: 45, step: 9
	action: tensor([[ 2.1623, -0.6761, -0.8748,  1.4448, -1.7340,  0.8784, -0.6299]],
       dtype=torch.float64)
	q_value: tensor([[-32.7392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 10
	action: tensor([[ 1.8767, -0.0372, -0.5356,  0.9241, -0.9991,  0.8316, -0.9691]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9429998776963999, distance: 0.27320873914326416 entropy 0.03264415264129639
epoch: 45, step: 11
	action: tensor([[ 2.7380,  0.2797, -0.7618,  1.6509, -1.6330,  0.6893, -0.9618]],
       dtype=torch.float64)
	q_value: tensor([[-32.1051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 12
	action: tensor([[ 1.4919,  0.1981, -0.8639,  1.5019, -1.3287,  0.6166, -0.3796]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.916877589051916, distance: 0.3299254053486054 entropy 0.03264415264129639
epoch: 45, step: 13
	action: tensor([[ 2.5149, -0.1047, -0.9201,  1.8363, -1.6610,  0.6510, -0.9303]],
       dtype=torch.float64)
	q_value: tensor([[-32.6462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 14
	action: tensor([[ 1.7253,  0.0157, -0.5454,  1.0279, -1.2160,  0.7210, -0.5754]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9487584804968469, distance: 0.2590405139847875 entropy 0.03264415264129639
epoch: 45, step: 15
	action: tensor([[ 2.5662, -0.1757, -0.8571,  1.2115, -1.2142,  0.8142, -0.6915]],
       dtype=torch.float64)
	q_value: tensor([[-31.0438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 16
	action: tensor([[ 1.7912,  0.0606, -1.0200,  1.1486, -1.0449,  0.7724, -0.4814]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9087742544209988, distance: 0.3456331616372569 entropy 0.03264415264129639
epoch: 45, step: 17
	action: tensor([[ 2.8925, -0.3378, -0.7252,  1.7921, -1.3519,  0.6855, -1.1474]],
       dtype=torch.float64)
	q_value: tensor([[-33.0416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 18
	action: tensor([[ 1.9665, -0.0672, -0.0229,  1.5660, -0.7629,  0.5161, -0.4726]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8334131474270146, distance: 0.46706471017495366 entropy 0.03264415264129639
epoch: 45, step: 19
	action: tensor([[ 3.1581,  0.0463, -0.9209,  2.5040, -1.4572,  1.1703, -0.8984]],
       dtype=torch.float64)
	q_value: tensor([[-26.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 20
	action: tensor([[ 1.5115, -0.1359, -0.4329,  1.6614, -1.1971,  0.8508, -0.6192]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9169759840075897, distance: 0.3297300753055066 entropy 0.03264415264129639
epoch: 45, step: 21
	action: tensor([[ 2.5749,  0.1095, -1.2053,  1.6563, -1.5480,  1.0801, -0.8194]],
       dtype=torch.float64)
	q_value: tensor([[-30.6786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 22
	action: tensor([[ 1.4588, -0.2197, -0.6938,  1.2897, -1.0863,  0.4195, -0.5475]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9622099338937988, distance: 0.22245671466327951 entropy 0.03264415264129639
epoch: 45, step: 23
	action: tensor([[ 2.4856, -0.0532, -0.4203,  2.6691, -1.8094,  1.0719, -0.9282]],
       dtype=torch.float64)
	q_value: tensor([[-27.5535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 24
	action: tensor([[ 1.8946,  0.1069, -0.3139,  1.1668, -1.0044,  0.5457, -0.6504]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9353895095285291, distance: 0.29087621623499077 entropy 0.03264415264129639
epoch: 45, step: 25
	action: tensor([[ 2.7599, -0.4484, -0.7000,  1.8738, -0.9420,  1.3204, -0.5655]],
       dtype=torch.float64)
	q_value: tensor([[-29.3134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 26
	action: tensor([[ 1.8871,  0.4684, -0.5950,  1.3789, -1.3250,  0.9782, -0.7916]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.944670415786315, distance: 0.2691754182759639 entropy 0.03264415264129639
epoch: 45, step: 27
	action: tensor([[ 1.9215,  0.0251, -0.8632,  2.0856, -1.5603,  0.9466, -0.5500]],
       dtype=torch.float64)
	q_value: tensor([[-37.5135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 28
	action: tensor([[ 1.9375,  0.2317, -0.4477,  1.6233, -0.7339,  0.3629, -0.2272]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8617274233083201, distance: 0.4255246481072802 entropy 0.03264415264129639
epoch: 45, step: 29
	action: tensor([[ 2.4212, -0.0812, -0.5455,  2.3252, -1.4954,  1.1118, -0.8006]],
       dtype=torch.float64)
	q_value: tensor([[-27.8815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 30
	action: tensor([[ 1.6254, -0.4766, -0.9711,  1.3214, -1.1840,  1.1559, -0.5799]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6905545750979163, distance: 0.6365737505910681 entropy 0.03264415264129639
epoch: 45, step: 31
	action: tensor([[ 2.8362,  0.1598, -0.7913,  2.7756, -1.9722,  0.9815, -0.6083]],
       dtype=torch.float64)
	q_value: tensor([[-32.9588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 32
	action: tensor([[ 1.8797,  0.3098, -0.2782,  1.1979, -0.8887,  0.0901, -0.7514]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8417208464690538, distance: 0.45526949613107476 entropy 0.03264415264129639
epoch: 45, step: 33
	action: tensor([[ 2.4976,  0.0078, -0.6872,  1.8127, -1.6749,  0.6428, -1.5673]],
       dtype=torch.float64)
	q_value: tensor([[-28.5509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 34
	action: tensor([[ 2.0749, -0.1038, -0.7408,  1.4431, -0.9869,  0.5112,  0.1969]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9653362342247466, distance: 0.21305638994887333 entropy 0.03264415264129639
epoch: 45, step: 35
	action: tensor([[ 2.7021,  0.3009, -1.0409,  2.2843, -1.6444,  1.0782, -0.4697]],
       dtype=torch.float64)
	q_value: tensor([[-26.5082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 36
	action: tensor([[ 1.3007, -0.2845, -0.4459,  1.3374, -1.4291,  0.8755, -0.4151]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9300546150989768, distance: 0.3026468969707029 entropy 0.03264415264129639
epoch: 45, step: 37
	action: tensor([[ 2.8683,  0.0138, -0.4173,  1.9368, -1.4864,  1.1702, -0.8074]],
       dtype=torch.float64)
	q_value: tensor([[-27.6387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 38
	action: tensor([[ 1.7181, -0.0914, -0.6803,  1.7342, -0.6915,  0.3726, -1.2941]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9235098736929928, distance: 0.3164895554303766 entropy 0.03264415264129639
epoch: 45, step: 39
	action: tensor([[ 3.6446, -0.0527, -0.6391,  2.9492, -1.8713,  1.1251, -1.2479]],
       dtype=torch.float64)
	q_value: tensor([[-32.9377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 40
	action: tensor([[ 1.9248,  0.3136, -0.1119,  1.7157, -1.1199,  0.3973, -0.5508]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7352724715369653, distance: 0.5887843336365947 entropy 0.03264415264129639
epoch: 45, step: 41
	action: tensor([[ 2.9856, -0.1134, -0.4501,  2.2767, -1.3740,  0.8691, -0.9042]],
       dtype=torch.float64)
	q_value: tensor([[-30.2136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 42
	action: tensor([[ 1.8737, -0.0637, -0.3908,  0.7612, -1.4660,  0.4966, -0.4700]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8027249986043228, distance: 0.5082679449684532 entropy 0.03264415264129639
epoch: 45, step: 43
	action: tensor([[ 2.2021,  0.5658, -0.6686,  1.3822, -1.2769,  0.8838, -0.2748]],
       dtype=torch.float64)
	q_value: tensor([[-28.9665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 44
	action: tensor([[ 2.1827,  0.0322, -0.4247,  1.1896, -0.6421,  0.1641, -0.4828]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9132259791279451, distance: 0.33709441273139074 entropy 0.03264415264129639
epoch: 45, step: 45
	action: tensor([[ 3.0531, -0.1951, -0.8987,  2.2989, -1.4819,  0.7137, -0.5856]],
       dtype=torch.float64)
	q_value: tensor([[-25.7079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 46
	action: tensor([[ 1.6397,  0.2110, -0.2644,  1.3608, -0.8544,  0.5867, -0.7423]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.892324103233835, distance: 0.37550518831356305 entropy 0.03264415264129639
epoch: 45, step: 47
	action: tensor([[ 2.8852, -0.3975, -1.0436,  1.8002, -1.8175,  1.1027, -0.5562]],
       dtype=torch.float64)
	q_value: tensor([[-30.0879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 48
	action: tensor([[ 1.8555e+00,  1.6693e-03, -7.4408e-01,  1.2695e+00, -1.0353e+00,
          5.1324e-01, -6.5326e-01]], dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9723672109843251, distance: 0.1902256399473251 entropy 0.03264415264129639
epoch: 45, step: 49
	action: tensor([[ 3.1026,  0.2049, -0.8205,  1.9666, -1.7845,  1.1989, -1.4235]],
       dtype=torch.float64)
	q_value: tensor([[-31.1270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 50
	action: tensor([[ 1.6095, -0.8559, -0.6948,  1.0656, -0.7929,  0.9141, -0.3284]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.688885340207368, distance: 0.6382883695566146 entropy 0.03264415264129639
epoch: 45, step: 51
	action: tensor([[ 3.0494, -0.0514, -1.1468,  2.2511, -1.6378,  0.9984, -0.7078]],
       dtype=torch.float64)
	q_value: tensor([[-24.7907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 52
	action: tensor([[ 1.3152, -0.2985, -1.2464,  1.4489, -0.7679,  0.6278, -0.8672]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6757229372212253, distance: 0.6516506136832841 entropy 0.03264415264129639
epoch: 45, step: 53
	action: tensor([[ 3.1474,  0.1615, -0.5948,  2.1604, -1.4973,  1.1421, -0.6935]],
       dtype=torch.float64)
	q_value: tensor([[-30.4981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 54
	action: tensor([[ 1.8909,  0.2548, -0.7772,  1.4631, -0.6473,  0.6707, -0.4242]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9120944959269366, distance: 0.33928505308026613 entropy 0.03264415264129639
epoch: 45, step: 55
	action: tensor([[ 2.8360, -0.0902, -0.8808,  2.1930, -1.4004,  0.8731, -0.9982]],
       dtype=torch.float64)
	q_value: tensor([[-31.0013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 56
	action: tensor([[ 1.7210, -0.3252, -0.5239,  1.0640, -0.8520,  0.4490, -0.4956]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8963550143072084, distance: 0.3684095174915587 entropy 0.03264415264129639
epoch: 45, step: 57
	action: tensor([[ 2.8598, -0.2222, -0.9606,  2.0510, -1.2627,  1.1296, -0.6667]],
       dtype=torch.float64)
	q_value: tensor([[-25.8558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 58
	action: tensor([[ 1.8860,  0.3142, -0.8757,  1.5171, -1.1638,  0.7631, -0.4022]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.915277893424308, distance: 0.33308499297659633 entropy 0.03264415264129639
epoch: 45, step: 59
	action: tensor([[ 2.7633,  0.5252, -0.7061,  2.1544, -1.9550,  0.7560, -0.8113]],
       dtype=torch.float64)
	q_value: tensor([[-34.5735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 60
	action: tensor([[ 2.0463, -0.0934, -0.2358,  1.1479, -0.9746,  0.1718, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8359269656132243, distance: 0.4635272794759047 entropy 0.03264415264129639
epoch: 45, step: 61
	action: tensor([[ 2.0528,  0.3726, -0.4168,  2.1469, -1.3991,  0.8268, -0.9699]],
       dtype=torch.float64)
	q_value: tensor([[-24.7961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 62
	action: tensor([[ 2.2400, -0.1348, -0.4341,  1.1041, -0.7346,  0.5253, -0.1910]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9238532059248348, distance: 0.3157784618642352 entropy 0.03264415264129639
epoch: 45, step: 63
	action: tensor([[ 2.7870,  0.0360, -1.2149,  1.7403, -1.9362,  1.0575, -0.4041]],
       dtype=torch.float64)
	q_value: tensor([[-24.9342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 64
	action: tensor([[ 1.8788, -0.0868, -0.6799,  1.2565, -0.9378,  0.4434, -0.5929]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9722388081429447, distance: 0.19066709382194416 entropy 0.03264415264129639
epoch: 45, step: 65
	action: tensor([[ 2.7057,  0.1356, -1.1899,  2.1239, -1.7192,  1.4035, -0.9637]],
       dtype=torch.float64)
	q_value: tensor([[-29.1871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 66
	action: tensor([[ 1.6300, -0.5837, -0.4069,  1.2203, -0.5433,  0.7635, -0.7907]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.879681777814139, distance: 0.3969377433762043 entropy 0.03264415264129639
epoch: 45, step: 67
	action: tensor([[ 3.3656, -0.1153, -1.2405,  1.7462, -1.6288,  1.3229, -0.5924]],
       dtype=torch.float64)
	q_value: tensor([[-25.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 68
	action: tensor([[ 1.6747, -0.3303, -1.5312,  0.9256, -0.8513,  0.8340, -0.5661]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5027877659596469, distance: 0.8069146423491261 entropy 0.03264415264129639
epoch: 45, step: 69
	action: tensor([[ 2.6166, -0.3800, -0.9451,  2.0714, -2.0142,  1.1121, -0.8612]],
       dtype=torch.float64)
	q_value: tensor([[-33.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 70
	action: tensor([[ 1.6326,  0.4889, -0.7802,  1.3746, -0.8135,  0.8096, -0.8986]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9045942205730331, distance: 0.3534630618800676 entropy 0.03264415264129639
epoch: 45, step: 71
	action: tensor([[ 2.8618, -0.0671, -0.7568,  2.4245, -1.8831,  1.1350, -1.0510]],
       dtype=torch.float64)
	q_value: tensor([[-36.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 72
	action: tensor([[ 1.6581, -0.5755, -0.7885,  1.4894, -0.9838,  0.3506, -0.1411]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8971963347988706, distance: 0.3669112200450462 entropy 0.03264415264129639
epoch: 45, step: 73
	action: tensor([[ 2.8351, -0.0643, -0.4092,  2.3068, -1.1283,  1.1776, -0.7354]],
       dtype=torch.float64)
	q_value: tensor([[-25.4089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 74
	action: tensor([[ 1.6943,  0.0039, -0.5982,  1.3248, -0.9312,  0.6546, -0.5265]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9711896693768264, distance: 0.19423648684145595 entropy 0.03264415264129639
epoch: 45, step: 75
	action: tensor([[ 2.4395, -0.1153, -0.3513,  2.1095, -2.0268,  1.1162, -0.9887]],
       dtype=torch.float64)
	q_value: tensor([[-30.0044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 76
	action: tensor([[ 2.2223,  0.1735, -0.6608,  1.2939, -1.1903,  1.1725, -0.7977]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9336464279057147, distance: 0.2947737776536595 entropy 0.03264415264129639
epoch: 45, step: 77
	action: tensor([[ 2.7856,  0.0225, -0.8215,  2.1948, -1.6027,  1.4007, -0.4426]],
       dtype=torch.float64)
	q_value: tensor([[-36.3061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 78
	action: tensor([[ 2.1468, -0.0957, -0.4147,  1.1160, -1.0615,  0.6762, -0.5254]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9429251909741673, distance: 0.27338767196500185 entropy 0.03264415264129639
epoch: 45, step: 79
	action: tensor([[ 2.2814, -0.1718, -0.7593,  2.3292, -1.4403,  1.1726, -1.0158]],
       dtype=torch.float64)
	q_value: tensor([[-28.7245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 80
	action: tensor([[ 1.8722, -0.0399, -0.4539,  1.8422, -0.4642,  0.5453, -0.2957]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8138946451729148, distance: 0.4936693085252631 entropy 0.03264415264129639
epoch: 45, step: 81
	action: tensor([[ 2.9412,  0.1415, -0.9999,  2.1742, -1.3591,  0.6580, -0.6709]],
       dtype=torch.float64)
	q_value: tensor([[-27.1051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 82
	action: tensor([[ 1.7572,  0.3697, -0.6243,  1.5870, -0.5969,  0.1639, -0.0993]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8519292685405172, distance: 0.4403432101596367 entropy 0.03264415264129639
epoch: 45, step: 83
	action: tensor([[ 2.6699, -0.2748, -1.0925,  2.1579, -1.5832,  1.0507, -1.0159]],
       dtype=torch.float64)
	q_value: tensor([[-27.5960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 84
	action: tensor([[ 1.7746,  0.1845, -0.9490,  0.7611, -0.5531,  0.6592, -0.2608]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9486110213121787, distance: 0.25941297036419725 entropy 0.03264415264129639
epoch: 45, step: 85
	action: tensor([[ 2.8210,  0.1257, -0.5679,  2.1753, -1.6195,  0.6213, -0.8761]],
       dtype=torch.float64)
	q_value: tensor([[-28.8524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 86
	action: tensor([[ 1.9532, -0.0728, -0.3897,  1.9060, -1.2281,  0.5613, -0.6527]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.87555985917857, distance: 0.40367972769849486 entropy 0.03264415264129639
epoch: 45, step: 87
	action: tensor([[ 2.9436,  0.0055, -0.9714,  2.3291, -1.5049,  1.6399, -0.7939]],
       dtype=torch.float64)
	q_value: tensor([[-31.4273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 88
	action: tensor([[ 1.7793, -0.1918, -0.4296,  0.9943, -0.9694,  1.0442, -0.3612]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9333116364693506, distance: 0.2955164924952194 entropy 0.03264415264129639
epoch: 45, step: 89
	action: tensor([[ 2.4683, -0.2378, -1.0534,  2.0623, -1.0011,  0.9795, -0.9550]],
       dtype=torch.float64)
	q_value: tensor([[-28.5553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 90
	action: tensor([[ 1.6646,  0.0949, -0.7075,  0.9965, -1.3620,  0.6529, -1.1209]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9564163209041519, distance: 0.23890136841913848 entropy 0.03264415264129639
epoch: 45, step: 91
	action: tensor([[ 2.5788, -0.2219, -0.8219,  2.0628, -1.3465,  0.9484, -0.9013]],
       dtype=torch.float64)
	q_value: tensor([[-35.6541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 92
	action: tensor([[ 1.6057,  0.0742, -0.4664,  1.2537, -1.0135,  0.5440, -1.0600]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9736603235715467, distance: 0.18572138385486545 entropy 0.03264415264129639
epoch: 45, step: 93
	action: tensor([[ 3.0486, -0.4128, -0.6902,  2.0830, -1.7342,  1.2667, -0.8254]],
       dtype=torch.float64)
	q_value: tensor([[-32.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 94
	action: tensor([[ 1.5390, -0.0994, -0.7441,  1.5824, -1.0380,  0.1469, -0.6819]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.967987851075425, distance: 0.2047453784551468 entropy 0.03264415264129639
epoch: 45, step: 95
	action: tensor([[ 2.8844, -0.0475, -1.1096,  2.5961, -1.6368,  0.9441, -0.6027]],
       dtype=torch.float64)
	q_value: tensor([[-29.3326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 96
	action: tensor([[ 1.9849,  0.3415, -0.5120,  1.5486, -1.0032,  0.5631, -0.7398]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8924441683084409, distance: 0.37529577453131757 entropy 0.03264415264129639
epoch: 45, step: 97
	action: tensor([[ 2.5765, -0.0638, -1.0947,  2.1227, -2.0792,  1.0006, -0.8931]],
       dtype=torch.float64)
	q_value: tensor([[-33.0374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 98
	action: tensor([[ 2.1100,  0.3218, -0.3703,  1.3520, -0.8468,  0.4601, -0.5489]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8909556406961544, distance: 0.3778838190160134 entropy 0.03264415264129639
epoch: 45, step: 99
	action: tensor([[ 2.9625,  0.0284, -0.6047,  1.6799, -1.7031,  1.3845, -0.8340]],
       dtype=torch.float64)
	q_value: tensor([[-29.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 100
	action: tensor([[ 1.7950,  0.0559, -0.5384,  1.2534, -0.8947,  0.9293, -0.6972]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9576401863852217, distance: 0.23552321094057432 entropy 0.03264415264129639
epoch: 45, step: 101
	action: tensor([[ 2.8996,  0.3715, -1.0904,  2.3076, -1.7247,  0.9378, -0.7094]],
       dtype=torch.float64)
	q_value: tensor([[-31.9029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 102
	action: tensor([[ 1.6776, -0.0571, -0.9063,  1.3671, -0.9420,  0.8039, -0.7328]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9068951450457164, distance: 0.3491747711674669 entropy 0.03264415264129639
epoch: 45, step: 103
	action: tensor([[ 2.8220e+00,  5.9619e-04, -4.9223e-01,  1.8198e+00, -2.0129e+00,
          1.0951e+00, -7.6739e-01]], dtype=torch.float64)
	q_value: tensor([[-33.1830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 104
	action: tensor([[ 1.7253,  0.1883, -0.5409,  1.6326, -0.7855,  0.9098, -0.2354]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8472644342352367, distance: 0.44722573064094484 entropy 0.03264415264129639
epoch: 45, step: 105
	action: tensor([[ 2.9371,  0.4961, -0.7425,  2.5587, -1.2539,  0.9757, -1.2806]],
       dtype=torch.float64)
	q_value: tensor([[-30.4728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 106
	action: tensor([[ 1.5923,  0.0929, -0.1233,  1.2665, -0.9275,  0.1863, -0.3408]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8446203038719658, distance: 0.45108026526706485 entropy 0.03264415264129639
epoch: 45, step: 107
	action: tensor([[ 2.7407, -0.1457, -0.9034,  1.9632, -1.2510,  0.8005, -0.8208]],
       dtype=torch.float64)
	q_value: tensor([[-25.5185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 108
	action: tensor([[ 1.8312, -0.0521, -0.5833,  1.3967, -0.8779,  0.7210, -0.7293]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9616431124306953, distance: 0.22411884364997606 entropy 0.03264415264129639
epoch: 45, step: 109
	action: tensor([[ 3.1182, -0.5882, -0.7495,  2.3354, -1.7182,  0.9588, -1.0281]],
       dtype=torch.float64)
	q_value: tensor([[-30.8730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 110
	action: tensor([[ 1.7265,  0.1154, -0.6428,  1.1294, -1.0469,  0.8166, -0.2516]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9737721253857233, distance: 0.1853268067161008 entropy 0.03264415264129639
epoch: 45, step: 111
	action: tensor([[ 2.4430e+00, -2.3408e-04, -6.3402e-01,  1.7804e+00, -1.7613e+00,
          9.1856e-01, -8.4866e-01]], dtype=torch.float64)
	q_value: tensor([[-30.3763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 112
	action: tensor([[ 1.3924, -0.4222, -0.7713,  1.2087, -1.2047,  0.0832, -0.6825]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8978365959593926, distance: 0.3657668740949192 entropy 0.03264415264129639
epoch: 45, step: 113
	action: tensor([[ 2.2752, -0.1481, -0.6050,  1.5640, -1.5135,  1.2951, -0.8259]],
       dtype=torch.float64)
	q_value: tensor([[-26.2884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 114
	action: tensor([[ 0.9926,  0.0183, -0.7690,  0.9568, -0.8310,  0.6486, -0.7637]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8627628920444872, distance: 0.42392836095500874 entropy 0.03264415264129639
epoch: 45, step: 115
	action: tensor([[ 2.6286,  0.1199, -1.3057,  1.8084, -1.5312,  0.4302, -0.6880]],
       dtype=torch.float64)
	q_value: tensor([[-25.4201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 116
	action: tensor([[ 1.9458, -0.0704,  0.0073,  1.4605, -1.2086,  0.5358, -0.9117]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8504868627571979, distance: 0.44248277638269096 entropy 0.03264415264129639
epoch: 45, step: 117
	action: tensor([[ 2.5714, -0.2088, -0.7822,  1.7566, -1.3565,  1.0443, -0.6636]],
       dtype=torch.float64)
	q_value: tensor([[-29.8784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 118
	action: tensor([[ 1.9070,  0.4664, -0.6633,  1.1498, -0.8639,  0.6663, -0.4498]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9566628451589255, distance: 0.23822475618755345 entropy 0.03264415264129639
epoch: 45, step: 119
	action: tensor([[ 2.3456,  0.1969, -0.5789,  1.9954, -1.2302,  1.0174, -1.1711]],
       dtype=torch.float64)
	q_value: tensor([[-32.1245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 120
	action: tensor([[ 2.0934,  0.0710, -0.7771,  1.4343, -1.3889,  0.4814, -0.4760]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9738347342432407, distance: 0.18510547668924143 entropy 0.03264415264129639
epoch: 45, step: 121
	action: tensor([[ 2.5436,  0.0285, -0.5077,  1.7021, -0.9531,  0.5829, -0.8908]],
       dtype=torch.float64)
	q_value: tensor([[-32.5579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 122
	action: tensor([[ 1.7124,  0.0674, -0.4105,  1.1411, -0.9911,  0.8896, -0.5804]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9635699921219498, distance: 0.21841694375247156 entropy 0.03264415264129639
epoch: 45, step: 123
	action: tensor([[ 2.7575,  0.1697, -0.8235,  2.3078, -1.4791,  0.8567, -1.1212]],
       dtype=torch.float64)
	q_value: tensor([[-30.7120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 124
	action: tensor([[ 1.4292, -0.2505, -0.4777,  1.2561, -0.9262,  0.2918, -0.7212]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9790904504183677, distance: 0.16547372066859983 entropy 0.03264415264129639
epoch: 45, step: 125
	action: tensor([[ 2.6345,  0.0623, -0.8775,  1.9506, -1.7011,  1.6000, -0.8804]],
       dtype=torch.float64)
	q_value: tensor([[-25.8503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 45, step: 126
	action: tensor([[ 1.4794, -0.0491, -0.5219,  1.1475, -0.9348,  0.7508, -0.4242]],
       dtype=torch.float64)
	q_value: tensor([[-21.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9803947377113713, distance: 0.1602297007943401 entropy 0.03264415264129639
epoch: 45, step: 127
	action: tensor([[ 2.4281,  0.1172, -0.7159,  2.1841, -1.7280,  0.8831, -0.3570]],
       dtype=torch.float64)
	q_value: tensor([[-27.7109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 45 actor 224.8460525884034 critic 551.964440972548 
epoch: 46, step: 0
	action: tensor([[ 1.6550,  0.1355, -0.3214,  1.4759, -0.6110,  0.8931, -0.2122]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8513290302868692, distance: 0.44123482296335964 entropy 0.03264415264129639
epoch: 46, step: 1
	action: tensor([[ 3.1613,  0.3054, -0.5855,  2.6467, -1.8615,  0.8092, -1.2481]],
       dtype=torch.float64)
	q_value: tensor([[-30.0207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 2
	action: tensor([[ 2.4585, -0.2147, -0.4810,  1.0081, -1.3797,  0.3965, -0.4301]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 3
	action: tensor([[ 2.1543, -0.0532, -0.6001,  0.9947, -1.4243,  0.4742, -0.6051]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9024273522186946, distance: 0.35745447505060346 entropy 0.03264415264129639
epoch: 46, step: 4
	action: tensor([[ 2.7628, -0.0976, -0.7410,  1.7925, -1.9779,  0.9583, -0.8839]],
       dtype=torch.float64)
	q_value: tensor([[-33.4968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 5
	action: tensor([[ 1.8818, -0.3164, -0.5561,  1.5519, -0.8309,  0.3912, -0.8306]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9525152538438482, distance: 0.249363998770583 entropy 0.03264415264129639
epoch: 46, step: 6
	action: tensor([[ 3.4639, -0.0646, -1.1523,  2.6549, -1.9423,  1.2121, -1.0141]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 7
	action: tensor([[ 1.9295,  0.2606, -0.8466,  1.1606, -1.1414,  0.3270, -0.4655]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9901922896075931, distance: 0.1133288573331672 entropy 0.03264415264129639
epoch: 46, step: 8
	action: tensor([[ 2.8222,  0.0540, -0.7415,  1.9122, -1.5238,  0.4346, -0.8069]],
       dtype=torch.float64)
	q_value: tensor([[-34.2621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 9
	action: tensor([[ 1.6844, -0.2169, -0.6141,  1.2019, -1.5366,  0.5842, -0.4002]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9100266053035674, distance: 0.3432525297389367 entropy 0.03264415264129639
epoch: 46, step: 10
	action: tensor([[ 2.9279, -0.0313, -0.3056,  2.1366, -1.6910,  0.7864, -1.2037]],
       dtype=torch.float64)
	q_value: tensor([[-32.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 11
	action: tensor([[ 1.9588, -0.2469, -0.3236,  1.5062, -1.0292,  0.3741, -0.6821]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9372267582332944, distance: 0.2867107467777206 entropy 0.03264415264129639
epoch: 46, step: 12
	action: tensor([[ 3.3353,  0.3099, -1.1285,  2.1257, -1.6993,  0.6421, -1.0596]],
       dtype=torch.float64)
	q_value: tensor([[-30.4385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 13
	action: tensor([[ 2.1551,  0.0198, -0.9096,  0.9864, -1.0858,  0.6405, -0.4781]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9141587562221877, distance: 0.3352777196834708 entropy 0.03264415264129639
epoch: 46, step: 14
	action: tensor([[ 3.3552, -0.0965, -1.1611,  2.1433, -1.5020,  0.5446, -0.8024]],
       dtype=torch.float64)
	q_value: tensor([[-33.9754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 15
	action: tensor([[ 1.8581,  0.0808, -0.2120,  1.7077, -0.8548,  0.5678, -0.5676]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8320505527425912, distance: 0.46897099443484486 entropy 0.03264415264129639
epoch: 46, step: 16
	action: tensor([[ 3.3731,  0.2800, -1.3267,  2.6091, -1.5520,  0.9845, -1.1343]],
       dtype=torch.float64)
	q_value: tensor([[-31.3621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 17
	action: tensor([[ 2.3103,  0.1712, -0.7559,  1.4379, -1.0916,  0.4591, -0.0424]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9681668342118147, distance: 0.20417220018139612 entropy 0.03264415264129639
epoch: 46, step: 18
	action: tensor([[ 2.7466,  0.1096, -0.8173,  2.5163, -2.1600,  0.6511, -0.5038]],
       dtype=torch.float64)
	q_value: tensor([[-31.6639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 19
	action: tensor([[ 2.3299, -0.5218, -0.6580,  1.4035, -1.2528,  0.4837, -0.4404]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8827741134375007, distance: 0.3918036307997178 entropy 0.03264415264129639
epoch: 46, step: 20
	action: tensor([[ 3.0242,  0.0982, -0.7095,  2.2278, -1.3830,  0.7606, -1.2018]],
       dtype=torch.float64)
	q_value: tensor([[-30.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 21
	action: tensor([[ 1.5271, -0.1779, -0.7208,  1.4569, -0.7680,  0.3937, -0.5634]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.966924553989108, distance: 0.2081179540780013 entropy 0.03264415264129639
epoch: 46, step: 22
	action: tensor([[ 3.2388,  0.0274, -0.6869,  2.3357, -2.0452,  0.9068, -1.1152]],
       dtype=torch.float64)
	q_value: tensor([[-29.8680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 23
	action: tensor([[ 2.0061, -0.1873, -0.6194,  1.7645, -0.6090, -0.0368, -0.8994]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9425971314019601, distance: 0.2741722469375233 entropy 0.03264415264129639
epoch: 46, step: 24
	action: tensor([[ 3.8487,  0.2898, -1.1481,  2.7223, -2.5624,  1.1482, -1.1282]],
       dtype=torch.float64)
	q_value: tensor([[-31.4773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 25
	action: tensor([[ 1.8566,  0.1152, -1.0260,  0.9818, -0.6277,  0.3606, -0.4580]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9522275332142668, distance: 0.2501183337680188 entropy 0.03264415264129639
epoch: 46, step: 26
	action: tensor([[ 3.2562,  0.2459, -1.1040,  2.6101, -1.8422,  0.9358, -0.7813]],
       dtype=torch.float64)
	q_value: tensor([[-31.6335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 27
	action: tensor([[ 1.4708,  0.4088, -0.4279,  1.5190, -0.8650,  0.8290, -0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7929678509232585, distance: 0.5206856256076531 entropy 0.03264415264129639
epoch: 46, step: 28
	action: tensor([[ 3.6095, -0.0876, -0.9347,  2.6475, -1.9518,  0.3812, -0.5629]],
       dtype=torch.float64)
	q_value: tensor([[-31.6941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 29
	action: tensor([[ 1.9202, -0.0712, -0.2312,  1.4955, -0.8385,  0.5162, -0.2210]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8984752972524025, distance: 0.3646217377762606 entropy 0.03264415264129639
epoch: 46, step: 30
	action: tensor([[ 3.3028, -0.0881, -1.2859,  2.2291, -2.1918,  1.1736, -1.0628]],
       dtype=torch.float64)
	q_value: tensor([[-28.0735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 31
	action: tensor([[ 1.6194,  0.3669, -0.3591,  1.3797, -0.6047,  0.7039, -0.5888]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8414110232594652, distance: 0.45571486147086737 entropy 0.03264415264129639
epoch: 46, step: 32
	action: tensor([[ 3.6532,  0.6592, -0.8888,  2.1865, -2.4818,  0.4395, -1.1029]],
       dtype=torch.float64)
	q_value: tensor([[-32.6641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 33
	action: tensor([[ 1.8863,  0.5419, -0.3397,  1.2535, -1.3118,  0.3448, -0.6633]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8460105199693194, distance: 0.449057774329955 entropy 0.03264415264129639
epoch: 46, step: 34
	action: tensor([[ 3.1849, -0.0184, -0.8199,  1.9717, -1.5152,  0.6511, -0.4248]],
       dtype=torch.float64)
	q_value: tensor([[-35.5874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 35
	action: tensor([[ 1.9201,  0.2415, -0.8051,  1.1556, -1.2952,  0.5673, -0.6536]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9833278419598332, distance: 0.14775854384270862 entropy 0.03264415264129639
epoch: 46, step: 36
	action: tensor([[ 3.5849, -0.2156, -1.1651,  2.3398, -1.8311,  0.9523, -0.9599]],
       dtype=torch.float64)
	q_value: tensor([[-36.8675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 37
	action: tensor([[ 1.6772,  0.1235, -0.9265,  1.3143, -1.0832,  0.8715, -0.5840]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.918043017114715, distance: 0.327604360993628 entropy 0.03264415264129639
epoch: 46, step: 38
	action: tensor([[ 2.9086,  0.3842, -1.0074,  1.9697, -1.7658,  1.2912, -0.9412]],
       dtype=torch.float64)
	q_value: tensor([[-37.1765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 39
	action: tensor([[ 2.2063,  0.2224, -0.8724,  1.5594, -1.0288,  0.7382, -0.5554]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9072181561802606, distance: 0.3485685442788199 entropy 0.03264415264129639
epoch: 46, step: 40
	action: tensor([[ 3.6225,  0.0656, -1.2195,  2.2358, -2.4802,  1.0982, -0.9183]],
       dtype=torch.float64)
	q_value: tensor([[-36.9345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 41
	action: tensor([[ 2.2100,  0.1531, -0.8194,  0.9942, -1.1390,  0.3086, -0.4822]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9657642576656058, distance: 0.21173690851481755 entropy 0.03264415264129639
epoch: 46, step: 42
	action: tensor([[ 2.7224,  0.4026, -0.9165,  2.1644, -1.8228,  0.7999, -0.6617]],
       dtype=torch.float64)
	q_value: tensor([[-33.0902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 43
	action: tensor([[ 2.1658,  0.0486, -0.7744,  1.5577, -1.3724,  0.1259, -0.7227]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.974515902905913, distance: 0.18268013286635804 entropy 0.03264415264129639
epoch: 46, step: 44
	action: tensor([[ 3.1830, -0.1729, -1.2771,  2.7018, -2.4472,  1.0292, -0.7161]],
       dtype=torch.float64)
	q_value: tensor([[-35.3654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 45
	action: tensor([[ 2.1830,  0.0572, -0.7969,  1.4723, -1.2325,  0.2205, -0.4038]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9810155251009923, distance: 0.15767251241984648 entropy 0.03264415264129639
epoch: 46, step: 46
	action: tensor([[ 3.0762,  0.4554, -0.9223,  2.1529, -1.9964,  1.0463, -0.9840]],
       dtype=torch.float64)
	q_value: tensor([[-33.0297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 47
	action: tensor([[ 1.8876, -0.2458, -0.1900,  1.2683, -1.0516,  0.1628, -0.5354]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8226120432584685, distance: 0.48196861532648666 entropy 0.03264415264129639
epoch: 46, step: 48
	action: tensor([[ 2.8900,  0.1082, -0.6824,  2.1833, -2.0719,  0.9130, -1.0127]],
       dtype=torch.float64)
	q_value: tensor([[-27.9125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 49
	action: tensor([[ 1.6934,  0.5445, -0.5128,  1.4064, -1.5398,  0.1277, -0.2907]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8497945730270039, distance: 0.44350600588256034 entropy 0.03264415264129639
epoch: 46, step: 50
	action: tensor([[ 2.5004, -0.2945, -0.7251,  1.8775, -1.5036,  1.0089, -0.4247]],
       dtype=torch.float64)
	q_value: tensor([[-34.7291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 51
	action: tensor([[ 1.4940,  0.0765, -0.5138,  1.4853, -1.1536,  0.6940, -0.4014]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9407978562548599, distance: 0.2784360152129391 entropy 0.03264415264129639
epoch: 46, step: 52
	action: tensor([[ 3.0492, -0.2535, -0.8569,  2.3620, -1.8055,  0.7030, -1.2016]],
       dtype=torch.float64)
	q_value: tensor([[-32.2041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 53
	action: tensor([[ 2.0628,  0.2129, -0.3761,  1.7008, -1.2741,  0.4369, -0.4080]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8695846978837394, distance: 0.4132577141564707 entropy 0.03264415264129639
epoch: 46, step: 54
	action: tensor([[ 2.5301,  0.3023, -0.6569,  2.4174, -2.3287,  0.5974, -0.6004]],
       dtype=torch.float64)
	q_value: tensor([[-33.4036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 55
	action: tensor([[ 1.9567, -0.1620, -0.3885,  0.8432, -1.6700,  0.3673, -0.6729]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7623239307399894, distance: 0.5578910849362642 entropy 0.03264415264129639
epoch: 46, step: 56
	action: tensor([[ 2.5637,  0.4633, -1.0052,  2.0109, -2.0201,  1.3635, -0.8046]],
       dtype=torch.float64)
	q_value: tensor([[-32.8662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 57
	action: tensor([[ 1.8068,  0.4254, -0.3362,  2.1490, -1.0446,  0.5430, -0.8112]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.609875061701995, distance: 0.7147572179556685 entropy 0.03264415264129639
epoch: 46, step: 58
	action: tensor([[ 3.6247,  0.3883, -0.8822,  2.8169, -1.9441,  0.9423, -1.1890]],
       dtype=torch.float64)
	q_value: tensor([[-37.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 59
	action: tensor([[ 1.7971, -0.1236, -0.3887,  1.3159, -1.0857,  0.7093, -0.4788]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9546930366141628, distance: 0.2435786229857017 entropy 0.03264415264129639
epoch: 46, step: 60
	action: tensor([[ 3.5950, -0.2318, -0.7920,  2.3032, -2.1078,  0.5088, -0.5713]],
       dtype=torch.float64)
	q_value: tensor([[-31.2022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 61
	action: tensor([[ 2.2588,  0.1852, -0.5284,  1.5994, -0.9854,  0.5773, -0.3032]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.912580002762212, distance: 0.3383468109818404 entropy 0.03264415264129639
epoch: 46, step: 62
	action: tensor([[ 3.2481, -0.1143, -0.6870,  2.4813, -1.7151,  1.0050, -0.9664]],
       dtype=torch.float64)
	q_value: tensor([[-32.4544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 63
	action: tensor([[ 2.0475, -0.0995, -0.3190,  1.3382, -0.9029,  0.1044, -0.6829]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8933702251303295, distance: 0.3736766315397417 entropy 0.03264415264129639
epoch: 46, step: 64
	action: tensor([[ 3.2614,  0.0357, -0.9394,  2.3646, -1.9078,  0.9848, -0.7500]],
       dtype=torch.float64)
	q_value: tensor([[-29.4277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 65
	action: tensor([[ 1.9009, -0.0588, -0.5337,  1.1917, -1.4538,  0.1953, -0.4094]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9346829162647021, distance: 0.2924624315139695 entropy 0.03264415264129639
epoch: 46, step: 66
	action: tensor([[ 2.6334, -0.1566, -1.0152,  2.1220, -1.6385,  1.1999, -0.3771]],
       dtype=torch.float64)
	q_value: tensor([[-31.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 67
	action: tensor([[ 1.4996,  0.0055, -0.5143,  1.2767, -0.8620,  0.4711, -0.5079]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9790042569091633, distance: 0.16581442844691166 entropy 0.03264415264129639
epoch: 46, step: 68
	action: tensor([[ 3.2623,  0.7739, -0.4935,  2.6665, -1.9789,  1.1611, -0.7244]],
       dtype=torch.float64)
	q_value: tensor([[-29.5624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 69
	action: tensor([[ 1.5342, -0.2879, -0.4720,  0.6297, -1.2641,  0.0437, -0.6884]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.658483266804144, distance: 0.6687482938599553 entropy 0.03264415264129639
epoch: 46, step: 70
	action: tensor([[ 2.6694,  0.1454, -0.5211,  1.7439, -1.8645,  0.8684, -0.9283]],
       dtype=torch.float64)
	q_value: tensor([[-28.3590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 71
	action: tensor([[ 1.8058, -0.0826, -0.4617,  1.4798, -1.1878,  0.5668, -0.9835]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.962855207671934, distance: 0.2205492882488653 entropy 0.03264415264129639
epoch: 46, step: 72
	action: tensor([[ 3.5696,  0.1249, -1.0662,  2.5101, -2.2332,  0.8291, -0.5122]],
       dtype=torch.float64)
	q_value: tensor([[-35.2650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 73
	action: tensor([[ 2.3275,  0.6015, -0.6403,  1.2773, -1.4484,  0.5420, -0.4091]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9408196209822104, distance: 0.2783848292199322 entropy 0.03264415264129639
epoch: 46, step: 74
	action: tensor([[ 2.8985, -0.3711, -0.5925,  2.3581, -1.5296,  0.6577, -0.4839]],
       dtype=torch.float64)
	q_value: tensor([[-37.7264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 75
	action: tensor([[ 1.5910,  0.1168, -0.4935,  1.1842, -1.2510,  0.5792, -0.3407]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.980940518954686, distance: 0.15798368113319636 entropy 0.03264415264129639
epoch: 46, step: 76
	action: tensor([[ 2.7318, -0.0458, -0.8070,  2.0717, -1.5267,  0.4100, -1.0324]],
       dtype=torch.float64)
	q_value: tensor([[-32.2397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 77
	action: tensor([[ 1.7927,  0.1171, -0.4546,  1.2692, -0.9433,  0.2708, -0.4220]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9529917005841204, distance: 0.24810982558241748 entropy 0.03264415264129639
epoch: 46, step: 78
	action: tensor([[ 3.4714,  0.2610, -0.7918,  2.1281, -1.8017,  0.7249, -0.6098]],
       dtype=torch.float64)
	q_value: tensor([[-30.0363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 79
	action: tensor([[ 2.0029, -0.1413, -0.3588,  1.1759, -1.1789,  0.9413, -0.3935]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9400049368533143, distance: 0.280294419089993 entropy 0.03264415264129639
epoch: 46, step: 80
	action: tensor([[ 3.3339, -0.0119, -0.6583,  2.3858, -1.8766,  0.8645, -0.2413]],
       dtype=torch.float64)
	q_value: tensor([[-31.6447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 81
	action: tensor([[ 2.3433,  0.0548, -0.1133,  1.1171, -1.0180,  0.7269, -0.7909]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9084735481384983, distance: 0.34620234610497486 entropy 0.03264415264129639
epoch: 46, step: 82
	action: tensor([[ 3.2276, -0.2010, -0.6772,  2.2167, -2.6663,  0.7114, -0.8890]],
       dtype=torch.float64)
	q_value: tensor([[-32.0083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 83
	action: tensor([[ 1.8634,  0.1378, -0.5452,  1.7014, -0.8732,  0.5016, -0.5803]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8833710422197151, distance: 0.39080480124678874 entropy 0.03264415264129639
epoch: 46, step: 84
	action: tensor([[ 3.9386,  0.0625, -1.1366,  2.8992, -2.4405,  0.8795, -0.7284]],
       dtype=torch.float64)
	q_value: tensor([[-33.3004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 85
	action: tensor([[ 1.6818, -0.1111, -0.2823,  1.5201, -0.5575,  0.4866, -0.4391]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9010907295990757, distance: 0.3598944856868965 entropy 0.03264415264129639
epoch: 46, step: 86
	action: tensor([[ 3.7697,  0.2009, -0.8711,  2.9333, -2.0288,  1.0108, -1.5308]],
       dtype=torch.float64)
	q_value: tensor([[-28.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 87
	action: tensor([[ 2.3852,  0.4313, -0.1622,  1.3094, -1.2463,  0.5095, -0.8015]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8260523230590601, distance: 0.4772720586622317 entropy 0.03264415264129639
epoch: 46, step: 88
	action: tensor([[ 3.1190,  0.4580, -0.6443,  2.2002, -2.3806,  0.8138, -1.2338]],
       dtype=torch.float64)
	q_value: tensor([[-35.2609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 89
	action: tensor([[ 2.1408,  0.5246, -1.0707,  1.3783, -1.1170,  0.7807, -0.3396]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9330307217338073, distance: 0.2961382478854789 entropy 0.03264415264129639
epoch: 46, step: 90
	action: tensor([[ 3.3602,  0.6410, -0.9606,  2.5106, -1.9093,  0.6508, -1.0522]],
       dtype=torch.float64)
	q_value: tensor([[-38.9859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 91
	action: tensor([[ 1.9730,  0.2464, -0.2670,  1.3833, -1.2186,  1.0659, -0.8231]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9297098294867177, distance: 0.3033919069477234 entropy 0.03264415264129639
epoch: 46, step: 92
	action: tensor([[ 3.7416,  0.0969, -1.3231,  2.5851, -2.0003,  0.6757, -1.1596]],
       dtype=torch.float64)
	q_value: tensor([[-37.4250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 93
	action: tensor([[ 1.6414,  0.1286, -0.3431,  1.0495, -1.1536,  0.0693, -0.4989]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8866600247049404, distance: 0.38525497002447207 entropy 0.03264415264129639
epoch: 46, step: 94
	action: tensor([[ 3.0826, -0.0235, -0.8604,  2.1283, -1.4671,  0.7750, -1.0481]],
       dtype=torch.float64)
	q_value: tensor([[-29.7528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 95
	action: tensor([[ 1.8742,  0.4328, -0.7935,  1.2283, -1.1311,  0.1238, -0.3067]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9706666554641693, distance: 0.19599161199409995 entropy 0.03264415264129639
epoch: 46, step: 96
	action: tensor([[ 3.1950,  0.2098, -0.8432,  2.0545, -1.6141,  0.8116, -0.9492]],
       dtype=torch.float64)
	q_value: tensor([[-33.3330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 97
	action: tensor([[ 1.6240,  0.0936, -0.5125,  1.5603, -1.3165,  0.5346, -0.3859]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9517485957626841, distance: 0.25136897355257887 entropy 0.03264415264129639
epoch: 46, step: 98
	action: tensor([[ 3.1658, -0.1703, -1.1505,  2.1068, -2.1881,  0.6564, -0.8370]],
       dtype=torch.float64)
	q_value: tensor([[-33.5564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 99
	action: tensor([[ 2.2618, -0.2493, -0.7431,  1.6436, -1.3018,  0.3676, -0.5256]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9509268940676323, distance: 0.2535002934273706 entropy 0.03264415264129639
epoch: 46, step: 100
	action: tensor([[ 3.5120, -0.2981, -0.8911,  2.1469, -2.4568,  0.8282, -0.6527]],
       dtype=torch.float64)
	q_value: tensor([[-33.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 101
	action: tensor([[ 1.6967,  0.2741, -0.7327,  1.4890, -1.2225,  0.4775, -0.4995]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9543693709556745, distance: 0.24444711789970439 entropy 0.03264415264129639
epoch: 46, step: 102
	action: tensor([[ 3.3961,  0.2135, -0.7758,  2.1997, -1.5015,  1.0679, -0.7611]],
       dtype=torch.float64)
	q_value: tensor([[-35.7254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 103
	action: tensor([[ 1.5760,  0.1095, -0.5999,  1.1070, -0.8184,  0.6288, -0.8701]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.983656912906269, distance: 0.14629306563059147 entropy 0.03264415264129639
epoch: 46, step: 104
	action: tensor([[ 3.2672, -0.2340, -1.2786,  1.9302, -1.9666,  1.0817, -1.0297]],
       dtype=torch.float64)
	q_value: tensor([[-33.7879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 105
	action: tensor([[ 1.9091,  0.1008, -0.5377,  1.7104, -1.1879,  0.7439, -0.4028]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9036157829181917, distance: 0.3552709156147422 entropy 0.03264415264129639
epoch: 46, step: 106
	action: tensor([[ 3.3404, -0.3401, -1.0586,  2.1069, -1.9478,  0.8050, -0.8889]],
       dtype=torch.float64)
	q_value: tensor([[-34.5144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 107
	action: tensor([[ 1.6827, -0.0849, -0.7321,  1.4115, -1.1852,  0.5365, -0.2579]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9642990047580113, distance: 0.2162204938052942 entropy 0.03264415264129639
epoch: 46, step: 108
	action: tensor([[ 2.8958, -0.0680, -1.3217,  2.7590, -1.5216,  1.0515, -0.8681]],
       dtype=torch.float64)
	q_value: tensor([[-31.9402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 109
	action: tensor([[ 2.0912,  0.1588, -0.6597,  1.3206, -0.8676,  0.4472, -0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9733310979262096, distance: 0.1868784664689228 entropy 0.03264415264129639
epoch: 46, step: 110
	action: tensor([[ 2.6170,  0.3026, -1.0380,  2.1841, -2.4364,  0.3494, -1.0882]],
       dtype=torch.float64)
	q_value: tensor([[-29.7441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 111
	action: tensor([[ 2.1502,  0.5132, -0.4818,  1.7379, -1.0307,  0.6128, -0.7322]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7846694752249647, distance: 0.5310183056708477 entropy 0.03264415264129639
epoch: 46, step: 112
	action: tensor([[ 3.3598,  0.5143, -1.1052,  2.6757, -2.1944,  1.0904, -0.9659]],
       dtype=torch.float64)
	q_value: tensor([[-37.5089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 113
	action: tensor([[ 1.7075, -0.1948, -0.5061,  1.4039, -1.2502,  0.7810, -0.3199]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9564235585769807, distance: 0.238881531157007 entropy 0.03264415264129639
epoch: 46, step: 114
	action: tensor([[ 3.3785,  0.0222, -0.5874,  2.6892, -1.6658,  0.3287, -0.9596]],
       dtype=torch.float64)
	q_value: tensor([[-31.7801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 115
	action: tensor([[ 1.7989,  0.0140, -0.6862,  1.4880, -0.6315,  0.1589, -0.7992]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9682458875236052, distance: 0.20391852572999317 entropy 0.03264415264129639
epoch: 46, step: 116
	action: tensor([[ 3.6991,  0.0102, -1.7616,  2.8413, -2.2598,  1.0241, -0.9691]],
       dtype=torch.float64)
	q_value: tensor([[-31.7446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 117
	action: tensor([[ 1.9355,  0.0154, -0.7540,  1.4810, -1.5339,  0.6610, -0.8529]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9591399533409979, distance: 0.23131623896997613 entropy 0.03264415264129639
epoch: 46, step: 118
	action: tensor([[ 3.4251, -0.1608, -1.1566,  2.7938, -2.4769,  0.7330, -0.5946]],
       dtype=torch.float64)
	q_value: tensor([[-38.7493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 119
	action: tensor([[ 2.1127,  0.2984, -1.1091,  1.4625, -0.9178,  0.3284, -0.1150]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9470048023202, distance: 0.2634358957045685 entropy 0.03264415264129639
epoch: 46, step: 120
	action: tensor([[ 2.7626,  0.3759, -0.9934,  2.2672, -1.7246,  0.5596, -0.7342]],
       dtype=torch.float64)
	q_value: tensor([[-33.5615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 121
	action: tensor([[ 1.8450, -0.0386, -0.6968,  0.9234, -0.8287,  0.6153, -0.5480]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9298591670193195, distance: 0.30306944443878286 entropy 0.03264415264129639
epoch: 46, step: 122
	action: tensor([[ 2.8504,  0.1360, -0.5310,  2.2270, -1.7102,  0.8983, -0.4822]],
       dtype=torch.float64)
	q_value: tensor([[-31.3322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 123
	action: tensor([[ 1.9503,  0.1838, -0.7908,  1.8147, -0.8670,  0.5959, -0.6081]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8342572953904862, distance: 0.4658798253475316 entropy 0.03264415264129639
epoch: 46, step: 124
	action: tensor([[ 3.9876,  0.0138, -1.3781,  2.5713, -2.5813,  0.8535, -1.1718]],
       dtype=torch.float64)
	q_value: tensor([[-35.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 125
	action: tensor([[ 1.8184,  0.1112, -0.5208,  1.3088, -1.1181,  0.2512, -0.4650]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9666359051880596, distance: 0.20902410214194914 entropy 0.03264415264129639
epoch: 46, step: 126
	action: tensor([[ 3.4005,  0.2629, -1.0283,  2.0159, -2.1811,  0.5985, -0.8791]],
       dtype=torch.float64)
	q_value: tensor([[-31.3950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 46, step: 127
	action: tensor([[ 1.8273,  0.5481, -0.8938,  1.2556, -0.6568,  0.3730, -0.5732]],
       dtype=torch.float64)
	q_value: tensor([[-23.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9421060839272783, distance: 0.2753424401211716 entropy 0.03264415264129639
LOSS epoch 46 actor 225.89574501984634 critic 438.01130794510334 
epoch: 47, step: 0
	action: tensor([[ 4.6740, -0.2189, -1.5474,  3.0731, -2.8691,  0.6985, -1.1532]],
       dtype=torch.float64)
	q_value: tensor([[-43.1212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 1
	action: tensor([[ 1.8810, -0.3504, -0.3834,  1.1053, -1.2437,  0.4165, -0.7762]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8357926738371315, distance: 0.4637169363862676 entropy 0.03264415264129639
epoch: 47, step: 2
	action: tensor([[ 4.4518, -0.2059, -0.8852,  2.8741, -3.1293,  0.8062, -0.8694]],
       dtype=torch.float64)
	q_value: tensor([[-38.4477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 3
	action: tensor([[ 1.7710,  0.1295, -0.4118,  1.8549, -1.4967,  0.6851, -1.0369]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8899099424849002, distance: 0.3796913843370031 entropy 0.03264415264129639
epoch: 47, step: 4
	action: tensor([[ 5.5010,  0.3000, -1.6096,  3.3242, -3.3469,  1.1911, -1.1121]],
       dtype=torch.float64)
	q_value: tensor([[-48.9455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 5
	action: tensor([[ 2.4068,  0.1431, -0.7864,  1.1466, -1.4764,  0.0834, -0.4685]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 6
	action: tensor([[ 1.9988,  0.3315, -0.6776,  1.0476, -1.1857,  0.3944, -0.8027]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9835541278979752, distance: 0.1467523780499921 entropy 0.03264415264129639
epoch: 47, step: 7
	action: tensor([[ 4.6424,  0.3633, -1.4779,  2.9877, -2.4950,  1.1473, -1.0524]],
       dtype=torch.float64)
	q_value: tensor([[-44.6232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 8
	action: tensor([[ 1.7366,  0.2530, -0.9203,  1.4516, -1.2880,  0.3338, -0.4604]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9721677529824678, distance: 0.19091094543857343 entropy 0.03264415264129639
epoch: 47, step: 9
	action: tensor([[ 4.2303, -0.0538, -1.0861,  3.1872, -2.7622,  0.7573, -1.6128]],
       dtype=torch.float64)
	q_value: tensor([[-44.4644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 10
	action: tensor([[ 1.9264,  0.3860, -0.3582,  1.5802, -1.1105,  0.6287, -0.4467]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8317877934143898, distance: 0.469337706995414 entropy 0.03264415264129639
epoch: 47, step: 11
	action: tensor([[ 4.8329,  0.4643, -1.5885,  3.1476, -2.9985,  0.5947, -1.5699]],
       dtype=torch.float64)
	q_value: tensor([[-42.3959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 12
	action: tensor([[ 2.0763,  0.2698, -1.0049,  1.6403, -1.3756,  0.7359, -0.8891]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.891419385174702, distance: 0.37707942963716784 entropy 0.03264415264129639
epoch: 47, step: 13
	action: tensor([[ 5.1610,  0.0195, -1.5252,  3.6694, -3.2137,  0.9259, -1.3210]],
       dtype=torch.float64)
	q_value: tensor([[-51.6919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 14
	action: tensor([[ 1.9207, -0.1139, -0.4206,  0.9763, -1.8075,  0.0841, -0.2152]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.815666175922453, distance: 0.4913140793332299 entropy 0.03264415264129639
epoch: 47, step: 15
	action: tensor([[ 3.8558,  0.3716, -0.9589,  2.4070, -2.0390,  0.3260, -0.9711]],
       dtype=torch.float64)
	q_value: tensor([[-37.8047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 16
	action: tensor([[ 2.0528,  0.3010, -0.5830,  1.4176, -1.0265,  0.5689, -0.8985]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9457961482144188, distance: 0.2664230328423323 entropy 0.03264415264129639
epoch: 47, step: 17
	action: tensor([[ 4.6526,  0.0632, -1.2590,  3.3640, -2.8409,  0.7237, -1.3629]],
       dtype=torch.float64)
	q_value: tensor([[-45.4697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 18
	action: tensor([[ 2.1501,  0.4867, -0.9940,  1.4950, -1.2588, -0.1550, -0.8103]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.947332105998196, distance: 0.2626211324299319 entropy 0.03264415264129639
epoch: 47, step: 19
	action: tensor([[ 4.7520,  0.1548, -1.3280,  3.1991, -2.9689,  0.5026, -1.4873]],
       dtype=torch.float64)
	q_value: tensor([[-47.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 20
	action: tensor([[ 2.0713, -0.2701, -0.4562,  1.5063, -1.0738, -0.0633, -0.4934]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9103480724701202, distance: 0.34263877520760794 entropy 0.03264415264129639
epoch: 47, step: 21
	action: tensor([[ 4.0681,  0.3981, -1.4563,  3.1595, -3.0489,  0.9652, -1.1691]],
       dtype=torch.float64)
	q_value: tensor([[-36.0492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 22
	action: tensor([[ 2.2933, -0.1938, -0.6161,  1.5010, -1.2063,  0.7139, -0.5907]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9478630728365584, distance: 0.2612939829042753 entropy 0.03264415264129639
epoch: 47, step: 23
	action: tensor([[ 4.5044,  0.1662, -1.7290,  2.9791, -2.8557,  1.4439, -1.5800]],
       dtype=torch.float64)
	q_value: tensor([[-41.7484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 24
	action: tensor([[ 1.7659,  0.2033, -0.6804,  1.7260, -1.4089,  0.6636, -0.7458]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.916937900176553, distance: 0.329805691647421 entropy 0.03264415264129639
epoch: 47, step: 25
	action: tensor([[ 5.3624,  0.0455, -1.4777,  3.5592, -3.2230,  1.2035, -1.3326]],
       dtype=torch.float64)
	q_value: tensor([[-47.9674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 26
	action: tensor([[ 2.0104,  0.1772, -0.5005,  1.7117, -1.1875,  1.3010, -0.2502]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8467452453603206, distance: 0.44798520547113224 entropy 0.03264415264129639
epoch: 47, step: 27
	action: tensor([[ 4.7022,  0.1594, -1.8208,  3.1491, -2.8388,  1.2961, -1.1866]],
       dtype=torch.float64)
	q_value: tensor([[-44.7621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 28
	action: tensor([[ 1.9323,  0.3526, -0.6066,  1.3644, -1.3643,  0.6598, -0.3818]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9663656686220197, distance: 0.20986890308821302 entropy 0.03264415264129639
epoch: 47, step: 29
	action: tensor([[ 4.2519, -0.1597, -1.0137,  2.7195, -2.4611,  1.4789, -0.6718]],
       dtype=torch.float64)
	q_value: tensor([[-44.3142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 30
	action: tensor([[ 2.0048, -0.1003, -0.4729,  0.9670, -1.0216,  0.5820, -0.3600]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9311967438731111, distance: 0.30016578663459675 entropy 0.03264415264129639
epoch: 47, step: 31
	action: tensor([[ 4.0819,  0.0639, -1.1221,  2.8137, -2.2704,  1.2891, -1.2179]],
       dtype=torch.float64)
	q_value: tensor([[-36.2028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 32
	action: tensor([[ 1.8360,  0.1674, -0.3146,  1.6054, -1.3493,  0.9233, -0.4793]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9198969644255861, distance: 0.32387779989842963 entropy 0.03264415264129639
epoch: 47, step: 33
	action: tensor([[ 4.2652,  0.3020, -1.2222,  3.7554, -3.4345,  0.9647, -1.2002]],
       dtype=torch.float64)
	q_value: tensor([[-43.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 34
	action: tensor([[ 2.0325,  0.1182, -0.6978,  1.6010, -1.5433,  0.2093,  0.1005]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9617566582772521, distance: 0.2237868742660364 entropy 0.03264415264129639
epoch: 47, step: 35
	action: tensor([[ 4.1728,  0.0346, -0.8364,  2.6749, -2.5211,  0.6401, -0.8949]],
       dtype=torch.float64)
	q_value: tensor([[-39.3978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 36
	action: tensor([[ 2.2914,  0.1280, -0.7040,  1.2005, -0.9410,  0.6077, -0.5863]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9803592841437772, distance: 0.16037451264524125 entropy 0.03264415264129639
epoch: 47, step: 37
	action: tensor([[ 4.2966,  0.3090, -1.4726,  3.0012, -2.5489,  0.6957, -1.0474]],
       dtype=torch.float64)
	q_value: tensor([[-41.5801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 38
	action: tensor([[ 2.2330,  0.5426, -0.7695,  1.9909, -1.0897,  0.3447, -1.0159]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7248260982078283, distance: 0.6002889018947944 entropy 0.03264415264129639
epoch: 47, step: 39
	action: tensor([[ 5.6442,  0.7162, -1.5548,  3.7804, -3.4988,  1.2057, -1.4429]],
       dtype=torch.float64)
	q_value: tensor([[-50.4744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 40
	action: tensor([[ 2.2685,  0.1926, -0.6653,  1.5587, -1.3868, -0.0803, -0.3441]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9365719135949355, distance: 0.28820233680266194 entropy 0.03264415264129639
epoch: 47, step: 41
	action: tensor([[ 4.3253,  0.1770, -0.9971,  3.0053, -2.5019,  1.1509, -0.9292]],
       dtype=torch.float64)
	q_value: tensor([[-40.4808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 42
	action: tensor([[ 2.2580,  0.5295, -0.3827,  1.7469, -1.1120,  0.3665, -0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.735359703750309, distance: 0.5886873184287736 entropy 0.03264415264129639
epoch: 47, step: 43
	action: tensor([[ 5.1163,  0.5157, -1.3625,  3.4809, -2.8526,  0.9455, -1.3929]],
       dtype=torch.float64)
	q_value: tensor([[-43.2813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 44
	action: tensor([[ 2.0560,  0.4575, -0.3256,  1.3223, -0.9875,  0.3851, -0.4305]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8494814377531588, distance: 0.4439680566648325 entropy 0.03264415264129639
epoch: 47, step: 45
	action: tensor([[ 4.2039, -0.2469, -1.4928,  2.6047, -2.5716,  0.7782, -1.3341]],
       dtype=torch.float64)
	q_value: tensor([[-39.8437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 46
	action: tensor([[ 1.9413,  0.2314, -0.9227,  1.2634, -1.4529,  0.1588, -0.7908]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.990536028879413, distance: 0.11132517755089841 entropy 0.03264415264129639
epoch: 47, step: 47
	action: tensor([[ 4.5451,  0.5837, -1.1561,  3.0623, -2.4830,  0.8808, -0.7792]],
       dtype=torch.float64)
	q_value: tensor([[-46.6744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 48
	action: tensor([[ 2.1774, -0.0642, -0.8283,  1.2365, -1.0246, -0.0324, -0.5509]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9524293064284266, distance: 0.24958957122937417 entropy 0.03264415264129639
epoch: 47, step: 49
	action: tensor([[ 4.4355, -0.1066, -1.0411,  2.4873, -2.6007,  0.3064, -1.3596]],
       dtype=torch.float64)
	q_value: tensor([[-38.2843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 50
	action: tensor([[ 2.2263,  0.0210, -0.5960,  1.9627, -1.6398,  0.4160, -0.4145]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.887028537389758, distance: 0.3846281526456807 entropy 0.03264415264129639
epoch: 47, step: 51
	action: tensor([[ 5.2202,  0.2923, -1.2737,  3.2984, -3.1812,  0.6754, -1.2064]],
       dtype=torch.float64)
	q_value: tensor([[-44.5572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 52
	action: tensor([[ 2.2974,  0.2594, -0.8231,  1.4509, -1.3312,  0.6362, -0.4787]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9627145239577171, distance: 0.22096655225425849 entropy 0.03264415264129639
epoch: 47, step: 53
	action: tensor([[ 4.4351,  0.1569, -1.7124,  3.1038, -3.1733,  1.0755, -1.1924]],
       dtype=torch.float64)
	q_value: tensor([[-45.7917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 54
	action: tensor([[ 1.8089,  0.6567, -0.7400,  1.1389, -1.3481,  0.3872, -0.8078]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9488873450066313, distance: 0.2587145854926168 entropy 0.03264415264129639
epoch: 47, step: 55
	action: tensor([[ 4.2254,  0.1000, -1.3417,  3.2274, -2.7486,  0.9038, -1.2368]],
       dtype=torch.float64)
	q_value: tensor([[-48.7420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 56
	action: tensor([[ 1.9910,  0.6081, -0.6360,  1.5415, -1.3553,  0.1618, -0.0545]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8407333838620845, distance: 0.4566874422018818 entropy 0.03264415264129639
epoch: 47, step: 57
	action: tensor([[ 3.7591,  0.4481, -0.9744,  2.7709, -2.8099,  0.4793, -0.9572]],
       dtype=torch.float64)
	q_value: tensor([[-41.9508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 58
	action: tensor([[ 1.9905,  0.2017, -0.5065,  1.2919, -1.3465,  0.6404, -0.8335]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9794388607070063, distance: 0.16408930701494742 entropy 0.03264415264129639
epoch: 47, step: 59
	action: tensor([[ 4.5841,  0.4741, -1.4185,  3.4808, -2.6770,  0.9785, -0.9475]],
       dtype=torch.float64)
	q_value: tensor([[-45.6169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 60
	action: tensor([[ 2.4324, -0.0723, -0.6803,  1.5604, -0.8867,  0.0457, -0.7167]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 61
	action: tensor([[ 1.8714,  0.0150, -0.7583,  1.2258, -0.8364,  0.0816, -1.2999]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9840430362863716, distance: 0.14455456861184782 entropy 0.03264415264129639
epoch: 47, step: 62
	action: tensor([[ 5.1461, -0.3836, -1.3284,  3.8695, -2.6037,  0.7560, -1.3090]],
       dtype=torch.float64)
	q_value: tensor([[-43.9080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 63
	action: tensor([[ 2.0590,  0.5239, -0.4049,  1.5346, -0.9095,  0.8649, -0.4192]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8092224275903878, distance: 0.49982773649579665 entropy 0.03264415264129639
epoch: 47, step: 64
	action: tensor([[ 4.9322,  0.0406, -1.6384,  3.3798, -3.3643,  0.6058, -1.5004]],
       dtype=torch.float64)
	q_value: tensor([[-43.5013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 65
	action: tensor([[ 1.9665,  0.3893, -0.9024,  1.5717, -1.4047,  0.7408, -0.5797]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9262203078654937, distance: 0.3108315632558897 entropy 0.03264415264129639
epoch: 47, step: 66
	action: tensor([[ 4.6846,  0.0481, -1.2709,  3.5277, -2.8413,  0.4541, -1.1534]],
       dtype=torch.float64)
	q_value: tensor([[-49.3983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 67
	action: tensor([[ 2.3278, -0.1130, -0.7354,  1.2430, -1.6669,  0.4648, -0.0710]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9319547206327328, distance: 0.29850780706854857 entropy 0.03264415264129639
epoch: 47, step: 68
	action: tensor([[ 3.6934,  0.3025, -1.2174,  3.1133, -2.4617,  0.4548, -0.7744]],
       dtype=torch.float64)
	q_value: tensor([[-39.9977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 69
	action: tensor([[ 1.5692, -0.1818, -0.8158,  1.7090, -1.2397,  0.1969, -0.7111]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9439922130223878, distance: 0.27082010347352414 entropy 0.03264415264129639
epoch: 47, step: 70
	action: tensor([[ 5.3232,  0.3501, -1.8615,  3.5977, -2.9967,  1.2681, -1.0661]],
       dtype=torch.float64)
	q_value: tensor([[-42.1711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 71
	action: tensor([[ 1.8856, -0.1162, -0.9519,  1.5266, -1.5081,  0.4632, -0.4331]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9210072302669134, distance: 0.32162541839065495 entropy 0.03264415264129639
epoch: 47, step: 72
	action: tensor([[ 4.3334,  0.4017, -1.7633,  3.1480, -2.9181,  1.0744, -1.3041]],
       dtype=torch.float64)
	q_value: tensor([[-44.0122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 73
	action: tensor([[ 2.3370,  0.3425, -0.7691,  1.6056, -1.6785,  0.7176, -0.2610]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9537601530925218, distance: 0.24607352300357713 entropy 0.03264415264129639
epoch: 47, step: 74
	action: tensor([[ 3.7882,  0.3517, -1.3440,  3.1145, -2.5370,  1.0022, -0.8388]],
       dtype=torch.float64)
	q_value: tensor([[-47.3552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 75
	action: tensor([[ 2.2624,  0.3922, -0.5536,  1.1067, -1.1511,  0.5701, -0.5609]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9681358417620497, distance: 0.20427156600129504 entropy 0.03264415264129639
epoch: 47, step: 76
	action: tensor([[ 4.3022,  0.3625, -1.2956,  2.6522, -2.5051,  0.8801, -1.2628]],
       dtype=torch.float64)
	q_value: tensor([[-43.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 77
	action: tensor([[ 2.1120,  0.0980, -0.9461,  1.6771, -0.6438,  0.9285, -1.0782]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8209467565347063, distance: 0.4842256485473812 entropy 0.03264415264129639
epoch: 47, step: 78
	action: tensor([[ 6.1444e+00,  6.9811e-04, -1.7132e+00,  4.1930e+00, -3.6290e+00,
          1.5310e+00, -1.1845e+00]], dtype=torch.float64)
	q_value: tensor([[-48.4923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 79
	action: tensor([[ 1.5959,  0.0253, -0.0864,  1.8065, -1.1614,  0.8192, -0.8699]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8446671525480154, distance: 0.45101225732617045 entropy 0.03264415264129639
epoch: 47, step: 80
	action: tensor([[ 5.3887,  0.3042, -1.3363,  3.5805, -3.7339,  0.9348, -1.1828]],
       dtype=torch.float64)
	q_value: tensor([[-43.1165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 81
	action: tensor([[ 2.2191,  0.5361, -1.0751,  1.2951, -1.8355,  0.2700, -1.1347]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9923594922489521, distance: 0.10002712895795413 entropy 0.03264415264129639
epoch: 47, step: 82
	action: tensor([[ 4.7055, -0.0214, -1.1703,  2.5576, -2.7701,  1.0139, -1.7039]],
       dtype=torch.float64)
	q_value: tensor([[-55.8671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 83
	action: tensor([[ 1.9955,  0.0965, -0.5936,  1.9848, -1.4474,  0.1528, -0.7586]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8460937751906785, distance: 0.44893636519521113 entropy 0.03264415264129639
epoch: 47, step: 84
	action: tensor([[ 4.7459,  0.0539, -1.1201,  3.8248, -3.0983,  1.1384, -1.0027]],
       dtype=torch.float64)
	q_value: tensor([[-45.4565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 85
	action: tensor([[ 1.6936,  0.1236, -0.5936,  1.1793, -1.1873,  0.5926, -0.7172]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9869500911650111, distance: 0.13072553628672026 entropy 0.03264415264129639
epoch: 47, step: 86
	action: tensor([[ 4.2161,  0.4149, -1.7393,  3.4052, -3.1872,  0.6874, -1.2241]],
       dtype=torch.float64)
	q_value: tensor([[-43.1733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 87
	action: tensor([[ 1.5616, -0.3274, -0.4888,  1.4565, -0.9286,  0.6922, -0.5497]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9530762047722765, distance: 0.24788671869896975 entropy 0.03264415264129639
epoch: 47, step: 88
	action: tensor([[ 4.9810,  0.4775, -1.9075,  3.5934, -2.9790,  1.1203, -1.4254]],
       dtype=torch.float64)
	q_value: tensor([[-37.1858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 89
	action: tensor([[ 1.9575,  0.0168, -0.6089,  1.2737, -0.8962,  0.2277, -0.5292]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9726847775467212, distance: 0.1891294084240379 entropy 0.03264415264129639
epoch: 47, step: 90
	action: tensor([[ 4.4591, -0.0255, -1.6971,  3.4564, -3.1745,  0.7585, -0.7426]],
       dtype=torch.float64)
	q_value: tensor([[-37.7714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 91
	action: tensor([[ 2.3235,  0.1312, -0.8132,  1.5857, -0.8268,  0.8256, -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8821969969636426, distance: 0.39276689377972895 entropy 0.03264415264129639
epoch: 47, step: 92
	action: tensor([[ 4.5360,  0.6257, -1.7669,  3.5654, -3.0031,  0.8043, -0.8371]],
       dtype=torch.float64)
	q_value: tensor([[-41.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 93
	action: tensor([[ 2.1127,  0.0569, -0.7361,  1.5308, -1.5756,  0.6911, -0.1661]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9586859358302352, distance: 0.23259782701975115 entropy 0.03264415264129639
epoch: 47, step: 94
	action: tensor([[ 4.3896,  0.0212, -1.4640,  2.8785, -2.1871,  0.5814, -1.2495]],
       dtype=torch.float64)
	q_value: tensor([[-43.2954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 95
	action: tensor([[ 1.9932,  0.1472, -0.5238,  1.4169, -1.6440,  0.6385, -0.5540]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9780562513191267, distance: 0.16951654944871372 entropy 0.03264415264129639
epoch: 47, step: 96
	action: tensor([[ 4.2777,  0.0871, -1.0430,  2.9465, -2.3585,  0.9385, -0.7139]],
       dtype=torch.float64)
	q_value: tensor([[-45.3310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 97
	action: tensor([[ 1.8871, -0.0080, -0.7785,  1.7640, -1.3648,  0.3506, -0.4582]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9312680780915888, distance: 0.300010142513777 entropy 0.03264415264129639
epoch: 47, step: 98
	action: tensor([[ 4.5011,  0.1687, -1.1301,  3.1626, -2.8197,  0.9273, -1.4325]],
       dtype=torch.float64)
	q_value: tensor([[-43.1077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 99
	action: tensor([[ 1.9962, -0.0556, -0.6409,  1.2983, -0.9853,  0.3988, -0.6221]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9726253219398892, distance: 0.1893351305524985 entropy 0.03264415264129639
epoch: 47, step: 100
	action: tensor([[ 4.9348, -0.0052, -1.6311,  3.2824, -2.9365,  0.7833, -1.0354]],
       dtype=torch.float64)
	q_value: tensor([[-39.6242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 101
	action: tensor([[ 1.8312,  0.2751, -0.9532,  1.5760, -1.3829,  0.6889, -0.5498]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.920126696070498, distance: 0.3234130334679354 entropy 0.03264415264129639
epoch: 47, step: 102
	action: tensor([[ 4.6602, -0.0536, -1.4635,  3.2586, -2.6094,  1.1231, -0.8647]],
       dtype=torch.float64)
	q_value: tensor([[-48.3108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 103
	action: tensor([[ 2.2271,  0.0555, -0.5429,  1.2548, -1.5213,  0.4583, -0.1111]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9720764096982405, distance: 0.19122396630738342 entropy 0.03264415264129639
epoch: 47, step: 104
	action: tensor([[ 3.8241,  0.1458, -1.2147,  2.7708, -2.1363,  0.6503, -1.1088]],
       dtype=torch.float64)
	q_value: tensor([[-39.2813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 105
	action: tensor([[ 1.7938,  0.0505, -0.6479,  1.6487, -1.2189,  0.4283, -0.2454]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9402854450628552, distance: 0.27963839007628466 entropy 0.03264415264129639
epoch: 47, step: 106
	action: tensor([[ 4.9807,  0.1730, -1.3170,  3.3747, -2.5977,  0.9059, -1.3626]],
       dtype=torch.float64)
	q_value: tensor([[-40.1214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 107
	action: tensor([[ 1.8292, -0.1497, -0.4121,  1.3058, -1.2699,  0.4685, -0.6064]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9464354477122495, distance: 0.2648472286796392 entropy 0.03264415264129639
epoch: 47, step: 108
	action: tensor([[ 4.4241, -0.0755, -1.6486,  3.3226, -3.0041,  0.6141, -1.2905]],
       dtype=torch.float64)
	q_value: tensor([[-39.5008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 109
	action: tensor([[ 2.2318,  0.5096, -0.1173,  0.9515, -1.3469,  0.3627, -0.4721]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 110
	action: tensor([[ 2.1055,  0.1318, -0.8549,  1.6649, -1.1518,  0.5672, -0.9387]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9212004864640824, distance: 0.32123174838541413 entropy 0.03264415264129639
epoch: 47, step: 111
	action: tensor([[ 5.2490,  0.1429, -1.2463,  3.6552, -2.8160,  0.7509, -1.2036]],
       dtype=torch.float64)
	q_value: tensor([[-47.9312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 112
	action: tensor([[ 1.8232,  0.1807, -0.6701,  1.0675, -0.8361,  0.6041, -0.2031]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9876801140768725, distance: 0.12701646932783783 entropy 0.03264415264129639
epoch: 47, step: 113
	action: tensor([[ 4.2530,  0.5013, -1.3302,  2.6704, -2.1917,  0.9754, -1.1044]],
       dtype=torch.float64)
	q_value: tensor([[-37.7745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 114
	action: tensor([[ 2.2910, -0.0416, -0.3020,  1.2440, -0.9834,  0.4597, -1.1545]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9287149054462841, distance: 0.305531546461773 entropy 0.03264415264129639
epoch: 47, step: 115
	action: tensor([[ 4.8695,  0.7296, -1.1660,  3.6868, -3.0365,  1.2670, -1.2538]],
       dtype=torch.float64)
	q_value: tensor([[-41.9025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 116
	action: tensor([[ 2.3633,  0.5627, -0.2397,  1.8442, -1.2165,  0.1714, -0.4565]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6039274158752479, distance: 0.7201850208212356 entropy 0.03264415264129639
epoch: 47, step: 117
	action: tensor([[ 4.6671,  0.3839, -1.2104,  3.3118, -2.7481,  0.9506, -0.9348]],
       dtype=torch.float64)
	q_value: tensor([[-42.3779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 118
	action: tensor([[ 1.9150,  0.1161, -0.6104,  1.4757, -1.9168,  0.6270, -0.5499]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9803467110870279, distance: 0.1604258365192286 entropy 0.03264415264129639
epoch: 47, step: 119
	action: tensor([[ 4.5435, -0.0596, -1.3437,  3.1486, -2.5759,  0.4760, -1.4802]],
       dtype=torch.float64)
	q_value: tensor([[-47.5003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 120
	action: tensor([[ 1.8059,  0.5302,  0.0520,  1.5627, -1.0970,  0.2880, -0.2860]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 121
	action: tensor([[ 2.2455, -0.5806, -0.5864,  1.5568, -1.3271,  0.2034, -0.5760]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9026084352715141, distance: 0.3571226248382902 entropy 0.03264415264129639
epoch: 47, step: 122
	action: tensor([[ 4.6056,  0.1127, -1.3393,  3.2675, -2.3553,  1.3756, -0.6730]],
       dtype=torch.float64)
	q_value: tensor([[-37.8037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 123
	action: tensor([[ 1.4798,  0.2104, -0.5022,  1.2792, -0.7342,  0.0016, -0.5496]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9488132935688884, distance: 0.25890192904778936 entropy 0.03264415264129639
epoch: 47, step: 124
	action: tensor([[ 4.4690,  0.4777, -1.3926,  2.5135, -2.6150,  1.1518, -0.9082]],
       dtype=torch.float64)
	q_value: tensor([[-35.4011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 125
	action: tensor([[ 1.9205,  0.4423, -0.2926,  1.3333, -0.9391,  0.6113, -0.1351]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8411639707347862, distance: 0.4560696834030172 entropy 0.03264415264129639
epoch: 47, step: 126
	action: tensor([[ 4.0587,  0.5238, -1.2128,  2.7225, -2.2699,  1.0393, -1.2145]],
       dtype=torch.float64)
	q_value: tensor([[-38.2907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 47, step: 127
	action: tensor([[ 2.0887,  0.3345, -0.3910,  1.2561, -1.8797,  0.3600, -0.7459]],
       dtype=torch.float64)
	q_value: tensor([[-28.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9285087066981516, distance: 0.30597311654079884 entropy 0.03264415264129639
LOSS epoch 47 actor 236.53954344165572 critic 222.35455056409222 
epoch: 48, step: 0
	action: tensor([[ 6.1800,  0.1153, -2.0087,  4.8489, -4.2615,  1.4927, -2.0815]],
       dtype=torch.float64)
	q_value: tensor([[-58.6408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 1
	action: tensor([[ 2.2527, -0.1338, -0.8215,  1.4058, -1.7016,  0.4934, -0.7728]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.933937929510094, distance: 0.2941255713327157 entropy 0.03264415264129639
epoch: 48, step: 2
	action: tensor([[ 6.0318,  0.4472, -2.3795,  5.3157, -4.7734,  1.2541, -1.8683]],
       dtype=torch.float64)
	q_value: tensor([[-58.2205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 3
	action: tensor([[ 2.5459, -0.2965, -1.0729,  1.6916, -1.7457,  0.5599, -0.4735]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 4
	action: tensor([[ 2.8608, -0.3055, -0.6150,  1.0689, -1.2296,  0.3987, -0.8121]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 5
	action: tensor([[ 2.5366, -0.0879, -0.8595,  1.7936, -1.8066,  0.4416, -0.5766]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 6
	action: tensor([[ 2.3464,  0.0880, -0.5320,  1.5060, -1.6296,  0.1664, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9502489058504641, distance: 0.2552454513895442 entropy 0.03264415264129639
epoch: 48, step: 7
	action: tensor([[ 6.1773,  0.2657, -1.9426,  4.5775, -4.1459,  1.2776, -1.9316]],
       dtype=torch.float64)
	q_value: tensor([[-48.6766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 8
	action: tensor([[ 2.6627,  0.7040, -0.6090,  1.4910, -1.6824,  0.2279, -0.5552]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 9
	action: tensor([[ 2.6309,  0.0710, -0.7789,  2.1490, -1.3731,  0.2165, -0.8588]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 10
	action: tensor([[ 2.9328,  0.0652, -0.6157,  1.4161, -1.5428,  0.4679, -0.4251]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 11
	action: tensor([[ 2.3649, -0.2817, -0.7757,  1.3429, -1.4473,  0.7358, -0.1064]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8820218590464712, distance: 0.3930587489361257 entropy 0.03264415264129639
epoch: 48, step: 12
	action: tensor([[ 6.1800, -0.0478, -1.9569,  4.5940, -4.0912,  1.2286, -1.4855]],
       dtype=torch.float64)
	q_value: tensor([[-49.0811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 13
	action: tensor([[ 2.2908, -0.1332, -0.6688,  1.8281, -1.7408, -0.0129, -1.0574]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9343208891723671, distance: 0.2932718140743532 entropy 0.03264415264129639
epoch: 48, step: 14
	action: tensor([[ 6.1800,  0.2383, -2.3613,  6.0570, -4.7730,  1.4076, -1.7226]],
       dtype=torch.float64)
	q_value: tensor([[-59.7569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9077808753838683 entropy 0.03264415264129639
epoch: 48, step: 15
	action: tensor([[ 2.1896,  0.0347, -0.3258,  1.5613, -1.5023,  1.1091, -0.4060]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9441474691700664, distance: 0.2704444801236075 entropy 0.03264415264129639
epoch: 48, step: 16
	action: tensor([[ 6.1800,  0.2269, -2.1315,  5.4046, -4.1705,  1.6518, -1.4036]],
       dtype=torch.float64)
	q_value: tensor([[-54.5549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.49566767529285954 entropy 0.03264415264129639
epoch: 48, step: 17
	action: tensor([[ 2.4097,  0.5636, -0.6819,  1.3371, -1.3044,  0.3505, -0.5116]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 18
	action: tensor([[ 2.1472,  0.5333, -1.0515,  1.6366, -1.1706,  0.2028, -0.7138]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9009180977735671, distance: 0.3602084206498854 entropy 0.03264415264129639
epoch: 48, step: 19
	action: tensor([[ 6.1800,  0.6548, -2.7052,  5.2484, -4.8308,  1.4413, -1.6604]],
       dtype=torch.float64)
	q_value: tensor([[-60.4945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 20
	action: tensor([[ 2.7312,  0.0273, -0.7491,  1.7160, -1.3090,  0.5674, -0.7114]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 21
	action: tensor([[ 2.5808, -0.2092, -0.3439,  1.6800, -1.5971,  0.3516, -0.7517]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 22
	action: tensor([[ 2.5801, -0.2702, -0.2843,  1.7097, -1.5132,  0.4989, -0.4031]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 23
	action: tensor([[ 2.2287,  0.0133, -0.7224,  1.8469, -1.7563,  0.8325, -0.5963]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9125227964231716, distance: 0.33845749742389264 entropy 0.03264415264129639
epoch: 48, step: 24
	action: tensor([[ 6.1800,  0.3346, -2.3423,  5.9579, -4.7701,  1.6267, -1.8280]],
       dtype=torch.float64)
	q_value: tensor([[-61.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8314137071261166 entropy 0.03264415264129639
epoch: 48, step: 25
	action: tensor([[ 2.7352, -0.0701, -0.3574,  1.4962, -0.8311,  0.0310, -0.9300]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 26
	action: tensor([[ 2.5668,  0.1787, -1.4535,  1.7085, -0.9643, -0.0349, -0.7748]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 27
	action: tensor([[ 2.2746, -0.0267, -0.5877,  1.4469, -1.7885,  0.2803, -0.6856]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9724842634749769, distance: 0.18982231447943262 entropy 0.03264415264129639
epoch: 48, step: 28
	action: tensor([[ 6.1800,  0.6231, -2.3376,  5.3991, -4.6547,  1.0959, -1.6371]],
       dtype=torch.float64)
	q_value: tensor([[-56.1753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.645786562003732 entropy 0.03264415264129639
epoch: 48, step: 29
	action: tensor([[ 2.3489,  0.3472, -0.7112,  1.3478, -1.0041,  0.5792, -0.5259]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9593740048693811, distance: 0.23065278314168108 entropy 0.03264415264129639
epoch: 48, step: 30
	action: tensor([[ 6.1800, -0.0783, -2.1513,  5.2041, -4.5048,  1.2996, -1.6804]],
       dtype=torch.float64)
	q_value: tensor([[-53.8582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 31
	action: tensor([[ 2.3324, -0.1070, -0.6233,  1.6082, -1.9037,  0.0226, -0.6014]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9615595940303386, distance: 0.22436270948988127 entropy 0.03264415264129639
epoch: 48, step: 32
	action: tensor([[ 6.1800,  0.0794, -2.0627,  5.1318, -4.7663,  1.8826, -1.4436]],
       dtype=torch.float64)
	q_value: tensor([[-55.4421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 33
	action: tensor([[ 2.3942, -0.2106, -0.9428,  2.1430, -1.1124,  1.0525, -0.7021]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.627749368266469, distance: 0.6981912712656609 entropy 0.03264415264129639
epoch: 48, step: 34
	action: tensor([[ 6.1800e+00, -7.2197e-04, -3.0131e+00,  6.1800e+00, -5.7036e+00,
          2.2427e+00, -1.7897e+00]], dtype=torch.float64)
	q_value: tensor([[-59.8604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 35
	action: tensor([[ 2.5405,  0.2832, -0.6374,  1.5461, -1.1889,  0.3932, -0.5204]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 36
	action: tensor([[ 2.6284, -0.1138, -0.5512,  0.9040, -0.5489,  0.8287, -1.4495]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 37
	action: tensor([[ 2.4528,  0.0565, -0.8813,  1.6181, -1.4841,  0.6615, -0.7544]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 38
	action: tensor([[ 2.2378, -0.1286, -0.9601,  1.7841, -1.3413,  0.3733, -0.8005]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.909071482450216, distance: 0.3450696382666864 entropy 0.03264415264129639
epoch: 48, step: 39
	action: tensor([[ 6.1800,  0.4436, -2.5523,  6.1800, -5.3590,  1.1974, -1.7956]],
       dtype=torch.float64)
	q_value: tensor([[-57.7932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0031254166802466 entropy 0.03264415264129639
epoch: 48, step: 40
	action: tensor([[ 2.6194,  0.0493, -1.2107,  1.5691, -0.8752,  0.4088, -0.8767]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 41
	action: tensor([[ 3.0754,  0.1898, -1.0646,  1.7759, -1.5511,  0.4948, -0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 42
	action: tensor([[ 2.5525,  0.5601, -0.5812,  2.1820, -1.3712,  0.2713, -0.4306]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 43
	action: tensor([[ 2.8643, -0.1820, -0.9316,  1.2808, -1.6450,  0.1503, -0.7516]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 44
	action: tensor([[ 2.8349, -0.0632, -0.7079,  1.7107, -1.2364,  0.3635, -0.3834]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 45
	action: tensor([[ 2.9338e+00, -1.5791e-03, -5.8378e-01,  1.7248e+00, -1.3181e+00,
          4.7035e-01, -1.2868e+00]], dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 46
	action: tensor([[ 2.3177, -0.1013, -1.1045,  1.7100, -0.7797,  0.7121, -0.4891]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.804627905462267, distance: 0.5058106385958251 entropy 0.03264415264129639
epoch: 48, step: 47
	action: tensor([[ 6.1800, -0.0469, -2.1322,  6.1800, -4.7403,  1.4837, -2.4988]],
       dtype=torch.float64)
	q_value: tensor([[-53.5032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9004983521890997 entropy 0.03264415264129639
epoch: 48, step: 48
	action: tensor([[ 2.5983,  0.1078, -0.6213,  1.9131, -1.4901,  0.1722, -0.4309]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 49
	action: tensor([[ 2.2274,  0.2609, -0.7120,  1.3629, -1.6359,  0.5481, -0.7323]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9890995974575499, distance: 0.11947525371403109 entropy 0.03264415264129639
epoch: 48, step: 50
	action: tensor([[ 5.9369, -0.1357, -1.9705,  5.0778, -4.3833,  1.4812, -1.4210]],
       dtype=torch.float64)
	q_value: tensor([[-59.9554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 51
	action: tensor([[ 2.6893, -0.0031, -0.6526,  1.6547, -1.3221,  0.3015, -0.4658]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 52
	action: tensor([[ 2.5884, -0.0706, -0.5237,  2.0198, -1.0265,  0.6589, -0.4362]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 53
	action: tensor([[ 2.7653,  0.3711, -0.6810,  1.4208, -1.1433,  0.9187, -0.0923]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 54
	action: tensor([[ 2.4629e+00,  6.6897e-04, -8.1079e-01,  1.6627e+00, -1.2672e+00,
          2.2772e-01, -1.6595e-01]], dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 55
	action: tensor([[ 2.0798,  0.5183, -0.3769,  2.1478, -1.1066,  0.2884, -0.2314]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5744278820883582, distance: 0.7465231442910435 entropy 0.03264415264129639
epoch: 48, step: 56
	action: tensor([[ 6.1800,  0.0904, -2.3439,  5.9485, -4.9305,  2.0376, -1.9961]],
       dtype=torch.float64)
	q_value: tensor([[-52.7325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9027135082888521 entropy 0.03264415264129639
epoch: 48, step: 57
	action: tensor([[ 2.4685,  0.0034, -0.5105,  1.1561, -1.8164,  0.2558, -0.6106]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 58
	action: tensor([[ 2.2196,  0.1965, -0.8048,  1.5423, -1.2616,  0.1525, -0.8276]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9662440554286876, distance: 0.21024797704393233 entropy 0.03264415264129639
epoch: 48, step: 59
	action: tensor([[ 5.9061,  0.1478, -2.5533,  5.8225, -4.6721,  1.8099, -1.9655]],
       dtype=torch.float64)
	q_value: tensor([[-56.7221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0199926586538353 entropy 0.03264415264129639
epoch: 48, step: 60
	action: tensor([[ 2.0409,  0.2073, -1.1453,  1.6417, -1.6292,  0.5196, -0.6305]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9068734361968013, distance: 0.34921547656649293 entropy 0.03264415264129639
epoch: 48, step: 61
	action: tensor([[ 6.1800,  0.2182, -2.3751,  5.5676, -4.6201,  1.1426, -1.9223]],
       dtype=torch.float64)
	q_value: tensor([[-62.9140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7295005004694065 entropy 0.03264415264129639
epoch: 48, step: 62
	action: tensor([[ 2.9539,  0.2908, -1.0781,  1.4939, -1.1110,  0.6749, -0.5103]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 63
	action: tensor([[ 2.8243, -0.0962, -0.6772,  1.3542, -1.1815,  0.2588, -0.4998]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 64
	action: tensor([[ 2.6684, -0.0083, -0.5466,  1.4949, -1.0624,  0.4277, -0.3640]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 65
	action: tensor([[ 2.5838,  0.2913, -0.6887,  1.8018, -1.4752,  0.4082, -0.0206]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 66
	action: tensor([[ 2.9072, -0.2122, -0.7454,  2.0910, -1.6286,  0.1720, -0.8213]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 67
	action: tensor([[ 2.4707,  0.1174, -1.0141,  1.7580, -1.3975,  0.4455, -0.2384]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 68
	action: tensor([[ 2.4201,  0.4348, -0.6358,  1.2861, -1.3155,  0.5586, -0.4880]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 69
	action: tensor([[ 2.2341, -0.2449, -0.6232,  1.7941, -1.3879,  0.6083, -0.5419]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9309747511627409, distance: 0.3006496369486117 entropy 0.03264415264129639
epoch: 48, step: 70
	action: tensor([[ 6.1800, -0.1328, -2.3362,  6.1800, -4.4027,  1.5926, -1.8680]],
       dtype=torch.float64)
	q_value: tensor([[-53.1982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8860313727947099 entropy 0.03264415264129639
epoch: 48, step: 71
	action: tensor([[ 2.7343,  0.1630, -1.0875,  1.4690, -1.1723,  0.7146, -0.6250]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 72
	action: tensor([[ 2.4373, -0.0731, -0.6456,  1.5871, -1.3770,  0.6206, -0.5616]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 73
	action: tensor([[ 2.6129,  0.2999, -0.6273,  1.6055, -1.2852,  0.2935, -0.5742]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 74
	action: tensor([[ 2.3115,  0.4309, -0.6727,  1.2224, -1.4263,  0.5141, -0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9790179954988149, distance: 0.1657601691343192 entropy 0.03264415264129639
epoch: 48, step: 75
	action: tensor([[ 6.1800, -0.0896, -1.8751,  4.8596, -4.0857,  0.9437, -1.2711]],
       dtype=torch.float64)
	q_value: tensor([[-52.5159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 76
	action: tensor([[ 2.3020,  0.0995, -0.6465,  1.3632, -1.5433,  0.5772, -0.7382]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9827568846753278, distance: 0.15026732650573646 entropy 0.03264415264129639
epoch: 48, step: 77
	action: tensor([[ 6.1800, -0.1165, -2.1620,  4.9570, -4.3377,  0.9415, -2.0334]],
       dtype=torch.float64)
	q_value: tensor([[-57.4860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 78
	action: tensor([[ 2.5015, -0.2999, -0.4430,  1.7939, -1.5368,  0.2667, -0.1285]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 79
	action: tensor([[ 2.2736, -0.0726, -0.3730,  1.5753, -1.3154,  0.2602, -1.1614]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9337229427211764, distance: 0.29460377122921844 entropy 0.03264415264129639
epoch: 48, step: 80
	action: tensor([[ 5.6682,  0.4755, -2.3749,  5.8475, -4.6371,  1.3520, -2.4208]],
       dtype=torch.float64)
	q_value: tensor([[-55.8966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0126971537217162 entropy 0.03264415264129639
epoch: 48, step: 81
	action: tensor([[ 2.6086,  0.0294, -0.9170,  1.5768, -1.6061, -0.1038, -0.5404]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 82
	action: tensor([[ 2.8138,  0.4389, -0.4423,  1.8054, -1.5271,  0.4878, -0.5134]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 83
	action: tensor([[ 2.3619,  0.3076, -0.7475,  1.7214, -1.5542,  0.1442, -0.6646]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9077681680908322, distance: 0.3475338492709226 entropy 0.03264415264129639
epoch: 48, step: 84
	action: tensor([[ 6.1800, -0.0710, -1.7370,  5.0701, -5.4285,  1.2543, -1.6509]],
       dtype=torch.float64)
	q_value: tensor([[-58.4645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 85
	action: tensor([[ 2.8127,  0.1403, -0.6958,  1.7288, -1.4041,  0.6938, -1.0533]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 86
	action: tensor([[ 2.5219,  0.1399, -0.9329,  1.9763, -1.6596,  0.2788, -0.4238]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 87
	action: tensor([[ 2.7453, -0.4293, -0.9151,  1.5484, -1.4623,  0.1401, -0.5252]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 88
	action: tensor([[ 2.2099, -0.3548, -0.7341,  1.4302, -1.4524,  0.1426, -0.8155]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9272276066460369, distance: 0.3087024117460241 entropy 0.03264415264129639
epoch: 48, step: 89
	action: tensor([[ 6.1800,  0.5460, -2.2051,  5.4385, -4.4420,  1.7109, -1.8680]],
       dtype=torch.float64)
	q_value: tensor([[-52.7609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6104768029620985 entropy 0.03264415264129639
epoch: 48, step: 90
	action: tensor([[ 2.0692,  0.0224, -0.4503,  1.6261, -1.7840,  0.8277, -0.5779]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9645786557324169, distance: 0.2153719859746215 entropy 0.03264415264129639
epoch: 48, step: 91
	action: tensor([[ 6.1800,  0.1529, -2.6465,  5.6015, -4.5359,  1.5018, -1.8008]],
       dtype=torch.float64)
	q_value: tensor([[-57.9653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8436169145878026 entropy 0.03264415264129639
epoch: 48, step: 92
	action: tensor([[ 2.9443, -0.2198, -0.4549,  1.7944, -1.1019,  0.6155, -0.6490]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 93
	action: tensor([[ 2.5537, -0.3406, -0.8047,  1.8598, -1.3917,  0.4406,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 94
	action: tensor([[ 2.3999, -0.0702, -1.1366,  1.0063, -1.3735,  0.6334, -1.0227]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 95
	action: tensor([[ 2.6026,  0.0043, -0.4253,  1.5210, -1.2775,  0.5165, -0.6883]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 96
	action: tensor([[ 2.8696, -0.3788, -0.4467,  1.7831, -1.1919,  0.6326, -0.5204]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 97
	action: tensor([[ 2.4398,  0.0736, -1.1157,  1.5805, -1.3991,  0.4974, -0.8754]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 98
	action: tensor([[ 2.5593,  0.1282, -0.6070,  1.3951, -1.0061,  0.4423, -0.7754]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 99
	action: tensor([[ 2.1362, -0.0344, -0.7387,  1.7909, -1.6209,  0.3295, -0.5120]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9450227331567318, distance: 0.268317047226429 entropy 0.03264415264129639
epoch: 48, step: 100
	action: tensor([[ 6.1800, -0.0452, -2.4518,  5.2969, -4.4078,  1.0728, -2.2867]],
       dtype=torch.float64)
	q_value: tensor([[-55.8213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 101
	action: tensor([[ 2.1631e+00, -5.4406e-04, -5.2150e-01,  1.6599e+00, -1.2515e+00,
          6.1442e-02, -1.0221e+00]], dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9304105355372508, distance: 0.30187589829473765 entropy 0.03264415264129639
epoch: 48, step: 102
	action: tensor([[ 5.7995, -0.0090, -2.9403,  5.7638, -4.9290,  1.6250, -1.7564]],
       dtype=torch.float64)
	q_value: tensor([[-54.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2594055028421363 entropy 0.03264415264129639
epoch: 48, step: 103
	action: tensor([[ 2.6914,  0.5889, -0.5628,  1.7677, -1.2938,  0.3302, -0.3827]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 104
	action: tensor([[ 2.4746,  0.3720, -0.9424,  1.2214, -1.5178,  0.4996, -0.7032]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 105
	action: tensor([[ 2.6914,  0.3628, -0.9901,  1.4593, -0.9775, -0.0371, -1.0468]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 106
	action: tensor([[ 2.7073, -0.2737, -0.9874,  1.8495, -1.4116,  1.0589, -1.1665]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 107
	action: tensor([[ 2.4725, -0.3510, -1.1246,  1.7625, -1.1723,  0.0951, -0.4197]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 108
	action: tensor([[ 2.8833, -0.0087, -1.1864,  1.7981, -1.2634,  0.3751, -0.4055]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 109
	action: tensor([[ 2.7311, -0.0891, -0.9172,  1.3965, -1.3728,  0.2534, -0.7422]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 110
	action: tensor([[ 2.3617, -0.0389, -0.8004,  1.4530, -1.2332,  0.3632, -0.8583]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9705669783341345, distance: 0.19632432743630504 entropy 0.03264415264129639
epoch: 48, step: 111
	action: tensor([[ 6.1800,  0.2006, -1.7881,  5.8088, -5.2099,  1.4474, -1.8855]],
       dtype=torch.float64)
	q_value: tensor([[-55.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7656019788688815 entropy 0.03264415264129639
epoch: 48, step: 112
	action: tensor([[ 2.6651,  0.1463, -0.4510,  1.5699, -1.6851,  0.7484, -0.6890]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 113
	action: tensor([[ 3.0607, -0.3806, -1.0067,  1.6259, -1.3733,  0.2981, -0.3121]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 114
	action: tensor([[ 2.6306,  0.1175, -1.1558,  1.9643, -1.0673,  0.3144, -0.6722]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 115
	action: tensor([[ 2.6282,  0.2233, -0.8121,  1.8015, -1.1077,  0.7270, -0.6201]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 116
	action: tensor([[ 2.8391,  0.3257, -0.9381,  1.5042, -1.3702, -0.1692, -0.4022]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 117
	action: tensor([[ 2.5155, -0.2854, -0.5015,  1.4996, -1.3797,  0.4542, -0.4023]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 118
	action: tensor([[ 2.5621,  0.4762, -0.9122,  1.3545, -1.4929,  0.5060, -0.2569]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 119
	action: tensor([[ 2.2530,  0.2292, -0.6556,  1.4102, -1.8273,  0.4985, -0.5695]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9868829460268568, distance: 0.1310614130551599 entropy 0.03264415264129639
epoch: 48, step: 120
	action: tensor([[ 6.1031,  0.5756, -2.2128,  5.0282, -4.4115,  1.6491, -2.0939]],
       dtype=torch.float64)
	q_value: tensor([[-58.9925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 121
	action: tensor([[ 2.4179, -0.0291, -0.8015,  1.6808, -1.3685,  0.5777, -0.5391]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 122
	action: tensor([[ 2.5257, -0.3211, -1.0135,  1.4437, -1.7646,  0.6310, -0.9049]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 123
	action: tensor([[ 2.7709,  0.0854, -0.6288,  2.0129, -1.5609,  0.5340, -0.3434]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 124
	action: tensor([[ 3.0589,  0.3864, -0.6840,  1.4527, -1.2450,  0.2532, -0.6916]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 125
	action: tensor([[ 2.6873, -0.2294, -1.0438,  1.4810, -1.4384,  0.9371, -0.9622]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 126
	action: tensor([[ 2.6669, -0.0750, -0.5074,  1.9589, -1.3529,  0.9406, -0.7524]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 48, step: 127
	action: tensor([[ 2.5143, -0.0930, -0.7620,  1.7955, -1.4471,  0.2338, -0.4247]],
       dtype=torch.float64)
	q_value: tensor([[-35.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 48 actor 207.5902943025296 critic 170.46225371715784 
epoch: 49, step: 0
	action: tensor([[ 2.9652, -0.4917, -0.7233,  2.2684, -1.3523,  0.2604, -0.3129]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 1
	action: tensor([[ 2.8165, -0.1135, -0.8392,  1.9673, -1.5842,  0.3103, -0.6435]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 2
	action: tensor([[ 3.0571, -0.3034, -0.4843,  1.5211, -1.3750,  0.4703, -0.5118]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 3
	action: tensor([[ 2.5140,  0.1980, -0.8280,  1.9344, -1.2600,  0.3872, -0.2189]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 4
	action: tensor([[ 3.3104, -0.6125, -1.0568,  2.0102, -1.5082,  0.3420, -0.3427]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 5
	action: tensor([[ 2.8808, -0.0669, -1.0021,  1.5806, -1.1495,  0.3883, -0.6673]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 6
	action: tensor([[ 2.5445, -0.5172, -0.5423,  2.0504, -1.9530,  0.5079, -0.3532]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 7
	action: tensor([[ 3.0639, -0.4827, -0.8639,  2.2386, -1.1168,  0.2205, -0.8119]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 8
	action: tensor([[ 3.3050, -0.4029, -0.7293,  1.7070, -1.4059,  0.3891, -0.4296]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 9
	action: tensor([[ 2.7578,  0.3321, -0.4862,  2.2591, -1.3818,  0.6402, -0.4424]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 10
	action: tensor([[ 2.8507, -0.1733, -1.4862,  1.9963, -1.4729,  0.4555, -0.3180]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 11
	action: tensor([[ 3.1454,  0.2819, -1.1176,  1.8273, -1.5805,  0.4961, -0.3484]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 12
	action: tensor([[ 3.2720, -0.3393, -0.6401,  1.7736, -1.4254,  0.2885,  0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 13
	action: tensor([[ 2.6711,  0.1521, -0.9841,  1.9321, -1.3119,  0.4355, -0.9892]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 14
	action: tensor([[ 2.6370, -0.4019, -0.7238,  2.0979, -1.2485,  0.3894, -0.4835]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 15
	action: tensor([[ 2.6542,  0.1372, -1.0699,  2.5000, -0.9585,  0.8394, -0.4038]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 16
	action: tensor([[ 3.1538,  0.3896, -0.8242,  1.7779, -1.2929,  0.6291, -0.3096]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 17
	action: tensor([[ 2.6428, -0.0329, -0.3859,  2.4298, -1.4419, -0.0849, -0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 18
	action: tensor([[ 2.6778, -0.0493, -0.7786,  1.9436, -1.7815,  0.6350, -0.5423]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 19
	action: tensor([[ 2.5063, -0.1924, -0.9391,  2.0110, -1.1935,  0.5364,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 20
	action: tensor([[ 3.2605, -0.1321, -0.3167,  1.8950, -1.5810,  0.4611, -0.5754]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 21
	action: tensor([[ 2.6106, -0.0337, -0.4314,  1.8677, -1.7323,  0.2335, -0.6582]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 22
	action: tensor([[ 2.4268,  0.0380, -0.9277,  2.0250, -1.4391,  0.3506, -0.5181]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 23
	action: tensor([[ 3.1103, -0.2210, -0.8643,  1.7534, -1.4937,  0.5427, -0.6008]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 24
	action: tensor([[ 2.9940, -0.4230, -0.5498,  1.6005, -1.5420,  0.7070, -0.4020]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 25
	action: tensor([[ 3.2254,  0.2193, -0.8747,  1.9381, -1.2751,  0.7336, -0.3820]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 26
	action: tensor([[ 2.9528, -0.1783, -1.0260,  1.3852, -1.4354,  0.2368, -0.3601]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 27
	action: tensor([[ 2.6640, -0.5625, -0.7271,  2.1466, -1.0479,  0.2077, -0.7031]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 28
	action: tensor([[ 2.8838, -0.5464, -0.6752,  2.2797, -1.6910,  0.4529, -0.7903]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 29
	action: tensor([[ 2.5757, -0.0474, -0.9684,  2.3353, -1.7901,  0.6970, -0.5683]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 30
	action: tensor([[ 2.9440,  0.2209, -0.7181,  1.9739, -1.2480,  0.2714, -0.9091]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 31
	action: tensor([[ 3.1050, -0.0124, -0.7526,  2.2376, -1.0942,  0.1384, -0.6619]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 32
	action: tensor([[ 2.8459, -0.0075, -1.0130,  1.3474, -1.8343,  0.4788, -0.6236]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 33
	action: tensor([[ 2.8830,  0.4261, -1.0299,  1.6448, -1.5208,  0.3883, -0.8681]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 34
	action: tensor([[ 2.9517, -0.3983, -0.7241,  1.7349, -1.3897,  0.1524, -0.4565]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 35
	action: tensor([[ 3.4140, -0.0447, -0.9787,  2.1534, -1.6051, -0.2148, -0.5985]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 36
	action: tensor([[ 3.1195, -0.2603, -0.9033,  1.9345, -0.8786,  0.5242, -0.5146]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 37
	action: tensor([[ 2.7570, -0.3459, -1.0993,  2.3366, -1.4512,  0.9893, -0.3310]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 38
	action: tensor([[ 3.3212,  0.2344, -0.9717,  2.3264, -1.4452,  0.6590, -0.6373]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 39
	action: tensor([[ 2.7644, -0.0905, -0.8515,  2.4025, -1.8574,  0.6272, -0.7567]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 40
	action: tensor([[ 3.1121,  0.0634, -0.9599,  2.3432, -1.0897,  0.3104, -0.2441]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 41
	action: tensor([[ 2.9512,  0.1958, -1.0023,  2.1740, -1.5713,  0.0866, -0.5586]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 42
	action: tensor([[ 2.9820,  0.0545, -0.8135,  1.7162, -1.3639,  0.0105, -0.7429]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 43
	action: tensor([[ 2.3740, -0.0687, -1.0781,  1.7565, -1.3414,  1.0615, -0.2181]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7315247794541759, distance: 0.5929373351435833 entropy 0.03264415264129639
epoch: 49, step: 44
	action: tensor([[ 6.1274, -0.1349, -3.3044,  6.1800, -5.4436,  1.8938, -2.1284]],
       dtype=torch.float64)
	q_value: tensor([[-67.0824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.164631729842416 entropy 0.03264415264129639
epoch: 49, step: 45
	action: tensor([[ 3.2033,  0.2273, -0.8129,  2.2699, -1.5383,  0.6144, -0.8640]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 46
	action: tensor([[ 3.1447, -0.1482, -1.0522,  1.7167, -1.3965,  0.4446, -0.5778]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 47
	action: tensor([[ 2.9587, -0.3138, -0.7403,  1.9922, -1.3242,  0.4765, -0.4886]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 48
	action: tensor([[ 3.0714, -0.1638, -0.7032,  2.1982, -1.2594,  0.5570, -0.3524]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 49
	action: tensor([[ 3.1166,  0.0956, -0.6660,  2.2786, -1.0299,  0.6324, -0.8125]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 50
	action: tensor([[ 3.1712, -0.2901, -1.3295,  2.1074, -1.3675,  0.3770, -0.4644]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 51
	action: tensor([[ 2.4893,  0.3568, -0.2888,  1.7335, -1.3046,  0.5040, -0.8445]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 52
	action: tensor([[ 2.9437,  0.0863, -0.6365,  1.6328, -1.3787,  0.6609, -0.7581]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 53
	action: tensor([[ 3.1226,  0.0554, -0.9537,  1.9341, -0.9397,  0.4612, -1.0887]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 54
	action: tensor([[ 3.0513, -0.2670, -0.8057,  1.7610, -1.4846,  0.4117, -0.6654]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 55
	action: tensor([[ 2.8645, -0.2130, -1.0338,  2.0390, -1.5481,  0.1627, -0.3755]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 56
	action: tensor([[ 3.0902,  0.5527, -0.3699,  2.3294, -1.6230,  0.2766, -0.3782]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 57
	action: tensor([[ 2.9284,  0.5821, -0.7460,  2.0456, -1.7900,  0.7327, -0.4571]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 58
	action: tensor([[ 2.9602,  0.4866, -1.3608,  2.3623, -1.4351,  0.6326, -0.5571]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 59
	action: tensor([[ 2.9711,  0.1634, -1.0839,  2.2193, -1.4137,  0.5121, -0.4491]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 60
	action: tensor([[ 2.6149, -0.3200, -0.8676,  2.1353, -1.5721,  0.4699, -0.6174]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 61
	action: tensor([[ 2.8967,  0.1204, -0.5434,  2.0661, -1.2316,  0.4226, -0.3659]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 62
	action: tensor([[ 3.1091, -0.2397, -1.1968,  1.9584, -1.6996,  0.4570, -0.7678]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 63
	action: tensor([[ 2.7239, -0.0599, -0.2835,  1.9168, -1.3025, -0.0421, -0.4399]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 64
	action: tensor([[ 3.1465, -0.0509, -0.9968,  1.7427, -1.4313,  0.4544, -0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 65
	action: tensor([[ 3.0689,  0.3533, -1.0290,  1.7014, -1.3987,  0.2235, -0.9110]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 66
	action: tensor([[ 2.9334,  0.1147, -0.6408,  2.2177, -0.9610,  0.6239, -0.3516]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 67
	action: tensor([[ 3.1521, -0.3294, -1.2736,  1.9869, -1.3505,  0.5273, -0.9440]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 68
	action: tensor([[ 2.8726,  0.1397, -1.1078,  2.1815, -1.5238,  0.1466, -0.2509]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 69
	action: tensor([[ 2.6622,  0.0688, -0.8480,  2.5005, -1.4499,  0.8671, -0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 70
	action: tensor([[ 3.3193,  0.0912, -1.1283,  2.2798, -1.2110,  0.1484, -1.1296]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 71
	action: tensor([[ 3.0341,  0.0185, -0.9071,  2.0645, -1.5993,  0.1520, -0.4330]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 72
	action: tensor([[ 3.1666, -0.0535, -0.7005,  2.4508, -1.4241,  0.3467, -0.7763]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 73
	action: tensor([[ 2.9528,  0.1431, -1.0702,  1.4673, -1.7046,  0.2649, -0.5276]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 74
	action: tensor([[ 2.4125,  0.1167, -1.0424,  1.4088, -1.2092,  0.5285, -0.1226]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 75
	action: tensor([[ 2.8668, -0.1798, -1.1081,  2.1628, -1.6900,  0.5159, -0.7167]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 76
	action: tensor([[ 3.0974, -0.3209, -0.5897,  1.8769, -1.4976,  0.6130,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 77
	action: tensor([[ 2.9265, -0.0471, -0.8813,  1.4363, -1.7088,  0.4255,  0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 78
	action: tensor([[ 2.9051, -0.1322, -0.7908,  1.7808, -1.0275,  0.1716, -0.8027]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 79
	action: tensor([[ 2.8499, -0.1235, -0.7234,  2.0025, -1.3204,  0.5926, -0.5250]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 80
	action: tensor([[ 2.2862,  0.3987, -0.9915,  2.3273, -1.6231,  0.6131, -0.4417]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6904398604793345, distance: 0.6366917318935196 entropy 0.03264415264129639
epoch: 49, step: 81
	action: tensor([[ 6.1800, -0.3898, -3.9032,  5.9965, -5.7850,  1.6816, -1.8418]],
       dtype=torch.float64)
	q_value: tensor([[-75.4860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1607161475755787 entropy 0.03264415264129639
epoch: 49, step: 82
	action: tensor([[ 3.0878, -0.0365, -1.0259,  1.7589, -1.3150,  0.3999,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 83
	action: tensor([[ 3.3218,  0.2938, -0.9609,  2.0240, -1.6386,  0.8397, -0.8921]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 84
	action: tensor([[ 2.4748, -0.5968, -0.9248,  2.2657, -1.5274,  0.4625, -0.6630]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 85
	action: tensor([[ 2.8872, -0.2358, -1.4807,  2.3522, -1.3860,  0.7586, -0.5044]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 86
	action: tensor([[ 3.1632,  0.0576, -0.7270,  2.2031, -1.5005,  0.0864, -0.6984]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 87
	action: tensor([[ 2.9834, -0.4984, -0.5348,  1.8850, -1.5721,  0.0787, -0.7489]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 88
	action: tensor([[ 2.8718,  0.1995, -0.5141,  2.1028, -1.5166,  0.4955, -0.2792]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 89
	action: tensor([[ 2.7602, -0.2839, -1.1720,  2.2252, -1.2876,  0.5696, -0.3087]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 90
	action: tensor([[ 2.9570,  0.2259, -0.7604,  1.8335, -1.3579,  0.1728, -0.7239]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 91
	action: tensor([[ 2.9484,  0.1710, -0.8879,  2.1419, -1.8291,  0.4196, -0.7496]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 92
	action: tensor([[ 3.3198, -0.5203, -1.2714,  1.6489, -1.5201,  0.6087, -0.5695]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 93
	action: tensor([[ 2.5030, -0.0925, -0.2929,  1.8470, -0.8991,  0.0074, -0.1216]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 94
	action: tensor([[ 3.2129, -0.4010, -0.7757,  2.1749, -1.6318,  0.3977, -0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 95
	action: tensor([[ 2.8249,  0.4375, -1.1573,  1.7616, -1.0664,  0.3889, -0.2953]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 96
	action: tensor([[ 3.2036,  0.0879, -0.7637,  2.0707, -1.5127,  0.7660, -0.2065]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 97
	action: tensor([[ 3.1073, -0.1323, -0.7414,  1.9124, -0.9972,  0.2001, -0.1259]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 98
	action: tensor([[ 2.6094, -0.0189, -1.3015,  1.5235, -0.9614,  0.0390, -0.6588]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 99
	action: tensor([[ 3.3586, -0.1087, -0.7344,  2.1317, -1.0569,  0.4483, -0.3489]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 100
	action: tensor([[ 3.1919, -0.4960, -0.5583,  1.6753, -1.4045,  0.7242, -0.4402]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 101
	action: tensor([[ 3.2785, -0.1720, -0.6527,  1.9858, -1.3918,  0.4721, -0.4603]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 102
	action: tensor([[ 3.1482,  0.3132, -0.6797,  2.1074, -1.0222, -0.1299, -0.6231]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 103
	action: tensor([[ 2.8371,  0.1463, -0.6542,  1.3529, -1.6383,  0.2928, -0.5680]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 104
	action: tensor([[ 2.8626,  0.0981, -0.8065,  1.9246, -1.5378,  0.6584, -0.3979]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 105
	action: tensor([[ 2.7244, -0.2500, -0.5172,  1.7287, -1.0304,  0.5277, -0.4302]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 106
	action: tensor([[ 2.6474,  0.2742, -1.0940,  2.1807, -1.4636,  0.0996, -0.4934]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 107
	action: tensor([[ 2.9005,  0.0294, -0.9992,  1.8139, -1.0215,  0.5243, -0.5367]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 108
	action: tensor([[ 2.9152, -0.0137, -0.9946,  2.3163, -1.1919,  0.4641, -0.2016]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 109
	action: tensor([[ 2.8274,  0.0506, -1.1005,  2.0637, -1.5269,  0.3674, -1.0604]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 110
	action: tensor([[ 2.4818,  0.0390, -0.6015,  1.9124, -1.7973,  0.4624, -0.2954]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 111
	action: tensor([[ 2.8668,  0.0120, -0.8741,  2.0850, -1.1213,  0.0831, -0.5340]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 112
	action: tensor([[ 3.1149,  0.1568, -0.5605,  1.8251, -0.8369,  0.5603, -0.3020]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 113
	action: tensor([[ 3.2247,  0.1209, -0.3629,  1.9419, -1.4085,  0.4884, -0.3828]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 114
	action: tensor([[ 3.2521, -0.4107, -0.3410,  1.5695, -1.8024,  0.2355, -0.2661]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 115
	action: tensor([[ 2.9780, -0.0385, -1.2227,  1.8963, -1.4563,  0.6195,  0.2240]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 116
	action: tensor([[ 2.9078, -0.2583, -0.4723,  1.2572, -1.5959,  0.4296, -0.3440]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 117
	action: tensor([[ 2.7544, -0.1095, -1.2604,  1.6856, -1.3390,  0.5241, -0.3745]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 118
	action: tensor([[ 2.7350,  0.0531, -1.2355,  2.2824, -1.7345,  0.4131, -0.5974]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 119
	action: tensor([[ 3.0867,  0.2114, -1.3228,  2.0230, -0.8833, -0.0143, -0.4006]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 120
	action: tensor([[ 2.8799, -0.3454, -1.0529,  1.7652, -1.6267,  0.5111, -0.4904]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 121
	action: tensor([[ 3.2065,  0.0833, -0.6455,  1.8092, -1.3982,  0.2428, -0.5583]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 122
	action: tensor([[ 3.0873, -0.3368, -1.2337,  1.8662, -1.3782,  0.6034, -0.6944]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 123
	action: tensor([[ 2.3999,  0.1699, -0.7639,  1.7078, -1.9521,  0.6695, -0.6645]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 124
	action: tensor([[ 3.1389, -0.0312, -0.8069,  1.9046, -1.7157,  0.5384, -0.4649]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 125
	action: tensor([[ 2.5012,  0.0995, -0.6936,  1.8764, -1.3824,  0.2347, -1.0089]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 126
	action: tensor([[ 2.9614,  0.0145, -1.1809,  1.8597, -1.3034,  0.1081, -0.9775]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 49, step: 127
	action: tensor([[ 3.1119, -0.7661, -1.3005,  1.5698, -1.4707, -0.2413, -0.7740]],
       dtype=torch.float64)
	q_value: tensor([[-44.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 49 actor 35.355170809462216 critic 39.54681729532154 
epoch: 50, step: 0
	action: tensor([[ 2.1048, -0.1639, -0.7202,  1.4621, -1.1319,  0.4385, -0.0330]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9641491304263148, distance: 0.2166738700017239 entropy 0.03264415264129639
epoch: 50, step: 1
	action: tensor([[ 6.1800, -0.3215, -2.5649,  5.3081, -2.8770,  0.6777, -0.7852]],
       dtype=torch.float64)
	q_value: tensor([[-59.2309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 2
	action: tensor([[ 2.1444,  0.0893, -1.0105,  1.6803, -0.8143,  0.6260, -0.5041]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8569278945600947, distance: 0.4328467663450947 entropy 0.03264415264129639
epoch: 50, step: 3
	action: tensor([[ 6.1800, -0.3972, -3.1945,  5.6550, -3.2832,  1.5903, -1.0809]],
       dtype=torch.float64)
	q_value: tensor([[-71.1093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8083080505320714 entropy 0.03264415264129639
epoch: 50, step: 4
	action: tensor([[ 1.9713,  0.1031, -0.8778,  1.7916, -1.0000,  0.2035, -0.3304]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9040145214647493, distance: 0.3545352815677755 entropy 0.03264415264129639
epoch: 50, step: 5
	action: tensor([[ 6.1800, -0.0608, -2.8033,  5.8530, -2.8289,  0.4929, -0.4597]],
       dtype=torch.float64)
	q_value: tensor([[-67.0073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7684150526466088 entropy 0.03264415264129639
epoch: 50, step: 6
	action: tensor([[ 2.7375, -0.0650, -0.8999,  1.2778, -0.5993,  0.3426, -0.0471]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 7
	action: tensor([[ 2.2037,  0.3757, -1.1282,  1.4980, -0.5607,  0.8349, -0.2575]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.847110843585066, distance: 0.4474505388920998 entropy 0.03264415264129639
epoch: 50, step: 8
	action: tensor([[ 6.1800, -0.5757, -2.8817,  5.9549, -2.8693,  1.0956, -0.7532]],
       dtype=torch.float64)
	q_value: tensor([[-70.6727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9256048462929839 entropy 0.03264415264129639
epoch: 50, step: 9
	action: tensor([[ 1.9112, -0.3881, -0.8824,  2.0486, -0.9718,  0.3460, -0.2716]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8396194696070336, distance: 0.4582817005527062 entropy 0.03264415264129639
epoch: 50, step: 10
	action: tensor([[ 6.0082, -0.7302, -2.6908,  5.3636, -3.0302,  0.7213, -0.3043]],
       dtype=torch.float64)
	q_value: tensor([[-63.3689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 11
	action: tensor([[ 2.3423,  0.0220, -0.7736,  1.6034, -0.6190,  0.4057,  0.2028]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9360982498278358, distance: 0.289276443737314 entropy 0.03264415264129639
epoch: 50, step: 12
	action: tensor([[ 6.1800, -0.2289, -2.4506,  5.2708, -2.3447,  1.0402, -0.6261]],
       dtype=torch.float64)
	q_value: tensor([[-55.2212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 13
	action: tensor([[ 2.4986, -0.2897, -0.6497,  1.4957, -0.9589,  0.3549, -0.1661]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 14
	action: tensor([[ 2.4798,  0.3771, -1.1448,  1.8202, -0.8596,  0.4036, -0.3383]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 15
	action: tensor([[ 2.5814, -0.4215, -1.0622,  1.7964, -0.5799,  0.5626, -0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 16
	action: tensor([[ 2.1438, -0.3394, -1.3121,  1.6582, -0.7072,  0.3046, -0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7860096504320575, distance: 0.5293632492586321 entropy 0.03264415264129639
epoch: 50, step: 17
	action: tensor([[ 6.1564e+00, -4.1236e-03, -2.6990e+00,  5.6376e+00, -2.8377e+00,
          1.1246e+00, -5.2467e-01]], dtype=torch.float64)
	q_value: tensor([[-60.5613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.525893851319433 entropy 0.03264415264129639
epoch: 50, step: 18
	action: tensor([[ 2.4426, -0.1403, -0.9766,  1.5473, -0.7479,  0.1962, -0.4139]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 19
	action: tensor([[ 1.9613, -0.3376, -0.7214,  1.1168, -0.4028,  0.0540, -0.2365]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8287293832586409, distance: 0.47358520248225794 entropy 0.03264415264129639
epoch: 50, step: 20
	action: tensor([[ 6.1800, -0.9825, -2.4371,  4.7975, -2.4134,  0.3854, -0.5031]],
       dtype=torch.float64)
	q_value: tensor([[-49.6411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 21
	action: tensor([[ 2.3379, -0.1445, -0.7561,  1.5824, -0.9475,  0.5018, -0.0865]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9474375855414133, distance: 0.26235802108857387 entropy 0.03264415264129639
epoch: 50, step: 22
	action: tensor([[ 5.7833, -0.1949, -2.6031,  5.3241, -3.0756,  1.0346, -0.5163]],
       dtype=torch.float64)
	q_value: tensor([[-59.8700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 23
	action: tensor([[ 2.2373, -0.4135, -0.7054,  1.4441, -0.8303,  0.6184, -0.6534]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.923271065459547, distance: 0.31698322314451194 entropy 0.03264415264129639
epoch: 50, step: 24
	action: tensor([[ 5.7993, -0.3797, -3.1915,  5.7718, -2.7620,  0.6367, -0.8936]],
       dtype=torch.float64)
	q_value: tensor([[-62.6615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1671584746943313 entropy 0.03264415264129639
epoch: 50, step: 25
	action: tensor([[ 2.3620, -0.2406, -0.6883,  1.5650, -0.6791,  0.5720, -0.5042]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9411344223867578, distance: 0.2776434281901936 entropy 0.03264415264129639
epoch: 50, step: 26
	action: tensor([[ 6.1767, -0.8615, -3.3640,  5.9247, -3.0243,  0.5693, -1.0876]],
       dtype=torch.float64)
	q_value: tensor([[-61.3324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2648828393863478 entropy 0.03264415264129639
epoch: 50, step: 27
	action: tensor([[ 2.1711, -0.1197, -0.8841,  1.4809, -0.5066,  0.3538, -0.2430]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9621355119375966, distance: 0.22267565475273698 entropy 0.03264415264129639
epoch: 50, step: 28
	action: tensor([[ 6.1800, -0.3270, -3.1164,  5.1803, -2.9875,  0.6440, -0.7921]],
       dtype=torch.float64)
	q_value: tensor([[-58.1180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 29
	action: tensor([[ 2.4575,  0.1505, -1.1580,  1.7822, -1.3068,  0.1813, -0.4286]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 30
	action: tensor([[ 2.6127, -0.6040, -0.9292,  1.5937, -0.5727,  0.4309,  0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 31
	action: tensor([[ 2.6130, -0.0614, -1.1479,  1.6276, -1.1257,  0.3789, -0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 32
	action: tensor([[ 2.3018, -0.1653, -0.7760,  1.7073, -1.0111,  0.7846, -0.4004]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8779664071232327, distance: 0.3997572898816843 entropy 0.03264415264129639
epoch: 50, step: 33
	action: tensor([[ 6.1800, -0.5496, -2.8877,  5.6945, -2.8707,  1.3292, -0.7134]],
       dtype=torch.float64)
	q_value: tensor([[-67.4103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6898291471730459 entropy 0.03264415264129639
epoch: 50, step: 34
	action: tensor([[ 2.2362, -0.3140, -1.0614,  1.3159, -0.6598,  0.4519, -0.0995]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8823265361194315, distance: 0.3925508861861228 entropy 0.03264415264129639
epoch: 50, step: 35
	action: tensor([[ 6.1800, -0.1903, -2.2204,  5.0373, -2.1375,  0.7015, -0.8367]],
       dtype=torch.float64)
	q_value: tensor([[-56.8829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 36
	action: tensor([[ 2.2703, -0.3210, -0.7437,  1.6766, -0.5716,  0.6982, -0.5185]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8960946689276601, distance: 0.3688719304063731 entropy 0.03264415264129639
epoch: 50, step: 37
	action: tensor([[ 6.1800, -0.2376, -3.1922,  6.1800, -3.4248,  1.3311, -0.7346]],
       dtype=torch.float64)
	q_value: tensor([[-61.7314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9604947697106108 entropy 0.03264415264129639
epoch: 50, step: 38
	action: tensor([[ 2.3758, -0.5696, -0.6205,  1.2361, -0.4749,  0.2449, -0.2182]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8200366775310959, distance: 0.48545468254045193 entropy 0.03264415264129639
epoch: 50, step: 39
	action: tensor([[ 6.1444,  0.2132, -2.4639,  5.3184, -2.3489,  0.5565, -0.7118]],
       dtype=torch.float64)
	q_value: tensor([[-48.6457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 40
	action: tensor([[ 1.8830,  0.2449, -0.6322,  1.3326, -0.3848,  0.6616, -0.1837]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.915508037227175, distance: 0.3326322801014117 entropy 0.03264415264129639
epoch: 50, step: 41
	action: tensor([[ 5.9830, -0.4433, -2.6266,  5.2915, -3.1930,  0.6905, -0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-58.9117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 42
	action: tensor([[ 2.4737,  0.0825, -0.8730,  1.5570, -0.5829,  0.2435, -0.0720]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 43
	action: tensor([[ 2.1888, -0.1134, -0.9288,  1.4986, -1.1151,  0.3096, -0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9525846148325053, distance: 0.24918180916628468 entropy 0.03264415264129639
epoch: 50, step: 44
	action: tensor([[ 6.1800, -0.3293, -2.5124,  4.9100, -2.5065,  0.8719, -0.6955]],
       dtype=torch.float64)
	q_value: tensor([[-62.1414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 45
	action: tensor([[ 2.3669, -0.0850, -0.9398,  1.3256, -0.9732,  0.2315, -0.7622]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9569988499683046, distance: 0.2372994483249894 entropy 0.03264415264129639
epoch: 50, step: 46
	action: tensor([[ 5.9716, -0.4477, -2.9562,  5.4025, -3.2263,  0.9790, -0.7112]],
       dtype=torch.float64)
	q_value: tensor([[-68.5054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9139995685437599 entropy 0.03264415264129639
epoch: 50, step: 47
	action: tensor([[ 2.1250, -0.1104, -0.1926,  1.5124, -0.8771, -0.0695,  0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8300913580625899, distance: 0.4716984265693328 entropy 0.03264415264129639
epoch: 50, step: 48
	action: tensor([[ 6.1800, -0.4177, -2.1353,  4.9983, -2.7573,  0.8407, -0.6916]],
       dtype=torch.float64)
	q_value: tensor([[-50.6398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 49
	action: tensor([[ 2.5926, -0.1429, -0.9726,  1.9468, -0.8840,  0.4992, -0.6583]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 50
	action: tensor([[ 2.6094, -0.8949, -0.8972,  1.5499, -1.1525,  0.3605,  0.4390]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 51
	action: tensor([[ 2.5088,  0.0036, -1.1992,  1.5342, -1.2224,  0.2248, -0.1522]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 52
	action: tensor([[ 2.3468,  0.0157, -0.6195,  2.1025, -1.0529,  0.4778, -0.5361]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7771396763772525, distance: 0.5402230016802841 entropy 0.03264415264129639
epoch: 50, step: 53
	action: tensor([[ 6.1800, -0.5890, -3.3907,  5.8927, -3.3451,  0.9263, -1.1445]],
       dtype=torch.float64)
	q_value: tensor([[-70.0677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1037032180742852 entropy 0.03264415264129639
epoch: 50, step: 54
	action: tensor([[ 2.2225,  0.0743, -0.6978,  2.2191, -0.8079,  0.6469, -0.0700]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6352951648313399, distance: 0.6910786144091927 entropy 0.03264415264129639
epoch: 50, step: 55
	action: tensor([[ 6.0485, -0.4139, -3.2322,  6.1554, -3.2738,  1.2894, -1.1746]],
       dtype=torch.float64)
	q_value: tensor([[-65.2478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0958557137434541 entropy 0.03264415264129639
epoch: 50, step: 56
	action: tensor([[ 2.3128,  0.0270, -0.5528,  1.3823, -1.0859,  0.3475, -0.2164]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9791014255759438, distance: 0.16543028743904836 entropy 0.03264415264129639
epoch: 50, step: 57
	action: tensor([[ 6.1800, -0.2096, -2.5059,  5.1715, -2.4063,  0.8690, -0.3751]],
       dtype=torch.float64)
	q_value: tensor([[-60.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 58
	action: tensor([[ 2.1329,  0.2663, -0.8245,  1.4126, -0.5679,  0.5330, -0.4055]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.940528855924824, distance: 0.2790678714836718 entropy 0.03264415264129639
epoch: 50, step: 59
	action: tensor([[ 6.1800, -0.1780, -3.0546,  5.1461, -2.4909,  1.1333, -1.1318]],
       dtype=torch.float64)
	q_value: tensor([[-65.1674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 60
	action: tensor([[ 2.3561,  0.0863, -1.0818,  1.6556, -0.4471,  0.0466, -0.1504]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.947254718760071, distance: 0.2628140019609456 entropy 0.03264415264129639
epoch: 50, step: 61
	action: tensor([[ 6.1800, -0.3407, -2.7758,  5.5728, -2.5538,  0.5592, -1.0486]],
       dtype=torch.float64)
	q_value: tensor([[-60.3600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7320642086817789 entropy 0.03264415264129639
epoch: 50, step: 62
	action: tensor([[ 2.3623, -0.1767, -0.8719,  1.9021, -0.8433,  0.5918, -0.6273]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.834037081380428, distance: 0.4661892181795617 entropy 0.03264415264129639
epoch: 50, step: 63
	action: tensor([[ 6.1800, -0.4125, -3.0677,  6.0025, -3.1336,  1.1130, -1.1722]],
       dtype=torch.float64)
	q_value: tensor([[-69.3756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9081827359444473 entropy 0.03264415264129639
epoch: 50, step: 64
	action: tensor([[ 2.4983, -0.1151, -0.4476,  1.3057, -0.6943,  0.1545, -0.3279]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 65
	action: tensor([[ 2.0896, -0.0027, -1.1437,  1.6350, -0.8352,  0.0924, -0.5324]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9283977573666216, distance: 0.30621044860157515 entropy 0.03264415264129639
epoch: 50, step: 66
	action: tensor([[ 6.1800, -0.0424, -3.0028,  5.6119, -3.1196,  0.7011, -1.0429]],
       dtype=torch.float64)
	q_value: tensor([[-68.0612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6862514954488091 entropy 0.03264415264129639
epoch: 50, step: 67
	action: tensor([[ 2.4022, -0.0927, -0.6733,  2.0250, -0.6496,  0.5200, -0.7172]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 68
	action: tensor([[ 2.0468, -0.2454, -0.8388,  1.3498, -0.5345,  0.5448,  0.1976]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9490708925335132, distance: 0.25824964065209227 entropy 0.03264415264129639
epoch: 50, step: 69
	action: tensor([[ 6.1800,  0.2433, -2.6312,  4.8089, -2.9286,  0.5005, -0.3843]],
       dtype=torch.float64)
	q_value: tensor([[-51.5030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 70
	action: tensor([[ 2.6608, -0.5536, -0.6585,  1.4472, -0.8059,  0.4119, -0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 71
	action: tensor([[ 1.9775, -0.1694, -0.6824,  1.2684, -0.4732,  0.2999, -0.7558]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9569514131355112, distance: 0.23743030100545218 entropy 0.03264415264129639
epoch: 50, step: 72
	action: tensor([[ 6.1739, -0.3433, -2.7139,  5.9436, -2.8845,  1.1444, -0.9777]],
       dtype=torch.float64)
	q_value: tensor([[-59.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8002641392595907 entropy 0.03264415264129639
epoch: 50, step: 73
	action: tensor([[ 2.2016,  0.3687, -1.0321,  1.5681, -1.2054, -0.0786, -0.2478]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9623012800343607, distance: 0.2221876908297675 entropy 0.03264415264129639
epoch: 50, step: 74
	action: tensor([[ 5.7311, -0.5189, -2.5024,  5.2325, -2.5647,  0.8768, -0.4964]],
       dtype=torch.float64)
	q_value: tensor([[-69.5206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 75
	action: tensor([[ 2.2452,  0.3949, -0.2529,  1.6767, -1.1066,  0.4273, -0.1590]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7735528182677627, distance: 0.5445529978839659 entropy 0.03264415264129639
epoch: 50, step: 76
	action: tensor([[ 6.1374, -0.0789, -2.6187,  5.1159, -2.8758,  1.3069, -0.8069]],
       dtype=torch.float64)
	q_value: tensor([[-63.3706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 77
	action: tensor([[ 2.5810,  0.0921, -0.6875,  1.7778, -0.7446,  0.1503, -0.2251]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 78
	action: tensor([[ 2.3304, -0.2256, -0.9391,  1.5268, -0.7991,  0.1331, -0.2203]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9547429493526631, distance: 0.24344441595164132 entropy 0.03264415264129639
epoch: 50, step: 79
	action: tensor([[ 6.1800, -0.2466, -2.5995,  5.4733, -2.7821,  0.6383, -0.8550]],
       dtype=torch.float64)
	q_value: tensor([[-58.7349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6158059648834238 entropy 0.03264415264129639
epoch: 50, step: 80
	action: tensor([[ 2.5463, -0.2148, -1.3226,  1.8348, -0.7084,  0.1295, -0.0933]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 81
	action: tensor([[ 2.3313, -0.4141, -1.0528,  1.3734, -0.9499,  0.4801,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8405903808870634, distance: 0.45689242241484557 entropy 0.03264415264129639
epoch: 50, step: 82
	action: tensor([[ 6.1800, -0.2716, -2.2912,  5.1651, -2.7640,  1.0227, -1.3640]],
       dtype=torch.float64)
	q_value: tensor([[-57.3650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 83
	action: tensor([[ 2.4779, -0.2756, -0.7899,  1.5385, -0.8781,  0.3572, -0.4309]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 84
	action: tensor([[ 2.0245,  0.1046, -0.5198,  1.9537, -0.5054,  0.2322, -0.0750]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7907176067138384, distance: 0.5235076589685342 entropy 0.03264415264129639
epoch: 50, step: 85
	action: tensor([[ 6.1800, -0.0672, -2.7691,  6.0966, -3.0229,  1.1742, -0.6619]],
       dtype=torch.float64)
	q_value: tensor([[-57.7283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7135081772670452 entropy 0.03264415264129639
epoch: 50, step: 86
	action: tensor([[ 2.1994, -0.1424, -1.0554,  1.3839, -1.1138,  0.5153, -0.2311]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.890702139145173, distance: 0.37832280790522094 entropy 0.03264415264129639
epoch: 50, step: 87
	action: tensor([[ 6.1800, -0.5280, -2.5666,  5.4241, -2.7343,  0.5902, -0.8185]],
       dtype=torch.float64)
	q_value: tensor([[-65.8395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6918360067677759 entropy 0.03264415264129639
epoch: 50, step: 88
	action: tensor([[ 2.2897,  0.0859, -0.7372,  1.7999, -0.8712,  0.3962, -0.2079]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8779455451576521, distance: 0.3997914582046499 entropy 0.03264415264129639
epoch: 50, step: 89
	action: tensor([[ 6.1800, -0.4421, -2.7641,  5.2668, -2.9441,  0.6547, -1.0815]],
       dtype=torch.float64)
	q_value: tensor([[-63.8800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 90
	action: tensor([[ 2.3832, -0.2289, -1.0207,  1.5161, -0.4957, -0.1480, -0.5029]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9487385249649087, distance: 0.25909094953114814 entropy 0.03264415264129639
epoch: 50, step: 91
	action: tensor([[ 5.5767, -0.8635, -2.9811,  5.9375, -2.9217,  0.4385, -0.9269]],
       dtype=torch.float64)
	q_value: tensor([[-59.2724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5861358014680031 entropy 0.03264415264129639
epoch: 50, step: 92
	action: tensor([[ 1.9701,  0.0493, -0.8230,  1.2167, -0.6926,  0.7693, -0.3521]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9607234434539554, distance: 0.22678973624506046 entropy 0.03264415264129639
epoch: 50, step: 93
	action: tensor([[ 6.1800, -0.3961, -2.4885,  5.0046, -2.5166,  1.0838, -0.7954]],
       dtype=torch.float64)
	q_value: tensor([[-63.8650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 94
	action: tensor([[ 2.3186,  0.1136, -0.6967,  1.0164, -0.6458,  0.6084, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9777763623585995, distance: 0.17059420208270384 entropy 0.03264415264129639
epoch: 50, step: 95
	action: tensor([[ 6.1087, -0.1793, -2.3430,  4.6903, -2.9881,  0.4321, -0.2715]],
       dtype=torch.float64)
	q_value: tensor([[-55.9235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 96
	action: tensor([[ 2.0936, -0.0556, -0.9947,  1.6902, -0.8997, -0.1453, -0.3784]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9656001437092617, distance: 0.21224379744648958 entropy 0.03264415264129639
epoch: 50, step: 97
	action: tensor([[ 6.1800, -0.2509, -3.2939,  5.2141, -2.6666,  0.6916, -0.8072]],
       dtype=torch.float64)
	q_value: tensor([[-63.5889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 98
	action: tensor([[ 2.5186, -0.2412, -0.8525,  1.4937, -0.6964, -0.0110, -0.5419]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 99
	action: tensor([[ 2.1776,  0.1671, -1.0126,  1.3309, -1.1135,  0.3264, -0.1653]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9729153006180609, distance: 0.18832965164569945 entropy 0.03264415264129639
epoch: 50, step: 100
	action: tensor([[ 6.1552, -0.2283, -2.5559,  4.8414, -2.0881,  0.5996, -0.6613]],
       dtype=torch.float64)
	q_value: tensor([[-66.3625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 101
	action: tensor([[ 2.2185, -0.0591, -1.0594,  2.1892, -0.8756,  0.3437,  0.2082]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7044091655515183, distance: 0.6221601276166248 entropy 0.03264415264129639
epoch: 50, step: 102
	action: tensor([[ 6.0094, -0.4785, -2.4205,  5.7903, -2.9205,  1.1680, -0.9135]],
       dtype=torch.float64)
	q_value: tensor([[-62.8709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8046067978053231 entropy 0.03264415264129639
epoch: 50, step: 103
	action: tensor([[ 2.0654, -0.0825, -1.0677,  1.6775, -1.1417,  0.1534, -0.5213]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9247190213731469, distance: 0.3139780741801959 entropy 0.03264415264129639
epoch: 50, step: 104
	action: tensor([[ 6.0442, -0.7579, -2.7917,  5.7245, -3.2277,  0.6677, -0.9764]],
       dtype=torch.float64)
	q_value: tensor([[-69.7450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0629778682448914 entropy 0.03264415264129639
epoch: 50, step: 105
	action: tensor([[ 2.4842, -0.0996, -0.8016,  0.9969, -0.9122,  0.2238, -0.4769]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 106
	action: tensor([[ 2.6329,  0.0525, -0.9231,  1.0316, -0.7417,  0.5042, -0.5107]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 107
	action: tensor([[ 2.3071, -0.1722, -1.1892,  1.6285, -0.3287,  0.7220,  0.1047]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.833300886605268, distance: 0.4672220582394595 entropy 0.03264415264129639
epoch: 50, step: 108
	action: tensor([[ 6.1800,  0.0458, -2.9052,  5.5046, -3.0337,  0.9699, -0.8780]],
       dtype=torch.float64)
	q_value: tensor([[-58.0812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5513576422399509 entropy 0.03264415264129639
epoch: 50, step: 109
	action: tensor([[ 2.0266, -0.2825, -1.1706,  1.5452, -0.8173,  0.6342, -0.0851]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8104895462951892, distance: 0.4981650771632385 entropy 0.03264415264129639
epoch: 50, step: 110
	action: tensor([[ 6.1348, -0.4337, -2.7772,  5.3989, -2.7234,  0.8537, -0.9175]],
       dtype=torch.float64)
	q_value: tensor([[-62.3990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6337667210998484 entropy 0.03264415264129639
epoch: 50, step: 111
	action: tensor([[ 2.0199,  0.0272, -0.5851,  1.5157, -0.7515,  0.2005,  0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9578902171510101, distance: 0.23482708885542022 entropy 0.03264415264129639
epoch: 50, step: 112
	action: tensor([[ 5.8888, -0.5891, -2.4822,  4.6181, -2.6520,  1.0036, -0.7599]],
       dtype=torch.float64)
	q_value: tensor([[-54.8675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 113
	action: tensor([[ 2.2269, -0.2074, -0.4005,  1.8963, -0.6530,  0.3970, -0.1308]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8705805630192454, distance: 0.41167685050475694 entropy 0.03264415264129639
epoch: 50, step: 114
	action: tensor([[ 6.1800, -0.4018, -3.1012,  5.8453, -2.9293,  0.8865, -0.7924]],
       dtype=torch.float64)
	q_value: tensor([[-55.9608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8543450433499472 entropy 0.03264415264129639
epoch: 50, step: 115
	action: tensor([[ 2.3967,  0.1664, -0.5089,  1.8185, -0.5360,  0.0095,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8591734521089229, distance: 0.42943650537020905 entropy 0.03264415264129639
epoch: 50, step: 116
	action: tensor([[ 6.1697, -0.5556, -2.8745,  5.6388, -2.5499,  0.6742, -0.7382]],
       dtype=torch.float64)
	q_value: tensor([[-55.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9405782744385659 entropy 0.03264415264129639
epoch: 50, step: 117
	action: tensor([[ 2.3861, -0.0713, -0.9371,  1.7816, -0.4652,  0.0669, -0.0232]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9369506262534412, distance: 0.287340657947274 entropy 0.03264415264129639
epoch: 50, step: 118
	action: tensor([[ 6.0213, -0.1375, -2.8171,  5.3100, -2.4563,  0.9501, -0.3694]],
       dtype=torch.float64)
	q_value: tensor([[-56.9223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 119
	action: tensor([[ 2.3957, -0.2909, -0.5859,  1.6972, -0.4062,  0.1846, -0.2927]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9467453685192097, distance: 0.2640799232717964 entropy 0.03264415264129639
epoch: 50, step: 120
	action: tensor([[ 6.1800, -0.0890, -2.6436,  5.9672, -2.8097,  0.4346, -0.8296]],
       dtype=torch.float64)
	q_value: tensor([[-54.3067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8209828389005978 entropy 0.03264415264129639
epoch: 50, step: 121
	action: tensor([[ 2.5697, -0.0772, -0.9002,  1.8030, -0.9284,  0.3741, -0.1870]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 122
	action: tensor([[ 2.5486,  0.1465, -1.2340,  1.4891, -0.6696,  1.1226, -0.1605]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 123
	action: tensor([[ 2.4669e+00, -1.6507e-01, -7.8116e-01,  1.3138e+00, -6.1200e-01,
          1.5865e-03, -3.0633e-01]], dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 124
	action: tensor([[ 2.2795,  0.0488, -1.3769,  1.4575, -0.9433, -0.2379, -0.5906]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9298665906650087, distance: 0.3030534057091753 entropy 0.03264415264129639
epoch: 50, step: 125
	action: tensor([[ 6.1800, -0.0394, -3.0045,  5.0875, -2.8807,  0.8932, -0.4991]],
       dtype=torch.float64)
	q_value: tensor([[-70.0194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 126
	action: tensor([[ 2.6594, -0.3811, -1.4277,  1.3640, -0.8583,  0.5564, -0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 50, step: 127
	action: tensor([[ 2.4370,  0.1762, -0.3458,  1.3532, -0.9602, -0.1179, -0.2668]],
       dtype=torch.float64)
	q_value: tensor([[-56.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 50 actor 690.783354885486 critic 94.9288441259261 
epoch: 51, step: 0
	action: tensor([[ 1.3328e+00, -1.9554e-04, -9.1080e-01,  5.5783e-01, -5.2704e-02,
          2.8914e-01, -5.5584e-01]], dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9191606736901214, distance: 0.32536290428995335 entropy 0.03264415264129639
epoch: 51, step: 1
	action: tensor([[ 3.1617, -0.1570, -1.6021,  2.0819, -1.3358,  0.6058, -0.2590]],
       dtype=torch.float64)
	q_value: tensor([[-46.6186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 2
	action: tensor([[ 1.8929, -0.1841, -0.7918,  1.0236,  0.1627,  0.0395,  0.3526]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8516422162618225, distance: 0.44076983167675476 entropy 0.03264415264129639
epoch: 51, step: 3
	action: tensor([[ 3.3555, -0.0129, -1.5273,  1.9339, -1.0913,  0.1711, -0.1534]],
       dtype=torch.float64)
	q_value: tensor([[-39.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 4
	action: tensor([[ 1.5165, -0.0023, -0.9152,  0.6651, -0.5098, -0.0381, -0.2052]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8436607648424249, distance: 0.4524709326381394 entropy 0.03264415264129639
epoch: 51, step: 5
	action: tensor([[ 3.1349, -0.2852, -1.3192,  2.2223, -0.9419,  0.2672, -0.1404]],
       dtype=torch.float64)
	q_value: tensor([[-48.3713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 6
	action: tensor([[ 1.1390,  0.1594, -0.8861,  1.3925, -0.8749, -0.0181, -0.0992]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8975641609582552, distance: 0.3662542372388214 entropy 0.03264415264129639
epoch: 51, step: 7
	action: tensor([[ 3.2454, -0.3256, -1.3835,  2.0135, -1.1215,  0.3173,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-48.5505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 8
	action: tensor([[ 1.3295,  0.0671, -0.7503,  0.5650, -0.3623,  0.0620, -0.4203]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9041619840957571, distance: 0.3542628403538892 entropy 0.03264415264129639
epoch: 51, step: 9
	action: tensor([[ 3.2404, -0.3310, -1.3520,  2.0515, -1.0476,  0.4651,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[-45.2159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 10
	action: tensor([[ 1.4874e+00, -9.7248e-03, -5.6877e-01,  1.3624e+00, -1.2552e-01,
          1.4093e-03, -1.8925e-01]], dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9830852996555156, distance: 0.14882943993597333 entropy 0.03264415264129639
epoch: 51, step: 11
	action: tensor([[ 3.7427, -0.5563, -1.4003,  2.3032, -1.2210,  0.5100, -0.4965]],
       dtype=torch.float64)
	q_value: tensor([[-45.4917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 12
	action: tensor([[ 1.4298,  0.5822, -0.8009,  1.0496, -0.2408, -0.0290, -0.3360]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9468403056216117, distance: 0.26384443045633854 entropy 0.03264415264129639
epoch: 51, step: 13
	action: tensor([[ 3.6862, -0.7033, -1.3824,  1.6749, -1.2902,  0.4561, -0.0868]],
       dtype=torch.float64)
	q_value: tensor([[-53.5491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 14
	action: tensor([[ 1.4166,  0.0437, -0.7371,  1.1143, -0.5870,  0.9357, -0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9511064818867324, distance: 0.25303601374023893 entropy 0.03264415264129639
epoch: 51, step: 15
	action: tensor([[ 3.8621, -0.4566, -1.2897,  2.0805, -1.2547,  0.0267, -0.0774]],
       dtype=torch.float64)
	q_value: tensor([[-52.7471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 16
	action: tensor([[ 1.5104,  0.1757, -0.8110,  1.1807,  0.0509, -0.1667, -0.2028]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9906050058036246, distance: 0.11091874609274924 entropy 0.03264415264129639
epoch: 51, step: 17
	action: tensor([[ 3.5481, -0.4502, -1.4120,  2.3535, -0.7542,  0.4283,  0.2088]],
       dtype=torch.float64)
	q_value: tensor([[-47.2296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 18
	action: tensor([[ 1.8222,  0.1728, -1.1902,  0.6548, -0.4150,  0.3649, -0.2753]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8917913646861636, distance: 0.3764329691228722 entropy 0.03264415264129639
epoch: 51, step: 19
	action: tensor([[ 3.6715, -0.1467, -0.9338,  2.1745, -0.6968,  0.6413, -0.2736]],
       dtype=torch.float64)
	q_value: tensor([[-57.3729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 20
	action: tensor([[ 1.8432,  0.4451, -0.5807,  0.9178,  0.0048,  0.1665, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9127503504944902, distance: 0.3380169967571364 entropy 0.03264415264129639
epoch: 51, step: 21
	action: tensor([[ 3.5523, -0.2498, -1.7431,  2.0647, -0.9914, -0.2087, -0.7071]],
       dtype=torch.float64)
	q_value: tensor([[-48.2269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 22
	action: tensor([[ 1.4308,  0.0413, -0.8765,  0.6082, -0.2107,  0.2483, -0.2964]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9093698562369306, distance: 0.3445030154976266 entropy 0.03264415264129639
epoch: 51, step: 23
	action: tensor([[ 3.3225, -0.4383, -1.4675,  2.3481, -0.8762,  0.3706, -0.3579]],
       dtype=torch.float64)
	q_value: tensor([[-46.6238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 24
	action: tensor([[ 1.6309, -0.2661, -0.2506,  1.0472, -0.8259,  0.2838,  0.0417]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8194945293136399, distance: 0.4861853605148701 entropy 0.03264415264129639
epoch: 51, step: 25
	action: tensor([[ 3.6712, -0.4260, -1.4666,  2.2109, -0.6117,  0.4102, -0.7262]],
       dtype=torch.float64)
	q_value: tensor([[-44.7971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 26
	action: tensor([[ 1.6200, -0.6417, -1.3504,  1.0906, -0.6071,  0.3454, -0.1367]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5649364813284772, distance: 0.7548019739531877 entropy 0.03264415264129639
epoch: 51, step: 27
	action: tensor([[ 4.0516, -0.3075, -1.4000,  2.2351, -1.0292,  0.2420, -0.6398]],
       dtype=torch.float64)
	q_value: tensor([[-51.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 28
	action: tensor([[ 1.4253, -0.0969, -0.7547,  1.0607, -0.6647,  0.5050,  0.0395]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9748576813986402, distance: 0.18145099581760615 entropy 0.03264415264129639
epoch: 51, step: 29
	action: tensor([[ 3.4016, -0.1437, -1.6194,  2.1528, -0.9021,  0.4313, -0.2319]],
       dtype=torch.float64)
	q_value: tensor([[-47.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 30
	action: tensor([[ 1.9518,  0.3446, -0.6397,  1.1376, -0.2842,  0.3073, -0.7685]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9505718840651649, distance: 0.25441559072740066 entropy 0.03264415264129639
epoch: 51, step: 31
	action: tensor([[ 4.2262, -0.0456, -1.6942,  3.3270, -0.7709,  0.4117, -0.1047]],
       dtype=torch.float64)
	q_value: tensor([[-59.8809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 32
	action: tensor([[ 1.5772, -0.0981, -0.5735,  0.7415, -0.3978,  0.4389,  0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8679089818169944, distance: 0.41590422976184216 entropy 0.03264415264129639
epoch: 51, step: 33
	action: tensor([[ 3.1260, -0.2162, -1.3970,  2.1792, -1.0571,  0.1624, -0.3295]],
       dtype=torch.float64)
	q_value: tensor([[-43.6903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 34
	action: tensor([[ 1.9245e+00, -9.4608e-04, -1.0072e+00,  1.2886e+00, -9.6608e-01,
          1.2345e-01, -3.9167e-01]], dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9649900808285617, distance: 0.2141175410819771 entropy 0.03264415264129639
epoch: 51, step: 35
	action: tensor([[ 4.3054, -0.2802, -1.6230,  2.2630, -1.5235,  0.0380, -0.5307]],
       dtype=torch.float64)
	q_value: tensor([[-60.8133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 36
	action: tensor([[ 2.0495, -0.0558, -0.9952,  1.1240, -0.6434,  0.2454, -0.3760]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9368587712456706, distance: 0.287549891368079 entropy 0.03264415264129639
epoch: 51, step: 37
	action: tensor([[ 3.7491, -0.6349, -1.7311,  2.4708, -0.9675,  0.5287, -0.4389]],
       dtype=torch.float64)
	q_value: tensor([[-56.8259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 38
	action: tensor([[ 1.3128, -0.1155, -0.7426,  0.7937, -0.1295,  0.2200, -0.5269]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9419194673932398, distance: 0.27578585554627294 entropy 0.03264415264129639
epoch: 51, step: 39
	action: tensor([[ 3.8740, -0.3320, -1.2338,  1.9679, -0.6633,  0.2900, -0.2117]],
       dtype=torch.float64)
	q_value: tensor([[-44.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 40
	action: tensor([[ 1.8305,  0.0896, -1.2180,  0.6421, -0.2154,  0.5107, -0.2176]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.879379454827715, distance: 0.397436122231027 entropy 0.03264415264129639
epoch: 51, step: 41
	action: tensor([[ 3.7052, -0.2664, -1.6553,  1.8527, -1.3623,  0.4047, -0.1630]],
       dtype=torch.float64)
	q_value: tensor([[-55.2090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 42
	action: tensor([[ 0.9642, -0.3453, -0.2903,  1.4113, -0.0308, -0.4263,  0.3285]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9698635386710281, distance: 0.19865651890659355 entropy 0.03264415264129639
epoch: 51, step: 43
	action: tensor([[ 2.9882, -0.8813, -0.9895,  1.8214, -0.5273,  0.2666, -0.3806]],
       dtype=torch.float64)
	q_value: tensor([[-26.7564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 44
	action: tensor([[ 1.4043, -0.2471, -0.8693,  0.6949,  0.0025,  0.1352, -0.2239]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7937875716041216, distance: 0.5196538049477932 entropy 0.03264415264129639
epoch: 51, step: 45
	action: tensor([[ 3.7248, -0.2198, -1.6371,  2.6819, -1.0466,  0.4584,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[-40.3522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 46
	action: tensor([[ 1.5280, -0.0390, -1.0745,  1.1917, -0.7218,  0.2407, -0.1805]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.946313865303642, distance: 0.26514763735242414 entropy 0.03264415264129639
epoch: 51, step: 47
	action: tensor([[ 4.1735, -0.2507, -1.4736,  2.1591, -0.4200,  0.1602, -0.2328]],
       dtype=torch.float64)
	q_value: tensor([[-55.2084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 48
	action: tensor([[ 1.3357, -0.2110, -0.8637,  0.4760, -0.4221, -0.0488, -0.4685]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7054624248983719, distance: 0.6210506873384041 entropy 0.03264415264129639
epoch: 51, step: 49
	action: tensor([[ 3.1704, -0.0644, -1.3253,  2.0029, -1.0668,  0.4632, -0.1118]],
       dtype=torch.float64)
	q_value: tensor([[-43.4120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 50
	action: tensor([[ 1.7121, -0.2126, -0.7619,  0.9468, -0.0758,  0.0203,  0.2177]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7997692031783762, distance: 0.5120615082750352 entropy 0.03264415264129639
epoch: 51, step: 51
	action: tensor([[ 3.3267,  0.0760, -1.1865,  1.9953, -1.0043,  0.0769, -0.3266]],
       dtype=torch.float64)
	q_value: tensor([[-40.6593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 52
	action: tensor([[ 1.5131, -0.2848, -0.2825,  0.7692, -0.4787, -0.1911, -0.0391]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6470111681998494, distance: 0.6798876865878546 entropy 0.03264415264129639
epoch: 51, step: 53
	action: tensor([[ 3.2309, -0.2408, -1.0471,  1.7523, -0.9835,  0.5331, -0.1100]],
       dtype=torch.float64)
	q_value: tensor([[-38.2056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 54
	action: tensor([[ 1.2047,  0.1540, -0.6385,  0.5284, -0.6308,  0.3086,  0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9735524929080677, distance: 0.18610115324227913 entropy 0.03264415264129639
epoch: 51, step: 55
	action: tensor([[ 2.3566, -0.2852, -1.1560,  1.7722, -1.2083, -0.0625,  0.0751]],
       dtype=torch.float64)
	q_value: tensor([[-38.5661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 56
	action: tensor([[ 1.4814, -0.3693, -0.9854,  0.9483, -0.1245,  0.1416, -0.2252]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8000815850557621, distance: 0.5116619164669483 entropy 0.03264415264129639
epoch: 51, step: 57
	action: tensor([[ 4.0873, -0.2669, -1.4513,  2.3286, -1.0116,  0.6276, -0.2327]],
       dtype=torch.float64)
	q_value: tensor([[-43.9386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 58
	action: tensor([[ 1.3060,  0.2858, -0.2753,  1.0993, -0.3292,  0.0797, -0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9287246900525775, distance: 0.30551057708168616 entropy 0.03264415264129639
epoch: 51, step: 59
	action: tensor([[ 3.1049, -0.3883, -1.4288,  1.7001, -1.1055, -0.1459, -0.0369]],
       dtype=torch.float64)
	q_value: tensor([[-42.8433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 60
	action: tensor([[ 2.1201,  0.3435, -0.6889,  0.9974, -0.4086, -0.0999, -0.0854]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.969634916594047, distance: 0.19940862205185494 entropy 0.03264415264129639
epoch: 51, step: 61
	action: tensor([[ 3.3168, -0.3138, -1.4608,  2.0558, -1.5841,  0.4275,  0.1697]],
       dtype=torch.float64)
	q_value: tensor([[-50.9618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 62
	action: tensor([[ 1.6323, -0.0573, -0.6315,  0.9980, -0.5650, -0.0849, -0.4367]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8794218258156161, distance: 0.3973663114031566 entropy 0.03264415264129639
epoch: 51, step: 63
	action: tensor([[ 3.9256,  0.2680, -1.4474,  2.1785, -1.3654,  0.7063,  0.1012]],
       dtype=torch.float64)
	q_value: tensor([[-51.0156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 64
	action: tensor([[ 2.0242, -0.3102, -0.3391,  1.2001, -0.6039,  0.6026,  0.2190]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9010133872179358, distance: 0.36003516844121697 entropy 0.03264415264129639
epoch: 51, step: 65
	action: tensor([[ 3.0509, -0.6588, -1.2014,  2.2421, -0.7023,  0.5503, -0.3154]],
       dtype=torch.float64)
	q_value: tensor([[-43.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 66
	action: tensor([[ 1.4700, -0.0222, -0.5677,  0.8715, -0.0612, -0.2878, -0.4069]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8682948844405176, distance: 0.41529625531027897 entropy 0.03264415264129639
epoch: 51, step: 67
	action: tensor([[ 3.9000, -0.4357, -1.5676,  2.3493, -1.2848,  0.1744, -0.1820]],
       dtype=torch.float64)
	q_value: tensor([[-43.0682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 68
	action: tensor([[ 1.5274, -0.2830, -0.6181,  1.1614, -0.2875,  0.2866, -0.4430]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9440179637662116, distance: 0.27075783873091286 entropy 0.03264415264129639
epoch: 51, step: 69
	action: tensor([[ 3.7229, -0.2150, -1.7934,  2.5762, -1.3027,  0.9263, -0.7638]],
       dtype=torch.float64)
	q_value: tensor([[-47.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 70
	action: tensor([[ 1.9910, -0.6633, -0.5973,  0.9636, -0.2347,  0.4929, -0.1573]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.712111827452351, distance: 0.6140003302374625 entropy 0.03264415264129639
epoch: 51, step: 71
	action: tensor([[ 4.0972,  0.1120, -1.8289,  2.0702, -0.9558,  0.7808, -0.5337]],
       dtype=torch.float64)
	q_value: tensor([[-41.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 72
	action: tensor([[ 1.4001,  0.2773, -1.0879,  1.0961, -0.5388,  0.3891, -0.7525]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9628914033264598, distance: 0.22044180519368042 entropy 0.03264415264129639
epoch: 51, step: 73
	action: tensor([[ 4.0771,  0.0189, -1.6892,  2.5115, -1.2157,  0.4913, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-63.0585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 74
	action: tensor([[ 1.8092, -0.3485, -0.8193,  0.9945, -0.1800,  0.1861, -0.6621]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8049880474645561, distance: 0.5053442268429952 entropy 0.03264415264129639
epoch: 51, step: 75
	action: tensor([[ 4.3994, -0.1509, -1.7101,  2.7627, -1.3220,  0.6162, -0.3061]],
       dtype=torch.float64)
	q_value: tensor([[-50.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 76
	action: tensor([[ 1.5946,  0.2481, -0.9357,  0.7738, -0.6298,  0.4106, -0.3778]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9660436248612255, distance: 0.21087124138612498 entropy 0.03264415264129639
epoch: 51, step: 77
	action: tensor([[ 3.2603, -0.4960, -1.2081,  2.1250, -0.9415,  0.2885, -0.2882]],
       dtype=torch.float64)
	q_value: tensor([[-59.0185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 78
	action: tensor([[ 1.5278, -0.2382, -0.8020,  0.6115, -0.2836,  0.0655, -0.3172]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6915201986772797, distance: 0.6355797612494636 entropy 0.03264415264129639
epoch: 51, step: 79
	action: tensor([[ 3.5734, -0.1630, -1.5696,  2.1921, -0.6184,  0.5128, -0.5596]],
       dtype=torch.float64)
	q_value: tensor([[-44.3566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 80
	action: tensor([[ 1.4323, -0.5594, -0.7854,  1.2178, -0.1139,  0.3055, -0.3083]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8709012860039789, distance: 0.41116643207938175 entropy 0.03264415264129639
epoch: 51, step: 81
	action: tensor([[ 3.7427, -0.5592, -1.9343,  2.6163, -1.1006,  0.4734,  0.2048]],
       dtype=torch.float64)
	q_value: tensor([[-42.6071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 82
	action: tensor([[ 1.5994,  0.1059, -0.8252,  1.1850, -0.2405,  0.3850, -0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9870495167722367, distance: 0.1302265934487908 entropy 0.03264415264129639
epoch: 51, step: 83
	action: tensor([[ 4.1105, -0.4194, -1.1204,  2.2934, -0.7301,  0.3915, -0.2870]],
       dtype=torch.float64)
	q_value: tensor([[-50.5436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 84
	action: tensor([[ 0.8369, -0.4660, -0.8684,  1.6991, -0.2319,  0.1761, -0.1961]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6995245754607022, distance: 0.6272796116842052 entropy 0.03264415264129639
epoch: 51, step: 85
	action: tensor([[ 4.0244, -0.1020, -1.0635,  2.2897, -0.8949,  0.2040, -0.1845]],
       dtype=torch.float64)
	q_value: tensor([[-37.2071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 86
	action: tensor([[ 1.5194, -0.2060, -0.6338,  0.7334, -0.8395,  0.1794,  0.1796]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8128742012623968, distance: 0.49502089020392187 entropy 0.03264415264129639
epoch: 51, step: 87
	action: tensor([[ 2.5837,  0.2159, -1.4938,  1.8943, -0.5226,  0.5135, -0.6603]],
       dtype=torch.float64)
	q_value: tensor([[-43.3602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 88
	action: tensor([[ 2.0217e+00,  2.9284e-04, -1.7037e-01,  8.7045e-01, -3.1104e-01,
          2.1368e-01, -4.4674e-01]], dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7592367981102883, distance: 0.5615025700887142 entropy 0.03264415264129639
epoch: 51, step: 89
	action: tensor([[ 4.3626, -0.1166, -1.9626,  2.5618, -1.2821,  0.2346, -1.2011]],
       dtype=torch.float64)
	q_value: tensor([[-46.4127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 90
	action: tensor([[ 1.3605,  0.1916, -0.4696,  0.7965, -0.3729, -0.1913, -0.5365]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.935872107734685, distance: 0.28978785231279447 entropy 0.03264415264129639
epoch: 51, step: 91
	action: tensor([[ 3.3185, -0.3161, -1.2014,  2.5042, -0.9713,  0.1369, -0.4975]],
       dtype=torch.float64)
	q_value: tensor([[-45.7546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 92
	action: tensor([[ 1.6337,  0.1419, -0.6645,  1.0312, -0.4870,  0.5006, -0.0646]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9769487919930048, distance: 0.17374148857053243 entropy 0.03264415264129639
epoch: 51, step: 93
	action: tensor([[ 4.0140, -0.4659, -1.4998,  2.3339, -1.1994,  0.4347, -0.5324]],
       dtype=torch.float64)
	q_value: tensor([[-51.9445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 94
	action: tensor([[ 1.3805, -0.0320, -0.7178,  1.1383, -0.6591,  0.3743, -0.1245]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9896279810152646, distance: 0.11654357734619734 entropy 0.03264415264129639
epoch: 51, step: 95
	action: tensor([[ 3.8350, -0.3862, -1.4513,  1.9522, -1.3784,  0.2267, -0.3974]],
       dtype=torch.float64)
	q_value: tensor([[-48.2962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 96
	action: tensor([[ 1.0734, -0.3550, -0.7310,  0.9392, -0.2628,  0.1117, -0.1319]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8620319647302318, distance: 0.42505578682730033 entropy 0.03264415264129639
epoch: 51, step: 97
	action: tensor([[ 3.2458, -0.0264, -1.4421,  2.2376, -0.6567,  0.4170,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[-34.2714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 98
	action: tensor([[ 1.5312,  0.0151, -0.6851,  0.8397,  0.2834,  0.4007, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9498812334221606, distance: 0.2561868775720188 entropy 0.03264415264129639
epoch: 51, step: 99
	action: tensor([[ 3.7303, -0.0979, -1.1652,  2.5288, -1.0055,  0.0547, -0.7376]],
       dtype=torch.float64)
	q_value: tensor([[-42.0454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 100
	action: tensor([[ 1.7030,  0.3273, -1.0668,  0.4977,  0.0472,  0.0883,  0.0480]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8276100629893682, distance: 0.47513021426854957 entropy 0.03264415264129639
epoch: 51, step: 101
	action: tensor([[ 3.1130,  0.1110, -1.5418,  1.7875, -0.7886,  0.4110, -0.5544]],
       dtype=torch.float64)
	q_value: tensor([[-48.9333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 102
	action: tensor([[ 1.7514, -0.3280, -0.4997,  1.1989,  0.1450, -0.0688, -0.4280]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8545878892293413, distance: 0.4363721069704665 entropy 0.03264415264129639
epoch: 51, step: 103
	action: tensor([[ 3.9171, -0.2699, -1.6651,  2.8492, -1.1165,  0.2869, -0.0688]],
       dtype=torch.float64)
	q_value: tensor([[-43.6781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 104
	action: tensor([[ 1.7211, -0.2001, -0.9079,  0.9190, -0.3165,  0.3989, -0.8166]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8678594499379193, distance: 0.41598220094962607 entropy 0.03264415264129639
epoch: 51, step: 105
	action: tensor([[ 4.5173,  0.1869, -1.4563,  2.8903, -1.0436,  0.6120, -0.3865]],
       dtype=torch.float64)
	q_value: tensor([[-57.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 106
	action: tensor([[ 1.1789, -0.0810, -1.1076,  1.1321, -0.2139, -0.3329, -0.1295]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9099771997593079, distance: 0.3433467589869452 entropy 0.03264415264129639
epoch: 51, step: 107
	action: tensor([[ 3.5896, -0.0680, -1.7605,  2.2514, -0.5057,  0.0610, -0.2341]],
       dtype=torch.float64)
	q_value: tensor([[-42.0070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 108
	action: tensor([[ 1.5410, -0.0272, -0.5784,  0.6597, -0.5325, -0.0334, -0.1272]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.777129101387344, distance: 0.5402358186432351 entropy 0.03264415264129639
epoch: 51, step: 109
	action: tensor([[ 3.4490, -0.4142, -1.4908,  1.7276, -0.4847,  0.5004, -0.4127]],
       dtype=torch.float64)
	q_value: tensor([[-44.6784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 110
	action: tensor([[ 1.4323, -0.3823, -1.1068,  1.0629,  0.2684,  0.2221,  0.5142]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.819674679655862, distance: 0.48594268553334674 entropy 0.03264415264129639
epoch: 51, step: 111
	action: tensor([[ 3.8299, -0.3087, -1.1793,  2.2326, -1.0413,  0.2615, -0.4534]],
       dtype=torch.float64)
	q_value: tensor([[-36.1703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 112
	action: tensor([[ 1.3531,  0.4692, -0.7338,  0.9897, -0.1682,  0.3830,  0.5780]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.93947049015392, distance: 0.28154010732139073 entropy 0.03264415264129639
epoch: 51, step: 113
	action: tensor([[ 2.9014, -0.5135, -1.0905,  1.7775, -0.7371,  0.4225, -0.4441]],
       dtype=torch.float64)
	q_value: tensor([[-42.5462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 114
	action: tensor([[ 1.6037, -0.3456, -0.7568,  1.0622, -0.3179, -0.0343, -0.0306]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8009626977158218, distance: 0.51053313192071 entropy 0.03264415264129639
epoch: 51, step: 115
	action: tensor([[ 3.9276, -0.1504, -1.4756,  2.3765, -0.6986,  0.2369, -0.4139]],
       dtype=torch.float64)
	q_value: tensor([[-43.7648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 116
	action: tensor([[ 1.4601, -0.4522, -0.8209,  0.5977,  0.0797,  0.2748, -0.3015]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6215440954989366, distance: 0.7039865080172313 entropy 0.03264415264129639
epoch: 51, step: 117
	action: tensor([[ 3.5249, -0.1155, -1.6269,  2.4511, -0.6962,  0.5707, -0.5012]],
       dtype=torch.float64)
	q_value: tensor([[-39.1981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 118
	action: tensor([[ 1.3910, -0.1440, -0.6009,  0.8342, -0.0835,  0.4295, -0.3410]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9479538462286964, distance: 0.2610664198765143 entropy 0.03264415264129639
epoch: 51, step: 119
	action: tensor([[ 3.9572, -0.5849, -1.4385,  2.1436, -0.7944,  0.1242, -0.2562]],
       dtype=torch.float64)
	q_value: tensor([[-42.7189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 120
	action: tensor([[ 1.8420,  0.0607, -0.1797,  1.0346, -0.3891, -0.4142, -0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.739402721075425, distance: 0.5841732034100643 entropy 0.03264415264129639
epoch: 51, step: 121
	action: tensor([[ 3.5906, -0.3399, -1.1155,  2.2403, -0.6602,  0.3743, -0.5361]],
       dtype=torch.float64)
	q_value: tensor([[-44.4806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 122
	action: tensor([[ 0.9531, -0.2954, -0.5082,  0.5056, -0.3002, -0.2258, -0.1054]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7178495205996718, distance: 0.6078509356419464 entropy 0.03264415264129639
epoch: 51, step: 123
	action: tensor([[ 2.4383,  0.0668, -0.9030,  1.5173, -0.4511,  0.2816, -0.2150]],
       dtype=torch.float64)
	q_value: tensor([[-27.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 124
	action: tensor([[ 1.4263, -0.0175, -0.6636,  0.4931, -0.2878,  0.2361, -0.0664]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8718760724339192, distance: 0.4096111923111813 entropy 0.03264415264129639
epoch: 51, step: 125
	action: tensor([[ 3.1375, -0.0291, -0.9941,  1.7816, -0.8745,  0.3990,  0.0369]],
       dtype=torch.float64)
	q_value: tensor([[-41.3369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 51, step: 126
	action: tensor([[ 1.3396, -0.0109, -0.8195,  1.0189, -0.2682,  0.1878,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[-55.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9858099145622402, distance: 0.13631674219505377 entropy 0.03264415264129639
epoch: 51, step: 127
	action: tensor([[ 3.5235,  0.4189, -1.0719,  1.8388, -0.9194,  0.3534, -0.4930]],
       dtype=torch.float64)
	q_value: tensor([[-42.1583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 51 actor 794.6758063475627 critic 54.59177817183965 
epoch: 52, step: 0
	action: tensor([[ 1.1435, -0.1829, -0.3620,  0.1542, -0.7564,  0.1526, -0.5360]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6882696299173353, distance: 0.6389196584645809 entropy 0.03264415264129639
epoch: 52, step: 1
	action: tensor([[ 2.2441, -0.4696, -0.6388,  1.0064, -0.2522,  0.0669, -0.0949]],
       dtype=torch.float64)
	q_value: tensor([[-33.8407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 2
	action: tensor([[ 0.9983, -0.1043, -0.5030,  0.4384, -0.2248,  0.4813, -0.2193]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.91338797766912, distance: 0.33677960471809754 entropy 0.03264415264129639
epoch: 52, step: 3
	action: tensor([[ 1.9487,  0.0858, -1.4763,  0.8636, -0.1817,  0.1637,  0.3935]],
       dtype=torch.float64)
	q_value: tensor([[-28.7482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 4
	action: tensor([[ 1.4224, -0.0775, -0.4411,  0.6156, -0.2881,  0.2516, -0.1264]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8596910415409859, distance: 0.4286466102505998 entropy 0.03264415264129639
epoch: 52, step: 5
	action: tensor([[ 2.0266,  0.2494, -1.1203,  1.4178, -0.5044, -0.3572, -0.3462]],
       dtype=torch.float64)
	q_value: tensor([[-34.4783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 6
	action: tensor([[ 0.8608, -0.7083, -0.7234,  0.8644, -0.0079, -0.0469, -0.1072]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.516597734168907, distance: 0.795629787263828 entropy 0.03264415264129639
epoch: 52, step: 7
	action: tensor([[ 1.8638, -0.7663, -0.9083,  1.4260, -0.2380,  0.3988, -0.3721]],
       dtype=torch.float64)
	q_value: tensor([[-21.6976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 8
	action: tensor([[ 1.0246,  0.0623, -0.5138,  0.9085, -0.2649,  0.0994,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9882042263468049, distance: 0.12428534106648635 entropy 0.03264415264129639
epoch: 52, step: 9
	action: tensor([[ 1.7627, -0.1461, -0.6184,  0.7897, -0.7893,  0.5316,  0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-29.1354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 10
	action: tensor([[ 0.8253, -0.4272, -0.5756,  0.8584, -0.1893, -0.0673, -0.3112]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7643269173295775, distance: 0.5555353310978289 entropy 0.03264415264129639
epoch: 52, step: 11
	action: tensor([[ 2.2545, -0.5914, -0.8661,  0.6667, -0.2847,  0.0081, -0.4042]],
       dtype=torch.float64)
	q_value: tensor([[-24.6196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 12
	action: tensor([[ 1.1441, -0.3651, -0.1615,  0.4525, -0.1918,  0.1801,  0.2424]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7083977348910453, distance: 0.6179482972149845 entropy 0.03264415264129639
epoch: 52, step: 13
	action: tensor([[ 1.4413,  0.2400, -0.8930,  1.4998, -0.7964,  0.0317,  0.1290]],
       dtype=torch.float64)
	q_value: tensor([[-21.6154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9482100497515205, distance: 0.26042306154336353 entropy 0.03264415264129639
epoch: 52, step: 14
	action: tensor([[ 2.3414, -0.1780, -0.7958,  1.0707, -0.4437,  0.3566, -0.4467]],
       dtype=torch.float64)
	q_value: tensor([[-45.1252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 15
	action: tensor([[ 0.9616, -0.1997, -0.9389,  0.1635,  0.2181, -0.0278,  0.2229]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6278491318119477, distance: 0.6980977070132877 entropy 0.03264415264129639
epoch: 52, step: 16
	action: tensor([[ 1.5955,  0.3042, -0.7256,  1.1881, -0.3403,  0.0790, -0.0553]],
       dtype=torch.float64)
	q_value: tensor([[-23.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9785404862770137, distance: 0.16763574603459436 entropy 0.03264415264129639
epoch: 52, step: 17
	action: tensor([[ 2.2180,  0.0827, -1.0985,  1.4758, -0.7711,  0.5497, -0.3691]],
       dtype=torch.float64)
	q_value: tensor([[-44.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 18
	action: tensor([[ 1.5425,  0.1331, -0.4676,  0.5475, -0.3169,  0.0408,  0.2512]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8050042646718292, distance: 0.5053232141759475 entropy 0.03264415264129639
epoch: 52, step: 19
	action: tensor([[ 1.4164,  0.2515, -0.7898,  1.0872, -0.7960,  0.2491, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-34.7627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06691513756889779 entropy 0.03264415264129639
epoch: 52, step: 20
	action: tensor([[ 1.0341,  0.1218, -0.6190,  0.6637, -0.3822,  0.1409,  0.2016]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9829375931768447, distance: 0.14947784898339864 entropy 0.03264415264129639
epoch: 52, step: 21
	action: tensor([[ 1.8593,  0.0258, -0.5678,  0.6584, -0.2753,  0.0266,  0.2627]],
       dtype=torch.float64)
	q_value: tensor([[-28.9513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 22
	action: tensor([[ 0.9954, -0.1889, -0.2447,  0.5089, -0.6688, -0.0313,  0.2475]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8265848263863046, distance: 0.4765409659349011 entropy 0.03264415264129639
epoch: 52, step: 23
	action: tensor([[ 1.5664, -0.0241, -0.8271,  1.2485,  0.0876,  0.1044, -0.2605]],
       dtype=torch.float64)
	q_value: tensor([[-23.4468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9828092474841222, distance: 0.15003899183983554 entropy 0.03264415264129639
epoch: 52, step: 24
	action: tensor([[ 2.8389, -0.1605, -0.9508,  1.5015, -0.5585,  0.6593, -0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-41.5877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 25
	action: tensor([[ 0.7464, -0.1545, -0.6143,  0.9724, -0.7097,  0.4958, -0.5495]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8062664660944102, distance: 0.5036850881429924 entropy 0.03264415264129639
epoch: 52, step: 26
	action: tensor([[ 1.8093, -0.1948, -1.2272,  1.2631, -0.4220,  0.6017, -0.5643]],
       dtype=torch.float64)
	q_value: tensor([[-35.0665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8563275339529114, distance: 0.4337539736095583 entropy 0.03264415264129639
epoch: 52, step: 27
	action: tensor([[ 3.1230, -0.3100, -1.2731,  1.9112, -0.2858,  0.5441, -0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-52.8489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 28
	action: tensor([[ 1.1527,  0.1519, -0.8391,  0.9929, -0.3189, -0.0919,  0.1134]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9865369693774219, distance: 0.1327786076101513 entropy 0.03264415264129639
epoch: 52, step: 29
	action: tensor([[ 2.0837, -0.4316, -1.0513,  1.0022, -0.3049,  0.4029, -0.0940]],
       dtype=torch.float64)
	q_value: tensor([[-33.8533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 30
	action: tensor([[ 1.0483, -0.3603, -0.5309,  0.6238, -0.3862, -0.1875, -0.1056]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7286248483599326, distance: 0.5961310360241864 entropy 0.03264415264129639
epoch: 52, step: 31
	action: tensor([[ 1.8530,  0.4752, -0.3929,  0.6315, -0.5827, -0.2850, -0.2038]],
       dtype=torch.float64)
	q_value: tensor([[-26.0294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 32
	action: tensor([[ 1.3121,  0.4441, -0.4592,  0.6996, -0.1831,  0.5743, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9482399036179114, distance: 0.26034799141899184 entropy 0.03264415264129639
epoch: 52, step: 33
	action: tensor([[ 1.7654, -0.1467, -0.9290,  1.1022, -0.0955,  0.7936, -0.1757]],
       dtype=torch.float64)
	q_value: tensor([[-38.1882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 34
	action: tensor([[ 1.3910, -0.2272, -0.0023,  0.6845, -0.2928,  0.3365, -0.4934]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7789909830128151, distance: 0.5379744989277371 entropy 0.03264415264129639
epoch: 52, step: 35
	action: tensor([[ 2.6184,  0.1126, -0.7132,  1.1625, -0.3331, -0.0241, -0.1248]],
       dtype=torch.float64)
	q_value: tensor([[-33.9567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 36
	action: tensor([[ 1.3337, -0.1089, -0.5998,  0.8222, -0.4370, -0.0263, -0.2819]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.916738121958959, distance: 0.33020207232546783 entropy 0.03264415264129639
epoch: 52, step: 37
	action: tensor([[ 2.4471,  0.0150, -1.1990,  1.5890, -0.6842,  0.4708, -0.1267]],
       dtype=torch.float64)
	q_value: tensor([[-36.1454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 38
	action: tensor([[ 0.9544, -0.1640, -0.4169,  0.3850, -0.2540,  0.2839,  0.2595]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8410961116003347, distance: 0.4561670957763361 entropy 0.03264415264129639
epoch: 52, step: 39
	action: tensor([[ 1.6555, -0.3349, -0.5983,  0.9363, -0.3644,  0.8564, -0.4008]],
       dtype=torch.float64)
	q_value: tensor([[-21.9965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8983120693122852, distance: 0.3649147332278641 entropy 0.03264415264129639
epoch: 52, step: 40
	action: tensor([[ 2.5386, -0.2012, -1.6829,  1.3273, -0.4627,  0.2915,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-43.9662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 41
	action: tensor([[ 1.4414, -0.4039, -0.6701,  0.6113, -0.3109, -0.0680, -0.1355]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5869420983469775, distance: 0.735465254983398 entropy 0.03264415264129639
epoch: 52, step: 42
	action: tensor([[ 1.8472, -0.3131, -0.5475,  1.2301, -0.5980,  0.6640, -0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-32.8649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 43
	action: tensor([[ 1.0016, -0.1081, -0.4839,  0.5337, -0.3678,  0.2412, -0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081714635181236, distance: 0.34677319884907964 entropy 0.03264415264129639
epoch: 52, step: 44
	action: tensor([[ 1.6075, -0.1107, -0.8598,  1.0225, -0.4062,  0.2453, -0.7321]],
       dtype=torch.float64)
	q_value: tensor([[-27.2685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9328034349794521, distance: 0.29664035340542183 entropy 0.03264415264129639
epoch: 52, step: 45
	action: tensor([[ 2.9720, -0.0666, -1.5375,  1.7956, -0.6802,  0.3931, -0.3927]],
       dtype=torch.float64)
	q_value: tensor([[-48.9591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 46
	action: tensor([[ 1.3501, -0.3700, -0.6242,  0.5717, -0.3271,  0.4832, -0.7233]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7855608999893032, distance: 0.5299180116187668 entropy 0.03264415264129639
epoch: 52, step: 47
	action: tensor([[ 2.4816,  0.0674, -1.1544,  1.4804, -0.1712,  0.3502, -0.6536]],
       dtype=torch.float64)
	q_value: tensor([[-39.3639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 48
	action: tensor([[ 1.2576, -0.3336, -0.3018,  0.8284, -0.4941,  0.1523, -0.3616]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8621817272920255, distance: 0.42482502784158266 entropy 0.03264415264129639
epoch: 52, step: 49
	action: tensor([[ 1.9950,  0.0345, -1.0844,  1.2299, -0.3120, -0.2415,  0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-33.1482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 50
	action: tensor([[ 1.3673, -0.0211, -0.3041, -0.0186, -0.3599, -0.1048, -0.0594]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49360214628195254, distance: 0.8143341005771604 entropy 0.03264415264129639
epoch: 52, step: 51
	action: tensor([[ 1.4408, -0.3163, -0.9645,  1.1714, -0.1457,  0.1305,  0.3562]],
       dtype=torch.float64)
	q_value: tensor([[-29.5821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9025550462212573, distance: 0.3572204968949123 entropy 0.03264415264129639
epoch: 52, step: 52
	action: tensor([[ 2.1685,  0.0723, -0.9031,  1.1019, -0.2787,  0.3188, -0.7171]],
       dtype=torch.float64)
	q_value: tensor([[-33.8123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 53
	action: tensor([[ 1.1888, -0.3951, -0.1792,  0.6268, -0.1079,  0.2630,  0.1855]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7870616159749896, distance: 0.5280604849638125 entropy 0.03264415264129639
epoch: 52, step: 54
	action: tensor([[ 2.0706,  0.1313, -0.7095,  1.1873, -0.6234,  0.2135, -0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-23.1288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 55
	action: tensor([[ 0.9964,  0.1080, -0.6176,  0.8830, -0.8712,  0.4324, -0.2029]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8756556319186709, distance: 0.4035243559872087 entropy 0.03264415264129639
epoch: 52, step: 56
	action: tensor([[ 2.2395, -0.3559, -0.9144,  0.9192, -0.7313,  0.1957, -0.2622]],
       dtype=torch.float64)
	q_value: tensor([[-38.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 57
	action: tensor([[ 0.9105, -0.3668, -0.6423,  0.8832, -0.0886,  0.0978, -0.0739]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8247352595862316, distance: 0.47907550960797657 entropy 0.03264415264129639
epoch: 52, step: 58
	action: tensor([[ 2.1546, -0.2103, -1.3312,  1.5940, -0.4566, -0.0794, -0.1032]],
       dtype=torch.float64)
	q_value: tensor([[-24.3700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 59
	action: tensor([[ 0.5006, -0.1173, -0.3400,  0.7650, -0.1442,  0.4894, -0.2738]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.813751737242368, distance: 0.493858813338409 entropy 0.03264415264129639
epoch: 52, step: 60
	action: tensor([[ 1.8804, -0.0632, -1.0281,  1.3047, -0.7730,  0.4176,  0.1272]],
       dtype=torch.float64)
	q_value: tensor([[-21.6353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9431610329266353, distance: 0.27282224714881864 entropy 0.03264415264129639
epoch: 52, step: 61
	action: tensor([[ 2.5034, -0.5756, -1.0311,  1.1866, -0.6424,  0.5640, -0.1946]],
       dtype=torch.float64)
	q_value: tensor([[-46.7902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 62
	action: tensor([[ 1.4613,  0.1840, -0.6090,  0.6396, -0.2921, -0.2959, -0.1331]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8444732587943067, distance: 0.4512936568777654 entropy 0.03264415264129639
epoch: 52, step: 63
	action: tensor([[ 2.2425, -0.1657, -0.8522,  1.2896, -0.7276,  0.2594, -0.9077]],
       dtype=torch.float64)
	q_value: tensor([[-37.0219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 64
	action: tensor([[ 1.0789, -0.0763, -0.6489,  0.5890, -0.0019, -0.0116, -0.3063]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8959984690561762, distance: 0.3690426493784469 entropy 0.03264415264129639
epoch: 52, step: 65
	action: tensor([[ 1.9920,  0.1324, -1.1242,  1.1658, -0.3238,  0.0758,  0.0707]],
       dtype=torch.float64)
	q_value: tensor([[-29.1377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 66
	action: tensor([[ 1.0935, -0.2960, -0.8385,  0.7965, -0.8390,  0.0877,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8282579195100646, distance: 0.47423658335940216 entropy 0.03264415264129639
epoch: 52, step: 67
	action: tensor([[ 2.1879, -0.1062, -1.1330,  0.8596, -0.3946,  0.3010, -0.2757]],
       dtype=torch.float64)
	q_value: tensor([[-32.6330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 68
	action: tensor([[ 0.9615, -0.3629, -0.8184,  0.5498, -0.6541,  0.1347,  0.1274]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6929712630346709, distance: 0.6340831409521219 entropy 0.03264415264129639
epoch: 52, step: 69
	action: tensor([[ 1.6005, -0.1779, -1.1049,  1.1537, -0.1946,  0.1181,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-26.8112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9023452896717811, distance: 0.3576047603005993 entropy 0.03264415264129639
epoch: 52, step: 70
	action: tensor([[ 2.4614, -0.1991, -1.3965,  1.1629, -0.1983,  0.3922, -0.0463]],
       dtype=torch.float64)
	q_value: tensor([[-41.6133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 71
	action: tensor([[ 1.1362, -0.5046, -0.4626,  0.5509, -0.3728,  0.2681, -0.0950]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.665959242459603, distance: 0.6613881709732984 entropy 0.03264415264129639
epoch: 52, step: 72
	action: tensor([[ 1.7393,  0.0636, -0.4492,  1.0605, -0.8087,  0.4196, -0.8766]],
       dtype=torch.float64)
	q_value: tensor([[-26.2685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 73
	action: tensor([[ 0.9917, -0.1168, -0.1831,  0.1145, -0.1567,  0.2816, -0.1658]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7640188128531036, distance: 0.5558983488319359 entropy 0.03264415264129639
epoch: 52, step: 74
	action: tensor([[ 1.8332,  0.0094, -0.5206,  1.4021, -0.4342,  0.1652, -0.6341]],
       dtype=torch.float64)
	q_value: tensor([[-22.9809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 75
	action: tensor([[ 1.1106,  0.4825, -0.8509,  0.9206, -0.2197,  0.0563, -0.0293]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8854112051855076, distance: 0.38737159262879695 entropy 0.03264415264129639
epoch: 52, step: 76
	action: tensor([[ 1.8170, -0.5397, -0.8962,  1.2237, -0.0869,  0.3711, -0.2960]],
       dtype=torch.float64)
	q_value: tensor([[-37.4096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 77
	action: tensor([[ 0.9548,  0.2760, -0.3455,  0.9081, -0.2389, -0.0956, -0.1340]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9755435996914533, distance: 0.17895875942055384 entropy 0.03264415264129639
epoch: 52, step: 78
	action: tensor([[ 2.2843,  0.3409, -0.8731,  0.8852, -0.8507,  0.2806, -0.4264]],
       dtype=torch.float64)
	q_value: tensor([[-29.1928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 79
	action: tensor([[ 1.2873,  0.0660, -0.7932,  0.3367, -0.1616,  0.0333, -0.0615]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8286935872143911, distance: 0.4736346902213116 entropy 0.03264415264129639
epoch: 52, step: 80
	action: tensor([[ 1.6519, -0.0693, -1.0992,  1.4280, -0.4709, -0.0325, -0.1816]],
       dtype=torch.float64)
	q_value: tensor([[-33.1921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9582128928712045, distance: 0.23392565086834427 entropy 0.03264415264129639
epoch: 52, step: 81
	action: tensor([[ 2.6824, -0.0219, -1.3615,  1.7631, -0.0273, -0.0162, -0.0801]],
       dtype=torch.float64)
	q_value: tensor([[-46.8475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 82
	action: tensor([[ 0.7581, -0.2342, -0.7549,  0.6483,  0.0072,  0.0437, -0.2978]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7643199829344609, distance: 0.5555435040149616 entropy 0.03264415264129639
epoch: 52, step: 83
	action: tensor([[ 1.7804, -0.1439, -0.7829,  1.4785, -0.0815,  0.1843,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-24.1563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9760910145362408, distance: 0.17694458121438317 entropy 0.03264415264129639
epoch: 52, step: 84
	action: tensor([[ 3.0375, -0.6240, -0.9996,  1.2209, -0.3656,  0.4052, -0.2692]],
       dtype=torch.float64)
	q_value: tensor([[-40.5313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 85
	action: tensor([[ 0.9780, -0.1780, -0.3334,  0.6217, -0.3564,  0.2451, -0.4033]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9171315624400573, distance: 0.3294209903835832 entropy 0.03264415264129639
epoch: 52, step: 86
	action: tensor([[ 1.6970, -0.1779, -0.9113,  1.2162, -0.6537,  0.1035,  0.0391]],
       dtype=torch.float64)
	q_value: tensor([[-28.7653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9435307712933244, distance: 0.27193344288420274 entropy 0.03264415264129639
epoch: 52, step: 87
	action: tensor([[ 2.0230, -0.1434, -1.0979,  0.7324, -0.5840,  0.6284, -0.1002]],
       dtype=torch.float64)
	q_value: tensor([[-42.9427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 88
	action: tensor([[ 0.8926, -0.2306, -0.3980,  0.6346, -0.2797,  0.1108, -0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8633307161049655, distance: 0.42305044157485744 entropy 0.03264415264129639
epoch: 52, step: 89
	action: tensor([[ 1.8556, -0.1910, -0.7241,  1.0736, -0.4163,  0.2685, -0.3946]],
       dtype=torch.float64)
	q_value: tensor([[-23.7821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 90
	action: tensor([[ 1.2917, -0.1688, -0.7146,  0.8418, -0.4478,  0.0927,  0.1516]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9155794406989515, distance: 0.33249169797424055 entropy 0.03264415264129639
epoch: 52, step: 91
	action: tensor([[ 2.1592, -0.2909, -0.9040,  0.8719, -0.3930,  0.2566, -0.5188]],
       dtype=torch.float64)
	q_value: tensor([[-32.4873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 92
	action: tensor([[ 1.0617,  0.0325, -0.6129,  0.1589,  0.1223,  0.1971,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.845085175319532, distance: 0.4504049794374508 entropy 0.03264415264129639
epoch: 52, step: 93
	action: tensor([[ 1.8040, -0.2657, -0.7804,  0.6122, -0.4315,  0.0980, -0.3561]],
       dtype=torch.float64)
	q_value: tensor([[-25.4394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 94
	action: tensor([[ 1.0767, -0.2337, -0.3676,  0.4796, -0.3024,  0.6199, -0.0690]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8905665047772892, distance: 0.3785574770233565 entropy 0.03264415264129639
epoch: 52, step: 95
	action: tensor([[ 1.8427, -0.2697, -1.0867,  0.7933, -0.3343,  0.2786, -0.4746]],
       dtype=torch.float64)
	q_value: tensor([[-27.7988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 96
	action: tensor([[ 1.0627, -0.3564, -0.4523,  0.3414, -0.2046,  0.1818, -0.0203]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6564468125446874, distance: 0.6707391935791911 entropy 0.03264415264129639
epoch: 52, step: 97
	action: tensor([[ 1.4083, -0.0711, -0.6696,  0.7641, -0.1009, -0.0535, -0.3882]],
       dtype=torch.float64)
	q_value: tensor([[-23.3261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8744520029306233, distance: 0.4054726706536489 entropy 0.03264415264129639
epoch: 52, step: 98
	action: tensor([[ 2.4107, -0.3419, -1.0612,  1.5385, -0.4322,  0.4915, -0.0718]],
       dtype=torch.float64)
	q_value: tensor([[-36.7063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 99
	action: tensor([[ 1.1665,  0.0670, -0.4734,  0.7555, -0.6159,  0.1370,  0.0702]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9875717104134406, distance: 0.12757405948980774 entropy 0.03264415264129639
epoch: 52, step: 100
	action: tensor([[ 1.3947,  0.0685, -0.8898,  0.7706, -0.2852,  0.6143,  0.0025]],
       dtype=torch.float64)
	q_value: tensor([[-32.3369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9818650789042915, distance: 0.15410421886522055 entropy 0.03264415264129639
epoch: 52, step: 101
	action: tensor([[ 2.1861, -0.4548, -1.3479,  1.3586, -0.8766,  0.3635, -0.1025]],
       dtype=torch.float64)
	q_value: tensor([[-40.2233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 102
	action: tensor([[ 0.6011, -0.0231, -0.7117,  0.5342, -0.2160, -0.0708,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7917570080937674, distance: 0.5222060399721542 entropy 0.03264415264129639
epoch: 52, step: 103
	action: tensor([[ 1.3576, -0.2407, -0.8407,  0.8116, -0.2392,  0.1187,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-21.1250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8623464704789663, distance: 0.4245710413594914 entropy 0.03264415264129639
epoch: 52, step: 104
	action: tensor([[ 2.1445, -0.1431, -1.0685,  1.3749, -0.4469,  0.6128,  0.0884]],
       dtype=torch.float64)
	q_value: tensor([[-33.7839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 105
	action: tensor([[ 1.1218,  0.3492, -0.5167,  0.7925, -0.2351,  0.3317,  0.1250]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9708889251688866, distance: 0.1952476491791849 entropy 0.03264415264129639
epoch: 52, step: 106
	action: tensor([[ 1.6534,  0.6147, -1.3539,  1.1406, -0.4193, -0.0734, -0.3601]],
       dtype=torch.float64)
	q_value: tensor([[-32.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05678919369609756 entropy 0.03264415264129639
epoch: 52, step: 107
	action: tensor([[ 1.4246, -0.3402, -0.3592,  0.5220, -0.1362, -0.0800, -0.3527]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5578203640708248, distance: 0.7609498962444665 entropy 0.03264415264129639
epoch: 52, step: 108
	action: tensor([[ 2.2331,  0.1416, -0.8245,  1.5673, -1.2377,  0.1187, -0.4585]],
       dtype=torch.float64)
	q_value: tensor([[-31.5341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 109
	action: tensor([[ 0.9344, -0.2715,  0.0082,  0.6509, -0.5088,  0.1776, -0.0921]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8788024288191795, distance: 0.3983856178742911 entropy 0.03264415264129639
epoch: 52, step: 110
	action: tensor([[ 1.7693, -0.7031, -0.7311,  0.9144, -0.2449,  0.0749,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-23.6105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5362739441776372, distance: 0.7792690765318627 entropy 0.03264415264129639
epoch: 52, step: 111
	action: tensor([[ 2.0473,  0.0956, -1.1487,  0.9885, -0.4034,  0.2554, -0.2647]],
       dtype=torch.float64)
	q_value: tensor([[-34.0426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 112
	action: tensor([[ 0.6787,  0.1000, -0.7454,  1.0976, -0.1659,  0.0284, -0.0503]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7986516731574032, distance: 0.5134884812582605 entropy 0.03264415264129639
epoch: 52, step: 113
	action: tensor([[ 1.7083, -0.0837, -0.8861,  1.0678, -0.3218, -0.0912, -0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-27.1718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.912624149239858, distance: 0.3382613688254521 entropy 0.03264415264129639
epoch: 52, step: 114
	action: tensor([[ 2.2015, -0.0167, -1.3685,  1.5371, -0.5573,  0.2522, -0.1621]],
       dtype=torch.float64)
	q_value: tensor([[-43.6442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 115
	action: tensor([[ 1.2247, -0.3018, -0.3924,  0.1580,  0.2795,  0.5555, -0.2493]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6636005306609449, distance: 0.6637191438208253 entropy 0.03264415264129639
epoch: 52, step: 116
	action: tensor([[ 2.0105, -0.3297, -0.9087,  1.1017, -0.3548,  0.2304,  0.1815]],
       dtype=torch.float64)
	q_value: tensor([[-26.2055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 117
	action: tensor([[ 1.4637,  0.2070, -0.3596,  1.1039, -0.5950,  0.7269,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9389004167743634, distance: 0.28286278771280915 entropy 0.03264415264129639
epoch: 52, step: 118
	action: tensor([[ 2.3600, -0.3865, -1.3669,  1.3373, -0.5346, -0.4065,  0.1411]],
       dtype=torch.float64)
	q_value: tensor([[-42.5922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 119
	action: tensor([[ 0.8851, -0.0031, -0.6853,  0.4584, -0.5179,  0.0017,  0.2429]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8877613257161463, distance: 0.3833786797340542 entropy 0.03264415264129639
epoch: 52, step: 120
	action: tensor([[ 1.0793, -0.3881, -1.0190,  1.0217, -0.2516,  0.0915,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-25.2114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7776108382246693, distance: 0.5396516412510606 entropy 0.03264415264129639
epoch: 52, step: 121
	action: tensor([[ 2.3862,  0.0509, -1.2812,  1.6511, -0.0969,  0.3853, -0.2578]],
       dtype=torch.float64)
	q_value: tensor([[-29.8200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 122
	action: tensor([[ 1.1109,  0.3616, -0.6074,  0.5885, -0.2803,  0.5520, -0.1230]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9848133466502527, distance: 0.1410222804151773 entropy 0.03264415264129639
epoch: 52, step: 123
	action: tensor([[ 2.0832, -0.0349, -0.8827,  1.1143, -0.3809,  0.4132, -0.1883]],
       dtype=torch.float64)
	q_value: tensor([[-36.6696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 124
	action: tensor([[ 1.1647,  0.2386, -0.6019,  0.6702, -0.4073,  0.1644,  0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06475191706439708 entropy 0.03264415264129639
epoch: 52, step: 125
	action: tensor([[ 0.9871,  0.0224, -0.3374,  0.3194, -0.0404,  0.1389, -0.5205]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8988115796747463, distance: 0.36401736474092633 entropy 0.03264415264129639
epoch: 52, step: 126
	action: tensor([[ 2.1314, -0.1324, -0.7957,  1.7119, -0.0516,  0.2528, -0.4609]],
       dtype=torch.float64)
	q_value: tensor([[-27.8444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 52, step: 127
	action: tensor([[ 1.1677, -0.0405, -0.9367,  0.7015, -0.1104,  0.1184,  0.2296]],
       dtype=torch.float64)
	q_value: tensor([[-42.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9161111960442365, distance: 0.33144288038278036 entropy 0.03264415264129639
LOSS epoch 52 actor 721.7195760318506 critic 924.8900654202868 
epoch: 53, step: 0
	action: tensor([[ 1.2764,  0.2259, -0.1250,  0.9977, -0.0907, -0.2681, -0.6411]],
       dtype=torch.float64)
	q_value: tensor([[-27.0265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9313230043476104, distance: 0.29989024387796964 entropy 0.03264415264129639
epoch: 53, step: 1
	action: tensor([[ 1.7950, -0.0719, -1.1800,  0.7192, -0.3390,  0.1357, -0.1791]],
       dtype=torch.float64)
	q_value: tensor([[-32.7000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 2
	action: tensor([[ 0.9414,  0.2996, -0.4290,  0.4141, -0.0658,  0.0444,  0.2807]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9872029124899067, distance: 0.12945304284765377 entropy 0.03264415264129639
epoch: 53, step: 3
	action: tensor([[ 1.1571, -0.4959, -0.5878,  0.4549,  0.1149,  0.4107, -0.1336]],
       dtype=torch.float64)
	q_value: tensor([[-21.1083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6627898966896537, distance: 0.664518356653729 entropy 0.03264415264129639
epoch: 53, step: 4
	action: tensor([[ 1.4627, -0.2906, -0.3894,  0.6234, -0.5186,  0.1229,  0.1326]],
       dtype=torch.float64)
	q_value: tensor([[-22.7158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.702490232588805, distance: 0.6241763474586611 entropy 0.03264415264129639
epoch: 53, step: 5
	action: tensor([[ 1.4963, -0.3948, -1.0389,  0.2279, -0.2280,  0.0353, -0.3032]],
       dtype=torch.float64)
	q_value: tensor([[-28.7133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3184317192339281, distance: 0.9447379878434696 entropy 0.03264415264129639
epoch: 53, step: 6
	action: tensor([[ 2.1496, -0.1650, -1.3258,  0.8005, -0.3085,  0.4278, -0.1540]],
       dtype=torch.float64)
	q_value: tensor([[-32.6073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 7
	action: tensor([[ 1.3752, -0.1563, -0.8034,  0.5736, -0.2063,  0.1189, -0.1027]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7974331360827973, distance: 0.5150399243270017 entropy 0.03264415264129639
epoch: 53, step: 8
	action: tensor([[ 1.7539, -0.5654, -1.1062,  0.8225, -0.6668,  0.0613, -0.1366]],
       dtype=torch.float64)
	q_value: tensor([[-30.7245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 9
	action: tensor([[ 0.9011, -0.6942, -0.6558,  0.4714,  0.3545,  0.1367,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38706469139605504, distance: 0.8959092348605132 entropy 0.03264415264129639
epoch: 53, step: 10
	action: tensor([[ 1.8991e+00, -5.3059e-01, -8.0119e-01,  5.6590e-01, -1.1789e-03,
          1.0521e-01,  2.1150e-02]], dtype=torch.float64)
	q_value: tensor([[-16.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 11
	action: tensor([[ 0.6071, -0.0945, -0.5716, -0.1330, -0.0651,  0.2661, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6411013169918585, distance: 0.6855555098003656 entropy 0.03264415264129639
epoch: 53, step: 12
	action: tensor([[ 1.2741,  0.1244, -0.3106,  0.6159, -0.2658, -0.0207, -0.0232]],
       dtype=torch.float64)
	q_value: tensor([[-15.9757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.929292545308734, distance: 0.30429113156972615 entropy 0.03264415264129639
epoch: 53, step: 13
	action: tensor([[ 1.5527,  0.2355, -0.6946,  0.5588,  0.1783,  0.1202, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[-27.7088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8725196800827425, distance: 0.40858109274760807 entropy 0.03264415264129639
epoch: 53, step: 14
	action: tensor([[ 2.0081, -0.1757, -0.7255,  1.0309, -0.5781,  0.2293,  0.0890]],
       dtype=torch.float64)
	q_value: tensor([[-33.2399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 15
	action: tensor([[ 1.2502,  0.2114, -0.8053,  0.1112, -0.3989,  0.6790, -0.5421]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9339778355152732, distance: 0.29403672194027447 entropy 0.03264415264129639
epoch: 53, step: 16
	action: tensor([[ 1.2569,  0.2398, -0.4576,  1.0058, -0.1788,  0.7491, -0.4916]],
       dtype=torch.float64)
	q_value: tensor([[-38.1253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9393853907451292, distance: 0.2817379486851323 entropy 0.03264415264129639
epoch: 53, step: 17
	action: tensor([[ 1.5559,  0.1832, -1.0839,  1.1068, -0.4607,  0.5848, -0.1070]],
       dtype=torch.float64)
	q_value: tensor([[-37.4771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9665868154752525, distance: 0.20917781771046629 entropy 0.03264415264129639
epoch: 53, step: 18
	action: tensor([[ 1.2511,  0.0204, -0.8511,  1.4600, -0.3579, -0.0394, -0.1933]],
       dtype=torch.float64)
	q_value: tensor([[-43.5095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9498050205339517, distance: 0.25638158831889085 entropy 0.03264415264129639
epoch: 53, step: 19
	action: tensor([[ 1.4132, -0.5470, -0.6997,  0.6397, -0.2101,  0.1236, -0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-35.5649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5707796178832971, distance: 0.7497161420591264 entropy 0.03264415264129639
epoch: 53, step: 20
	action: tensor([[ 1.5374, -0.1023, -1.0676,  0.7442, -0.4951, -0.4351, -0.0258]],
       dtype=torch.float64)
	q_value: tensor([[-28.3078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7340672529305091, distance: 0.5901230837300754 entropy 0.03264415264129639
epoch: 53, step: 21
	action: tensor([[ 1.5319, -0.4025, -0.8631,  1.2329, -0.6202,  0.3817, -0.2596]],
       dtype=torch.float64)
	q_value: tensor([[-35.4911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9014533448376079, distance: 0.35923416813133296 entropy 0.03264415264129639
epoch: 53, step: 22
	action: tensor([[ 1.8745, -0.2225, -0.7183,  0.8225, -0.5328,  0.1675, -0.2207]],
       dtype=torch.float64)
	q_value: tensor([[-38.9760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 23
	action: tensor([[ 1.1901,  0.2618, -0.6694,  0.3598,  0.0465, -0.0119,  0.0631]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9421665067220315, distance: 0.27519871774011123 entropy 0.03264415264129639
epoch: 53, step: 24
	action: tensor([[ 1.4827,  0.1814, -1.0638,  0.8242,  0.4948,  0.2172, -0.1186]],
       dtype=torch.float64)
	q_value: tensor([[-26.7368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9729749465393809, distance: 0.18812216758569184 entropy 0.03264415264129639
epoch: 53, step: 25
	action: tensor([[ 1.7583,  0.0797, -0.6834,  1.1447, -0.3519,  0.3539, -0.3276]],
       dtype=torch.float64)
	q_value: tensor([[-34.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 26
	action: tensor([[ 0.5433,  0.0070, -0.5699,  0.3993, -0.2875,  0.2492,  0.0251]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7900692486606207, distance: 0.5243179467479183 entropy 0.03264415264129639
epoch: 53, step: 27
	action: tensor([[ 1.2223,  0.2978, -0.4824,  0.8408, -0.1879,  0.1531, -0.3117]],
       dtype=torch.float64)
	q_value: tensor([[-17.7160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9834057386820345, distance: 0.14741295624557754 entropy 0.03264415264129639
epoch: 53, step: 28
	action: tensor([[ 1.8787, -0.3292, -0.4802,  0.9732, -0.3340,  0.4987, -0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-32.6641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 29
	action: tensor([[ 0.8936,  0.0115, -0.5244,  0.2414, -0.1350,  0.3395, -0.1390]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8912117585124735, distance: 0.37743978103177067 entropy 0.03264415264129639
epoch: 53, step: 30
	action: tensor([[ 1.5311, -0.2343, -1.0182,  0.6890,  0.1294,  0.0291, -0.0307]],
       dtype=torch.float64)
	q_value: tensor([[-22.5281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7100704029090762, distance: 0.616173432970935 entropy 0.03264415264129639
epoch: 53, step: 31
	action: tensor([[ 1.5057,  0.1087, -1.3053,  0.6387, -0.0879,  0.3887, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[-32.0659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8951967950572249, distance: 0.3704622628811387 entropy 0.03264415264129639
epoch: 53, step: 32
	action: tensor([[ 1.7218, -0.1709, -0.7120,  1.1497, -0.1857, -0.0791, -0.2447]],
       dtype=torch.float64)
	q_value: tensor([[-38.3455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 33
	action: tensor([[ 0.7166, -0.3761, -0.3345,  0.6313, -0.7236, -0.2162, -0.1696]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7350987207256326, distance: 0.5889775227915532 entropy 0.03264415264129639
epoch: 53, step: 34
	action: tensor([[ 1.1833,  0.2702, -0.4866,  1.2708, -0.0183,  0.1032, -0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-20.1744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9197092922324137, distance: 0.3242569821380669 entropy 0.03264415264129639
epoch: 53, step: 35
	action: tensor([[ 1.9737, -0.1177, -1.3734,  0.9308, -0.6388,  0.4345, -0.3197]],
       dtype=torch.float64)
	q_value: tensor([[-32.2842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 36
	action: tensor([[ 0.8604, -0.0494, -0.3161,  0.9139, -0.2320,  0.0824, -0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9737445255281533, distance: 0.1854242917127352 entropy 0.03264415264129639
epoch: 53, step: 37
	action: tensor([[ 1.7476, -0.1161, -0.9072,  0.8322, -0.5438, -0.0108,  0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-22.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8188764631084978, distance: 0.4870170193813243 entropy 0.03264415264129639
epoch: 53, step: 38
	action: tensor([[ 1.5565, -0.2311, -0.9443,  0.5999, -0.6145, -0.1821, -0.0148]],
       dtype=torch.float64)
	q_value: tensor([[-36.7019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.621637292079794, distance: 0.7038998226539234 entropy 0.03264415264129639
epoch: 53, step: 39
	action: tensor([[ 1.3811, -0.0244, -0.7536,  0.8910, -0.1895,  0.1387, -0.2533]],
       dtype=torch.float64)
	q_value: tensor([[-34.7158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9653550903381832, distance: 0.21299843372489827 entropy 0.03264415264129639
epoch: 53, step: 40
	action: tensor([[ 1.7517, -0.1774, -1.2278,  1.1934, -0.1626,  0.0445, -0.3361]],
       dtype=torch.float64)
	q_value: tensor([[-33.8211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 41
	action: tensor([[ 1.3414, -0.3704, -0.7290,  0.2662, -0.5059,  0.2884, -0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5339555036683084, distance: 0.781214661024084 entropy 0.03264415264129639
epoch: 53, step: 42
	action: tensor([[ 1.2343, -0.0106, -0.8333,  0.2694, -0.2608,  0.1210, -0.3261]],
       dtype=torch.float64)
	q_value: tensor([[-29.1698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8050023856276871, distance: 0.5053256489018567 entropy 0.03264415264129639
epoch: 53, step: 43
	action: tensor([[ 1.6717, -0.4974, -0.6658,  0.6646, -0.2381,  0.1990,  0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-30.9622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5374149931223635, distance: 0.7783097474149172 entropy 0.03264415264129639
epoch: 53, step: 44
	action: tensor([[ 1.5385,  0.1881, -1.2053,  0.9188,  0.1890, -0.0890, -0.1435]],
       dtype=torch.float64)
	q_value: tensor([[-28.9885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9391038415114463, distance: 0.28239151393713546 entropy 0.03264415264129639
epoch: 53, step: 45
	action: tensor([[ 2.1042, -0.3009, -0.7779,  0.8908, -0.1029, -0.1973, -0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-37.5864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 46
	action: tensor([[ 1.2393, -0.3191, -0.3474,  0.1342, -0.0708,  0.1612,  0.5303]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5186285974220011, distance: 0.7939567335028788 entropy 0.03264415264129639
epoch: 53, step: 47
	action: tensor([[ 1.4214, -0.6910, -0.5161,  0.7796, -0.1286, -0.2297, -0.0404]],
       dtype=torch.float64)
	q_value: tensor([[-19.5142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41310865780647466, distance: 0.8766687880753585 entropy 0.03264415264129639
epoch: 53, step: 48
	action: tensor([[ 1.5258, -0.3411, -1.3235,  0.8842, -0.1166, -0.0826, -0.3057]],
       dtype=torch.float64)
	q_value: tensor([[-26.1802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6459148612372676, distance: 0.6809426601427795 entropy 0.03264415264129639
epoch: 53, step: 49
	action: tensor([[ 2.4451e+00, -3.3111e-01, -1.0128e+00,  1.5629e+00, -1.9509e-03,
          6.7102e-01, -3.7627e-01]], dtype=torch.float64)
	q_value: tensor([[-37.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 50
	action: tensor([[ 1.0028,  0.0741, -0.8131,  0.2740, -0.3142, -0.1055, -0.4135]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8763078881648036, distance: 0.4024646081525247 entropy 0.03264415264129639
epoch: 53, step: 51
	action: tensor([[ 1.7252, -0.0951, -0.7989,  1.0072, -0.3354,  0.2304, -0.5279]],
       dtype=torch.float64)
	q_value: tensor([[-28.4084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.930988131712204, distance: 0.3006204950593558 entropy 0.03264415264129639
epoch: 53, step: 52
	action: tensor([[ 1.9087, -0.3530, -0.3576,  0.8719, -0.0559,  0.8313, -0.2125]],
       dtype=torch.float64)
	q_value: tensor([[-41.0138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 53
	action: tensor([[ 1.1796, -0.3423, -0.5050,  0.4895, -0.0291,  0.4339, -0.4115]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7898656365370473, distance: 0.5245721534717325 entropy 0.03264415264129639
epoch: 53, step: 54
	action: tensor([[ 1.8764, -0.1190, -0.9198,  0.8155,  0.1197,  0.1397, -0.4460]],
       dtype=torch.float64)
	q_value: tensor([[-26.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 55
	action: tensor([[ 0.9567, -0.2228, -0.2786,  1.0262,  0.3074,  0.1887, -0.2125]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9859050419984509, distance: 0.1358590540362238 entropy 0.03264415264129639
epoch: 53, step: 56
	action: tensor([[ 1.8294, -0.1186, -1.1163,  1.0195, -0.4137, -0.0734, -0.0615]],
       dtype=torch.float64)
	q_value: tensor([[-21.8381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 57
	action: tensor([[ 0.9637,  0.0725, -0.4521,  0.2822, -0.7831,  0.0127, -0.3663]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9064232148908822, distance: 0.35005860163145935 entropy 0.03264415264129639
epoch: 53, step: 58
	action: tensor([[ 1.5234, -0.1431, -0.3727,  0.8977, -0.0094,  0.4875,  0.0760]],
       dtype=torch.float64)
	q_value: tensor([[-28.4526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9040821140588138, distance: 0.35441042841263287 entropy 0.03264415264129639
epoch: 53, step: 59
	action: tensor([[ 1.9635,  0.0188, -1.1579,  1.1012,  0.2502,  0.4526, -0.1292]],
       dtype=torch.float64)
	q_value: tensor([[-30.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 60
	action: tensor([[ 1.1827, -0.2382, -0.6190,  0.5444,  0.0040, -0.1412,  0.2446]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7432190621474539, distance: 0.5798799377696916 entropy 0.03264415264129639
epoch: 53, step: 61
	action: tensor([[ 1.3399, -0.2652, -0.9043,  1.2121, -0.0516,  0.0804, -0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-22.2219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9337813381815273, distance: 0.29447395772630025 entropy 0.03264415264129639
epoch: 53, step: 62
	action: tensor([[ 1.5950, -0.4024, -0.9618,  0.8892,  0.0902,  0.2387, -0.1592]],
       dtype=torch.float64)
	q_value: tensor([[-31.5794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7549548252143049, distance: 0.5664737253510859 entropy 0.03264415264129639
epoch: 53, step: 63
	action: tensor([[ 2.0469, -0.6060, -0.6888,  0.9578, -0.2485,  0.2893, -0.2883]],
       dtype=torch.float64)
	q_value: tensor([[-33.8999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 64
	action: tensor([[ 1.0261,  0.2854, -0.4095,  0.6053,  0.0825, -0.0043, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08004227783157969 entropy 0.03264415264129639
epoch: 53, step: 65
	action: tensor([[ 0.4641, -0.0371, -0.7592,  0.4229, -0.0052,  0.2772, -0.0991]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6921921539215319, distance: 0.6348871486682364 entropy 0.03264415264129639
epoch: 53, step: 66
	action: tensor([[ 0.9263,  0.0633, -0.6791,  0.4395, -0.6364,  0.1866, -0.2750]],
       dtype=torch.float64)
	q_value: tensor([[-17.2407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9273402851191486, distance: 0.3084633266349786 entropy 0.03264415264129639
epoch: 53, step: 67
	action: tensor([[ 1.4094,  0.1706, -0.6707,  0.4091, -0.4160,  0.0771, -0.2941]],
       dtype=torch.float64)
	q_value: tensor([[-28.8365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.867538186746678, distance: 0.4164875667925584 entropy 0.03264415264129639
epoch: 53, step: 68
	action: tensor([[ 1.7459, -0.2194, -1.0594,  0.8744, -0.4018, -0.0183, -0.6353]],
       dtype=torch.float64)
	q_value: tensor([[-34.7906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 69
	action: tensor([[ 1.0762, -0.1454, -0.2891,  0.5797, -0.1242,  0.3066,  0.4547]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9315431609398, distance: 0.29940918169325975 entropy 0.03264415264129639
epoch: 53, step: 70
	action: tensor([[ 1.1263, -0.5319, -0.7647,  0.8936, -0.0372, -0.5055, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[-19.9085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6042228117029201, distance: 0.7199164092843673 entropy 0.03264415264129639
epoch: 53, step: 71
	action: tensor([[ 1.7426,  0.0640, -0.9804,  1.1300, -0.7315,  0.1921,  0.0380]],
       dtype=torch.float64)
	q_value: tensor([[-23.8136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 72
	action: tensor([[ 1.0594, -0.5147, -0.9133,  0.6046, -0.0513, -0.0946, -0.4057]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5518761334617895, distance: 0.7660475545821173 entropy 0.03264415264129639
epoch: 53, step: 73
	action: tensor([[ 1.7295, -0.1116, -0.6034,  1.0509, -0.5357, -0.3357, -0.1158]],
       dtype=torch.float64)
	q_value: tensor([[-25.6405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8229611964309229, distance: 0.48149405165635256 entropy 0.03264415264129639
epoch: 53, step: 74
	action: tensor([[ 2.1011, -0.2883, -0.8111,  0.4720, -0.4289,  0.0705, -0.3770]],
       dtype=torch.float64)
	q_value: tensor([[-36.1273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 75
	action: tensor([[ 0.9541, -0.0908, -0.7607,  0.5937, -0.0094,  0.0393, -0.2093]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.879283474529649, distance: 0.3975942149176275 entropy 0.03264415264129639
epoch: 53, step: 76
	action: tensor([[ 1.7359,  0.0387, -0.5943,  0.9780, -0.1292, -0.0852, -0.2183]],
       dtype=torch.float64)
	q_value: tensor([[-24.3301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8935586184817842, distance: 0.3733463799030797 entropy 0.03264415264129639
epoch: 53, step: 77
	action: tensor([[ 2.2788,  0.2331, -0.8315,  0.9955, -0.5843,  0.1716, -0.3959]],
       dtype=torch.float64)
	q_value: tensor([[-35.9397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 78
	action: tensor([[ 1.0281,  0.0787, -0.1961,  0.6370, -0.1747,  0.3820, -0.3471]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07267809252522064 entropy 0.03264415264129639
epoch: 53, step: 79
	action: tensor([[ 1.1663, -0.3538, -0.6916,  0.6821, -0.2679,  0.1374,  0.0305]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7864580297602333, distance: 0.5288083644163434 entropy 0.03264415264129639
epoch: 53, step: 80
	action: tensor([[ 1.4041, -0.2498, -1.1068,  0.7682, -0.1332,  0.2165, -0.4091]],
       dtype=torch.float64)
	q_value: tensor([[-25.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8026842613326984, distance: 0.5083204209040193 entropy 0.03264415264129639
epoch: 53, step: 81
	action: tensor([[ 1.3574, -0.2886, -1.2516,  0.8992, -0.2391, -0.2292,  0.1041]],
       dtype=torch.float64)
	q_value: tensor([[-35.7954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7292657356695414, distance: 0.5954266998920857 entropy 0.03264415264129639
epoch: 53, step: 82
	action: tensor([[ 1.5149, -0.5347, -0.8411,  0.8808, -0.6421,  0.5835,  0.2658]],
       dtype=torch.float64)
	q_value: tensor([[-31.6489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7441265217761699, distance: 0.578854387797271 entropy 0.03264415264129639
epoch: 53, step: 83
	action: tensor([[ 1.2256,  0.0065, -0.8600,  0.4066, -0.2685,  0.4654, -0.0207]],
       dtype=torch.float64)
	q_value: tensor([[-32.6652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9114819200717248, distance: 0.3404651669231752 entropy 0.03264415264129639
epoch: 53, step: 84
	action: tensor([[ 2.0624, -0.1581, -1.0914,  0.7088, -0.5065,  0.3410,  0.4162]],
       dtype=torch.float64)
	q_value: tensor([[-30.7440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 85
	action: tensor([[ 0.9230, -0.0295, -0.1493,  0.2834, -0.0411,  0.0295, -0.0889]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.833840543222677, distance: 0.4664651739803142 entropy 0.03264415264129639
epoch: 53, step: 86
	action: tensor([[ 1.1111, -0.2393, -0.5243, -0.0510, -0.2634,  0.5179, -0.5039]],
       dtype=torch.float64)
	q_value: tensor([[-18.5870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6387073379248345, distance: 0.6878381564836092 entropy 0.03264415264129639
epoch: 53, step: 87
	action: tensor([[ 1.4004, -0.2661, -0.5881,  0.6032, -0.2788,  0.2600,  0.3075]],
       dtype=torch.float64)
	q_value: tensor([[-27.6366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8269395283630616, distance: 0.47605335997527565 entropy 0.03264415264129639
epoch: 53, step: 88
	action: tensor([[ 1.6430, -0.0082, -0.9516,  0.7599, -0.1867,  0.3606, -0.1710]],
       dtype=torch.float64)
	q_value: tensor([[-26.9655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9391712561732954, distance: 0.28223516088796846 entropy 0.03264415264129639
epoch: 53, step: 89
	action: tensor([[ 2.0545,  0.1196, -0.8328,  0.9690, -0.3542,  0.1966, -0.2577]],
       dtype=torch.float64)
	q_value: tensor([[-38.2996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 90
	action: tensor([[ 0.7708, -0.1249, -0.3709,  0.1559, -0.0128, -0.0921, -0.0806]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6944996200915192, distance: 0.6325029721212203 entropy 0.03264415264129639
epoch: 53, step: 91
	action: tensor([[ 1.4001,  0.1209, -0.4811,  0.3042, -0.2324,  0.2900,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-16.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.842214297142576, distance: 0.45455926865516916 entropy 0.03264415264129639
epoch: 53, step: 92
	action: tensor([[ 1.4345,  0.0570, -0.8395,  1.0464, -0.3792, -0.2681, -0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-30.1782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9653044527492229, distance: 0.21315403795317367 entropy 0.03264415264129639
epoch: 53, step: 93
	action: tensor([[ 1.7941,  0.0164, -0.5215,  0.8659, -0.3359,  0.2439, -0.4866]],
       dtype=torch.float64)
	q_value: tensor([[-35.4687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 94
	action: tensor([[ 1.2406e+00, -1.4711e-01, -8.3185e-01,  3.4874e-01,  6.2942e-04,
          5.4225e-01,  2.1410e-01]], dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8316305994013565, distance: 0.4695569534194369 entropy 0.03264415264129639
epoch: 53, step: 95
	action: tensor([[ 1.5331, -0.1978, -0.7537,  0.6883, -0.0372,  0.0548, -0.2831]],
       dtype=torch.float64)
	q_value: tensor([[-26.5538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7503467135829561, distance: 0.5717752296461125 entropy 0.03264415264129639
epoch: 53, step: 96
	action: tensor([[ 1.4997,  0.1728, -1.0187,  0.9266, -0.1941,  0.1948, -0.0956]],
       dtype=torch.float64)
	q_value: tensor([[-33.2658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9835181947359941, distance: 0.1469126127775727 entropy 0.03264415264129639
epoch: 53, step: 97
	action: tensor([[ 1.6265, -0.4123, -1.0199,  0.7468, -0.2281,  0.4021, -0.2218]],
       dtype=torch.float64)
	q_value: tensor([[-38.1061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6950741739627098, distance: 0.631907918758542 entropy 0.03264415264129639
epoch: 53, step: 98
	action: tensor([[ 1.9306, -0.2343, -0.9147,  1.1468, -0.3377,  0.3445, -0.4063]],
       dtype=torch.float64)
	q_value: tensor([[-36.4392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 99
	action: tensor([[ 0.6608, -0.3963, -0.7645,  0.0408, -0.3491,  0.3601, -0.4001]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4398350896807909, distance: 0.8564749068795735 entropy 0.03264415264129639
epoch: 53, step: 100
	action: tensor([[ 1.4728, -0.2073, -0.8486,  0.6855, -0.3271,  0.0926, -0.1851]],
       dtype=torch.float64)
	q_value: tensor([[-21.0702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7774435921129996, distance: 0.5398545236116049 entropy 0.03264415264129639
epoch: 53, step: 101
	action: tensor([[ 1.7156, -0.0633, -0.7981,  0.5078, -0.0179,  0.1192, -0.5436]],
       dtype=torch.float64)
	q_value: tensor([[-33.7593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 102
	action: tensor([[ 1.0268, -0.0244, -0.1949,  1.0026,  0.0278,  0.1347, -0.3654]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.02774239608889396 entropy 0.03264415264129639
epoch: 53, step: 103
	action: tensor([[ 0.6577, -0.0176, -0.4880,  0.5545, -0.0474, -0.4283,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8238413155017761, distance: 0.4802957265427982 entropy 0.03264415264129639
epoch: 53, step: 104
	action: tensor([[ 1.4222e+00, -2.3387e-03, -5.5937e-01,  7.5878e-01, -3.0427e-01,
         -7.7362e-04, -5.1269e-02]], dtype=torch.float64)
	q_value: tensor([[-16.8918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9014832957997744, distance: 0.3591795735507094 entropy 0.03264415264129639
epoch: 53, step: 105
	action: tensor([[ 1.3852,  0.1864, -1.0598,  1.0099, -0.4801, -0.0176,  0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-31.5349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9890115597664679, distance: 0.1199567576225134 entropy 0.03264415264129639
epoch: 53, step: 106
	action: tensor([[ 1.7052,  0.2387, -0.4709,  0.4952, -0.5218, -0.0121,  0.1556]],
       dtype=torch.float64)
	q_value: tensor([[-36.8832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 107
	action: tensor([[ 0.3800, -0.0146, -0.9352,  0.3312, -0.2912,  0.3603,  0.1911]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5738799289205756, distance: 0.747003589456674 entropy 0.03264415264129639
epoch: 53, step: 108
	action: tensor([[ 1.4526,  0.2468, -0.7443,  0.9071,  0.0382, -0.1305,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-18.8542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9752108905861225, distance: 0.1801719402565867 entropy 0.03264415264129639
epoch: 53, step: 109
	action: tensor([[ 1.9246, -0.5302, -0.8230,  0.8356, -0.3415, -0.2284, -0.3421]],
       dtype=torch.float64)
	q_value: tensor([[-33.1947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 110
	action: tensor([[ 0.7072, -0.0048, -0.3779,  0.4726, -0.4612,  0.3992,  0.5832]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8860035280005288, distance: 0.3863691110440923 entropy 0.03264415264129639
epoch: 53, step: 111
	action: tensor([[ 1.0786,  0.1640, -0.2602,  0.3092, -0.2926,  0.2891, -0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-18.3103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9558141925791696, distance: 0.24054597312716083 entropy 0.03264415264129639
epoch: 53, step: 112
	action: tensor([[ 1.0436,  0.1953, -0.9362,  0.1999, -0.1346,  0.2332, -0.2532]],
       dtype=torch.float64)
	q_value: tensor([[-25.5181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9347298652136539, distance: 0.2923573037951274 entropy 0.03264415264129639
epoch: 53, step: 113
	action: tensor([[ 1.3983, -0.0077, -0.8701,  1.1778, -0.5560,  0.3665, -0.1501]],
       dtype=torch.float64)
	q_value: tensor([[-29.6843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9758162345409347, distance: 0.17795846638180532 entropy 0.03264415264129639
epoch: 53, step: 114
	action: tensor([[ 1.6773, -0.4347, -0.6809,  0.8735, -0.4460,  0.0487,  0.0721]],
       dtype=torch.float64)
	q_value: tensor([[-38.4172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 115
	action: tensor([[ 1.4114, -0.2807, -0.1877,  0.3475, -0.5490,  0.1658,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5617854230606218, distance: 0.7575304651241954 entropy 0.03264415264129639
epoch: 53, step: 116
	action: tensor([[ 0.9309, -0.1184, -0.7818,  0.6517, -0.4259,  0.0995, -0.0198]],
       dtype=torch.float64)
	q_value: tensor([[-26.6984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8789833240061602, distance: 0.39808819874689844 entropy 0.03264415264129639
epoch: 53, step: 117
	action: tensor([[ 1.6024, -0.1294, -0.1795,  0.8742, -0.1996,  0.3384, -0.1741]],
       dtype=torch.float64)
	q_value: tensor([[-25.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.795088266333986, distance: 0.5180123418686906 entropy 0.03264415264129639
epoch: 53, step: 118
	action: tensor([[ 1.7518, -0.0695, -0.7507,  1.3957, -0.5540,  0.3993, -0.1359]],
       dtype=torch.float64)
	q_value: tensor([[-33.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 119
	action: tensor([[ 1.3327, -0.2252, -0.6563,  0.6981, -0.0174,  0.2318,  0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.862151246511259, distance: 0.4248720037700695 entropy 0.03264415264129639
epoch: 53, step: 120
	action: tensor([[ 1.2523,  0.1348, -0.7688,  0.9385, -0.5106, -0.0245,  0.0747]],
       dtype=torch.float64)
	q_value: tensor([[-26.1717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07749442577298775 entropy 0.03264415264129639
epoch: 53, step: 121
	action: tensor([[ 0.7772, -0.3400, -0.6955,  0.7776, -0.1057,  0.4151, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7752757267036708, distance: 0.5424774446474199 entropy 0.03264415264129639
epoch: 53, step: 122
	action: tensor([[ 1.2791, -0.2652, -0.8913,  0.7321, -0.1961,  0.5765,  0.1014]],
       dtype=torch.float64)
	q_value: tensor([[-20.6254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8813134917393032, distance: 0.394236989567237 entropy 0.03264415264129639
epoch: 53, step: 123
	action: tensor([[ 1.8369,  0.1999, -0.7909,  0.9568, -0.7284,  0.3063, -0.1381]],
       dtype=torch.float64)
	q_value: tensor([[-29.7318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 53, step: 124
	action: tensor([[ 0.7536, -0.4323, -0.5790,  0.2406, -0.1954, -0.1832, -0.4674]],
       dtype=torch.float64)
	q_value: tensor([[-30.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4529153656752042, distance: 0.8464161705448326 entropy 0.03264415264129639
epoch: 53, step: 125
	action: tensor([[ 1.7821, -0.0819, -1.3301,  0.8546, -0.4135,  0.3845, -0.3895]],
       dtype=torch.float64)
	q_value: tensor([[-19.3944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8038997022142422, distance: 0.5067524065859055 entropy 0.03264415264129639
epoch: 53, step: 126
	action: tensor([[ 1.6041, -0.3635, -0.4980,  0.8325, -0.1896,  0.2601,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[-44.3828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.740884901921059, distance: 0.5825095540256103 entropy 0.03264415264129639
epoch: 53, step: 127
	action: tensor([[ 1.9319,  0.0954, -0.7271,  1.1306, -0.4359,  0.1155, -0.4598]],
       dtype=torch.float64)
	q_value: tensor([[-31.2041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 53 actor 610.9025088515924 critic 958.8144896006186 
epoch: 54, step: 0
	action: tensor([[ 1.0443,  0.3975, -0.4630,  0.3344,  0.1581,  0.2249, -0.0490]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9912515352242632, distance: 0.10703421223787354 entropy 0.03264415264129639
epoch: 54, step: 1
	action: tensor([[ 1.2184, -0.3692, -0.6223,  0.7990,  0.0100,  0.0632, -0.3137]],
       dtype=torch.float64)
	q_value: tensor([[-23.7534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8300869384610821, distance: 0.47170456135299366 entropy 0.03264415264129639
epoch: 54, step: 2
	action: tensor([[ 1.5551, -0.1000, -0.6915,  0.8732, -0.0032,  0.4820, -0.3145]],
       dtype=torch.float64)
	q_value: tensor([[-26.3564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.936189367433602, distance: 0.2890701302980638 entropy 0.03264415264129639
epoch: 54, step: 3
	action: tensor([[ 1.6122, -0.2070, -0.5333,  0.6219,  0.4460, -0.0775, -0.3890]],
       dtype=torch.float64)
	q_value: tensor([[-34.7047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6116445312695988, distance: 0.713134432037688 entropy 0.03264415264129639
epoch: 54, step: 4
	action: tensor([[ 1.3580, -0.0076, -0.6904,  1.0848,  0.2623,  0.4005,  0.1465]],
       dtype=torch.float64)
	q_value: tensor([[-30.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08662514844878885 entropy 0.03264415264129639
epoch: 54, step: 5
	action: tensor([[ 0.4709, -0.1828, -0.5949,  0.3765,  0.3110,  0.1044,  0.1636]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6262962195902291, distance: 0.6995527028086276 entropy 0.03264415264129639
epoch: 54, step: 6
	action: tensor([[ 1.1959, -0.1673, -0.5513,  0.5426, -0.0382,  0.2110,  0.0698]],
       dtype=torch.float64)
	q_value: tensor([[-11.6839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8621697485086718, distance: 0.42484348974905883 entropy 0.03264415264129639
epoch: 54, step: 7
	action: tensor([[ 1.6053, -0.4943, -0.5330,  0.3205,  0.1165,  0.3123, -0.0528]],
       dtype=torch.float64)
	q_value: tensor([[-23.3826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32854886697541974, distance: 0.9376999627842691 entropy 0.03264415264129639
epoch: 54, step: 8
	action: tensor([[ 1.3462, -0.3521, -0.7000,  0.6119, -0.3009,  0.1538, -0.1440]],
       dtype=torch.float64)
	q_value: tensor([[-26.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7296097541635151, distance: 0.5950482791745807 entropy 0.03264415264129639
epoch: 54, step: 9
	action: tensor([[ 0.7632,  0.1441, -0.7101,  0.2668,  0.4062, -0.1817,  0.1374]],
       dtype=torch.float64)
	q_value: tensor([[-28.1274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8752420914875999, distance: 0.40419481305420374 entropy 0.03264415264129639
epoch: 54, step: 10
	action: tensor([[ 1.2186, -0.2656, -0.8825,  0.5754, -0.0820, -0.0059, -0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-16.8722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7481083249909443, distance: 0.5743327748860613 entropy 0.03264415264129639
epoch: 54, step: 11
	action: tensor([[ 1.3783e+00, -3.5602e-01, -7.5477e-01,  7.0622e-01, -1.7935e-01,
         -2.2451e-04, -1.0737e-01]], dtype=torch.float64)
	q_value: tensor([[-25.6358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7273167929923009, distance: 0.5975660145204332 entropy 0.03264415264129639
epoch: 54, step: 12
	action: tensor([[ 1.3055,  0.0201, -0.5768,  0.8643, -0.1614,  0.2845, -0.3070]],
       dtype=torch.float64)
	q_value: tensor([[-28.1573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9903457115157845, distance: 0.11243896241326112 entropy 0.03264415264129639
epoch: 54, step: 13
	action: tensor([[ 0.9837,  0.1329, -0.9723,  0.7752, -0.5320,  0.5130, -0.5991]],
       dtype=torch.float64)
	q_value: tensor([[-31.2090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8824463356934998, distance: 0.3923510135776799 entropy 0.03264415264129639
epoch: 54, step: 14
	action: tensor([[ 0.9068, -0.0342, -1.2193,  0.8111, -0.7513,  0.1740, -0.3656]],
       dtype=torch.float64)
	q_value: tensor([[-35.3114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7559802129874736, distance: 0.565287282582052 entropy 0.03264415264129639
epoch: 54, step: 15
	action: tensor([[ 1.3683, -0.3956, -0.3031,  0.4051,  0.1580,  0.0849,  0.0584]],
       dtype=torch.float64)
	q_value: tensor([[-32.7160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.525247323333989, distance: 0.7884794960902232 entropy 0.03264415264129639
epoch: 54, step: 16
	action: tensor([[ 0.8320, -0.0299, -0.5714,  0.7749, -0.3104,  0.4875, -0.1515]],
       dtype=torch.float64)
	q_value: tensor([[-21.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9224138230092875, distance: 0.31874902842483555 entropy 0.03264415264129639
epoch: 54, step: 17
	action: tensor([[ 1.2816,  0.0809, -0.5021,  0.4726, -0.3702, -0.4540,  0.1581]],
       dtype=torch.float64)
	q_value: tensor([[-24.1155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8004759792624286, distance: 0.5111569701897788 entropy 0.03264415264129639
epoch: 54, step: 18
	action: tensor([[ 0.7769,  0.0735, -0.0867,  0.8278, -0.2269,  0.4200, -0.1791]],
       dtype=torch.float64)
	q_value: tensor([[-25.1835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9834836794533386, distance: 0.14706636045083915 entropy 0.03264415264129639
epoch: 54, step: 19
	action: tensor([[ 1.9887,  0.0903, -0.5314,  1.0986, -0.4122,  0.2515, -0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-21.0999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 54, step: 20
	action: tensor([[ 0.9054, -0.5228, -0.5929,  0.3135,  0.0396,  0.4499,  0.2249]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5643371542251177, distance: 0.7553216884033449 entropy 0.03264415264129639
epoch: 54, step: 21
	action: tensor([[ 0.6693,  0.3595, -0.3905,  0.1096, -0.1488,  0.2712, -0.6986]],
       dtype=torch.float64)
	q_value: tensor([[-16.6736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9389095767399385, distance: 0.28284158371780066 entropy 0.03264415264129639
epoch: 54, step: 22
	action: tensor([[ 0.9518, -0.2190, -0.8650,  0.8374, -0.2497, -0.1386, -0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-23.9008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8400634322504913, distance: 0.4576469571909576 entropy 0.03264415264129639
epoch: 54, step: 23
	action: tensor([[ 1.1894, -0.3231, -0.7506,  1.0913, -0.0257, -0.2321, -0.0881]],
       dtype=torch.float64)
	q_value: tensor([[-25.8005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8907404581980697, distance: 0.37825648344822194 entropy 0.03264415264129639
epoch: 54, step: 24
	action: tensor([[ 1.4495, -0.0108, -0.6308,  1.0371, -0.2059,  0.4850, -0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-26.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9890058071064914, distance: 0.11998815334980303 entropy 0.03264415264129639
epoch: 54, step: 25
	action: tensor([[ 1.2377, -0.0646, -0.8196,  0.3665, -0.0956,  0.3210, -0.1554]],
       dtype=torch.float64)
	q_value: tensor([[-33.6064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.853504602507094, distance: 0.43799452674223416 entropy 0.03264415264129639
epoch: 54, step: 26
	action: tensor([[ 0.6891,  0.2314, -0.4635,  0.7619, -0.0470, -0.1485, -0.1809]],
       dtype=torch.float64)
	q_value: tensor([[-27.7664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.938194407702297, distance: 0.2844923414356567 entropy 0.03264415264129639
epoch: 54, step: 27
	action: tensor([[ 1.6460,  0.1675, -0.2104,  0.8288,  0.0994,  0.1704,  0.1518]],
       dtype=torch.float64)
	q_value: tensor([[-20.2164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8180523983389141, distance: 0.4881236621720248 entropy 0.03264415264129639
epoch: 54, step: 28
	action: tensor([[ 1.1106, -0.1125, -0.6162,  0.9400,  0.0467,  0.1035, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-29.6138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.977492836201237, distance: 0.17167896209841488 entropy 0.03264415264129639
epoch: 54, step: 29
	action: tensor([[ 1.8071, -0.3242, -0.2419,  0.8096, -0.1690,  0.1619, -0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-24.6090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 54, step: 30
	action: tensor([[ 0.9031, -0.2272, -0.5641,  0.5635,  0.1208,  0.1257,  0.2665]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8263904706619607, distance: 0.4768079337168748 entropy 0.03264415264129639
epoch: 54, step: 31
	action: tensor([[ 1.0621,  0.0486, -0.4418,  0.0970, -0.0965,  0.2804,  0.1105]],
       dtype=torch.float64)
	q_value: tensor([[-17.1370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8492843382566047, distance: 0.4442586429264695 entropy 0.03264415264129639
epoch: 54, step: 32
	action: tensor([[ 1.2420, -0.3285, -0.3849,  0.6033, -0.6088,  0.4107, -0.4769]],
       dtype=torch.float64)
	q_value: tensor([[-20.9058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8246629561055066, distance: 0.47917431800225463 entropy 0.03264415264129639
epoch: 54, step: 33
	action: tensor([[ 1.5604,  0.0243, -0.4223,  0.8634, -0.2066,  0.1778,  0.1158]],
       dtype=torch.float64)
	q_value: tensor([[-30.3241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8940266507046687, distance: 0.37252465698925663 entropy 0.03264415264129639
epoch: 54, step: 34
	action: tensor([[ 1.6610,  0.1488, -0.4996,  0.9510, -0.2964,  0.0696,  0.4961]],
       dtype=torch.float64)
	q_value: tensor([[-30.8981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 54, step: 35
	action: tensor([[ 1.4008, -0.0143, -0.4564,  0.6716, -0.0581,  0.2634, -0.0455]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9163303455740248, distance: 0.33100966970411516 entropy 0.03264415264129639
epoch: 54, step: 36
	action: tensor([[ 1.4100,  0.2243, -0.7284,  0.7483,  0.1449,  0.4852, -0.1203]],
       dtype=torch.float64)
	q_value: tensor([[-28.4383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9882719303957999, distance: 0.12392814830544416 entropy 0.03264415264129639
epoch: 54, step: 37
	action: tensor([[ 1.4093, -0.2526, -0.4931,  1.0969, -0.4112, -0.0653, -0.0709]],
       dtype=torch.float64)
	q_value: tensor([[-32.3871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9200704950594075, distance: 0.32352679452704747 entropy 0.03264415264129639
epoch: 54, step: 38
	action: tensor([[ 1.3142, -0.6437, -0.7965,  0.8836, -0.2748,  0.1527, -0.1785]],
       dtype=torch.float64)
	q_value: tensor([[-30.4116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6730430168870037, distance: 0.654337788682288 entropy 0.03264415264129639
epoch: 54, step: 39
	action: tensor([[ 1.1420, -0.7726, -0.5194,  0.5502, -0.1894, -0.0752, -0.2407]],
       dtype=torch.float64)
	q_value: tensor([[-28.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32312233890647346, distance: 0.9414814855903968 entropy 0.03264415264129639
epoch: 54, step: 40
	action: tensor([[ 1.2630,  0.3014, -0.7410,  0.8555,  0.0425, -0.3151,  0.1075]],
       dtype=torch.float64)
	q_value: tensor([[-22.0572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09638817298016222 entropy 0.03264415264129639
epoch: 54, step: 41
	action: tensor([[ 1.0585, -0.2625, -0.5288,  0.5287, -0.1372,  0.3724, -0.1908]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8489908542240224, distance: 0.44469097824459874 entropy 0.03264415264129639
epoch: 54, step: 42
	action: tensor([[ 1.2465, -0.3411, -0.5572,  1.1318, -0.0639,  0.0414, -0.1217]],
       dtype=torch.float64)
	q_value: tensor([[-23.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9450112634217118, distance: 0.26834503483942657 entropy 0.03264415264129639
epoch: 54, step: 43
	action: tensor([[ 1.2386,  0.1020, -0.7901,  1.0550, -0.4638,  0.3153, -0.3288]],
       dtype=torch.float64)
	q_value: tensor([[-27.0752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9782757469787532, distance: 0.1686666112570606 entropy 0.03264415264129639
epoch: 54, step: 44
	action: tensor([[ 1.4044, -0.0893, -0.3072,  0.6659, -0.3825,  0.5512, -0.1313]],
       dtype=torch.float64)
	q_value: tensor([[-34.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9176766694984996, distance: 0.328335740141352 entropy 0.03264415264129639
epoch: 54, step: 45
	action: tensor([[ 1.5066,  0.0927, -0.3182,  0.3218, -0.0685,  0.4005, -0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-30.3363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.769035711776696, distance: 0.5499574763866737 entropy 0.03264415264129639
epoch: 54, step: 46
	action: tensor([[ 1.4610, -0.1185, -1.5142,  0.3917,  0.2499,  0.0385,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-29.4051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5974243514081106, distance: 0.7260732619534035 entropy 0.03264415264129639
epoch: 54, step: 47
	action: tensor([[ 1.8432,  0.1883, -0.7006,  0.4694, -0.1669, -0.3251,  0.1063]],
       dtype=torch.float64)
	q_value: tensor([[-31.6453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 54, step: 48
	action: tensor([[ 1.2415,  0.0885, -0.9855,  0.0066, -0.0430,  0.1511, -0.2169]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.728783348517847, distance: 0.5959569215863413 entropy 0.03264415264129639
epoch: 54, step: 49
	action: tensor([[ 0.8945, -0.7578, -0.9569,  0.3557, -0.0684,  0.2412, -0.1543]],
       dtype=torch.float64)
	q_value: tensor([[-28.3388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22027005396365418, distance: 1.0104825084146483 entropy 0.03264415264129639
epoch: 54, step: 50
	action: tensor([[ 1.2531, -0.6014, -0.5428,  0.6185, -0.1033, -0.0026,  0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-19.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5160375301211714, distance: 0.7960906725360974 entropy 0.03264415264129639
epoch: 54, step: 51
	action: tensor([[ 1.1261,  0.3375, -0.3640,  0.1816,  0.1052,  0.4176,  0.2407]],
       dtype=torch.float64)
	q_value: tensor([[-22.1457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9693969357302725, distance: 0.20018851152584421 entropy 0.03264415264129639
epoch: 54, step: 52
	action: tensor([[ 1.3318,  0.0704, -0.6322,  0.7395, -0.1342, -0.2520, -0.4756]],
       dtype=torch.float64)
	q_value: tensor([[-22.4116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9147655515400129, distance: 0.33409061109663385 entropy 0.03264415264129639
epoch: 54, step: 53
	action: tensor([[ 1.4569, -0.0461, -0.3405,  0.6705, -0.3913,  0.2832, -0.4583]],
       dtype=torch.float64)
	q_value: tensor([[-31.4024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8710725600115565, distance: 0.4108935963050506 entropy 0.03264415264129639
epoch: 54, step: 54
	action: tensor([[ 1.9196, -0.0047, -0.2857,  1.2085, -0.1396,  0.1348,  0.1230]],
       dtype=torch.float64)
	q_value: tensor([[-33.1928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 54, step: 55
	action: tensor([[ 1.5220, -0.0773, -0.5337,  0.3854, -0.1550,  0.1936,  0.1718]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6874854297862979, distance: 0.6397227983164514 entropy 0.03264415264129639
epoch: 54, step: 56
	action: tensor([[ 1.4342,  0.0470, -0.6146,  0.6522, -0.0278,  0.2828, -0.3212]],
       dtype=torch.float64)
	q_value: tensor([[-27.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9197476288743757, distance: 0.3241795609246698 entropy 0.03264415264129639
epoch: 54, step: 57
	action: tensor([[ 1.1709,  0.2048, -0.6802,  0.6675, -0.0168,  0.5968,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[-32.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05117876420340016 entropy 0.03264415264129639
epoch: 54, step: 58
	action: tensor([[ 0.3560,  0.2599, -0.1175,  0.5512,  0.1965,  0.1813, -0.1385]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8391131194537533, distance: 0.4590045692844309 entropy 0.03264415264129639
epoch: 54, step: 59
	action: tensor([[ 1.2757,  0.0274, -0.5244,  0.9535, -0.1745,  0.0076, -0.2331]],
       dtype=torch.float64)
	q_value: tensor([[-12.9440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9879069282388887, distance: 0.1258418250964265 entropy 0.03264415264129639
epoch: 54, step: 60
	action: tensor([[ 1.6723, -0.1226, -0.5443,  0.4793, -0.2231,  0.3586,  0.0979]],
       dtype=torch.float64)
	q_value: tensor([[-29.7198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7092202353497378, distance: 0.6170761817863638 entropy 0.03264415264129639
epoch: 54, step: 61
	action: tensor([[ 1.3113, -0.2465, -0.5863,  0.8715, -0.2316,  0.2136,  0.0688]],
       dtype=torch.float64)
	q_value: tensor([[-30.5154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9217526498815722, distance: 0.3201043033914171 entropy 0.03264415264129639
epoch: 54, step: 62
	action: tensor([[ 1.3899,  0.0207, -0.7729,  0.8036, -0.4628,  0.0959, -0.1919]],
       dtype=torch.float64)
	q_value: tensor([[-27.0063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9551389365549877, distance: 0.24237703893586415 entropy 0.03264415264129639
epoch: 54, step: 63
	action: tensor([[ 0.8850, -0.0590, -0.0951,  0.4134, -0.2162,  0.0029, -0.1734]],
       dtype=torch.float64)
	q_value: tensor([[-33.3308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.888157366733854, distance: 0.3827016944663097 entropy 0.03264415264129639
epoch: 54, step: 64
	action: tensor([[ 1.3239, -0.3559, -1.0550,  0.6569,  0.1467,  0.6573,  0.1466]],
       dtype=torch.float64)
	q_value: tensor([[-18.7889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8109058853467797, distance: 0.4976175622101034 entropy 0.03264415264129639
epoch: 54, step: 65
	action: tensor([[ 1.3864, -0.2547, -0.7192,  1.0132, -0.4827, -0.1488, -0.2974]],
       dtype=torch.float64)
	q_value: tensor([[-27.5090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8992424051478752, distance: 0.36324160779628256 entropy 0.03264415264129639
epoch: 54, step: 66
	action: tensor([[ 1.6491, -0.5773, -1.0539,  0.9121,  0.2221,  0.2998, -0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-32.6412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6621047345344042, distance: 0.6651931166657661 entropy 0.03264415264129639
epoch: 54, step: 67
	action: tensor([[ 1.6485, -0.3519, -0.7347,  0.7098,  0.0261,  0.3467,  0.1842]],
       dtype=torch.float64)
	q_value: tensor([[-32.4494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7156385562644745, distance: 0.6102278839220878 entropy 0.03264415264129639
epoch: 54, step: 68
	action: tensor([[ 1.1982, -0.0125, -0.8157,  0.6132, -0.0592,  0.0856, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-29.2616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9262507157018833, distance: 0.31076750301713174 entropy 0.03264415264129639
epoch: 54, step: 69
	action: tensor([[ 1.1953, -0.0517, -0.6974,  1.1363, -0.4494,  0.0444, -0.0598]],
       dtype=torch.float64)
	q_value: tensor([[-27.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.990250465886918, distance: 0.1129922416970231 entropy 0.03264415264129639
epoch: 54, step: 70
	action: tensor([[ 1.5111, -0.0238, -0.9444,  0.2602, -0.0609, -0.0243,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[-29.8997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6005957570025955, distance: 0.7232076815881404 entropy 0.03264415264129639
epoch: 54, step: 71
	action: tensor([[ 0.9811,  0.0458, -0.8962,  0.8570,  0.0174,  0.0416, -0.1051]],
       dtype=torch.float64)
	q_value: tensor([[-29.9962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9355458510960679, distance: 0.2905240784719813 entropy 0.03264415264129639
epoch: 54, step: 72
	action: tensor([[ 1.6540,  0.2126, -0.6909,  0.7333, -0.2341, -0.0459, -0.3263]],
       dtype=torch.float64)
	q_value: tensor([[-25.5926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8722644437638984, distance: 0.4089899110953075 entropy 0.03264415264129639
epoch: 54, step: 73
	action: tensor([[ 1.9988, -0.0301, -0.5902,  1.1143, -0.4440,  0.5877, -0.2000]],
       dtype=torch.float64)
	q_value: tensor([[-36.4808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 54, step: 74
	action: tensor([[ 1.3639,  0.0575, -0.4989,  0.4518, -0.0248, -0.0519,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8226604416668268, distance: 0.4819028608479877 entropy 0.03264415264129639
epoch: 54, step: 75
	action: tensor([[ 1.0888,  0.0314, -1.1099,  0.6975, -0.4244,  0.0817,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[-26.1839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9058253704493502, distance: 0.35117505048638553 entropy 0.03264415264129639
epoch: 54, step: 76
	action: tensor([[ 1.3720, -0.4994,  0.3512,  0.6073, -0.0961,  0.6145,  0.1742]],
       dtype=torch.float64)
	q_value: tensor([[-28.7624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5702624592031666, distance: 0.7501676647586009 entropy 0.03264415264129639
epoch: 54, step: 77
	action: tensor([[ 1.0933, -0.1565, -1.0668,  0.6665, -0.0885,  0.4160, -0.3875]],
       dtype=torch.float64)
	q_value: tensor([[-22.1206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8513323992996908, distance: 0.4412298235535879 entropy 0.03264415264129639
epoch: 54, step: 78
	action: tensor([[ 1.1971,  0.1835, -0.3938,  0.7757, -0.2232,  0.0711,  0.2047]],
       dtype=torch.float64)
	q_value: tensor([[-30.1067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9884250379822714, distance: 0.12311656232331093 entropy 0.03264415264129639
epoch: 54, step: 79
	action: tensor([[ 1.3208,  0.2585, -0.4490,  0.8487, -0.2492,  0.3327,  0.1059]],
       dtype=torch.float64)
	q_value: tensor([[-25.2647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9727818243105729, distance: 0.1887931356977783 entropy 0.03264415264129639
epoch: 54, step: 80
	action: tensor([[ 1.4801, -0.6009, -0.3480,  0.8586,  0.0334,  0.5801, -0.1513]],
       dtype=torch.float64)
	q_value: tensor([[-29.8562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7395340065250214, distance: 0.5840260355200412 entropy 0.03264415264129639
epoch: 54, step: 81
	action: tensor([[ 1.7370,  0.0928, -1.0173,  0.7246,  0.3172,  0.4346, -0.3665]],
       dtype=torch.float64)
	q_value: tensor([[-27.6614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 54, step: 82
	action: tensor([[ 0.9311, -0.0409, -0.6772,  0.4892, -0.3539,  0.6238, -0.3593]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9153583080998533, distance: 0.33292688028605055 entropy 0.03264415264129639
epoch: 54, step: 83
	action: tensor([[ 1.1629, -0.1277, -0.7891,  0.4372, -0.1484,  0.0279, -0.0930]],
       dtype=torch.float64)
	q_value: tensor([[-27.7002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8086600321721913, distance: 0.5005639183358246 entropy 0.03264415264129639
epoch: 54, step: 84
	action: tensor([[ 1.2383, -0.0238, -0.6938,  0.7631, -0.3851,  0.3652,  0.1279]],
       dtype=torch.float64)
	q_value: tensor([[-25.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.976527450824244, distance: 0.1753221638218538 entropy 0.03264415264129639
epoch: 54, step: 85
	action: tensor([[ 1.1944, -0.5805, -0.3855,  0.1499,  0.0481,  0.0612,  0.3060]],
       dtype=torch.float64)
	q_value: tensor([[-28.3208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.227853512205913, distance: 1.0055566522937978 entropy 0.03264415264129639
epoch: 54, step: 86
	action: tensor([[ 0.8936, -0.0379, -0.1075,  0.4442,  0.2781,  0.3296, -0.1232]],
       dtype=torch.float64)
	q_value: tensor([[-17.4620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9451482329365064, distance: 0.26801062084804833 entropy 0.03264415264129639
epoch: 54, step: 87
	action: tensor([[ 1.5479, -0.0269, -0.4315,  1.2078, -0.4730,  0.4614, -0.2383]],
       dtype=torch.float64)
	q_value: tensor([[-17.6501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9547773410568118, distance: 0.24335189932257564 entropy 0.03264415264129639
epoch: 54, step: 88
	action: tensor([[ 1.8296,  0.0372, -0.3296,  1.5038, -0.3341,  0.0444,  0.3064]],
       dtype=torch.float64)
	q_value: tensor([[-36.7820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 54, step: 89
	action: tensor([[ 0.8800,  0.0058, -0.4707,  0.6574, -0.4220,  0.1440, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9494019725911894, distance: 0.2574088570564039 entropy 0.03264415264129639
epoch: 54, step: 90
	action: tensor([[ 1.1281, -0.4771, -0.3763,  0.4212, -0.2478,  0.2912, -0.1751]],
       dtype=torch.float64)
	q_value: tensor([[-22.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6434036482326448, distance: 0.683353056280271 entropy 0.03264415264129639
epoch: 54, step: 91
	action: tensor([[ 1.4428,  0.2584, -0.7701,  0.5436, -0.5017,  0.7288, -0.4931]],
       dtype=torch.float64)
	q_value: tensor([[-22.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.985342242855578, distance: 0.13854487150335476 entropy 0.03264415264129639
epoch: 54, step: 92
	action: tensor([[ 1.5855, -0.0330, -0.6017,  0.2602,  0.0316, -0.1653, -0.1791]],
       dtype=torch.float64)
	q_value: tensor([[-40.2519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4823521421575453, distance: 0.8233299309481156 entropy 0.03264415264129639
epoch: 54, step: 93
	action: tensor([[ 1.3021, -0.4499, -0.7016,  0.9933, -0.3347,  0.6118, -0.1119]],
       dtype=torch.float64)
	q_value: tensor([[-29.6493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9038543806100743, distance: 0.3548309092121953 entropy 0.03264415264129639
epoch: 54, step: 94
	action: tensor([[ 1.6664, -0.2519, -0.1383,  0.8848, -0.1995,  0.2277,  0.2802]],
       dtype=torch.float64)
	q_value: tensor([[-29.9552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7360618146448232, distance: 0.5879058836005561 entropy 0.03264415264129639
epoch: 54, step: 95
	action: tensor([[ 0.6768,  0.1841, -0.5012,  0.1623, -0.6655,  0.2018, -0.2169]],
       dtype=torch.float64)
	q_value: tensor([[-27.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8918453252646577, distance: 0.37633909919431785 entropy 0.03264415264129639
epoch: 54, step: 96
	action: tensor([[ 1.1634,  0.0656, -0.8598,  0.4961, -0.1348,  0.1707,  0.2108]],
       dtype=torch.float64)
	q_value: tensor([[-22.4079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9335948475346338, distance: 0.29488832753144134 entropy 0.03264415264129639
epoch: 54, step: 97
	action: tensor([[ 0.8910,  0.1941, -0.8063,  0.4845, -0.1423,  0.3586, -0.1170]],
       dtype=torch.float64)
	q_value: tensor([[-25.5429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9525917338114428, distance: 0.24916310229894434 entropy 0.03264415264129639
epoch: 54, step: 98
	action: tensor([[ 0.9580,  0.2657, -0.9844,  1.0655, -0.1754,  0.2754, -0.4635]],
       dtype=torch.float64)
	q_value: tensor([[-25.1139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.846170678533552, distance: 0.4488241896768431 entropy 0.03264415264129639
epoch: 54, step: 99
	action: tensor([[ 1.2898,  0.0391, -0.6571,  0.9934, -0.4181,  0.1904, -0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-32.7459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.04093752293029952 entropy 0.03264415264129639
epoch: 54, step: 100
	action: tensor([[ 0.7666,  0.1935, -0.5279,  0.8526, -0.0423,  0.1215,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9158313095919524, distance: 0.3319953326766304 entropy 0.03264415264129639
epoch: 54, step: 101
	action: tensor([[ 1.0327, -0.1521, -0.5918,  0.4321, -0.2456, -0.2547,  0.3945]],
       dtype=torch.float64)
	q_value: tensor([[-20.0826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7589484563907456, distance: 0.5618387015437213 entropy 0.03264415264129639
epoch: 54, step: 102
	action: tensor([[ 1.4161, -0.4546, -0.7947,  0.5723, -0.5079,  0.5238, -0.1567]],
       dtype=torch.float64)
	q_value: tensor([[-19.3106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6750620731966465, distance: 0.6523142950018984 entropy 0.03264415264129639
epoch: 54, step: 103
	action: tensor([[ 1.0095,  0.2110, -0.5942,  0.8014, -0.1082,  0.3128, -0.1264]],
       dtype=torch.float64)
	q_value: tensor([[-31.3779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9744067630100747, distance: 0.18307089400586668 entropy 0.03264415264129639
epoch: 54, step: 104
	action: tensor([[ 1.2199,  0.0241, -0.6611,  1.1950, -0.5600,  0.0433,  0.0589]],
       dtype=torch.float64)
	q_value: tensor([[-26.5869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9921355321164746, distance: 0.10148254847487433 entropy 0.03264415264129639
epoch: 54, step: 105
	action: tensor([[ 1.2464, -0.1474, -0.6014,  0.9950, -0.4066,  0.5686, -0.1399]],
       dtype=torch.float64)
	q_value: tensor([[-30.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9835903263345679, distance: 0.14659078330547456 entropy 0.03264415264129639
epoch: 54, step: 106
	action: tensor([[ 1.1152, -0.4510, -1.1282,  0.3015, -0.3530, -0.1233,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[-30.9410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.427587323096135, distance: 0.8657875071902796 entropy 0.03264415264129639
epoch: 54, step: 107
	action: tensor([[ 0.6961,  0.0445, -0.9908,  0.9337, -0.4918,  0.1023, -0.2175]],
       dtype=torch.float64)
	q_value: tensor([[-24.4306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7247000437338762, distance: 0.6004263793906984 entropy 0.03264415264129639
epoch: 54, step: 108
	action: tensor([[ 0.7317, -0.2095, -0.5573,  0.7059, -0.4293,  0.1963,  0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-26.3340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.822296771929407, distance: 0.482396726211591 entropy 0.03264415264129639
epoch: 54, step: 109
	action: tensor([[ 1.3745, -0.2069, -0.1602,  0.5479, -0.1020,  0.3226,  0.0410]],
       dtype=torch.float64)
	q_value: tensor([[-19.1620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.788371926670413, distance: 0.52643327476167 entropy 0.03264415264129639
epoch: 54, step: 110
	action: tensor([[ 1.1316,  0.0257, -0.8424,  0.6024,  0.2130, -0.2086, -0.4474]],
       dtype=torch.float64)
	q_value: tensor([[-24.5914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9050219961510886, distance: 0.3526697517866141 entropy 0.03264415264129639
epoch: 54, step: 111
	action: tensor([[ 1.4269, -0.7085, -0.4179,  0.6023, -0.3261,  0.5746, -0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-27.2288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5750317676028244, distance: 0.7459932992498188 entropy 0.03264415264129639
epoch: 54, step: 112
	action: tensor([[ 1.4354, -0.4151, -0.5793,  1.0696,  0.0376,  0.4105, -0.2122]],
       dtype=torch.float64)
	q_value: tensor([[-29.4279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9184272718647246, distance: 0.3268354733081024 entropy 0.03264415264129639
epoch: 54, step: 113
	action: tensor([[ 1.1827, -0.1607, -0.6949,  0.5277, -0.0747,  0.5758, -0.3341]],
       dtype=torch.float64)
	q_value: tensor([[-30.1918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9198421612404871, distance: 0.32398857285511967 entropy 0.03264415264129639
epoch: 54, step: 114
	action: tensor([[ 1.2614, -0.3314, -1.0531,  0.6727, -0.5332,  0.1307, -0.3552]],
       dtype=torch.float64)
	q_value: tensor([[-28.5053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.72155686338416, distance: 0.6038442733476899 entropy 0.03264415264129639
epoch: 54, step: 115
	action: tensor([[ 0.9995, -0.2821, -1.0384,  0.3943, -0.6201,  0.0862,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[-32.3795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6572239966106161, distance: 0.6699800928340873 entropy 0.03264415264129639
epoch: 54, step: 116
	action: tensor([[ 1.4121,  0.2758, -0.4208,  0.3344, -0.1797,  0.0865,  0.0533]],
       dtype=torch.float64)
	q_value: tensor([[-25.2838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8584363106285495, distance: 0.4305589581418614 entropy 0.03264415264129639
epoch: 54, step: 117
	action: tensor([[ 0.7854,  0.2812, -0.8139,  0.5450, -0.0196, -0.0283,  0.1902]],
       dtype=torch.float64)
	q_value: tensor([[-28.4154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9417848534502709, distance: 0.2761052666605707 entropy 0.03264415264129639
epoch: 54, step: 118
	action: tensor([[ 1.3616,  0.1766, -0.6151,  0.6203, -0.1436,  0.1203, -0.1824]],
       dtype=torch.float64)
	q_value: tensor([[-20.8436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9491179298943555, distance: 0.25813035535317297 entropy 0.03264415264129639
epoch: 54, step: 119
	action: tensor([[ 1.3775, -0.0951, -0.1126,  0.3998,  0.0102,  0.3358, -0.3636]],
       dtype=torch.float64)
	q_value: tensor([[-30.7755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7538575320358554, distance: 0.5677406212477671 entropy 0.03264415264129639
epoch: 54, step: 120
	action: tensor([[ 1.5002,  0.2907, -0.9246,  1.0440, -0.0419,  0.1096, -0.1312]],
       dtype=torch.float64)
	q_value: tensor([[-26.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07108895508410357 entropy 0.03264415264129639
epoch: 54, step: 121
	action: tensor([[ 0.7796,  0.6697, -0.2165,  1.0676, -0.2250,  0.5997, -0.3738]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 54, step: 122
	action: tensor([[ 1.0481,  0.0959, -0.7493,  0.2615, -0.1491,  0.1086, -0.0425]],
       dtype=torch.float64)
	q_value: tensor([[-23.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8924978322323901, distance: 0.375202137783048 entropy 0.03264415264129639
epoch: 54, step: 123
	action: tensor([[ 1.1477,  0.0533, -0.7256,  0.7745, -0.1381,  0.1138, -0.0967]],
       dtype=torch.float64)
	q_value: tensor([[-24.0856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9815918846048491, distance: 0.1552606347451351 entropy 0.03264415264129639
epoch: 54, step: 124
	action: tensor([[ 1.2497, -0.0347, -0.2619,  0.9771, -0.5501,  0.2988,  0.0569]],
       dtype=torch.float64)
	q_value: tensor([[-27.3612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9850462888805891, distance: 0.13993655771615662 entropy 0.03264415264129639
epoch: 54, step: 125
	action: tensor([[ 1.0936,  0.2217, -1.0524,  0.6820, -0.0934, -0.0369, -0.3512]],
       dtype=torch.float64)
	q_value: tensor([[-28.2125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9706905623902392, distance: 0.1959117282963171 entropy 0.03264415264129639
epoch: 54, step: 126
	action: tensor([[ 1.1842, -0.3403, -0.2822,  0.6771, -0.3396, -0.0444,  0.1680]],
       dtype=torch.float64)
	q_value: tensor([[-30.6993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7742960470652249, distance: 0.5436586168969949 entropy 0.03264415264129639
epoch: 54, step: 127
	action: tensor([[ 1.2543,  0.0905, -0.2019,  0.7364,  0.1008,  0.2415, -0.3573]],
       dtype=torch.float64)
	q_value: tensor([[-21.8742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9480750741629298, distance: 0.2607621995717021 entropy 0.03264415264129639
LOSS epoch 54 actor 669.6289509247454 critic 3668.976881503402 
epoch: 55, step: 0
	action: tensor([[ 0.6768, -0.3068, -0.6427,  0.9073, -0.2171,  0.3214,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-22.3806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7777100286969427, distance: 0.5395312795599517 entropy 0.03264415264129639
epoch: 55, step: 1
	action: tensor([[ 1.0026, -0.2511, -0.8061,  0.0974, -0.3526, -0.1846,  0.0732]],
       dtype=torch.float64)
	q_value: tensor([[-15.2907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5500741523631909, distance: 0.7675862120738907 entropy 0.03264415264129639
epoch: 55, step: 2
	action: tensor([[ 0.6679, -0.3619, -0.5352,  0.4857, -0.1616,  0.0032, -0.1317]],
       dtype=torch.float64)
	q_value: tensor([[-16.7164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6459903174318707, distance: 0.6808701012100421 entropy 0.03264415264129639
epoch: 55, step: 3
	action: tensor([[ 1.1080, -0.2465, -0.0272,  0.2648, -0.1355,  0.0698,  0.5846]],
       dtype=torch.float64)
	q_value: tensor([[-12.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6785537032485605, distance: 0.6488000978028517 entropy 0.03264415264129639
epoch: 55, step: 4
	action: tensor([[ 0.8273, -0.0781, -0.5783,  0.6543,  0.0735, -0.2250, -0.4844]],
       dtype=torch.float64)
	q_value: tensor([[-13.0923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8773366929099379, distance: 0.40078737074066156 entropy 0.03264415264129639
epoch: 55, step: 5
	action: tensor([[ 1.1170, -0.0519, -0.2653,  0.3722, -0.1065,  0.4353, -0.3284]],
       dtype=torch.float64)
	q_value: tensor([[-17.7237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9151607875063682, distance: 0.3333151144631215 entropy 0.03264415264129639
epoch: 55, step: 6
	action: tensor([[ 1.0063, -0.2927, -0.8491,  0.3963, -0.2407,  0.0877,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[-20.1874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6912429265048474, distance: 0.6358653374818805 entropy 0.03264415264129639
epoch: 55, step: 7
	action: tensor([[ 0.9364, -0.1920,  0.2131,  0.3299,  0.0179, -0.2089, -0.2846]],
       dtype=torch.float64)
	q_value: tensor([[-17.7028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7005366437709781, distance: 0.6262223117422712 entropy 0.03264415264129639
epoch: 55, step: 8
	action: tensor([[ 0.7985,  0.5986, -0.2864,  0.4853, -0.1263, -0.2582, -0.3037]],
       dtype=torch.float64)
	q_value: tensor([[-13.7186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9609262001403214, distance: 0.22620360245903318 entropy 0.03264415264129639
epoch: 55, step: 9
	action: tensor([[ 1.2345,  0.0478, -1.0358,  0.3014, -0.3564,  0.3802, -0.1235]],
       dtype=torch.float64)
	q_value: tensor([[-18.4610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8683286523226595, distance: 0.4152430129819849 entropy 0.03264415264129639
epoch: 55, step: 10
	action: tensor([[ 1.2115, -0.5301, -0.2600,  0.5348, -0.1805,  0.3842, -0.1249]],
       dtype=torch.float64)
	q_value: tensor([[-25.2587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6730786341166559, distance: 0.6543021473972986 entropy 0.03264415264129639
epoch: 55, step: 11
	action: tensor([[ 1.1883,  0.5164, -0.4421,  0.7848,  0.2713, -0.0641,  0.3427]],
       dtype=torch.float64)
	q_value: tensor([[-18.3128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9359970096953231, distance: 0.28950550471705916 entropy 0.03264415264129639
epoch: 55, step: 12
	action: tensor([[ 0.7355, -0.1368, -0.6335,  0.5599, -0.3027,  0.6398, -0.2339]],
       dtype=torch.float64)
	q_value: tensor([[-19.8042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8335989949937052, distance: 0.4668041041853922 entropy 0.03264415264129639
epoch: 55, step: 13
	action: tensor([[ 0.4521, -0.0370, -0.4151,  0.8214,  0.1783,  0.1153, -0.1293]],
       dtype=torch.float64)
	q_value: tensor([[-18.5997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8067406147166643, distance: 0.5030683443881868 entropy 0.03264415264129639
epoch: 55, step: 14
	action: tensor([[ 1.0740,  0.1235, -0.8372,  0.4325, -0.0446,  0.1013, -0.3321]],
       dtype=torch.float64)
	q_value: tensor([[-12.0491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9466118100953469, distance: 0.2644108617401941 entropy 0.03264415264129639
epoch: 55, step: 15
	action: tensor([[ 1.1742, -0.1592, -0.7345,  0.6184, -0.4262,  0.4459, -0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-22.1156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9050849912246192, distance: 0.3525527765866354 entropy 0.03264415264129639
epoch: 55, step: 16
	action: tensor([[ 0.7486,  0.1288, -0.1399,  0.1327,  0.0776,  0.1203,  0.1794]],
       dtype=torch.float64)
	q_value: tensor([[-22.5255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8942772752524678, distance: 0.37208389008644016 entropy 0.03264415264129639
epoch: 55, step: 17
	action: tensor([[ 0.8019, -0.2579, -0.1111,  0.6467, -0.2209, -0.2171,  0.3587]],
       dtype=torch.float64)
	q_value: tensor([[-11.0975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8200845707964954, distance: 0.48539008171872217 entropy 0.03264415264129639
epoch: 55, step: 18
	action: tensor([[ 0.6263,  0.0694,  0.0426,  0.7348,  0.2225,  0.4808, -0.0537]],
       dtype=torch.float64)
	q_value: tensor([[-11.7594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9888078955316564, distance: 0.1210633175500724 entropy 0.03264415264129639
epoch: 55, step: 19
	action: tensor([[ 0.3957,  0.2956, -0.4795,  0.4673, -0.1846,  0.0112, -0.0377]],
       dtype=torch.float64)
	q_value: tensor([[-12.8823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8127615887134565, distance: 0.4951698199083908 entropy 0.03264415264129639
epoch: 55, step: 20
	action: tensor([[ 0.5857, -0.4548, -0.6887,  0.4383,  0.1138,  0.3588, -0.1795]],
       dtype=torch.float64)
	q_value: tensor([[-12.3983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5526822125964517, distance: 0.765358266495215 entropy 0.03264415264129639
epoch: 55, step: 21
	action: tensor([[ 1.0489,  0.2959, -0.6638,  0.5256, -0.5132,  0.1018, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[-12.1395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9912854445399123, distance: 0.1068265769107054 entropy 0.03264415264129639
epoch: 55, step: 22
	action: tensor([[ 1.2273, -0.3639, -0.6301,  0.5869,  0.1944,  0.3710, -0.0350]],
       dtype=torch.float64)
	q_value: tensor([[-22.3116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7997460228142577, distance: 0.512091147643418 entropy 0.03264415264129639
epoch: 55, step: 23
	action: tensor([[ 1.0625, -0.1123, -0.6522,  0.3334, -0.2395,  0.2078,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-18.9630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8310857188498081, distance: 0.4703161346791248 entropy 0.03264415264129639
epoch: 55, step: 24
	action: tensor([[ 0.5906, -0.0620, -0.2794,  0.1168, -0.2027,  0.1227,  0.2068]],
       dtype=torch.float64)
	q_value: tensor([[-17.8266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7378867428103275, distance: 0.585869901221974 entropy 0.03264415264129639
epoch: 55, step: 25
	action: tensor([[ 0.3997, -0.2390, -0.5964,  0.3753, -0.0883,  0.0492,  0.1420]],
       dtype=torch.float64)
	q_value: tensor([[-10.1127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5473023037961806, distance: 0.7699470076283779 entropy 0.03264415264129639
epoch: 55, step: 26
	action: tensor([[ 0.3600, -0.0907, -0.2153,  0.5018,  0.1604,  0.0521,  0.3571]],
       dtype=torch.float64)
	q_value: tensor([[-9.1562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7246563516528879, distance: 0.600474023490534 entropy 0.03264415264129639
epoch: 55, step: 27
	action: tensor([[ 0.9232, -0.0016,  0.0642,  0.6697,  0.6128, -0.2244, -0.2673]],
       dtype=torch.float64)
	q_value: tensor([[-7.6259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9584219370232244, distance: 0.23333979910804178 entropy 0.03264415264129639
epoch: 55, step: 28
	action: tensor([[ 1.1575, -0.1330, -0.0664,  0.5947,  0.0012,  0.2590,  0.1126]],
       dtype=torch.float64)
	q_value: tensor([[-13.9233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9040006016066323, distance: 0.35456098807038 entropy 0.03264415264129639
epoch: 55, step: 29
	action: tensor([[ 1.1002,  0.1360, -0.3454,  0.7243, -0.1163, -0.0014,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[-16.7532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9899793870791784, distance: 0.11455230607214954 entropy 0.03264415264129639
epoch: 55, step: 30
	action: tensor([[ 1.3139,  0.0912, -0.9342, -0.1500, -0.1097, -0.1610, -0.3472]],
       dtype=torch.float64)
	q_value: tensor([[-19.3313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5507924351857502, distance: 0.7669732619427 entropy 0.03264415264129639
epoch: 55, step: 31
	action: tensor([[ 0.6863, -0.2702, -0.5141,  0.2253, -0.4794,  0.4770, -0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-23.4002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6702545283449528, distance: 0.6571221604317206 entropy 0.03264415264129639
epoch: 55, step: 32
	action: tensor([[ 1.2697, -0.0963, -0.5289,  0.4662, -0.0408, -0.1860,  0.3697]],
       dtype=torch.float64)
	q_value: tensor([[-15.3440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7676356915565522, distance: 0.5516217776635417 entropy 0.03264415264129639
epoch: 55, step: 33
	action: tensor([[ 1.1283, -0.1603, -0.6293,  0.4807,  0.1078,  0.5585,  0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-17.7978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9133259586955381, distance: 0.3369001595046782 entropy 0.03264415264129639
epoch: 55, step: 34
	action: tensor([[ 1.1245, -0.4079, -0.7326,  0.1655, -0.1153,  0.0879, -0.2231]],
       dtype=torch.float64)
	q_value: tensor([[-17.8050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4674500977524444, distance: 0.8350968546162074 entropy 0.03264415264129639
epoch: 55, step: 35
	action: tensor([[ 1.8464,  0.0227, -0.1123,  0.1618, -0.0358, -0.0954, -0.3163]],
       dtype=torch.float64)
	q_value: tensor([[-18.1754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 55, step: 36
	action: tensor([[ 0.2912,  0.3198, -0.4049,  0.4661,  0.0698,  0.0614, -0.2964]],
       dtype=torch.float64)
	q_value: tensor([[-16.9528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7418736688245258, distance: 0.5813970818351193 entropy 0.03264415264129639
epoch: 55, step: 37
	action: tensor([[ 0.9982,  0.1101, -0.4777,  0.6023, -0.4919, -0.0325,  0.0532]],
       dtype=torch.float64)
	q_value: tensor([[-11.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9741728242898668, distance: 0.1839056838363945 entropy 0.03264415264129639
epoch: 55, step: 38
	action: tensor([[ 0.3384, -0.1875, -0.2929,  0.4847, -0.4717,  0.4591, -0.2472]],
       dtype=torch.float64)
	q_value: tensor([[-19.2283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6525209737706159, distance: 0.6745606310789459 entropy 0.03264415264129639
epoch: 55, step: 39
	action: tensor([[ 0.8677,  0.1353, -0.5798,  0.2235, -0.1075,  0.2964, -0.0954]],
       dtype=torch.float64)
	q_value: tensor([[-12.8158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9374687137402911, distance: 0.2861576590443479 entropy 0.03264415264129639
epoch: 55, step: 40
	action: tensor([[ 0.9787, -0.0403, -0.5538,  0.1031,  0.0638,  0.3205,  0.1934]],
       dtype=torch.float64)
	q_value: tensor([[-17.2564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8225887120666325, distance: 0.48200031007257793 entropy 0.03264415264129639
epoch: 55, step: 41
	action: tensor([[ 1.1993, -0.2614, -0.5152,  0.2131, -0.0012,  0.1095,  0.1473]],
       dtype=torch.float64)
	q_value: tensor([[-15.2258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6057046828569335, distance: 0.718567387830936 entropy 0.03264415264129639
epoch: 55, step: 42
	action: tensor([[ 1.2085,  0.0267,  0.0227,  0.1528,  0.0706,  0.4933, -0.2996]],
       dtype=torch.float64)
	q_value: tensor([[-16.7490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8277252078781729, distance: 0.4749715102422488 entropy 0.03264415264129639
epoch: 55, step: 43
	action: tensor([[ 0.8826, -0.0145, -0.5050,  0.3953, -0.0214,  0.2988, -0.4292]],
       dtype=torch.float64)
	q_value: tensor([[-19.1509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9286593698337793, distance: 0.30565053752295523 entropy 0.03264415264129639
epoch: 55, step: 44
	action: tensor([[ 0.8559, -0.1312, -0.6166,  0.3091,  0.1467,  0.5483, -0.2121]],
       dtype=torch.float64)
	q_value: tensor([[-18.5285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8787280507339142, distance: 0.3985078423316489 entropy 0.03264415264129639
epoch: 55, step: 45
	action: tensor([[ 1.1374,  0.0610, -0.7116,  0.6467, -0.2431, -0.2237,  0.0353]],
       dtype=torch.float64)
	q_value: tensor([[-16.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9414521614937952, distance: 0.2768930954418163 entropy 0.03264415264129639
epoch: 55, step: 46
	action: tensor([[ 1.2005,  0.2910, -0.5958,  1.0904, -0.2334,  0.5746,  0.3886]],
       dtype=torch.float64)
	q_value: tensor([[-20.5540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9126841373532072, distance: 0.3381452317854276 entropy 0.03264415264129639
epoch: 55, step: 47
	action: tensor([[ 0.4986,  0.1263, -0.3773,  0.5883, -0.0307,  0.4192, -0.0176]],
       dtype=torch.float64)
	q_value: tensor([[-23.5247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8587495138255937, distance: 0.43008239838932605 entropy 0.03264415264129639
epoch: 55, step: 48
	action: tensor([[ 0.8221,  0.3909, -0.4812,  0.5900,  0.1663,  0.1583, -0.4414]],
       dtype=torch.float64)
	q_value: tensor([[-12.9087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9584431852673462, distance: 0.23328016796761286 entropy 0.03264415264129639
epoch: 55, step: 49
	action: tensor([[ 0.9568, -0.2329, -0.4883,  0.2329, -0.3701,  0.1516,  0.4266]],
       dtype=torch.float64)
	q_value: tensor([[-19.5621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7044246479626401, distance: 0.6221438336996745 entropy 0.03264415264129639
epoch: 55, step: 50
	action: tensor([[ 0.3778, -0.1167, -0.3577,  0.3394, -0.2466,  0.3435, -0.1543]],
       dtype=torch.float64)
	q_value: tensor([[-14.5872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6812560404910368, distance: 0.646067173017572 entropy 0.03264415264129639
epoch: 55, step: 51
	action: tensor([[ 0.8916, -0.8503, -0.7230,  0.3261, -0.2293,  0.2721, -0.3612]],
       dtype=torch.float64)
	q_value: tensor([[-10.9271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15892279772491946, distance: 1.049481105205269 entropy 0.03264415264129639
epoch: 55, step: 52
	action: tensor([[ 0.6005, -0.1041, -0.5517,  0.7739, -0.2038, -0.1610,  0.0176]],
       dtype=torch.float64)
	q_value: tensor([[-15.9861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8073067749953673, distance: 0.5023309255679351 entropy 0.03264415264129639
epoch: 55, step: 53
	action: tensor([[ 1.0187, -0.2299, -0.5490,  0.6035, -0.2135,  0.2147,  0.1421]],
       dtype=torch.float64)
	q_value: tensor([[-13.7070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8674117850755271, distance: 0.4166862360169051 entropy 0.03264415264129639
epoch: 55, step: 54
	action: tensor([[ 0.6927,  0.7623, -0.3209,  0.7632,  0.0655,  0.4431, -0.2396]],
       dtype=torch.float64)
	q_value: tensor([[-16.8937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 55, step: 55
	action: tensor([[ 1.0205, -0.0384, -0.0497,  0.2952, -0.0675, -0.1673,  0.4016]],
       dtype=torch.float64)
	q_value: tensor([[-16.9528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8056037271248062, distance: 0.5045458754689387 entropy 0.03264415264129639
epoch: 55, step: 56
	action: tensor([[ 0.5554, -0.3399, -0.8811,  0.2954, -0.5877,  0.1895, -0.0880]],
       dtype=torch.float64)
	q_value: tensor([[-12.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4654846796832701, distance: 0.8366364313563703 entropy 0.03264415264129639
epoch: 55, step: 57
	action: tensor([[ 0.7587, -0.0771, -0.5363,  0.4767,  0.0467,  0.5054, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-15.1391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8869477811351223, distance: 0.38476560142869426 entropy 0.03264415264129639
epoch: 55, step: 58
	action: tensor([[ 1.4321, -0.0737,  0.0898,  0.1212, -0.2680,  0.3307, -0.1514]],
       dtype=torch.float64)
	q_value: tensor([[-14.8750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6147160951313255, distance: 0.71030868801149 entropy 0.03264415264129639
epoch: 55, step: 59
	action: tensor([[ 1.0636, -0.0526, -0.6799,  0.7164,  0.1706,  0.4228, -0.2727]],
       dtype=torch.float64)
	q_value: tensor([[-21.0743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9771075503826657, distance: 0.1731421582558463 entropy 0.03264415264129639
epoch: 55, step: 60
	action: tensor([[ 0.8415,  0.0490, -0.6648,  0.5395,  0.1799,  0.0456, -0.3037]],
       dtype=torch.float64)
	q_value: tensor([[-20.9404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9276854487637838, distance: 0.30772979036965836 entropy 0.03264415264129639
epoch: 55, step: 61
	action: tensor([[ 1.2950, -0.1915, -0.6935,  0.8392,  0.1932,  0.0885, -0.3358]],
       dtype=torch.float64)
	q_value: tensor([[-17.2977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9155720901730061, distance: 0.332506172740076 entropy 0.03264415264129639
epoch: 55, step: 62
	action: tensor([[ 1.0677,  0.1563, -0.3870,  0.7059, -0.1143,  0.3286, -0.1595]],
       dtype=torch.float64)
	q_value: tensor([[-23.0416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05044841128777159 entropy 0.03264415264129639
epoch: 55, step: 63
	action: tensor([[ 1.1249, -0.3449, -0.0911,  0.8579,  0.2015,  0.1734, -0.5356]],
       dtype=torch.float64)
	q_value: tensor([[-16.9528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8903181640779326, distance: 0.3789867695046462 entropy 0.03264415264129639
epoch: 55, step: 64
	action: tensor([[ 9.4227e-01, -2.4532e-04, -7.4738e-01,  8.8054e-01, -6.3382e-02,
          1.8494e-01, -2.2943e-01]], dtype=torch.float64)
	q_value: tensor([[-19.6792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9420735614626293, distance: 0.27541976739597324 entropy 0.03264415264129639
epoch: 55, step: 65
	action: tensor([[ 1.1525,  0.4985, -0.5535,  0.6657, -0.4664,  0.3396, -0.2643]],
       dtype=torch.float64)
	q_value: tensor([[-20.7790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9529914721552601, distance: 0.2481104284054989 entropy 0.03264415264129639
epoch: 55, step: 66
	action: tensor([[ 0.3314, -0.1160, -0.4212,  0.9685, -0.3106, -0.0398,  0.1618]],
       dtype=torch.float64)
	q_value: tensor([[-26.8542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7141843981778371, distance: 0.6117861757985682 entropy 0.03264415264129639
epoch: 55, step: 67
	action: tensor([[ 1.5624, -0.4716, -0.4136,  0.3596, -0.0635,  0.0739, -0.1534]],
       dtype=torch.float64)
	q_value: tensor([[-11.5659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038889767407774, distance: 0.9547638098893777 entropy 0.03264415264129639
epoch: 55, step: 68
	action: tensor([[ 1.2248, -0.0190, -0.4861,  0.7523,  0.3328,  0.5196, -0.3359]],
       dtype=torch.float64)
	q_value: tensor([[-21.9719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9881051155883238, distance: 0.12480638564701484 entropy 0.03264415264129639
epoch: 55, step: 69
	action: tensor([[ 1.1349, -0.4847, -0.7234,  0.5511,  0.1006,  0.1425, -0.1883]],
       dtype=torch.float64)
	q_value: tensor([[-22.4234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6415153375476944, distance: 0.6851599720195386 entropy 0.03264415264129639
epoch: 55, step: 70
	action: tensor([[ 1.4376, -0.2602, -0.6714,  0.2961,  0.0960,  0.3399, -0.4291]],
       dtype=torch.float64)
	q_value: tensor([[-18.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6227414118709779, distance: 0.7028720290486071 entropy 0.03264415264129639
epoch: 55, step: 71
	action: tensor([[ 1.0338,  0.2772, -0.3132,  0.8978, -0.4584,  0.5207, -0.2186]],
       dtype=torch.float64)
	q_value: tensor([[-23.8288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9458780687979153, distance: 0.26622162849293346 entropy 0.03264415264129639
epoch: 55, step: 72
	action: tensor([[ 0.2766,  0.2653, -0.5604,  0.6717, -0.1159, -0.4500,  0.2301]],
       dtype=torch.float64)
	q_value: tensor([[-24.0865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7353678397892034, distance: 0.5886782691271362 entropy 0.03264415264129639
epoch: 55, step: 73
	action: tensor([[ 0.4424,  0.3100, -0.4922,  0.3073, -0.3938,  0.1390,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[-10.9313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8381286918104623, distance: 0.46040669632875403 entropy 0.03264415264129639
epoch: 55, step: 74
	action: tensor([[ 0.3915, -0.2062, -0.3801,  0.5017, -0.3449, -0.0009, -0.6806]],
       dtype=torch.float64)
	q_value: tensor([[-13.6390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6470000077342928, distance: 0.6798984345244029 entropy 0.03264415264129639
epoch: 55, step: 75
	action: tensor([[ 1.1463,  0.0938, -0.1637,  0.4634,  0.2955, -0.0489, -0.1755]],
       dtype=torch.float64)
	q_value: tensor([[-14.6318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9155880722672319, distance: 0.3324746997561522 entropy 0.03264415264129639
epoch: 55, step: 76
	action: tensor([[ 0.6111, -0.3035, -0.8012,  0.8470, -0.3272, -0.2252,  0.1011]],
       dtype=torch.float64)
	q_value: tensor([[-17.5332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.672619010922038, distance: 0.6547619319936644 entropy 0.03264415264129639
epoch: 55, step: 77
	action: tensor([[ 0.8397, -0.2802, -0.4239,  0.3053,  0.0795,  0.2441, -0.2421]],
       dtype=torch.float64)
	q_value: tensor([[-14.4289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7370908664873437, distance: 0.5867586898564833 entropy 0.03264415264129639
epoch: 55, step: 78
	action: tensor([[ 1.2022, -0.1547, -0.5420,  0.4062, -0.0687, -0.1348, -0.3969]],
       dtype=torch.float64)
	q_value: tensor([[-14.1618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7391828027265656, distance: 0.5844196436534562 entropy 0.03264415264129639
epoch: 55, step: 79
	action: tensor([[ 0.4446, -0.3294, -0.6886,  0.3717, -0.3404,  0.3168, -0.1079]],
       dtype=torch.float64)
	q_value: tensor([[-20.7701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4923446600678941, distance: 0.8153445501459248 entropy 0.03264415264129639
epoch: 55, step: 80
	action: tensor([[ 1.2000,  0.4012, -0.8076,  0.4114, -0.0453, -0.1150, -0.0462]],
       dtype=torch.float64)
	q_value: tensor([[-12.4565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9673769276236517, distance: 0.20668983715099454 entropy 0.03264415264129639
epoch: 55, step: 81
	action: tensor([[ 1.1640, -0.0021, -0.4131, -0.0032,  0.2029, -0.2485,  0.1864]],
       dtype=torch.float64)
	q_value: tensor([[-22.8160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.60652630342053, distance: 0.7178183330027578 entropy 0.03264415264129639
epoch: 55, step: 82
	action: tensor([[ 0.7215, -0.2313, -0.6944,  0.6164, -0.2922,  0.6376, -0.4409]],
       dtype=torch.float64)
	q_value: tensor([[-15.3255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.776798898226043, distance: 0.5406358743501288 entropy 0.03264415264129639
epoch: 55, step: 83
	action: tensor([[ 1.0736, -0.0837, -0.0684,  1.0568, -0.1154,  0.1474, -0.3956]],
       dtype=torch.float64)
	q_value: tensor([[-19.8527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9878196795746594, distance: 0.126294968797073 entropy 0.03264415264129639
epoch: 55, step: 84
	action: tensor([[ 1.0854,  0.2331, -0.4933,  0.2797,  0.0178,  0.1209,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[-21.1250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9509541630598074, distance: 0.25342985099289417 entropy 0.03264415264129639
epoch: 55, step: 85
	action: tensor([[ 0.8134,  0.1479, -0.5057,  0.3763, -0.0359, -0.0609, -0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-18.6090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388915694081988, distance: 0.28288326657646423 entropy 0.03264415264129639
epoch: 55, step: 86
	action: tensor([[ 1.2859,  0.6928, -0.6978,  0.6700,  0.0054, -0.1052, -0.4808]],
       dtype=torch.float64)
	q_value: tensor([[-15.6093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 55, step: 87
	action: tensor([[ 1.5432, -0.0143, -0.2854,  0.4914, -0.0226,  0.0219, -0.5532]],
       dtype=torch.float64)
	q_value: tensor([[-16.9528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6740948667449775, distance: 0.6532844092004567 entropy 0.03264415264129639
epoch: 55, step: 88
	action: tensor([[ 1.2821,  0.2370, -0.7939,  0.7792,  0.1466,  0.1513, -0.1198]],
       dtype=torch.float64)
	q_value: tensor([[-25.8288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06945824624354795 entropy 0.03264415264129639
epoch: 55, step: 89
	action: tensor([[ 0.9795,  0.1361, -0.6746,  0.6967, -0.4282,  0.1185,  0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-16.9528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9711313807701176, distance: 0.19443287565793313 entropy 0.03264415264129639
epoch: 55, step: 90
	action: tensor([[ 0.6599, -0.4123, -0.5352,  0.2205, -0.1315,  0.2830, -0.1654]],
       dtype=torch.float64)
	q_value: tensor([[-19.9715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.540527490383786, distance: 0.7756869044711302 entropy 0.03264415264129639
epoch: 55, step: 91
	action: tensor([[ 0.8734, -0.3699, -0.6364,  0.3693, -0.0197,  0.5451, -0.3568]],
       dtype=torch.float64)
	q_value: tensor([[-12.1642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7440300020047017, distance: 0.5789635542996868 entropy 0.03264415264129639
epoch: 55, step: 92
	action: tensor([[ 1.0996, -0.0438, -0.0774,  0.3711, -0.2749, -0.0129, -0.2106]],
       dtype=torch.float64)
	q_value: tensor([[-17.3506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8338844484146647, distance: 0.4664035416287889 entropy 0.03264415264129639
epoch: 55, step: 93
	action: tensor([[ 0.7658, -0.2191, -0.3186,  0.4940, -0.0266,  0.3178, -0.1807]],
       dtype=torch.float64)
	q_value: tensor([[-18.0311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8664425499276174, distance: 0.41820647502454433 entropy 0.03264415264129639
epoch: 55, step: 94
	action: tensor([[ 1.0163, -0.1008, -0.2937,  1.2965, -0.1228, -0.1125, -0.0504]],
       dtype=torch.float64)
	q_value: tensor([[-14.1041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07620683386219373 entropy 0.03264415264129639
epoch: 55, step: 95
	action: tensor([[ 1.1141,  0.0123, -0.2994,  0.3232, -0.3739, -0.1686, -0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-16.9528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8079889321446672, distance: 0.5014409813516645 entropy 0.03264415264129639
epoch: 55, step: 96
	action: tensor([[ 0.7707, -0.1437, -0.5680,  0.5372, -0.4250, -0.2035, -0.4995]],
       dtype=torch.float64)
	q_value: tensor([[-19.4655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8140120845371532, distance: 0.4935135221456572 entropy 0.03264415264129639
epoch: 55, step: 97
	action: tensor([[ 1.1725, -0.3405, -0.2662,  0.3438, -0.3271,  0.3422, -0.6253]],
       dtype=torch.float64)
	q_value: tensor([[-18.5329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.711833602102651, distance: 0.6142969544455062 entropy 0.03264415264129639
epoch: 55, step: 98
	action: tensor([[ 1.2505, -0.1388, -0.5029,  0.4937, -0.0434,  0.3441, -0.3958]],
       dtype=torch.float64)
	q_value: tensor([[-22.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8763924908472022, distance: 0.40232694614159004 entropy 0.03264415264129639
epoch: 55, step: 99
	action: tensor([[ 0.8999, -0.1185, -0.1722,  0.2314, -0.3651,  0.2020, -0.0280]],
       dtype=torch.float64)
	q_value: tensor([[-22.5077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8086260991295071, distance: 0.5006083024339028 entropy 0.03264415264129639
epoch: 55, step: 100
	action: tensor([[ 0.4966,  0.3516, -0.7435,  0.6563,  0.0753,  0.0178, -0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-14.9831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.787986940310983, distance: 0.5269118916533725 entropy 0.03264415264129639
epoch: 55, step: 101
	action: tensor([[ 1.0711, -0.2425, -1.0766,  0.6315, -0.4063,  0.1308, -0.4810]],
       dtype=torch.float64)
	q_value: tensor([[-15.7111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7466742976414653, distance: 0.5759653020900481 entropy 0.03264415264129639
epoch: 55, step: 102
	action: tensor([[ 0.9486, -0.4753, -0.5002,  0.8596, -0.4612,  0.0230, -0.1534]],
       dtype=torch.float64)
	q_value: tensor([[-24.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.798152045006407, distance: 0.5141251747635358 entropy 0.03264415264129639
epoch: 55, step: 103
	action: tensor([[ 0.6540, -0.0497, -0.6203,  0.8954, -0.1502,  0.2218, -0.2558]],
       dtype=torch.float64)
	q_value: tensor([[-18.4505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8360053329220553, distance: 0.4634165674255703 entropy 0.03264415264129639
epoch: 55, step: 104
	action: tensor([[ 0.8816, -0.3028, -0.7394,  0.5462,  0.0194, -0.1028,  0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-17.3326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7008782051701696, distance: 0.6258650820681009 entropy 0.03264415264129639
epoch: 55, step: 105
	action: tensor([[ 0.8658, -0.1374, -0.4601,  0.1284,  0.0156,  0.1807,  0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-14.8631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7344622585481348, distance: 0.5896846485239989 entropy 0.03264415264129639
epoch: 55, step: 106
	action: tensor([[ 0.7396,  0.0519, -0.6353,  0.5902, -0.3647,  0.1844, -0.3693]],
       dtype=torch.float64)
	q_value: tensor([[-13.0947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8883033395237654, distance: 0.382451869009657 entropy 0.03264415264129639
epoch: 55, step: 107
	action: tensor([[ 1.2896, -0.4216, -0.4301,  0.8015,  0.0414,  0.1907, -0.5247]],
       dtype=torch.float64)
	q_value: tensor([[-19.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8188556127609121, distance: 0.48704505047570296 entropy 0.03264415264129639
epoch: 55, step: 108
	action: tensor([[ 0.9053,  0.5366, -0.4069,  0.2153, -0.2851,  0.7245,  0.1883]],
       dtype=torch.float64)
	q_value: tensor([[-22.7981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9645148894020679, distance: 0.21556575764271152 entropy 0.03264415264129639
epoch: 55, step: 109
	action: tensor([[ 1.0111, -0.2871, -0.3935,  0.5500, -0.4470,  0.1344,  0.3620]],
       dtype=torch.float64)
	q_value: tensor([[-20.1950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8095725549467135, distance: 0.4993688677463719 entropy 0.03264415264129639
epoch: 55, step: 110
	action: tensor([[ 0.0817,  0.1535, -0.0639,  0.5251, -0.2918,  0.1032,  0.2912]],
       dtype=torch.float64)
	q_value: tensor([[-15.7367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6622649610637551, distance: 0.6650353840289098 entropy 0.03264415264129639
epoch: 55, step: 111
	action: tensor([[ 0.6753, -0.3289, -0.4973, -0.0205,  0.1966,  0.1744,  0.4706]],
       dtype=torch.float64)
	q_value: tensor([[-7.8588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4616222123273057, distance: 0.8396538043102226 entropy 0.03264415264129639
epoch: 55, step: 112
	action: tensor([[ 0.6975, -0.2857, -0.0410,  0.3232, -0.1130,  0.0750, -0.1233]],
       dtype=torch.float64)
	q_value: tensor([[-9.4187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6721687814911859, distance: 0.6552120067050657 entropy 0.03264415264129639
epoch: 55, step: 113
	action: tensor([[ 1.1711, -0.5774, -0.1978,  0.5714, -0.0938, -0.3121,  0.3867]],
       dtype=torch.float64)
	q_value: tensor([[-11.2777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4407582014750655, distance: 0.8557689111412432 entropy 0.03264415264129639
epoch: 55, step: 114
	action: tensor([[ 0.6604, -0.1827, -0.3349,  0.5579,  0.1172,  0.6800,  0.0637]],
       dtype=torch.float64)
	q_value: tensor([[-14.3803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9062928773794652, distance: 0.35030230465372736 entropy 0.03264415264129639
epoch: 55, step: 115
	action: tensor([[ 1.0443, -0.2739, -0.5237,  0.5445, -0.2451,  0.1101, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-13.2449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8054386123178736, distance: 0.504760103630873 entropy 0.03264415264129639
epoch: 55, step: 116
	action: tensor([[ 0.2846, -0.5653, -0.7238,  0.3748, -0.4768,  0.1948, -0.0597]],
       dtype=torch.float64)
	q_value: tensor([[-17.4756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12807818330759757, distance: 1.0685515177052203 entropy 0.03264415264129639
epoch: 55, step: 117
	action: tensor([[ 1.0614, -0.1149, -0.2077,  0.8199, -0.3503,  0.0095,  0.2166]],
       dtype=torch.float64)
	q_value: tensor([[-10.6817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9584716700486315, distance: 0.23320020426400548 entropy 0.03264415264129639
epoch: 55, step: 118
	action: tensor([[ 0.7684, -0.2022, -0.2152,  0.5368, -0.2498,  0.1076, -0.7250]],
       dtype=torch.float64)
	q_value: tensor([[-17.3370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8572030556938506, distance: 0.4324303332211027 entropy 0.03264415264129639
epoch: 55, step: 119
	action: tensor([[ 1.3201, -0.5132, -0.2403,  0.5228, -0.3696,  0.2191, -0.6055]],
       dtype=torch.float64)
	q_value: tensor([[-18.3467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5942427465788654, distance: 0.7289367385991663 entropy 0.03264415264129639
epoch: 55, step: 120
	action: tensor([[ 1.4685,  0.1788, -0.7332,  0.3526, -0.2547,  0.1700, -0.2730]],
       dtype=torch.float64)
	q_value: tensor([[-23.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8395963439307282, distance: 0.4583147397633399 entropy 0.03264415264129639
epoch: 55, step: 121
	action: tensor([[ 0.8834,  0.2333, -0.3025,  0.4657,  0.0979, -0.3125, -0.1073]],
       dtype=torch.float64)
	q_value: tensor([[-27.1784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9608919915624818, distance: 0.22630259987750917 entropy 0.03264415264129639
epoch: 55, step: 122
	action: tensor([[ 0.9394,  0.0413, -0.7924,  0.5022, -0.5666,  0.2928, -0.7086]],
       dtype=torch.float64)
	q_value: tensor([[-15.4415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9008101122983225, distance: 0.3604046557051002 entropy 0.03264415264129639
epoch: 55, step: 123
	action: tensor([[ 1.1175,  0.1860, -0.3031,  0.4110, -0.1539,  0.4397, -0.2000]],
       dtype=torch.float64)
	q_value: tensor([[-26.0172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9818274531443913, distance: 0.15426400130644813 entropy 0.03264415264129639
epoch: 55, step: 124
	action: tensor([[ 0.3981, -0.2732,  0.0880,  0.3861,  0.0524, -0.2561, -0.0424]],
       dtype=torch.float64)
	q_value: tensor([[-21.1673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5425664723472925, distance: 0.7739638741031785 entropy 0.03264415264129639
epoch: 55, step: 125
	action: tensor([[ 0.4684,  0.0271, -0.5205,  0.3497, -0.4563, -0.4441,  0.1088]],
       dtype=torch.float64)
	q_value: tensor([[-7.2350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7516070524739314, distance: 0.5703301407945939 entropy 0.03264415264129639
epoch: 55, step: 126
	action: tensor([[ 1.0628, -0.0597, -0.6797,  0.2494, -0.2733,  0.1117,  0.4126]],
       dtype=torch.float64)
	q_value: tensor([[-11.5744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7969173286443403, distance: 0.5156952450362751 entropy 0.03264415264129639
epoch: 55, step: 127
	action: tensor([[ 0.5207, -0.0957, -0.5319,  0.7164, -0.5653,  0.2418, -0.1943]],
       dtype=torch.float64)
	q_value: tensor([[-16.7760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7535602986414209, distance: 0.5680833100968066 entropy 0.03264415264129639
LOSS epoch 55 actor 343.7318213245553 critic 1525.9144348043633 
epoch: 56, step: 0
	action: tensor([[ 0.8214, -0.3324, -0.6177,  0.6874, -0.3398, -0.0159,  0.0454]],
       dtype=torch.float64)
	q_value: tensor([[-11.6903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7623326783613237, distance: 0.5578808183055581 entropy 0.03264415264129639
epoch: 56, step: 1
	action: tensor([[ 1.0530, -0.5147, -0.5205,  0.6268, -0.1325,  0.3083, -0.1520]],
       dtype=torch.float64)
	q_value: tensor([[-11.1175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7265534173264765, distance: 0.598401871897925 entropy 0.03264415264129639
epoch: 56, step: 2
	action: tensor([[ 0.9259, -0.0411, -0.3029,  0.1892, -0.0871,  0.3055, -0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-12.8052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8551813728469692, distance: 0.43548069398822725 entropy 0.03264415264129639
epoch: 56, step: 3
	action: tensor([[ 0.8791,  0.1445, -0.4135,  0.6428, -0.0368, -0.0780, -0.4182]],
       dtype=torch.float64)
	q_value: tensor([[-10.9931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.977529005381586, distance: 0.17154096194705307 entropy 0.03264415264129639
epoch: 56, step: 4
	action: tensor([[ 0.8953, -0.3913, -0.5370,  0.5655, -0.0596, -0.0450, -0.2562]],
       dtype=torch.float64)
	q_value: tensor([[-13.8220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6955580422442289, distance: 0.6314063515460936 entropy 0.03264415264129639
epoch: 56, step: 5
	action: tensor([[ 1.0481, -0.4810, -0.7043,  0.2927,  0.1696, -0.0654,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-11.5493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4236577977614284, distance: 0.8687541737706403 entropy 0.03264415264129639
epoch: 56, step: 6
	action: tensor([[ 0.8428,  0.1033, -0.5964,  0.4079, -0.3773,  0.3819, -0.2651]],
       dtype=torch.float64)
	q_value: tensor([[-10.6084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9277061902354365, distance: 0.3076856552242325 entropy 0.03264415264129639
epoch: 56, step: 7
	action: tensor([[ 0.7303, -0.3399, -0.6158,  0.6793,  0.0917, -0.1958,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-14.8075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.685558288089305, distance: 0.6416922132529005 entropy 0.03264415264129639
epoch: 56, step: 8
	action: tensor([[ 1.4225, -0.0786, -0.2160, -0.0753, -0.1129,  0.1144, -0.0217]],
       dtype=torch.float64)
	q_value: tensor([[-9.1269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46353003538960813, distance: 0.8381647638981359 entropy 0.03264415264129639
epoch: 56, step: 9
	action: tensor([[ 0.8192, -0.0923, -0.3343,  0.1432, -0.0658, -0.1052, -0.1951]],
       dtype=torch.float64)
	q_value: tensor([[-14.5813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7196724521486138, distance: 0.6058841372435293 entropy 0.03264415264129639
epoch: 56, step: 10
	action: tensor([[ 0.7953, -0.0178, -0.7865,  0.3476,  0.3783,  0.4746, -0.3580]],
       dtype=torch.float64)
	q_value: tensor([[-9.8291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8918843475155028, distance: 0.37627120141898324 entropy 0.03264415264129639
epoch: 56, step: 11
	action: tensor([[ 1.3053, -0.1315, -0.6613,  0.4278,  0.2407, -0.0111, -0.5172]],
       dtype=torch.float64)
	q_value: tensor([[-12.4292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7514176335820664, distance: 0.570547559840966 entropy 0.03264415264129639
epoch: 56, step: 12
	action: tensor([[ 1.2040, -0.1232, -0.4635,  0.7100, -0.0305,  0.4351, -0.1980]],
       dtype=torch.float64)
	q_value: tensor([[-16.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.963809738718283, distance: 0.2176970543004587 entropy 0.03264415264129639
epoch: 56, step: 13
	action: tensor([[ 1.2902, -0.2460, -0.5316,  0.4980,  0.0518,  0.1835, -0.1425]],
       dtype=torch.float64)
	q_value: tensor([[-15.8918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7666050084215896, distance: 0.5528438203136319 entropy 0.03264415264129639
epoch: 56, step: 14
	action: tensor([[ 1.2519, -0.0470, -0.5257,  0.2002, -0.1486,  0.6673, -0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-14.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8656314860919384, distance: 0.4194743891176343 entropy 0.03264415264129639
epoch: 56, step: 15
	action: tensor([[ 0.9160, -0.2633, -0.3590,  0.2252, -0.0797, -0.3023,  0.1105]],
       dtype=torch.float64)
	q_value: tensor([[-16.0371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5634163077679213, distance: 0.7561195165436677 entropy 0.03264415264129639
epoch: 56, step: 16
	action: tensor([[ 0.5303, -0.2085, -0.1862,  0.3873,  0.1708,  0.2335,  0.1242]],
       dtype=torch.float64)
	q_value: tensor([[-9.2098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7483666703892335, distance: 0.5740381754447725 entropy 0.03264415264129639
epoch: 56, step: 17
	action: tensor([[ 0.6365,  0.2327, -0.3451,  0.5703, -0.1333,  0.2894,  0.1322]],
       dtype=torch.float64)
	q_value: tensor([[-6.3204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9292833175145021, distance: 0.30431098693259667 entropy 0.03264415264129639
epoch: 56, step: 18
	action: tensor([[ 0.6888,  0.0815, -0.5600,  0.0083,  0.0565,  0.3616, -0.1860]],
       dtype=torch.float64)
	q_value: tensor([[-10.1840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8458193613331042, distance: 0.44933641235098587 entropy 0.03264415264129639
epoch: 56, step: 19
	action: tensor([[ 0.7664, -0.6182, -0.2278,  0.6526,  0.1397, -0.2109, -0.1887]],
       dtype=torch.float64)
	q_value: tensor([[-10.3406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5152113396410452, distance: 0.7967699008952476 entropy 0.03264415264129639
epoch: 56, step: 20
	action: tensor([[ 1.0496,  0.0908, -0.6944,  0.7190,  0.2345, -0.2840,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[-8.4474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9479942458397331, distance: 0.2609650768515241 entropy 0.03264415264129639
epoch: 56, step: 21
	action: tensor([[ 1.2022, -0.0270, -0.5787,  0.4348, -0.1653, -0.2941, -0.2512]],
       dtype=torch.float64)
	q_value: tensor([[-13.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7954964241966065, distance: 0.5174961776714806 entropy 0.03264415264129639
epoch: 56, step: 22
	action: tensor([[ 0.9994, -0.1653, -0.3086,  0.3658, -0.3714, -0.3757, -0.1620]],
       dtype=torch.float64)
	q_value: tensor([[-15.2852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7141418736761651, distance: 0.6118316857960644 entropy 0.03264415264129639
epoch: 56, step: 23
	action: tensor([[ 0.8657, -0.2708, -0.1826,  0.4642, -0.3509,  0.2553, -0.4751]],
       dtype=torch.float64)
	q_value: tensor([[-12.2788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8290741981585221, distance: 0.4731082336019352 entropy 0.03264415264129639
epoch: 56, step: 24
	action: tensor([[ 0.6838,  0.3496, -0.7322,  0.4969, -0.0907,  0.1723, -0.1315]],
       dtype=torch.float64)
	q_value: tensor([[-12.9550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9021111748408152, distance: 0.3580331598135817 entropy 0.03264415264129639
epoch: 56, step: 25
	action: tensor([[ 0.7970,  0.0230, -0.1938,  0.5652, -0.2544,  0.1763, -0.1526]],
       dtype=torch.float64)
	q_value: tensor([[-13.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9621094968507564, distance: 0.2227521371259976 entropy 0.03264415264129639
epoch: 56, step: 26
	action: tensor([[ 0.7356, -0.2465, -0.7553,  0.4719, -0.3798, -0.0530, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[-11.6132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6946642323652648, distance: 0.6323325438854146 entropy 0.03264415264129639
epoch: 56, step: 27
	action: tensor([[ 0.6804, -0.8509, -0.6577,  0.2688, -0.4462,  0.2325, -0.0913]],
       dtype=torch.float64)
	q_value: tensor([[-10.8788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04432108372120125, distance: 1.118697572601302 entropy 0.03264415264129639
epoch: 56, step: 28
	action: tensor([[ 0.6898, -0.1879, -0.4747,  0.3617, -0.1654,  0.0230,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-9.3525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7132419531855252, distance: 0.6127939939483362 entropy 0.03264415264129639
epoch: 56, step: 29
	action: tensor([[ 0.4015, -0.0214, -0.9811,  0.2998, -0.4105,  0.2618, -0.5838]],
       dtype=torch.float64)
	q_value: tensor([[-8.6280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5551537753634586, distance: 0.7632409225768808 entropy 0.03264415264129639
epoch: 56, step: 30
	action: tensor([[ 0.9630, -0.0059, -0.3760,  0.5596, -0.2209,  0.4311, -0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-13.2731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9667912195249967, distance: 0.20853701700165575 entropy 0.03264415264129639
epoch: 56, step: 31
	action: tensor([[ 0.5993, -0.2103, -0.2518,  0.3842,  0.4532,  0.3544,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[-13.4702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7913008938248531, distance: 0.5227776206582841 entropy 0.03264415264129639
epoch: 56, step: 32
	action: tensor([[ 0.7265, -0.2144, -0.1153,  0.3874,  0.1706,  0.4414,  0.1132]],
       dtype=torch.float64)
	q_value: tensor([[-6.9488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8506209666055122, distance: 0.4422842923031468 entropy 0.03264415264129639
epoch: 56, step: 33
	action: tensor([[ 0.8696, -0.2211, -0.2899,  0.6896,  0.0240,  0.4699, -0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-8.1149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9400870197881324, distance: 0.2801026094462611 entropy 0.03264415264129639
epoch: 56, step: 34
	action: tensor([[ 0.9935, -0.2683, -0.6050,  0.0214,  0.0756,  0.0888,  0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-11.4830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5291831693518965, distance: 0.785204324894252 entropy 0.03264415264129639
epoch: 56, step: 35
	action: tensor([[ 1.0027,  0.5449, -0.2957,  0.3378, -0.0730,  0.1077,  0.5488]],
       dtype=torch.float64)
	q_value: tensor([[-10.2850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9738167278098498, distance: 0.18516915875198955 entropy 0.03264415264129639
epoch: 56, step: 36
	action: tensor([[ 0.4429,  0.0211, -0.6554,  0.8004, -0.2653,  0.0716, -0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-12.0103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7135716952833339, distance: 0.6124415677463076 entropy 0.03264415264129639
epoch: 56, step: 37
	action: tensor([[ 1.1294, -0.4519, -0.5147,  0.7755, -0.2735,  0.2189, -0.1500]],
       dtype=torch.float64)
	q_value: tensor([[-10.3337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8157234785395913, distance: 0.49123770764441366 entropy 0.03264415264129639
epoch: 56, step: 38
	action: tensor([[ 1.1224, -0.1809, -0.3404,  0.6323, -0.1216, -0.1750, -0.1630]],
       dtype=torch.float64)
	q_value: tensor([[-14.4274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8363300934882799, distance: 0.46295748539463993 entropy 0.03264415264129639
epoch: 56, step: 39
	action: tensor([[ 0.9301,  0.0693,  0.1166,  0.5982, -0.2434, -0.0149,  0.2669]],
       dtype=torch.float64)
	q_value: tensor([[-13.5417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9566928466395931, distance: 0.23814228269776058 entropy 0.03264415264129639
epoch: 56, step: 40
	action: tensor([[ 0.8303,  0.0314, -0.7229,  0.6656, -0.1017, -0.0318, -0.3394]],
       dtype=torch.float64)
	q_value: tensor([[-10.3867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9082535193672143, distance: 0.34661822998081765 entropy 0.03264415264129639
epoch: 56, step: 41
	action: tensor([[ 1.0163, -0.2954, -0.8858,  0.5783, -0.1871,  0.0078, -0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-13.8706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.732044228352687, distance: 0.5923634464806296 entropy 0.03264415264129639
epoch: 56, step: 42
	action: tensor([[ 0.9815, -0.0014, -0.5026,  0.5245, -0.2668,  0.2995, -0.2927]],
       dtype=torch.float64)
	q_value: tensor([[-13.8494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9473199093267756, distance: 0.2626515391729887 entropy 0.03264415264129639
epoch: 56, step: 43
	action: tensor([[ 0.9955,  0.0357, -0.4088,  0.4723, -0.3867,  0.0558,  0.3206]],
       dtype=torch.float64)
	q_value: tensor([[-14.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9335423191191582, distance: 0.29500493711638753 entropy 0.03264415264129639
epoch: 56, step: 44
	action: tensor([[ 0.4823, -0.3931, -0.3729,  0.5893, -0.5473,  0.2362, -0.0666]],
       dtype=torch.float64)
	q_value: tensor([[-12.0677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6143065927110156, distance: 0.7106860667119862 entropy 0.03264415264129639
epoch: 56, step: 45
	action: tensor([[ 1.0137, -0.1005, -0.6066,  0.0948,  0.0720,  0.2050, -0.3859]],
       dtype=torch.float64)
	q_value: tensor([[-9.1769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.745768574490954, distance: 0.5769940166135349 entropy 0.03264415264129639
epoch: 56, step: 46
	action: tensor([[ 0.9283, -0.3691, -0.6105,  0.6657, -0.0843,  0.2919, -0.2933]],
       dtype=torch.float64)
	q_value: tensor([[-13.2210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.808004341432815, distance: 0.5014208601063447 entropy 0.03264415264129639
epoch: 56, step: 47
	action: tensor([[ 0.9924, -0.0375, -0.7536,  0.4688, -1.0461,  0.3940,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-13.1078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8536670340884152, distance: 0.43775163899081515 entropy 0.03264415264129639
epoch: 56, step: 48
	action: tensor([[ 1.2142,  0.2041, -0.3741,  0.5947, -0.3107, -0.0287, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-17.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.959045555292388, distance: 0.23158328717735296 entropy 0.03264415264129639
epoch: 56, step: 49
	action: tensor([[ 0.9436, -0.3113,  0.0016,  0.3119,  0.2122, -0.0035, -0.0774]],
       dtype=torch.float64)
	q_value: tensor([[-15.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6397125398590819, distance: 0.6868806254994761 entropy 0.03264415264129639
epoch: 56, step: 50
	action: tensor([[ 1.2400, -0.0371, -0.3124,  1.0184, -0.4176,  0.5237, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[-8.8753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9915380225551227, distance: 0.10526709181573042 entropy 0.03264415264129639
epoch: 56, step: 51
	action: tensor([[ 1.0322,  0.0048, -0.4925,  0.7402,  0.0103,  0.1226, -0.2995]],
       dtype=torch.float64)
	q_value: tensor([[-17.5982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.980877841605484, distance: 0.15824323360927742 entropy 0.03264415264129639
epoch: 56, step: 52
	action: tensor([[ 5.8044e-01,  2.4618e-01, -6.1832e-01,  5.5028e-01,  3.7838e-01,
          1.6541e-04,  5.6861e-02]], dtype=torch.float64)
	q_value: tensor([[-14.7763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8727843308759496, distance: 0.40815676258753236 entropy 0.03264415264129639
epoch: 56, step: 53
	action: tensor([[ 1.0528,  0.2462, -0.0018,  0.4347,  0.0606,  0.3944, -0.2016]],
       dtype=torch.float64)
	q_value: tensor([[-9.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9773130531441154, distance: 0.1723632681699668 entropy 0.03264415264129639
epoch: 56, step: 54
	action: tensor([[ 0.9239,  0.4486, -0.4004,  0.6481,  0.0045,  0.4842, -0.4143]],
       dtype=torch.float64)
	q_value: tensor([[-13.7814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9266998923683222, distance: 0.3098196788603414 entropy 0.03264415264129639
epoch: 56, step: 55
	action: tensor([[ 0.9553,  0.1832, -0.3535,  0.4965, -0.1650,  0.1178, -0.2641]],
       dtype=torch.float64)
	q_value: tensor([[-16.6666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.984623629687921, distance: 0.1419003962135337 entropy 0.03264415264129639
epoch: 56, step: 56
	action: tensor([[ 0.8505, -0.0234, -0.5177,  0.9287, -0.1173,  0.2418, -0.1638]],
       dtype=torch.float64)
	q_value: tensor([[-14.1350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9493655781481497, distance: 0.257501415684266 entropy 0.03264415264129639
epoch: 56, step: 57
	action: tensor([[ 0.7298, -0.1139, -0.4340,  0.3300, -0.0315,  0.3275, -0.2268]],
       dtype=torch.float64)
	q_value: tensor([[-13.6189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8381396041840199, distance: 0.4603911771654389 entropy 0.03264415264129639
epoch: 56, step: 58
	action: tensor([[ 0.9998,  0.2601, -0.1469,  0.3302, -0.2490,  0.2330, -0.1693]],
       dtype=torch.float64)
	q_value: tensor([[-10.4200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9786194686258511, distance: 0.16732696759706905 entropy 0.03264415264129639
epoch: 56, step: 59
	action: tensor([[ 0.8295,  0.1550, -0.0120,  0.3259, -0.0806, -0.1046, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-13.9637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9289221944643895, distance: 0.3050869974064919 entropy 0.03264415264129639
epoch: 56, step: 60
	action: tensor([[ 1.4069, -0.0993, -0.2635,  0.3960, -0.0457,  0.2239,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[-9.8768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7415319453303383, distance: 0.5817817991355234 entropy 0.03264415264129639
epoch: 56, step: 61
	action: tensor([[ 0.7745, -0.1069, -0.2744,  0.7481, -0.4930,  0.1662, -0.2931]],
       dtype=torch.float64)
	q_value: tensor([[-15.2097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9424187630753547, distance: 0.2745978851968306 entropy 0.03264415264129639
epoch: 56, step: 62
	action: tensor([[ 1.3657,  0.1643, -0.5913,  0.6562, -0.3790, -0.0764, -0.5352]],
       dtype=torch.float64)
	q_value: tensor([[-13.1833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9343868678597115, distance: 0.29312447231673 entropy 0.03264415264129639
epoch: 56, step: 63
	action: tensor([[ 1.1332,  0.0935, -0.4638,  0.3922,  0.1031,  0.0716,  0.3572]],
       dtype=torch.float64)
	q_value: tensor([[-20.5320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9097428459465834, distance: 0.34379338096534373 entropy 0.03264415264129639
epoch: 56, step: 64
	action: tensor([[ 0.8455,  0.1526, -0.2472,  0.2256, -0.1517,  0.0394,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-12.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9194127123321708, distance: 0.3248553045508909 entropy 0.03264415264129639
epoch: 56, step: 65
	action: tensor([[ 0.7252, -0.2082, -0.2822,  0.1216, -0.1483,  0.1510, -0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-10.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6655544780519373, distance: 0.6617887588325796 entropy 0.03264415264129639
epoch: 56, step: 66
	action: tensor([[ 1.1129,  0.0605, -0.7335,  0.0750, -0.0542,  0.1945, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-8.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8113409405780696, distance: 0.4970447897851517 entropy 0.03264415264129639
epoch: 56, step: 67
	action: tensor([[ 0.8727,  0.4024, -0.3554,  0.1740, -0.1725,  0.1231, -0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-14.0751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9814043391876216, distance: 0.15604954304190785 entropy 0.03264415264129639
epoch: 56, step: 68
	action: tensor([[ 0.5698,  0.1300, -0.7189,  0.9201,  0.0818,  0.0130, -0.2992]],
       dtype=torch.float64)
	q_value: tensor([[-13.0929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7832288732405159, distance: 0.5327916511185945 entropy 0.03264415264129639
epoch: 56, step: 69
	action: tensor([[ 1.0153, -0.2395, -0.8148,  0.2292,  0.0434,  0.2359,  0.2679]],
       dtype=torch.float64)
	q_value: tensor([[-12.3894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6868166717410318, distance: 0.6404069122390841 entropy 0.03264415264129639
epoch: 56, step: 70
	action: tensor([[ 0.4688,  0.0847, -0.4900, -0.2084, -0.0878,  0.1999, -0.4310]],
       dtype=torch.float64)
	q_value: tensor([[-11.4554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6910644840318488, distance: 0.6360490563401628 entropy 0.03264415264129639
epoch: 56, step: 71
	action: tensor([[ 0.6469,  0.0802, -0.2631,  0.1072,  0.2120,  0.2393,  0.3271]],
       dtype=torch.float64)
	q_value: tensor([[-9.0720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8480151757338541, distance: 0.4461252518166997 entropy 0.03264415264129639
epoch: 56, step: 72
	action: tensor([[ 0.7830, -0.1160, -0.1400,  0.0683, -0.2912,  0.2037,  0.3148]],
       dtype=torch.float64)
	q_value: tensor([[-7.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7215017555880603, distance: 0.6039040250104284 entropy 0.03264415264129639
epoch: 56, step: 73
	action: tensor([[ 1.3960,  0.0222, -0.3640,  0.3462, -0.2196,  0.0530,  0.0937]],
       dtype=torch.float64)
	q_value: tensor([[-8.5540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7624041899011481, distance: 0.5577968818234204 entropy 0.03264415264129639
epoch: 56, step: 74
	action: tensor([[ 0.7195,  0.2464, -0.5141,  0.1274, -0.3040,  0.3840, -0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-15.6827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9255014554876632, distance: 0.31234214411953054 entropy 0.03264415264129639
epoch: 56, step: 75
	action: tensor([[ 1.3527,  0.0895, -0.5572,  0.4234, -0.3257, -0.0193, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-12.1364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8409279744825571, distance: 0.45640836848336347 entropy 0.03264415264129639
epoch: 56, step: 76
	action: tensor([[ 0.9858,  0.2100, -0.8410,  0.3515,  0.2614,  0.1767,  0.0405]],
       dtype=torch.float64)
	q_value: tensor([[-17.0329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9580791807551458, distance: 0.23429961444423408 entropy 0.03264415264129639
epoch: 56, step: 77
	action: tensor([[ 1.0830, -0.1630, -0.4597,  0.5351, -0.1469, -0.0338, -0.4565]],
       dtype=torch.float64)
	q_value: tensor([[-13.3410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8456656408567104, distance: 0.4495603542156318 entropy 0.03264415264129639
epoch: 56, step: 78
	action: tensor([[ 1.1903, -0.0365, -0.5048,  0.6855,  0.4114,  0.1718, -0.1641]],
       dtype=torch.float64)
	q_value: tensor([[-14.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9503414315998602, distance: 0.25500799161345317 entropy 0.03264415264129639
epoch: 56, step: 79
	action: tensor([[ 0.2444,  0.0527, -0.5824,  0.7453, -0.3522,  0.2414,  0.0601]],
       dtype=torch.float64)
	q_value: tensor([[-14.2597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5793074903488548, distance: 0.7422309896175606 entropy 0.03264415264129639
epoch: 56, step: 80
	action: tensor([[ 0.4312,  0.0043, -0.2979,  0.9266, -0.3757, -0.1709, -0.1259]],
       dtype=torch.float64)
	q_value: tensor([[-8.8556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8410595690821104, distance: 0.45621954426055455 entropy 0.03264415264129639
epoch: 56, step: 81
	action: tensor([[ 0.5461, -0.1947, -0.6186,  0.8144, -0.5332,  0.0892, -0.2698]],
       dtype=torch.float64)
	q_value: tensor([[-10.1719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.723478202383715, distance: 0.6017573160016083 entropy 0.03264415264129639
epoch: 56, step: 82
	action: tensor([[ 0.9988, -0.3756,  0.0390,  0.2540,  0.0706, -0.0910,  0.3205]],
       dtype=torch.float64)
	q_value: tensor([[-12.0418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5231804145802872, distance: 0.7901940153541551 entropy 0.03264415264129639
epoch: 56, step: 83
	action: tensor([[ 0.6800, -0.1560, -0.4705,  0.3334, -0.2116, -0.2869, -0.3509]],
       dtype=torch.float64)
	q_value: tensor([[-8.2218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6950704203157952, distance: 0.6319118081501699 entropy 0.03264415264129639
epoch: 56, step: 84
	action: tensor([[ 0.2729,  0.8810, -0.3675,  0.5428, -0.1675,  0.3047, -0.4431]],
       dtype=torch.float64)
	q_value: tensor([[-10.2673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 56, step: 85
	action: tensor([[ 0.8635,  0.1145, -0.4654,  0.3141, -0.5012, -0.0833, -0.1974]],
       dtype=torch.float64)
	q_value: tensor([[-12.4234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9171219004107687, distance: 0.32944019421184284 entropy 0.03264415264129639
epoch: 56, step: 86
	action: tensor([[ 0.4176, -0.0983, -0.4058,  0.0457,  0.1181,  0.0235,  0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-13.2980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5865484189086465, distance: 0.7358156520968442 entropy 0.03264415264129639
epoch: 56, step: 87
	action: tensor([[ 0.6355, -0.2469, -0.8160,  0.4088, -0.2559, -0.2219, -0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-5.4263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6302817871081603, distance: 0.6958123228274691 entropy 0.03264415264129639
epoch: 56, step: 88
	action: tensor([[ 0.8772, -0.1449, -0.7327,  0.4432,  0.1972,  0.0192,  0.5341]],
       dtype=torch.float64)
	q_value: tensor([[-9.7267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7975700942539983, distance: 0.5148657821897399 entropy 0.03264415264129639
epoch: 56, step: 89
	action: tensor([[ 0.8598, -0.3130, -0.3887,  0.3027, -0.1767, -0.2462, -0.2240]],
       dtype=torch.float64)
	q_value: tensor([[-9.5229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5980200402605059, distance: 0.7255358798921866 entropy 0.03264415264129639
epoch: 56, step: 90
	action: tensor([[ 1.0670, -0.5391, -0.0517,  0.0703, -0.1964, -0.1321, -0.5131]],
       dtype=torch.float64)
	q_value: tensor([[-10.4230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20369643295871454, distance: 1.0211652415258063 entropy 0.03264415264129639
epoch: 56, step: 91
	action: tensor([[ 1.4069, -0.0735, -0.3990,  0.4345,  0.1081,  0.3755, -0.1587]],
       dtype=torch.float64)
	q_value: tensor([[-11.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.819139218248325, distance: 0.4866636344184131 entropy 0.03264415264129639
epoch: 56, step: 92
	action: tensor([[ 1.3562, -0.1779, -0.3040,  0.5170, -0.0443, -0.3436, -0.0487]],
       dtype=torch.float64)
	q_value: tensor([[-16.3785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6442284420787814, distance: 0.6825623139167225 entropy 0.03264415264129639
epoch: 56, step: 93
	action: tensor([[ 1.2369,  0.0915, -0.4920,  0.4788, -0.3896,  0.3066,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[-14.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9508669897843705, distance: 0.25365497205992027 entropy 0.03264415264129639
epoch: 56, step: 94
	action: tensor([[ 0.6052, -0.2602, -0.2347,  0.4246,  0.2192, -0.1623, -0.2708]],
       dtype=torch.float64)
	q_value: tensor([[-16.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6750526611109722, distance: 0.6523237423348587 entropy 0.03264415264129639
epoch: 56, step: 95
	action: tensor([[ 0.9509, -0.1150, -0.4951,  0.4894, -0.3226,  0.2373, -0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-7.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8969756806393154, distance: 0.3673047716300432 entropy 0.03264415264129639
epoch: 56, step: 96
	action: tensor([[ 0.9696, -0.1878, -0.3048,  0.6935,  0.1189,  0.8493,  0.1693]],
       dtype=torch.float64)
	q_value: tensor([[-13.0896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.986756507744058, distance: 0.1316915636211693 entropy 0.03264415264129639
epoch: 56, step: 97
	action: tensor([[ 0.5249,  0.0956, -1.1443, -0.0065, -0.3848,  0.0958,  0.1258]],
       dtype=torch.float64)
	q_value: tensor([[-12.4997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7537248472001461, distance: 0.5678936229271299 entropy 0.03264415264129639
epoch: 56, step: 98
	action: tensor([[ 0.7719,  0.2990, -0.1903,  0.2897, -0.2622,  0.1291, -0.4496]],
       dtype=torch.float64)
	q_value: tensor([[-11.6748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9735897649154044, distance: 0.18596997246459235 entropy 0.03264415264129639
epoch: 56, step: 99
	action: tensor([[ 1.1966,  0.0848, -0.7758,  0.6229,  0.0337,  0.0362,  0.3147]],
       dtype=torch.float64)
	q_value: tensor([[-13.2443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9539268019066339, distance: 0.24562969718621724 entropy 0.03264415264129639
epoch: 56, step: 100
	action: tensor([[ 0.9345, -0.2991, -0.5138,  0.1166, -0.0360, -0.1591, -0.2174]],
       dtype=torch.float64)
	q_value: tensor([[-14.3872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5149761196733231, distance: 0.7969631742508084 entropy 0.03264415264129639
epoch: 56, step: 101
	action: tensor([[ 0.6352, -0.2049, -0.3858,  0.5687, -0.1093,  0.0418, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[-10.6187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8039949680745987, distance: 0.506629301039569 entropy 0.03264415264129639
epoch: 56, step: 102
	action: tensor([[ 1.0356, -0.1400, -0.4051,  0.1127, -0.4382,  0.2663, -0.0559]],
       dtype=torch.float64)
	q_value: tensor([[-9.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7419639270019325, distance: 0.581295425354003 entropy 0.03264415264129639
epoch: 56, step: 103
	action: tensor([[ 1.1746, -0.0474, -0.3221,  0.6385, -0.5496, -0.1156, -0.0360]],
       dtype=torch.float64)
	q_value: tensor([[-13.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9030777797426304, distance: 0.3562610720095094 entropy 0.03264415264129639
epoch: 56, step: 104
	action: tensor([[ 1.0482, -0.3277, -0.4247,  0.2298, -0.0499,  0.0697, -0.4184]],
       dtype=torch.float64)
	q_value: tensor([[-15.2536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6079501464169486, distance: 0.7165183898155947 entropy 0.03264415264129639
epoch: 56, step: 105
	action: tensor([[ 1.0072, -0.2814, -0.5366,  0.8229, -0.0992,  0.0384, -0.2659]],
       dtype=torch.float64)
	q_value: tensor([[-12.6981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8988261336203102, distance: 0.363991185463661 entropy 0.03264415264129639
epoch: 56, step: 106
	action: tensor([[ 0.4062, -0.0435, -0.5705,  0.5365, -0.0172,  0.1908, -0.4361]],
       dtype=torch.float64)
	q_value: tensor([[-13.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.715607391217584, distance: 0.6102613224560256 entropy 0.03264415264129639
epoch: 56, step: 107
	action: tensor([[ 1.1284,  0.2017, -0.4292,  0.3187,  0.2243, -0.1433, -0.1529]],
       dtype=torch.float64)
	q_value: tensor([[-9.7564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9079239297418026, distance: 0.34724026668058705 entropy 0.03264415264129639
epoch: 56, step: 108
	action: tensor([[ 1.0174,  0.2887, -0.8210,  0.1205, -0.3264, -0.3166, -0.1491]],
       dtype=torch.float64)
	q_value: tensor([[-13.4586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.897333648056068, distance: 0.36666609936082173 entropy 0.03264415264129639
epoch: 56, step: 109
	action: tensor([[ 1.1432, -0.4139, -0.4438,  0.2168, -0.4539,  0.2861,  0.2467]],
       dtype=torch.float64)
	q_value: tensor([[-15.0189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5442720435934087, distance: 0.7725196387961584 entropy 0.03264415264129639
epoch: 56, step: 110
	action: tensor([[ 0.9844,  0.1992, -0.1454,  0.3207,  0.2429,  0.0265,  0.0281]],
       dtype=torch.float64)
	q_value: tensor([[-12.4655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.949528060170998, distance: 0.257087932443383 entropy 0.03264415264129639
epoch: 56, step: 111
	action: tensor([[ 0.7705,  0.2348, -0.1227,  0.4063, -0.8304,  0.1495,  0.1157]],
       dtype=torch.float64)
	q_value: tensor([[-10.7423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9805865437016754, distance: 0.1594439792303027 entropy 0.03264415264129639
epoch: 56, step: 112
	action: tensor([[ 0.9939,  0.2177, -0.2480,  0.7207,  0.2016, -0.2192, -0.3906]],
       dtype=torch.float64)
	q_value: tensor([[-12.7198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9923590935391376, distance: 0.10002973881577053 entropy 0.03264415264129639
epoch: 56, step: 113
	action: tensor([[ 1.0012,  0.1119, -1.1179,  0.6334, -0.4859,  0.1016,  0.0840]],
       dtype=torch.float64)
	q_value: tensor([[-13.7747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9117158360452964, distance: 0.34001501637436843 entropy 0.03264415264129639
epoch: 56, step: 114
	action: tensor([[ 0.8008,  0.0990, -0.3175,  0.4550, -0.1656,  0.1900, -0.0568]],
       dtype=torch.float64)
	q_value: tensor([[-16.6436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9605638245395187, distance: 0.22725010281398345 entropy 0.03264415264129639
epoch: 56, step: 115
	action: tensor([[ 0.9174, -0.2188, -0.3916,  0.4943,  0.1090,  0.3263, -0.2439]],
       dtype=torch.float64)
	q_value: tensor([[-11.2942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8820022306073819, distance: 0.3930914448608879 entropy 0.03264415264129639
epoch: 56, step: 116
	action: tensor([[ 0.7384, -0.1046, -0.2202,  0.6163, -0.1664,  0.2772, -0.1900]],
       dtype=torch.float64)
	q_value: tensor([[-11.6565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9420160915815734, distance: 0.275556358024723 entropy 0.03264415264129639
epoch: 56, step: 117
	action: tensor([[ 0.9897, -0.3999, -0.6092,  0.4920, -0.2633,  0.0798,  0.2163]],
       dtype=torch.float64)
	q_value: tensor([[-10.9395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.679332809291817, distance: 0.6480133555913021 entropy 0.03264415264129639
epoch: 56, step: 118
	action: tensor([[ 0.5610, -0.0658, -0.7650,  0.1821, -0.0136,  0.2962,  0.3478]],
       dtype=torch.float64)
	q_value: tensor([[-11.2106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7238510506888095, distance: 0.6013514892662312 entropy 0.03264415264129639
epoch: 56, step: 119
	action: tensor([[ 0.8543,  0.2928, -0.5393,  0.2924, -0.4171,  0.0733,  0.3680]],
       dtype=torch.float64)
	q_value: tensor([[-8.4827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9691657392914156, distance: 0.20094326905756274 entropy 0.03264415264129639
epoch: 56, step: 120
	action: tensor([[ 0.3596, -0.1749, -0.7686,  0.2586, -0.2512,  0.1193, -0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-11.9493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5141456490744929, distance: 0.7976451730580205 entropy 0.03264415264129639
epoch: 56, step: 121
	action: tensor([[ 0.3304, -0.0061, -0.3582,  0.1329, -0.2305,  0.1801,  0.1548]],
       dtype=torch.float64)
	q_value: tensor([[-8.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6505033012411638, distance: 0.6765162489862119 entropy 0.03264415264129639
epoch: 56, step: 122
	action: tensor([[ 0.8477, -0.0679, -0.0153,  0.6928,  0.2038,  0.1935, -0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-6.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9783427060820956, distance: 0.16840647616818144 entropy 0.03264415264129639
epoch: 56, step: 123
	action: tensor([[ 0.6303, -0.0361, -0.4159,  0.7463, -0.2064,  0.4332, -0.1211]],
       dtype=torch.float64)
	q_value: tensor([[-9.8827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8920566593299423, distance: 0.37597123639999147 entropy 0.03264415264129639
epoch: 56, step: 124
	action: tensor([[ 0.7418, -0.5125, -0.4052,  0.4987, -0.1693,  0.3679,  0.2072]],
       dtype=torch.float64)
	q_value: tensor([[-11.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6543727898871818, distance: 0.6727607656338429 entropy 0.03264415264129639
epoch: 56, step: 125
	action: tensor([[ 0.9214, -0.1577, -0.5439,  0.7973, -0.4342,  0.4697, -0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-8.8482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9330003462592587, distance: 0.2962054004684559 entropy 0.03264415264129639
epoch: 56, step: 126
	action: tensor([[ 0.7942,  0.2609, -0.3376,  0.1473,  0.1843,  0.2630,  0.1548]],
       dtype=torch.float64)
	q_value: tensor([[-14.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9632819809648867, distance: 0.21927863273663628 entropy 0.03264415264129639
epoch: 56, step: 127
	action: tensor([[ 0.7358,  0.0080, -0.2955,  0.4785, -0.2807,  0.1061,  0.1298]],
       dtype=torch.float64)
	q_value: tensor([[-9.6443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9251432940227456, distance: 0.31309205666319384 entropy 0.03264415264129639
LOSS epoch 56 actor 92.7151712119469 critic 194.15639589735474 
epoch: 57, step: 0
	action: tensor([[ 0.8698, -0.2015, -0.2545,  0.2571, -0.0200,  0.1843,  0.0955]],
       dtype=torch.float64)
	q_value: tensor([[-7.3601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7650880140052485, distance: 0.5546375662616532 entropy 0.03264415264129639
epoch: 57, step: 1
	action: tensor([[ 0.8080, -0.5611, -0.6803,  0.8473, -0.3407,  0.5533, -0.1489]],
       dtype=torch.float64)
	q_value: tensor([[-6.8287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7041037803842386, distance: 0.6224814322789082 entropy 0.03264415264129639
epoch: 57, step: 2
	action: tensor([[ 1.0474, -0.0426, -0.2856,  0.5221, -0.6104,  0.5826, -0.1593]],
       dtype=torch.float64)
	q_value: tensor([[-9.7405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9695572367034074, distance: 0.1996635224871891 entropy 0.03264415264129639
epoch: 57, step: 3
	action: tensor([[ 1.2883,  0.1551, -0.8037,  0.1654,  0.0579,  0.3009, -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-12.3848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.856413671272692, distance: 0.4336239277925072 entropy 0.03264415264129639
epoch: 57, step: 4
	action: tensor([[ 1.6924,  0.3505, -0.3483,  0.8859,  0.1154,  0.3953,  0.0795]],
       dtype=torch.float64)
	q_value: tensor([[-12.5369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 57, step: 5
	action: tensor([[ 1.0829, -0.1249, -1.0257,  0.4821, -0.1879,  0.5858, -0.4661]],
       dtype=torch.float64)
	q_value: tensor([[-9.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8518404919756182, distance: 0.44047519539219887 entropy 0.03264415264129639
epoch: 57, step: 6
	action: tensor([[ 1.1054, -0.1247, -0.7471,  0.3103, -0.2392, -0.1417,  0.1326]],
       dtype=torch.float64)
	q_value: tensor([[-14.1751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7227596599597043, distance: 0.6025386425339455 entropy 0.03264415264129639
epoch: 57, step: 7
	action: tensor([[ 0.8024,  0.1812, -0.7542,  0.5626, -0.0233,  0.8070, -0.1446]],
       dtype=torch.float64)
	q_value: tensor([[-9.8780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9205519430203887, distance: 0.3225509560166005 entropy 0.03264415264129639
epoch: 57, step: 8
	action: tensor([[ 1.2068, -0.1101, -0.0979,  0.6238, -0.4296,  0.0570,  0.0911]],
       dtype=torch.float64)
	q_value: tensor([[-11.6710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8519370423018974, distance: 0.4403316509273112 entropy 0.03264415264129639
epoch: 57, step: 9
	action: tensor([[ 0.7233,  0.0745, -0.6377,  0.0919,  0.0949,  0.3928,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-10.6419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8806496144389034, distance: 0.3953380414902903 entropy 0.03264415264129639
epoch: 57, step: 10
	action: tensor([[ 0.9354,  0.1516, -0.3790,  0.8981, -0.3233,  0.1230, -0.1956]],
       dtype=torch.float64)
	q_value: tensor([[-7.6690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9878699373383908, distance: 0.12603414381482847 entropy 0.03264415264129639
epoch: 57, step: 11
	action: tensor([[ 1.0361, -0.0424, -0.5238,  0.8786,  0.0514,  0.1689, -0.1828]],
       dtype=torch.float64)
	q_value: tensor([[-11.5038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.99068822740792, distance: 0.11042638980627342 entropy 0.03264415264129639
epoch: 57, step: 12
	action: tensor([[ 1.1925, -0.1594, -0.4117,  0.5566, -0.0928,  0.1932, -0.1887]],
       dtype=torch.float64)
	q_value: tensor([[-10.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.864133421843207, distance: 0.4218062515786181 entropy 0.03264415264129639
epoch: 57, step: 13
	action: tensor([[ 0.6883,  0.3685, -0.2835,  0.8544, -0.0073,  0.2253, -0.0650]],
       dtype=torch.float64)
	q_value: tensor([[-11.1196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8985606867706836, distance: 0.3644683690732849 entropy 0.03264415264129639
epoch: 57, step: 14
	action: tensor([[ 0.9398, -0.1061, -0.8512,  0.7263,  0.0766,  0.0235,  0.4400]],
       dtype=torch.float64)
	q_value: tensor([[-9.2891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.879892731068061, distance: 0.3965896163742478 entropy 0.03264415264129639
epoch: 57, step: 15
	action: tensor([[ 0.3291,  0.2188, -0.6894,  0.6661, -0.3676,  0.4830,  0.0384]],
       dtype=torch.float64)
	q_value: tensor([[-8.6067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6347533894383934, distance: 0.6915917285868123 entropy 0.03264415264129639
epoch: 57, step: 16
	action: tensor([[ 0.1504, -0.2678, -0.6518,  0.5989, -0.4185,  0.2555, -0.3113]],
       dtype=torch.float64)
	q_value: tensor([[-8.5170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.352197915713482, distance: 0.9210386684097008 entropy 0.03264415264129639
epoch: 57, step: 17
	action: tensor([[ 0.5229, -0.2653, -0.0534,  0.1923,  0.3571,  0.4895, -0.4053]],
       dtype=torch.float64)
	q_value: tensor([[-6.4304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7079309124766584, distance: 0.6184427322555288 entropy 0.03264415264129639
epoch: 57, step: 18
	action: tensor([[ 1.4130, -0.2358, -0.4543,  0.8709, -0.3672,  0.5200,  0.3026]],
       dtype=torch.float64)
	q_value: tensor([[-5.5271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9256384910736454, distance: 0.31205474461347016 entropy 0.03264415264129639
epoch: 57, step: 19
	action: tensor([[ 1.3792, -0.3243, -0.6718,  1.2672, -0.2699,  0.4445,  0.3810]],
       dtype=torch.float64)
	q_value: tensor([[-12.8292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9720064329044146, distance: 0.19146342095895802 entropy 0.03264415264129639
epoch: 57, step: 20
	action: tensor([[ 0.7574,  0.1771, -0.7614,  0.3648, -0.0145,  0.4922, -0.3106]],
       dtype=torch.float64)
	q_value: tensor([[-12.8409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9322988087371946, distance: 0.2977521105358192 entropy 0.03264415264129639
epoch: 57, step: 21
	action: tensor([[ 1.2419,  0.3448, -0.7311,  0.3842, -0.0145,  0.5473,  0.0976]],
       dtype=torch.float64)
	q_value: tensor([[-10.7044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9835950146900098, distance: 0.14656984081730995 entropy 0.03264415264129639
epoch: 57, step: 22
	action: tensor([[ 1.1923,  0.2439, -0.1271,  0.5088,  0.2714,  0.0534, -0.2056]],
       dtype=torch.float64)
	q_value: tensor([[-13.3393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9252300280157003, distance: 0.3129106193787664 entropy 0.03264415264129639
epoch: 57, step: 23
	action: tensor([[ 1.1372, -0.0627, -0.6431,  0.7704, -0.0326,  0.1869, -0.1662]],
       dtype=torch.float64)
	q_value: tensor([[-10.7647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9668804614198497, distance: 0.2082566279310469 entropy 0.03264415264129639
epoch: 57, step: 24
	action: tensor([[ 1.1931,  0.0121, -1.0481,  0.5290, -0.3647,  0.6173, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-11.7937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9074001291461592, distance: 0.34822655288764426 entropy 0.03264415264129639
epoch: 57, step: 25
	action: tensor([[ 1.2457, -0.2272, -0.3272,  1.0753,  0.0886,  0.2569,  0.2021]],
       dtype=torch.float64)
	q_value: tensor([[-14.6589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.974071932717506, distance: 0.18426453934411482 entropy 0.03264415264129639
epoch: 57, step: 26
	action: tensor([[ 1.6099, -0.0952, -0.7635,  0.9737, -0.4902,  0.2477, -0.0864]],
       dtype=torch.float64)
	q_value: tensor([[-10.5921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9133129264361491, distance: 0.33692548661053523 entropy 0.03264415264129639
epoch: 57, step: 27
	action: tensor([[ 0.8995,  0.0276, -0.8434,  1.0046, -0.2411,  0.5174, -0.1239]],
       dtype=torch.float64)
	q_value: tensor([[-16.7510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.87107608960232, distance: 0.4108879718384185 entropy 0.03264415264129639
epoch: 57, step: 28
	action: tensor([[ 1.4055, -0.4784, -0.5545,  0.8385,  0.2970,  0.1132, -0.3854]],
       dtype=torch.float64)
	q_value: tensor([[-12.5322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7251255329576813, distance: 0.599962206155574 entropy 0.03264415264129639
epoch: 57, step: 29
	action: tensor([[ 1.1679,  0.0822, -0.6562,  0.9275,  0.2121,  0.0178,  0.1529]],
       dtype=torch.float64)
	q_value: tensor([[-12.2726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0723241675359284 entropy 0.03264415264129639
epoch: 57, step: 30
	action: tensor([[ 0.7734, -0.1392, -0.3303,  0.4193,  0.2050,  0.0682,  0.1607]],
       dtype=torch.float64)
	q_value: tensor([[-9.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8288655780363121, distance: 0.4733968670272592 entropy 0.03264415264129639
epoch: 57, step: 31
	action: tensor([[ 0.6235, -0.0978, -0.4794,  0.6085, -0.3244,  0.6208, -0.2129]],
       dtype=torch.float64)
	q_value: tensor([[-5.9992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8310796399841239, distance: 0.4703245974416295 entropy 0.03264415264129639
epoch: 57, step: 32
	action: tensor([[ 0.6030,  0.0473, -0.5760,  0.7364,  0.2047, -0.1103, -0.3681]],
       dtype=torch.float64)
	q_value: tensor([[-9.5182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8557032934379334, distance: 0.43469525810121296 entropy 0.03264415264129639
epoch: 57, step: 33
	action: tensor([[ 1.1079, -0.1153, -0.5061,  0.2032,  0.3876,  0.0589, -0.2156]],
       dtype=torch.float64)
	q_value: tensor([[-8.1145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7316993540916004, distance: 0.5927445265622765 entropy 0.03264415264129639
epoch: 57, step: 34
	action: tensor([[ 1.0268, -0.1826, -0.7230,  0.4044, -0.1814,  0.3569, -0.1946]],
       dtype=torch.float64)
	q_value: tensor([[-9.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8362593058544414, distance: 0.4630575896928799 entropy 0.03264415264129639
epoch: 57, step: 35
	action: tensor([[ 0.9613,  0.0350, -0.3271,  0.2217,  0.3040,  0.7108,  0.0825]],
       dtype=torch.float64)
	q_value: tensor([[-10.8738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9615990858606817, distance: 0.22424743012801748 entropy 0.03264415264129639
epoch: 57, step: 36
	action: tensor([[ 0.2773,  0.1366, -0.8457,  0.6502, -0.1009,  0.0568,  0.5678]],
       dtype=torch.float64)
	q_value: tensor([[-8.7704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5673863773108386, distance: 0.7526737824958599 entropy 0.03264415264129639
epoch: 57, step: 37
	action: tensor([[ 0.8598, -0.2592, -0.4443,  0.2790, -0.2303,  0.1557, -0.4044]],
       dtype=torch.float64)
	q_value: tensor([[-5.9376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7180135445500365, distance: 0.6076742274431794 entropy 0.03264415264129639
epoch: 57, step: 38
	action: tensor([[ 1.0398, -0.1343, -0.8256,  0.5442, -0.1839,  0.1657, -0.0420]],
       dtype=torch.float64)
	q_value: tensor([[-9.1057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8631821318061442, distance: 0.42328034538362885 entropy 0.03264415264129639
epoch: 57, step: 39
	action: tensor([[ 0.9410,  0.3106, -0.1585,  0.4143,  0.1857,  0.0725, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-10.7336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9903421803630452, distance: 0.11245952337185225 entropy 0.03264415264129639
epoch: 57, step: 40
	action: tensor([[ 0.8056,  0.0232, -0.6302,  0.4590, -0.3496,  0.1153,  0.0367]],
       dtype=torch.float64)
	q_value: tensor([[-8.7389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9028267724831637, distance: 0.3567220926750914 entropy 0.03264415264129639
epoch: 57, step: 41
	action: tensor([[ 0.6464, -0.0778, -0.6563,  0.5854, -0.2861, -0.1704,  0.4350]],
       dtype=torch.float64)
	q_value: tensor([[-9.0633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8114719335971287, distance: 0.496872201456457 entropy 0.03264415264129639
epoch: 57, step: 42
	action: tensor([[ 1.3658,  0.1045, -0.2078,  0.3642, -0.3421,  0.0618, -0.3366]],
       dtype=torch.float64)
	q_value: tensor([[-6.7901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8058097610850192, distance: 0.5042784291282437 entropy 0.03264415264129639
epoch: 57, step: 43
	action: tensor([[ 1.3267,  0.6139, -0.4573,  0.1509, -0.1754,  0.0109, -0.2714]],
       dtype=torch.float64)
	q_value: tensor([[-13.5051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8893749191530542, distance: 0.3806128914148816 entropy 0.03264415264129639
epoch: 57, step: 44
	action: tensor([[ 1.3990, -0.2398, -0.7803,  0.3059,  0.0529,  0.0285,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-14.6023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5584908481222992, distance: 0.7603727571333273 entropy 0.03264415264129639
epoch: 57, step: 45
	action: tensor([[ 1.4595, -0.5396, -0.2779,  0.3861, -0.3186, -0.1053,  0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-11.4345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25188651386610605, distance: 0.9897840138100206 entropy 0.03264415264129639
epoch: 57, step: 46
	action: tensor([[ 1.1292,  0.1949, -0.1996,  0.5956,  0.0398,  0.4387, -0.1057]],
       dtype=torch.float64)
	q_value: tensor([[-10.7356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9732060011205895, distance: 0.18731625255298812 entropy 0.03264415264129639
epoch: 57, step: 47
	action: tensor([[ 1.0041,  0.1515, -0.7393,  0.4159,  0.1037,  0.3483, -0.6700]],
       dtype=torch.float64)
	q_value: tensor([[-11.3664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9770064796848164, distance: 0.1735239506435493 entropy 0.03264415264129639
epoch: 57, step: 48
	action: tensor([[ 1.2123, -0.1536, -0.1997,  0.5931,  0.0059,  0.1350, -0.2098]],
       dtype=torch.float64)
	q_value: tensor([[-13.0809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8583364367256548, distance: 0.4307108121306157 entropy 0.03264415264129639
epoch: 57, step: 49
	action: tensor([[ 1.1982, -0.3149, -0.6700,  0.8342,  0.0571, -0.0404, -0.6697]],
       dtype=torch.float64)
	q_value: tensor([[-10.6816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8556543970141889, distance: 0.4347689023434436 entropy 0.03264415264129639
epoch: 57, step: 50
	action: tensor([[ 1.6266, -0.0027, -0.5089,  0.4922, -0.1305,  0.1659, -0.4053]],
       dtype=torch.float64)
	q_value: tensor([[-13.0465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7212871092960204, distance: 0.6041367029453861 entropy 0.03264415264129639
epoch: 57, step: 51
	action: tensor([[ 1.0172, -0.2299, -0.7258,  0.8034,  0.2597, -0.0661, -0.1035]],
       dtype=torch.float64)
	q_value: tensor([[-15.5946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8663836191757309, distance: 0.4182987293465426 entropy 0.03264415264129639
epoch: 57, step: 52
	action: tensor([[ 1.7644, -0.2300, -0.8968,  0.4273,  0.2207,  0.0933, -0.1598]],
       dtype=torch.float64)
	q_value: tensor([[-9.4459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 57, step: 53
	action: tensor([[ 0.9938, -0.4001, -0.4706,  0.4549,  0.0120,  0.0903, -0.1577]],
       dtype=torch.float64)
	q_value: tensor([[-9.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6669748896615026, distance: 0.6603819338450247 entropy 0.03264415264129639
epoch: 57, step: 54
	action: tensor([[ 1.0268,  0.1182, -0.3738,  0.5691, -0.4398, -0.2800, -0.2256]],
       dtype=torch.float64)
	q_value: tensor([[-8.4270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9468748239249449, distance: 0.2637587551988699 entropy 0.03264415264129639
epoch: 57, step: 55
	action: tensor([[ 1.0550, -0.4452, -0.8224,  0.4861, -0.1579,  0.5507, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-11.2384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7045374906032458, distance: 0.6220250635546754 entropy 0.03264415264129639
epoch: 57, step: 56
	action: tensor([[ 1.2484, -0.5976, -0.0707,  0.4236, -0.0329,  0.0939, -0.1505]],
       dtype=torch.float64)
	q_value: tensor([[-10.2867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4064346108791297, distance: 0.8816393747389402 entropy 0.03264415264129639
epoch: 57, step: 57
	action: tensor([[ 0.8393, -0.1048, -0.6209,  0.6043,  0.0737,  0.0882, -0.2244]],
       dtype=torch.float64)
	q_value: tensor([[-9.1369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.889291841460614, distance: 0.3807557817346202 entropy 0.03264415264129639
epoch: 57, step: 58
	action: tensor([[ 0.7706, -0.0191,  0.0112,  0.4857, -0.4376,  0.1817, -0.3160]],
       dtype=torch.float64)
	q_value: tensor([[-8.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9476132754641499, distance: 0.26191918810817866 entropy 0.03264415264129639
epoch: 57, step: 59
	action: tensor([[ 1.1078, -0.0283, -0.4809,  0.4802, -0.2039,  0.0380,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[-9.0511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9012377397202073, distance: 0.35962692832926024 entropy 0.03264415264129639
epoch: 57, step: 60
	action: tensor([[ 0.9479, -0.3789, -1.5308,  0.5469, -0.1443,  0.6427, -0.5030]],
       dtype=torch.float64)
	q_value: tensor([[-10.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45700059203217525, distance: 0.843250041644056 entropy 0.03264415264129639
epoch: 57, step: 61
	action: tensor([[ 1.1926, -0.5839, -0.6406,  0.9474,  0.1931,  0.1278,  0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-14.2099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7681422813983664, distance: 0.5510201395161045 entropy 0.03264415264129639
epoch: 57, step: 62
	action: tensor([[ 1.3154, -0.3988, -0.7705,  0.5158,  0.0014, -0.2715, -0.3293]],
       dtype=torch.float64)
	q_value: tensor([[-9.4681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49778513093556764, distance: 0.8109638151745138 entropy 0.03264415264129639
epoch: 57, step: 63
	action: tensor([[ 0.8564, -0.0280, -0.5237,  0.7767, -0.1837,  0.3925, -0.3759]],
       dtype=torch.float64)
	q_value: tensor([[-11.8346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9339693433918694, distance: 0.2940556316190041 entropy 0.03264415264129639
epoch: 57, step: 64
	action: tensor([[ 1.6076,  0.1545, -0.9448,  0.8411, -0.3190,  0.1173, -0.5391]],
       dtype=torch.float64)
	q_value: tensor([[-11.2386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9407264241505674, distance: 0.2786039421944547 entropy 0.03264415264129639
epoch: 57, step: 65
	action: tensor([[ 1.2078, -0.0222, -0.8411,  0.7499, -0.4943,  0.1317,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-18.8494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9487965415105475, distance: 0.25894429146688747 entropy 0.03264415264129639
epoch: 57, step: 66
	action: tensor([[ 0.8425, -0.0660, -0.2708,  0.4162, -0.0881,  0.2015, -0.3748]],
       dtype=torch.float64)
	q_value: tensor([[-13.2791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9135945272098421, distance: 0.33637779448185395 entropy 0.03264415264129639
epoch: 57, step: 67
	action: tensor([[ 1.2477, -0.1141, -0.9692,  0.7159, -0.3199,  0.1887,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-9.0726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8911093537267442, distance: 0.3776173854665005 entropy 0.03264415264129639
epoch: 57, step: 68
	action: tensor([[ 0.6674, -0.4498, -0.4921,  0.6499,  0.1279,  0.1283,  0.2236]],
       dtype=torch.float64)
	q_value: tensor([[-13.2128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6748973809085719, distance: 0.6524795843013533 entropy 0.03264415264129639
epoch: 57, step: 69
	action: tensor([[ 0.8839, -0.7696, -0.7553,  0.2208, -0.0270,  0.1473,  0.0730]],
       dtype=torch.float64)
	q_value: tensor([[-5.7322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.136219249926819, distance: 1.0635513272642598 entropy 0.03264415264129639
epoch: 57, step: 70
	action: tensor([[ 0.8409, -0.0155, -0.4771,  0.5757, -0.2379,  0.3800, -0.1289]],
       dtype=torch.float64)
	q_value: tensor([[-6.9227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9345746799901783, distance: 0.29270464947026126 entropy 0.03264415264129639
epoch: 57, step: 71
	action: tensor([[ 0.8262, -0.1974, -0.1814,  0.3141, -0.6455, -0.1316,  0.3510]],
       dtype=torch.float64)
	q_value: tensor([[-9.8171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.736526790368706, distance: 0.587387803081133 entropy 0.03264415264129639
epoch: 57, step: 72
	action: tensor([[ 0.9787,  0.0624, -0.4288,  0.0891,  0.2025,  0.0356, -0.1285]],
       dtype=torch.float64)
	q_value: tensor([[-7.2944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8244259805506725, distance: 0.4794980210128964 entropy 0.03264415264129639
epoch: 57, step: 73
	action: tensor([[ 1.3925,  0.4956, -0.7892,  0.4637, -0.2994, -0.0659,  0.1019]],
       dtype=torch.float64)
	q_value: tensor([[-8.3844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9302179700054575, distance: 0.3022932799662293 entropy 0.03264415264129639
epoch: 57, step: 74
	action: tensor([[ 1.4474,  0.0759, -0.1214,  0.5862,  0.2032,  0.3888,  0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-14.9001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8316540884839931, distance: 0.46952419851638755 entropy 0.03264415264129639
epoch: 57, step: 75
	action: tensor([[ 0.9289, -0.3462, -0.4911,  0.4068, -0.0267,  0.0135, -0.1015]],
       dtype=torch.float64)
	q_value: tensor([[-11.3461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6799824429994374, distance: 0.6473566236949283 entropy 0.03264415264129639
epoch: 57, step: 76
	action: tensor([[ 0.9393,  0.3554, -0.3547,  0.6894, -0.6016, -0.1913,  0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-7.9216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9892961133494675, distance: 0.11839338671588243 entropy 0.03264415264129639
epoch: 57, step: 77
	action: tensor([[ 0.5495, -0.0269, -0.4497,  0.5461, -0.4995,  0.3463,  0.1662]],
       dtype=torch.float64)
	q_value: tensor([[-11.2971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8140807470881007, distance: 0.4934224166957997 entropy 0.03264415264129639
epoch: 57, step: 78
	action: tensor([[ 0.3086,  0.3791, -0.5869,  0.3512, -0.2257,  0.2698, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[-7.7239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.731801789099767, distance: 0.5926313632362807 entropy 0.03264415264129639
epoch: 57, step: 79
	action: tensor([[ 5.0714e-01,  3.9433e-02, -3.4505e-01,  7.1433e-01, -2.1580e-01,
          5.2886e-04, -1.8378e-01]], dtype=torch.float64)
	q_value: tensor([[-7.2362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8655751751106939, distance: 0.41956227630014314 entropy 0.03264415264129639
epoch: 57, step: 80
	action: tensor([[ 9.7060e-01,  3.1409e-01, -6.0846e-01,  6.1238e-01,  7.1152e-04,
         -1.0587e-01, -2.4462e-01]], dtype=torch.float64)
	q_value: tensor([[-7.3691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9921097953893951, distance: 0.10164846531488114 entropy 0.03264415264129639
epoch: 57, step: 81
	action: tensor([[ 1.3898,  0.1698, -0.3991,  0.4604, -0.4331,  0.3431, -0.6780]],
       dtype=torch.float64)
	q_value: tensor([[-11.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.920853301333381, distance: 0.32193863289852526 entropy 0.03264415264129639
epoch: 57, step: 82
	action: tensor([[ 0.8935, -0.4049, -0.2134,  0.8206, -0.4086,  0.6166, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-16.7051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9287985005562314, distance: 0.3053523474605227 entropy 0.03264415264129639
epoch: 57, step: 83
	action: tensor([[ 0.9583,  0.2678, -0.6261,  0.5156, -0.0366,  0.4650, -0.3941]],
       dtype=torch.float64)
	q_value: tensor([[-9.7325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9844026075069312, distance: 0.14291660591277935 entropy 0.03264415264129639
epoch: 57, step: 84
	action: tensor([[ 1.5780, -0.0746, -0.6716,  0.6639,  0.1752,  0.3247, -0.1180]],
       dtype=torch.float64)
	q_value: tensor([[-12.6548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8298759014980694, distance: 0.47199740584137173 entropy 0.03264415264129639
epoch: 57, step: 85
	action: tensor([[ 1.1622, -0.0052, -0.7577,  0.7997, -0.0494,  0.2850, -0.2229]],
       dtype=torch.float64)
	q_value: tensor([[-14.3309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.980739656919489, distance: 0.15881397014844328 entropy 0.03264415264129639
epoch: 57, step: 86
	action: tensor([[ 1.1005, -0.1967, -0.6634,  0.8566, -0.1943, -0.0724, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-12.9123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9172496473551552, distance: 0.3291861994402334 entropy 0.03264415264129639
epoch: 57, step: 87
	action: tensor([[ 1.1314, -0.2207, -0.5306,  0.8926, -0.2073,  0.6524,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[-10.7849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9769847773502073, distance: 0.17360582124461144 entropy 0.03264415264129639
epoch: 57, step: 88
	action: tensor([[ 1.0893,  0.1146, -0.5551,  0.8156,  0.2008,  0.3787,  0.3108]],
       dtype=torch.float64)
	q_value: tensor([[-11.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.055986552299188974 entropy 0.03264415264129639
epoch: 57, step: 89
	action: tensor([[ 1.0895,  0.0877, -0.3179,  0.8379,  0.4194, -0.1566, -0.2339]],
       dtype=torch.float64)
	q_value: tensor([[-9.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09865791586081 entropy 0.03264415264129639
epoch: 57, step: 90
	action: tensor([[ 0.9703, -0.1114, -0.5770,  0.3496,  0.3034,  0.1420,  0.1821]],
       dtype=torch.float64)
	q_value: tensor([[-9.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8349377691011006, distance: 0.4649224829644244 entropy 0.03264415264129639
epoch: 57, step: 91
	action: tensor([[ 1.0298, -0.3497, -0.2087,  0.2819,  0.1231, -0.1670,  0.0917]],
       dtype=torch.float64)
	q_value: tensor([[-7.6950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5456500337596775, distance: 0.7713508160661402 entropy 0.03264415264129639
epoch: 57, step: 92
	action: tensor([[ 0.8261,  0.1483, -0.7108,  0.2949, -0.2197,  0.2903, -0.3758]],
       dtype=torch.float64)
	q_value: tensor([[-6.8516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9146669951960766, distance: 0.33428370929220436 entropy 0.03264415264129639
epoch: 57, step: 93
	action: tensor([[ 1.0447, -0.0487, -0.6259,  0.9506,  0.1542, -0.6185, -0.1971]],
       dtype=torch.float64)
	q_value: tensor([[-11.1909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9308764954797469, distance: 0.3008635444016332 entropy 0.03264415264129639
epoch: 57, step: 94
	action: tensor([[ 1.0921, -0.0741, -0.7442,  0.6975, -0.2535,  0.4158,  0.1213]],
       dtype=torch.float64)
	q_value: tensor([[-10.6257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.948802429275094, distance: 0.2589294033431753 entropy 0.03264415264129639
epoch: 57, step: 95
	action: tensor([[ 0.6041, -0.0626, -0.1865,  0.5376, -0.1495,  0.5742, -0.5437]],
       dtype=torch.float64)
	q_value: tensor([[-11.4902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9167970133180373, distance: 0.3300852752447084 entropy 0.03264415264129639
epoch: 57, step: 96
	action: tensor([[ 1.0820,  0.0624, -0.6647,  0.7748, -0.0593,  0.3462, -0.0542]],
       dtype=torch.float64)
	q_value: tensor([[-9.1907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9903217513494904, distance: 0.11257840235100992 entropy 0.03264415264129639
epoch: 57, step: 97
	action: tensor([[ 1.1366,  0.0024, -0.5444,  0.6803, -0.6091,  0.0899,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-11.8506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9665576219592822, distance: 0.20926917839293613 entropy 0.03264415264129639
epoch: 57, step: 98
	action: tensor([[ 0.9765, -0.0056, -0.3635,  0.4610,  0.1679, -0.0243,  0.2918]],
       dtype=torch.float64)
	q_value: tensor([[-12.1133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9070613338028095, distance: 0.34886299988943564 entropy 0.03264415264129639
epoch: 57, step: 99
	action: tensor([[ 0.9655,  0.0851, -0.3181,  0.6369,  0.0254,  0.1921, -0.4215]],
       dtype=torch.float64)
	q_value: tensor([[-7.4739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08817806749760748 entropy 0.03264415264129639
epoch: 57, step: 100
	action: tensor([[ 1.1908,  0.1434, -0.3130,  0.1161, -0.1987, -0.0870, -0.6713]],
       dtype=torch.float64)
	q_value: tensor([[-9.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7936012293702023, distance: 0.5198885424604982 entropy 0.03264415264129639
epoch: 57, step: 101
	action: tensor([[ 1.3579, -0.3281, -0.4864,  0.4917, -0.2127,  0.2062, -0.1232]],
       dtype=torch.float64)
	q_value: tensor([[-12.7481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6917258847723285, distance: 0.6353678321090532 entropy 0.03264415264129639
epoch: 57, step: 102
	action: tensor([[ 1.1775,  0.4659, -0.4887,  1.0050,  0.2130, -0.0126,  0.0920]],
       dtype=torch.float64)
	q_value: tensor([[-11.8569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9144043186717934, distance: 0.33479781876673215 entropy 0.03264415264129639
epoch: 57, step: 103
	action: tensor([[ 0.9556, -0.0772, -0.1263,  0.8610, -0.4652,  0.2073, -0.3862]],
       dtype=torch.float64)
	q_value: tensor([[-12.4191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06795070775736957 entropy 0.03264415264129639
epoch: 57, step: 104
	action: tensor([[ 1.0733,  0.1622, -0.4232,  0.5165, -0.3595,  0.1005,  0.0732]],
       dtype=torch.float64)
	q_value: tensor([[-9.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9766512514927476, distance: 0.17485920395799504 entropy 0.03264415264129639
epoch: 57, step: 105
	action: tensor([[ 1.4407, -0.0332, -0.4934,  0.6332, -0.6520, -0.0510, -0.2030]],
       dtype=torch.float64)
	q_value: tensor([[-10.9896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8218243800785606, distance: 0.483037482829111 entropy 0.03264415264129639
epoch: 57, step: 106
	action: tensor([[ 0.7556, -0.4314, -0.7608,  0.8893, -0.2076, -0.0470,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-14.9142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6954628934170125, distance: 0.6315050121921717 entropy 0.03264415264129639
epoch: 57, step: 107
	action: tensor([[ 0.9086,  0.1416, -0.7835,  0.5857, -0.0335,  0.1349, -0.1943]],
       dtype=torch.float64)
	q_value: tensor([[-8.1866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.950553570267238, distance: 0.254462718603152 entropy 0.03264415264129639
epoch: 57, step: 108
	action: tensor([[ 1.1710,  0.0032, -0.1418,  0.7105, -0.5328, -0.1762, -0.5601]],
       dtype=torch.float64)
	q_value: tensor([[-10.8873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9167951614720753, distance: 0.3300889485724774 entropy 0.03264415264129639
epoch: 57, step: 109
	action: tensor([[ 1.1485,  0.0029, -0.6481,  0.5786, -0.3102,  0.4457, -0.5444]],
       dtype=torch.float64)
	q_value: tensor([[-13.4938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9696501062318532, distance: 0.19935874035724974 entropy 0.03264415264129639
epoch: 57, step: 110
	action: tensor([[ 1.8184,  0.1377, -0.6342,  0.6185, -0.3859,  0.1768, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-14.5699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 57, step: 111
	action: tensor([[ 1.1167, -0.0102,  0.2293,  0.4540, -0.3514,  0.4138,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[-9.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9137761527593612, distance: 0.3360240730153717 entropy 0.03264415264129639
epoch: 57, step: 112
	action: tensor([[ 1.1707e+00, -7.1437e-02, -5.9812e-01,  9.0422e-01, -3.3669e-01,
          1.2609e-01,  5.7256e-04]], dtype=torch.float64)
	q_value: tensor([[-10.1772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9839003687527995, distance: 0.1451993451542206 entropy 0.03264415264129639
epoch: 57, step: 113
	action: tensor([[ 0.9493,  0.1257, -0.7610,  0.0305,  0.0911, -0.1598,  0.1974]],
       dtype=torch.float64)
	q_value: tensor([[-12.2092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8104798492222585, distance: 0.4981778223210801 entropy 0.03264415264129639
epoch: 57, step: 114
	action: tensor([[ 0.7800, -0.3363, -0.2526,  0.6690, -0.4387,  0.5203, -0.3280]],
       dtype=torch.float64)
	q_value: tensor([[-8.4735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8803918849869161, distance: 0.3957646647863015 entropy 0.03264415264129639
epoch: 57, step: 115
	action: tensor([[ 1.1489,  0.2001, -0.4289,  0.7850,  0.5273,  0.4745, -0.0679]],
       dtype=torch.float64)
	q_value: tensor([[-9.7637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.978744760715568, distance: 0.16683597068781358 entropy 0.03264415264129639
epoch: 57, step: 116
	action: tensor([[ 0.7080,  0.0680, -0.8364,  0.3986, -0.3230,  0.2542, -0.2486]],
       dtype=torch.float64)
	q_value: tensor([[-11.2690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8436288277983037, distance: 0.4525171457535075 entropy 0.03264415264129639
epoch: 57, step: 117
	action: tensor([[ 1.3640,  0.0455, -0.1336,  0.6386,  0.1060,  0.2447, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[-10.3193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8842811895401361, distance: 0.3892769362879536 entropy 0.03264415264129639
epoch: 57, step: 118
	action: tensor([[ 1.0184,  0.0506, -0.1228,  0.7235, -0.1442,  0.2479, -0.1338]],
       dtype=torch.float64)
	q_value: tensor([[-11.6167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0982980225496018 entropy 0.03264415264129639
epoch: 57, step: 119
	action: tensor([[ 1.4120,  0.0447, -0.3164,  0.2808, -0.0312,  0.2411, -0.2961]],
       dtype=torch.float64)
	q_value: tensor([[-9.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.761732367801961, distance: 0.5585849339009644 entropy 0.03264415264129639
epoch: 57, step: 120
	action: tensor([[ 1.6070, -0.0282, -0.6382,  0.4364, -0.1880,  0.2545,  0.1553]],
       dtype=torch.float64)
	q_value: tensor([[-13.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7053420252163654, distance: 0.6211776094557758 entropy 0.03264415264129639
epoch: 57, step: 121
	action: tensor([[ 0.8461,  0.0301, -0.4578,  0.3643, -0.3299,  0.3269, -0.0574]],
       dtype=torch.float64)
	q_value: tensor([[-14.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9308493047439279, distance: 0.30092271310892693 entropy 0.03264415264129639
epoch: 57, step: 122
	action: tensor([[ 0.8879, -0.1514, -0.6799,  0.5404, -0.3769,  0.3508, -0.3900]],
       dtype=torch.float64)
	q_value: tensor([[-9.4715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8635113462832046, distance: 0.42277078494663983 entropy 0.03264415264129639
epoch: 57, step: 123
	action: tensor([[ 0.3972,  0.1776, -0.6321,  0.6834, -0.4674,  0.3812, -0.1147]],
       dtype=torch.float64)
	q_value: tensor([[-11.5367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6809547642642054, distance: 0.6463724316768081 entropy 0.03264415264129639
epoch: 57, step: 124
	action: tensor([[ 0.8941,  0.0465, -0.2708,  0.1372, -0.3111, -0.0979, -0.1381]],
       dtype=torch.float64)
	q_value: tensor([[-9.0602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8256563559394463, distance: 0.4778149708157079 entropy 0.03264415264129639
epoch: 57, step: 125
	action: tensor([[ 1.0389,  0.1932, -0.2450,  0.8094,  0.0505,  0.2944, -0.2097]],
       dtype=torch.float64)
	q_value: tensor([[-8.5412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9840485056203371, distance: 0.14452979306604735 entropy 0.03264415264129639
epoch: 57, step: 126
	action: tensor([[ 0.8175, -0.4696, -0.6579,  0.6828, -0.0648,  0.4228, -0.2820]],
       dtype=torch.float64)
	q_value: tensor([[-11.3274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7421663743220633, distance: 0.5810673471804025 entropy 0.03264415264129639
epoch: 57, step: 127
	action: tensor([[ 0.9114,  0.5082, -0.6079,  0.2518, -0.0279,  0.0493, -0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-9.0495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9891915652224571, distance: 0.11897017389102 entropy 0.03264415264129639
LOSS epoch 57 actor 366.906216795336 critic 3752.4279063419235 
epoch: 58, step: 0
	action: tensor([[ 1.2153, -0.0101, -0.8641,  0.4760, -0.2671,  0.2516,  0.1722]],
       dtype=torch.float64)
	q_value: tensor([[-7.7805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8965556539844257, distance: 0.36805275457671593 entropy 0.03264415264129639
epoch: 58, step: 1
	action: tensor([[ 1.0931, -0.2611, -0.5982,  0.5119, -0.4969,  0.1628, -0.2160]],
       dtype=torch.float64)
	q_value: tensor([[-8.7948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8040653042089032, distance: 0.5065383912715439 entropy 0.03264415264129639
epoch: 58, step: 2
	action: tensor([[ 1.2059, -0.1882, -0.1283,  0.3934, -0.0727,  0.1321,  0.3460]],
       dtype=torch.float64)
	q_value: tensor([[-8.3115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7477616267948486, distance: 0.5747278885156896 entropy 0.03264415264129639
epoch: 58, step: 3
	action: tensor([[ 1.3412,  0.2124, -0.8191,  1.1466, -0.0250,  0.6260, -0.1619]],
       dtype=torch.float64)
	q_value: tensor([[-6.1565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9454238197321877, distance: 0.26733650223502087 entropy 0.03264415264129639
epoch: 58, step: 4
	action: tensor([[ 1.7290, -0.1438, -0.8640,  1.1009,  0.0068,  0.0303, -0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-11.9042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 5
	action: tensor([[ 0.5739, -0.4543, -0.2057,  0.5157, -0.4042,  0.2033, -0.1691]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6125975830949291, distance: 0.7122588533228769 entropy 0.03264415264129639
epoch: 58, step: 6
	action: tensor([[ 1.4156, -0.3852, -1.0676,  0.4962, -0.2060,  0.3037, -0.1919]],
       dtype=torch.float64)
	q_value: tensor([[-4.6753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6297712188547688, distance: 0.6962926036253944 entropy 0.03264415264129639
epoch: 58, step: 7
	action: tensor([[ 1.6656, -0.5007, -0.2489,  0.6116, -0.2511,  0.6438,  0.2113]],
       dtype=torch.float64)
	q_value: tensor([[-10.0603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 8
	action: tensor([[ 1.1215, -0.0056, -0.7712,  1.0549, -0.3841,  0.4125,  0.1722]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9518353545853462, distance: 0.25114288388126704 entropy 0.03264415264129639
epoch: 58, step: 9
	action: tensor([[ 1.2901e+00, -5.8055e-04,  6.6335e-02,  8.3673e-01, -1.3038e-02,
          1.5218e-01,  1.5518e-01]], dtype=torch.float64)
	q_value: tensor([[-9.2748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8895487940338183, distance: 0.3803136597882607 entropy 0.03264415264129639
epoch: 58, step: 10
	action: tensor([[ 1.3358, -0.3744, -0.5672,  0.8958, -0.1219,  0.3679, -0.5229]],
       dtype=torch.float64)
	q_value: tensor([[-7.6085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9039263685921104, distance: 0.3546980464544494 entropy 0.03264415264129639
epoch: 58, step: 11
	action: tensor([[ 2.0410,  0.0790, -0.7354,  0.7171,  0.0997,  0.4994, -0.6329]],
       dtype=torch.float64)
	q_value: tensor([[-10.2123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 12
	action: tensor([[ 1.0765, -0.1526, -0.4552,  0.5811, -0.2241,  0.1735, -0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8937306884290279, distance: 0.3730444875620471 entropy 0.03264415264129639
epoch: 58, step: 13
	action: tensor([[ 0.9737, -0.0548, -0.4379,  0.9208,  0.3999,  0.3537, -0.2105]],
       dtype=torch.float64)
	q_value: tensor([[-7.3080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.991604892578191, distance: 0.10485033492803864 entropy 0.03264415264129639
epoch: 58, step: 14
	action: tensor([[ 1.9769, -0.4581, -0.7318,  0.9439, -0.2435,  0.4329, -0.1219]],
       dtype=torch.float64)
	q_value: tensor([[-7.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 15
	action: tensor([[ 0.9406,  0.2339, -0.2186,  0.2834, -0.3499,  0.2532, -0.2206]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9727070638959471, distance: 0.18905223782524141 entropy 0.03264415264129639
epoch: 58, step: 16
	action: tensor([[ 1.0224,  0.5074, -0.9102,  0.4370,  0.0705,  0.0928, -0.4952]],
       dtype=torch.float64)
	q_value: tensor([[-7.8189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9883945698109236, distance: 0.12327849249957948 entropy 0.03264415264129639
epoch: 58, step: 17
	action: tensor([[ 1.9473, -0.2992, -0.3904,  1.1703,  0.1803,  0.2524, -0.3376]],
       dtype=torch.float64)
	q_value: tensor([[-10.1698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 18
	action: tensor([[ 1.2151,  0.3563, -0.3862,  0.5854, -0.1903,  0.5128,  0.2106]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9786499202339102, distance: 0.1672077659099361 entropy 0.03264415264129639
epoch: 58, step: 19
	action: tensor([[ 1.2587, -0.2311, -0.2457,  0.6216, -0.1613,  0.0809, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-9.2506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8115412503126098, distance: 0.496780849765256 entropy 0.03264415264129639
epoch: 58, step: 20
	action: tensor([[ 1.7147, -0.2085, -1.0857,  0.8765, -0.0918,  0.0367, -0.3294]],
       dtype=torch.float64)
	q_value: tensor([[-7.8785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 21
	action: tensor([[ 1.0821, -0.2183, -0.3920,  0.3384, -0.6380,  0.0575, -0.2993]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7467870087164652, distance: 0.5758371569980859 entropy 0.03264415264129639
epoch: 58, step: 22
	action: tensor([[ 1.5365, -0.0561, -0.4842,  0.8638, -0.0867,  0.2873, -0.3761]],
       dtype=torch.float64)
	q_value: tensor([[-8.3172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9077036119162825, distance: 0.34765545333597864 entropy 0.03264415264129639
epoch: 58, step: 23
	action: tensor([[ 1.8113, -0.2880, -1.2351,  1.4075, -0.4395,  0.1099,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-11.3212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 24
	action: tensor([[ 1.2870, -0.1160, -0.4260,  0.7693, -0.5482, -0.3820,  0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8382975200287385, distance: 0.46016653665732515 entropy 0.03264415264129639
epoch: 58, step: 25
	action: tensor([[ 1.1855, -0.1961, -0.8313,  0.7197, -0.1812,  0.2866, -0.2297]],
       dtype=torch.float64)
	q_value: tensor([[-8.5086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8960647746001571, distance: 0.36892499017310726 entropy 0.03264415264129639
epoch: 58, step: 26
	action: tensor([[ 1.5615,  0.0567, -0.9856,  0.7201, -0.2732,  0.7706, -0.5377]],
       dtype=torch.float64)
	q_value: tensor([[-9.1894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9516142056768857, distance: 0.2517187872942858 entropy 0.03264415264129639
epoch: 58, step: 27
	action: tensor([[ 1.9228, -0.2093, -0.9793,  0.7794, -0.1900,  0.4806, -0.3382]],
       dtype=torch.float64)
	q_value: tensor([[-14.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 28
	action: tensor([[ 0.9599,  0.2302, -0.7377,  0.3064,  0.1105,  0.2088, -0.3152]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9633562121633787, distance: 0.21905686762990184 entropy 0.03264415264129639
epoch: 58, step: 29
	action: tensor([[ 1.2745,  0.0050, -1.3097,  0.9733, -0.4102,  0.4674, -0.2291]],
       dtype=torch.float64)
	q_value: tensor([[-8.0725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8497622611863743, distance: 0.4435537063054622 entropy 0.03264415264129639
epoch: 58, step: 30
	action: tensor([[ 1.4712, -0.3352, -1.0938,  1.1116, -0.0411,  0.0556, -0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-12.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.843885701995132, distance: 0.4521453132941086 entropy 0.03264415264129639
epoch: 58, step: 31
	action: tensor([[ 1.8386, -0.1842, -1.2515,  0.9393, -0.0325,  0.6943, -0.5707]],
       dtype=torch.float64)
	q_value: tensor([[-11.1018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 32
	action: tensor([[ 1.5775,  0.0795, -0.4542,  0.6587, -0.8924, -0.1550,  0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7558137249292652, distance: 0.5654800897634881 entropy 0.03264415264129639
epoch: 58, step: 33
	action: tensor([[ 0.7758, -0.0896, -0.6314,  0.7513, -0.1050,  0.5676, -0.2797]],
       dtype=torch.float64)
	q_value: tensor([[-11.6216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8837672741608018, distance: 0.390140381826333 entropy 0.03264415264129639
epoch: 58, step: 34
	action: tensor([[ 0.9418, -0.2999, -0.6253,  1.0019, -0.0144,  0.1025,  0.4597]],
       dtype=torch.float64)
	q_value: tensor([[-7.5424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9025859410050561, distance: 0.35716386428086905 entropy 0.03264415264129639
epoch: 58, step: 35
	action: tensor([[ 1.3259,  0.6106, -1.0353,  0.6548,  0.0677,  0.0746,  0.2765]],
       dtype=torch.float64)
	q_value: tensor([[-5.8021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9785524335118791, distance: 0.16758907530180192 entropy 0.03264415264129639
epoch: 58, step: 36
	action: tensor([[ 1.5569, -0.4932, -0.7708,  0.5953, -0.0171,  0.4755, -0.2671]],
       dtype=torch.float64)
	q_value: tensor([[-10.7287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6290798056035953, distance: 0.6969424739186764 entropy 0.03264415264129639
epoch: 58, step: 37
	action: tensor([[ 1.3555,  0.2343, -0.8931,  1.1547, -0.1444,  0.1030, -0.2702]],
       dtype=torch.float64)
	q_value: tensor([[-10.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9874657080814614, distance: 0.12811695136978327 entropy 0.03264415264129639
epoch: 58, step: 38
	action: tensor([[ 1.8304, -0.6968, -1.0703,  0.7829, -0.4472,  0.2333, -0.1860]],
       dtype=torch.float64)
	q_value: tensor([[-11.8689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 39
	action: tensor([[ 0.8373, -0.0244, -0.1885,  0.7006, -0.2278, -0.0433, -0.5000]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9579173994918375, distance: 0.23475128485021865 entropy 0.03264415264129639
epoch: 58, step: 40
	action: tensor([[ 1.8687,  0.1145, -0.4478,  0.9779,  0.0705, -0.4880, -0.5924]],
       dtype=torch.float64)
	q_value: tensor([[-7.3435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 41
	action: tensor([[ 1.2775, -0.1918, -0.6541,  0.4422,  0.0640,  0.3686,  0.1493]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8194157490348872, distance: 0.4862914449450442 entropy 0.03264415264129639
epoch: 58, step: 42
	action: tensor([[ 1.1362, -0.2446, -0.6161,  0.8143, -0.6396, -0.0752, -0.3923]],
       dtype=torch.float64)
	q_value: tensor([[-7.7516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8854796996022519, distance: 0.3872558013882011 entropy 0.03264415264129639
epoch: 58, step: 43
	action: tensor([[ 1.4357,  0.2913, -0.7909,  0.9962, -0.2750,  0.0213,  0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-9.6930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08765094146826058 entropy 0.03264415264129639
epoch: 58, step: 44
	action: tensor([[ 0.9249, -0.0453, -0.6335,  0.7914, -0.2073,  0.4521, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9374265647987288, distance: 0.2862540844611086 entropy 0.03264415264129639
epoch: 58, step: 45
	action: tensor([[ 1.6159,  0.0341, -0.5424,  0.6383,  0.0193,  0.1622, -0.2440]],
       dtype=torch.float64)
	q_value: tensor([[-7.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8045440126903178, distance: 0.5059192244774403 entropy 0.03264415264129639
epoch: 58, step: 46
	action: tensor([[ 1.7729, -0.3806, -1.1504,  0.8577, -0.5430,  0.1499,  0.0869]],
       dtype=torch.float64)
	q_value: tensor([[-10.9922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 47
	action: tensor([[ 1.6150,  0.2455, -0.6659,  0.1209,  0.1133,  0.2609, -0.0469]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6639103527426498, distance: 0.6634134325541434 entropy 0.03264415264129639
epoch: 58, step: 48
	action: tensor([[ 1.0499,  0.1072, -0.7442,  0.6569, -0.0932, -0.2569, -0.0360]],
       dtype=torch.float64)
	q_value: tensor([[-10.7455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9560791557075292, distance: 0.23982366370417163 entropy 0.03264415264129639
epoch: 58, step: 49
	action: tensor([[ 1.5734,  0.0241, -0.6801,  0.4582, -0.7073, -0.0218, -0.3155]],
       dtype=torch.float64)
	q_value: tensor([[-7.7686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7031488138644412, distance: 0.6234851120041423 entropy 0.03264415264129639
epoch: 58, step: 50
	action: tensor([[ 1.2121,  0.1541, -0.5089,  0.9601,  0.0364,  0.1328, -0.4288]],
       dtype=torch.float64)
	q_value: tensor([[-12.3854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0819209672483574 entropy 0.03264415264129639
epoch: 58, step: 51
	action: tensor([[ 0.9721,  0.5800, -0.7119,  0.1444, -0.1886,  0.0266,  0.1854]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9880664769045954, distance: 0.12500892837361519 entropy 0.03264415264129639
epoch: 58, step: 52
	action: tensor([[ 1.2923, -0.1358, -0.9863,  1.0859, -0.0724,  0.6189, -0.4991]],
       dtype=torch.float64)
	q_value: tensor([[-8.1758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9310141036658051, distance: 0.3005639219186079 entropy 0.03264415264129639
epoch: 58, step: 53
	action: tensor([[ 2.0130, -0.2012, -1.0019,  1.3097,  0.1793,  0.0907, -0.4795]],
       dtype=torch.float64)
	q_value: tensor([[-11.9453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 54
	action: tensor([[ 1.2116,  0.1989, -0.4898,  0.7837, -0.4757,  0.1626,  0.0599]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06522577999128558 entropy 0.03264415264129639
epoch: 58, step: 55
	action: tensor([[ 1.0369, -0.1959, -0.7235,  0.3924, -0.0914,  0.3414, -0.4516]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8247375465673155, distance: 0.4790723839359224 entropy 0.03264415264129639
epoch: 58, step: 56
	action: tensor([[ 1.4267, -0.5563, -1.1443,  0.4264, -0.2759,  0.1327, -0.0549]],
       dtype=torch.float64)
	q_value: tensor([[-8.3442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34951052584740916, distance: 0.9229471434590731 entropy 0.03264415264129639
epoch: 58, step: 57
	action: tensor([[ 1.3401e+00, -1.2852e-01, -3.8932e-01,  1.0223e+00, -1.3307e-04,
          3.0655e-01, -1.9613e-02]], dtype=torch.float64)
	q_value: tensor([[-9.3354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9768072507335229, distance: 0.17427408440346817 entropy 0.03264415264129639
epoch: 58, step: 58
	action: tensor([[ 1.4657, -0.0702, -0.6653,  0.7808,  0.2974,  0.1016, -0.2359]],
       dtype=torch.float64)
	q_value: tensor([[-9.0165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8914190665767769, distance: 0.37707998285118277 entropy 0.03264415264129639
epoch: 58, step: 59
	action: tensor([[ 1.9327,  0.1107, -1.2395,  0.9243, -0.0382,  0.3629,  0.0903]],
       dtype=torch.float64)
	q_value: tensor([[-9.7123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 60
	action: tensor([[ 1.1449,  0.0893, -0.6344,  0.6529, -0.2796,  0.1329,  0.2037]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9748390868182957, distance: 0.18151808154612628 entropy 0.03264415264129639
epoch: 58, step: 61
	action: tensor([[ 1.1670, -0.1618, -0.6294,  0.5137, -0.3498, -0.0249, -0.8150]],
       dtype=torch.float64)
	q_value: tensor([[-8.2045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8285001018996013, distance: 0.4739020926311919 entropy 0.03264415264129639
epoch: 58, step: 62
	action: tensor([[ 2.4102, -0.6575, -0.1946,  0.6878, -0.2735,  0.3312,  0.0489]],
       dtype=torch.float64)
	q_value: tensor([[-10.3697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 63
	action: tensor([[ 1.5906,  0.0201, -0.9284,  0.3541, -0.0173,  0.2576, -0.2008]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8032486146902785, distance: 0.5075929630823013 entropy 0.03264415264129639
epoch: 58, step: 64
	action: tensor([[ 1.3611, -0.0731, -1.0045,  1.1684, -0.4968,  0.1469, -0.1495]],
       dtype=torch.float64)
	q_value: tensor([[-11.3474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.912050167111432, distance: 0.33937058929929664 entropy 0.03264415264129639
epoch: 58, step: 65
	action: tensor([[ 1.3616,  0.0650, -1.0537,  1.1211,  0.0932,  0.4752, -0.2940]],
       dtype=torch.float64)
	q_value: tensor([[-11.7241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9187414567617065, distance: 0.32620544756523623 entropy 0.03264415264129639
epoch: 58, step: 66
	action: tensor([[ 1.4358,  0.0211, -1.0201,  0.9554, -0.5646,  0.2665,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-11.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9427800559106824, distance: 0.2737350489034583 entropy 0.03264415264129639
epoch: 58, step: 67
	action: tensor([[ 1.7538,  0.0087, -0.8582,  0.8414, -0.2383,  0.1834, -0.3791]],
       dtype=torch.float64)
	q_value: tensor([[-11.9635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 68
	action: tensor([[ 1.5630, -0.0800, -0.4019,  0.3622,  0.1033,  0.0783, -0.1503]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5880609585504002, distance: 0.7344684930303308 entropy 0.03264415264129639
epoch: 58, step: 69
	action: tensor([[ 1.4196,  0.0812, -0.6497,  0.7312, -0.2911,  0.3726, -0.1524]],
       dtype=torch.float64)
	q_value: tensor([[-9.3779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9622082523060885, distance: 0.22246166406340784 entropy 0.03264415264129639
epoch: 58, step: 70
	action: tensor([[ 1.7850, -0.1980, -0.6244,  0.8718, -0.4878,  0.1113,  0.2498]],
       dtype=torch.float64)
	q_value: tensor([[-11.1668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 71
	action: tensor([[ 1.3381,  0.2295, -0.4687,  1.1591,  0.2309,  0.0746, -0.0433]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9615316794000507, distance: 0.22444415849288682 entropy 0.03264415264129639
epoch: 58, step: 72
	action: tensor([[ 1.6805,  0.2656, -1.0546,  0.7332, -0.4674,  0.2130, -0.1478]],
       dtype=torch.float64)
	q_value: tensor([[-9.7057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 73
	action: tensor([[ 1.2148, -0.1734, -0.6899,  1.0882,  0.5063,  0.0407, -0.4698]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9673689106903378, distance: 0.20671523201316125 entropy 0.03264415264129639
epoch: 58, step: 74
	action: tensor([[ 2.1617, -0.5493, -1.0748,  1.2032,  0.0230,  0.3549, -0.1389]],
       dtype=torch.float64)
	q_value: tensor([[-8.8220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 75
	action: tensor([[ 1.1383,  0.1377, -0.5606,  0.5558,  0.2447, -0.4016,  0.0481]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9198676186405752, distance: 0.32393712085853504 entropy 0.03264415264129639
epoch: 58, step: 76
	action: tensor([[ 1.7137, -0.2996, -1.0778,  0.5079,  0.0608,  0.4269, -0.5647]],
       dtype=torch.float64)
	q_value: tensor([[-7.0322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6268438556273751, distance: 0.6990399427772308 entropy 0.03264415264129639
epoch: 58, step: 77
	action: tensor([[ 2.0098, -0.0194, -0.7115,  0.8993, -0.7328,  0.1697, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[-12.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 78
	action: tensor([[ 1.2942, -0.0370, -1.3370,  0.3099,  0.1266,  0.3289, -0.3662]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7852076061540031, distance: 0.5303543586260935 entropy 0.03264415264129639
epoch: 58, step: 79
	action: tensor([[ 1.4588,  0.1242, -1.2831,  1.1588,  0.0859,  0.4399, -0.0802]],
       dtype=torch.float64)
	q_value: tensor([[-10.5656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9428439343169788, distance: 0.2735822119704378 entropy 0.03264415264129639
epoch: 58, step: 80
	action: tensor([[ 1.2694,  0.0471, -0.8467,  0.8533, -0.2449,  0.3417, -0.1834]],
       dtype=torch.float64)
	q_value: tensor([[-12.3454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9855673177400026, distance: 0.13747705447936692 entropy 0.03264415264129639
epoch: 58, step: 81
	action: tensor([[ 1.8121,  0.2776, -0.6577,  0.9577, -0.1465,  0.2481, -0.4689]],
       dtype=torch.float64)
	q_value: tensor([[-10.6835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 82
	action: tensor([[ 1.1803,  0.1372, -0.3392,  0.8373, -0.0745, -0.1674, -0.1425]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.982252716800456, distance: 0.15244831659364086 entropy 0.03264415264129639
epoch: 58, step: 83
	action: tensor([[ 1.3777, -0.3134, -0.8451,  0.6948, -0.4480, -0.0123, -0.1418]],
       dtype=torch.float64)
	q_value: tensor([[-8.4437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7431629230642872, distance: 0.5799433228270738 entropy 0.03264415264129639
epoch: 58, step: 84
	action: tensor([[ 1.6679, -0.2570, -1.0314,  0.8949, -0.5845,  0.1756, -0.0299]],
       dtype=torch.float64)
	q_value: tensor([[-9.8605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 85
	action: tensor([[ 1.0187, -0.1464, -0.6475,  0.9833,  0.3021,  0.0168,  0.1383]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9423582572675703, distance: 0.27474211971990947 entropy 0.03264415264129639
epoch: 58, step: 86
	action: tensor([[ 1.3965, -0.1335, -1.0648,  0.6703, -0.0268,  0.4127, -0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-6.4196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8705604840403557, distance: 0.4117087843873175 entropy 0.03264415264129639
epoch: 58, step: 87
	action: tensor([[ 1.6648, -0.0395, -0.9304,  1.0636, -0.7284,  0.6570, -0.5654]],
       dtype=torch.float64)
	q_value: tensor([[-11.1369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 88
	action: tensor([[ 1.2644, -0.1701, -0.8902,  0.8085, -0.0878,  0.0437, -0.2067]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8888726178847802, distance: 0.38147601308933915 entropy 0.03264415264129639
epoch: 58, step: 89
	action: tensor([[ 1.2577, -0.2140, -0.5078,  0.9459, -0.2411, -0.0287, -0.0398]],
       dtype=torch.float64)
	q_value: tensor([[-9.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9332807420643335, distance: 0.29558493583817524 entropy 0.03264415264129639
epoch: 58, step: 90
	action: tensor([[ 0.6249, -0.0116, -0.6085,  0.6691, -0.2261, -0.3198, -0.2782]],
       dtype=torch.float64)
	q_value: tensor([[-8.5702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8506756308188387, distance: 0.44220335947534944 entropy 0.03264415264129639
epoch: 58, step: 91
	action: tensor([[ 1.2865,  0.0100, -0.4238,  0.5974, -0.3104,  0.0641, -0.0481]],
       dtype=torch.float64)
	q_value: tensor([[-5.9980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9060791282011869, distance: 0.3507016028988771 entropy 0.03264415264129639
epoch: 58, step: 92
	action: tensor([[ 1.1182,  0.2607, -0.1120,  0.6404,  0.2084,  0.2200, -0.5728]],
       dtype=torch.float64)
	q_value: tensor([[-8.9522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.964488421182744, distance: 0.2156461375273724 entropy 0.03264415264129639
epoch: 58, step: 93
	action: tensor([[ 1.6545, -0.7347, -0.6278,  0.7348, -0.2642,  0.4598, -0.3560]],
       dtype=torch.float64)
	q_value: tensor([[-8.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5299386504308469, distance: 0.7845740955452908 entropy 0.03264415264129639
epoch: 58, step: 94
	action: tensor([[ 1.6123,  0.0560, -0.9196,  0.6017,  0.0207, -0.1053, -0.1778]],
       dtype=torch.float64)
	q_value: tensor([[-10.4916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7578777373536296, distance: 0.5630851255220742 entropy 0.03264415264129639
epoch: 58, step: 95
	action: tensor([[ 1.3920, -0.4509, -1.0430,  0.6107, -0.5620,  0.0906, -0.6574]],
       dtype=torch.float64)
	q_value: tensor([[-11.1628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5766987325132311, distance: 0.7445287585774806 entropy 0.03264415264129639
epoch: 58, step: 96
	action: tensor([[ 1.6329, -0.1452, -0.6985,  1.2819, -0.3728,  0.2274, -0.2616]],
       dtype=torch.float64)
	q_value: tensor([[-11.8093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9667276939498078, distance: 0.20873637850352564 entropy 0.03264415264129639
epoch: 58, step: 97
	action: tensor([[ 1.6139, -0.0323, -0.8617,  0.7444, -0.0577,  0.0863, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[-12.6333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.831418256158985, distance: 0.46985295681540695 entropy 0.03264415264129639
epoch: 58, step: 98
	action: tensor([[ 1.9163,  0.0659, -0.9270,  0.7638, -0.0660,  0.6397, -0.1349]],
       dtype=torch.float64)
	q_value: tensor([[-10.9514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 99
	action: tensor([[ 1.2955,  0.1592, -0.2981,  0.6399, -0.4135,  0.0586,  0.2185]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9416031047981887, distance: 0.2765359333939701 entropy 0.03264415264129639
epoch: 58, step: 100
	action: tensor([[ 1.1890,  0.4293, -0.6266,  0.7015,  0.3872, -0.2202,  0.4193]],
       dtype=torch.float64)
	q_value: tensor([[-8.8164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9883834024339798, distance: 0.12333779087750806 entropy 0.03264415264129639
epoch: 58, step: 101
	action: tensor([[ 1.3344, -0.2714, -0.8822,  0.4980,  0.1794, -0.3932,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[-7.6924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.548506101236579, distance: 0.7689226184678305 entropy 0.03264415264129639
epoch: 58, step: 102
	action: tensor([[ 1.3119,  0.0666, -0.9375,  0.8712, -0.1523,  0.4787, -0.3875]],
       dtype=torch.float64)
	q_value: tensor([[-7.7585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9756757901821193, distance: 0.17847445470831078 entropy 0.03264415264129639
epoch: 58, step: 103
	action: tensor([[ 1.7765, -0.1795, -1.0247,  1.1528,  0.2181,  0.0813, -0.2187]],
       dtype=torch.float64)
	q_value: tensor([[-11.8057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 104
	action: tensor([[ 1.3027, -0.2415, -0.4266,  0.8244,  0.1381,  0.2488,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9135800299460959, distance: 0.33640601233030343 entropy 0.03264415264129639
epoch: 58, step: 105
	action: tensor([[ 1.6364, -0.2027, -1.0297,  0.8936, -0.3687,  0.5306, -0.1652]],
       dtype=torch.float64)
	q_value: tensor([[-7.9033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8682772037763632, distance: 0.4153241299572533 entropy 0.03264415264129639
epoch: 58, step: 106
	action: tensor([[ 1.6497,  0.0166, -0.5653,  0.9104, -0.4412,  0.3489, -0.1277]],
       dtype=torch.float64)
	q_value: tensor([[-12.7010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.929980006790164, distance: 0.3028082654089038 entropy 0.03264415264129639
epoch: 58, step: 107
	action: tensor([[ 1.9143,  0.0429, -0.7166,  1.0400, -0.2285,  0.1051, -0.1646]],
       dtype=torch.float64)
	q_value: tensor([[-12.2981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 108
	action: tensor([[ 1.8640,  0.4554, -0.8246,  0.8405, -0.1141,  0.3214,  0.1720]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9572358391595371, distance: 0.23664463905991676 entropy 0.03264415264129639
epoch: 58, step: 109
	action: tensor([[ 1.4026, -0.0477, -0.9911,  0.7702, -0.1469,  0.4912, -0.7041]],
       dtype=torch.float64)
	q_value: tensor([[-12.4664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388595638176628, distance: 0.28295733705031206 entropy 0.03264415264129639
epoch: 58, step: 110
	action: tensor([[ 1.7702,  0.1168, -0.5869,  0.5735, -0.1202,  0.7281,  0.0282]],
       dtype=torch.float64)
	q_value: tensor([[-12.9341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 111
	action: tensor([[ 1.4017, -0.2781, -0.8709,  0.4944, -0.2793,  0.4918,  0.2227]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7527622563876423, distance: 0.5690023747864846 entropy 0.03264415264129639
epoch: 58, step: 112
	action: tensor([[ 1.8062,  0.2341, -0.4595,  0.3530, -0.2293,  0.0800, -0.2078]],
       dtype=torch.float64)
	q_value: tensor([[-9.3324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 113
	action: tensor([[ 1.2579, -0.2962, -0.4232,  0.5252, -0.0737,  0.4511, -0.3876]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8179389859410364, distance: 0.48827576819175766 entropy 0.03264415264129639
epoch: 58, step: 114
	action: tensor([[ 1.5551, -0.5986, -0.8812,  0.9750,  0.1517,  0.2676,  0.1028]],
       dtype=torch.float64)
	q_value: tensor([[-8.8112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7115993367355171, distance: 0.6145466005993064 entropy 0.03264415264129639
epoch: 58, step: 115
	action: tensor([[ 1.7355, -0.2602, -1.2842,  1.0106, -0.2734, -0.0141, -0.3401]],
       dtype=torch.float64)
	q_value: tensor([[-9.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 116
	action: tensor([[ 1.6688e+00, -4.9852e-01, -8.7269e-01, -1.2389e-03,  4.2663e-01,
          3.8018e-01, -3.5368e-01]], dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06386160695455023, distance: 1.1072016434503904 entropy 0.03264415264129639
epoch: 58, step: 117
	action: tensor([[ 2.1599, -0.3947, -1.1426,  1.1309, -0.5791,  0.1778,  0.2493]],
       dtype=torch.float64)
	q_value: tensor([[-9.2057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 118
	action: tensor([[ 1.0123, -0.2535, -1.3357,  0.5350, -0.5132,  0.8014, -0.4192]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5844601831078834, distance: 0.7376715178808523 entropy 0.03264415264129639
epoch: 58, step: 119
	action: tensor([[ 1.4194,  0.6133, -0.7970,  0.4871, -0.0414,  0.0044, -0.5954]],
       dtype=torch.float64)
	q_value: tensor([[-11.5496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9309212871142339, distance: 0.30076604967944975 entropy 0.03264415264129639
epoch: 58, step: 120
	action: tensor([[ 1.6860,  0.1147, -0.6364,  0.7916, -0.3661, -0.3149, -0.1102]],
       dtype=torch.float64)
	q_value: tensor([[-12.8544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 121
	action: tensor([[ 1.2133,  0.3749, -0.7303,  0.9162,  0.4779, -0.1398, -0.4766]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9916385698871011, distance: 0.10463981793723874 entropy 0.03264415264129639
epoch: 58, step: 122
	action: tensor([[ 1.8437,  0.0184, -1.1001,  0.2918, -0.1135,  0.0253, -0.4749]],
       dtype=torch.float64)
	q_value: tensor([[-9.9834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 58, step: 123
	action: tensor([[ 0.9903, -0.3430, -0.1045,  0.8216, -0.1631, -0.2267, -0.5587]],
       dtype=torch.float64)
	q_value: tensor([[-7.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8268484407797194, distance: 0.476178624992176 entropy 0.03264415264129639
epoch: 58, step: 124
	action: tensor([[ 1.4837, -0.1967, -1.3971,  0.6608, -0.0697,  0.9192, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[-7.3930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7844026392816306, distance: 0.5313472207215052 entropy 0.03264415264129639
epoch: 58, step: 125
	action: tensor([[ 1.4763, -0.0980, -0.5464,  0.8406, -0.1274,  0.1220, -0.3328]],
       dtype=torch.float64)
	q_value: tensor([[-12.2012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8961034336427275, distance: 0.36885637237140356 entropy 0.03264415264129639
epoch: 58, step: 126
	action: tensor([[ 1.3551, -0.4105, -0.7626,  0.9148, -0.2528,  0.0396, -0.5078]],
       dtype=torch.float64)
	q_value: tensor([[-10.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8175984317653042, distance: 0.48873222688084667 entropy 0.03264415264129639
epoch: 58, step: 127
	action: tensor([[ 1.7307, -0.3497, -1.0766,  1.0143, -0.3057,  0.1556, -0.1979]],
       dtype=torch.float64)
	q_value: tensor([[-10.4367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 58 actor 372.69832156023364 critic 1871.8341135561875 
epoch: 59, step: 0
	action: tensor([[ 1.3779,  0.0388, -0.6196,  0.9725, -0.1121,  0.2086,  0.2591]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9914486701347835, distance: 0.10582140491333071 entropy 0.03264415264129639
epoch: 59, step: 1
	action: tensor([[ 1.7482,  0.1293, -1.0100,  1.1383, -0.3915,  0.5581, -0.2087]],
       dtype=torch.float64)
	q_value: tensor([[-7.5469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 2
	action: tensor([[ 1.5875, -0.1826, -1.0118,  0.6951, -0.3026, -0.1527,  0.0926]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6650600723591464, distance: 0.6622777342812616 entropy 0.03264415264129639
epoch: 59, step: 3
	action: tensor([[ 1.6040,  0.3265, -1.0767,  0.4914, -0.2233,  0.2150, -0.3140]],
       dtype=torch.float64)
	q_value: tensor([[-8.5874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8726280183760263, distance: 0.4084074408974364 entropy 0.03264415264129639
epoch: 59, step: 4
	action: tensor([[ 1.5526, -0.2165, -1.0830,  1.1841, -0.3942,  0.4790, -0.1533]],
       dtype=torch.float64)
	q_value: tensor([[-11.1134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9069584428879723, distance: 0.34905605680280616 entropy 0.03264415264129639
epoch: 59, step: 5
	action: tensor([[ 2.1160, -0.1270, -0.2956,  0.6106, -0.4937,  0.2767, -0.1831]],
       dtype=torch.float64)
	q_value: tensor([[-10.4399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 6
	action: tensor([[ 1.3964, -0.1491, -0.1683,  0.6284, -0.2776,  0.2672, -0.0384]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8250452103277204, distance: 0.4786517064025975 entropy 0.03264415264129639
epoch: 59, step: 7
	action: tensor([[ 2.2598,  0.7024, -0.7614,  0.7495, -0.1885,  0.1033, -0.3992]],
       dtype=torch.float64)
	q_value: tensor([[-7.2477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 8
	action: tensor([[ 1.6275,  0.1706, -0.3955,  0.9435, -0.1285,  0.1424, -0.3284]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8888256848431293, distance: 0.3815565600225685 entropy 0.03264415264129639
epoch: 59, step: 9
	action: tensor([[ 1.9019,  0.2672, -0.7995,  1.2363,  0.0903,  0.3005,  0.3435]],
       dtype=torch.float64)
	q_value: tensor([[-9.8876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 10
	action: tensor([[ 1.1921, -0.2049, -1.2120,  0.6897, -0.0669, -0.2946,  0.3920]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7209057841083529, distance: 0.6045498409922098 entropy 0.03264415264129639
epoch: 59, step: 11
	action: tensor([[ 1.3771, -0.2157, -0.6426,  0.6083, -0.4711,  0.1262, -0.1787]],
       dtype=torch.float64)
	q_value: tensor([[-6.3549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7953302916734108, distance: 0.5177063341248236 entropy 0.03264415264129639
epoch: 59, step: 12
	action: tensor([[ 1.5669,  0.1991, -0.7640,  1.0499, -0.0970,  0.4347, -0.2297]],
       dtype=torch.float64)
	q_value: tensor([[-8.1023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9859607913684486, distance: 0.13559010818769296 entropy 0.03264415264129639
epoch: 59, step: 13
	action: tensor([[ 1.8542, -0.1899, -1.1229,  1.2559, -0.0970,  0.1555, -0.1495]],
       dtype=torch.float64)
	q_value: tensor([[-10.5154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 14
	action: tensor([[ 1.1226, -0.0410, -0.8822,  0.8266, -0.3241, -0.4383, -0.3065]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9233558496270717, distance: 0.31680804420381914 entropy 0.03264415264129639
epoch: 59, step: 15
	action: tensor([[ 1.6666,  0.0171, -0.5717,  0.9695, -0.1061,  0.1294, -0.0427]],
       dtype=torch.float64)
	q_value: tensor([[-7.5533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9109024443896848, distance: 0.34157776137910006 entropy 0.03264415264129639
epoch: 59, step: 16
	action: tensor([[ 1.6505, -0.1884, -0.7713,  1.0834,  0.0111,  0.2936, -0.6178]],
       dtype=torch.float64)
	q_value: tensor([[-9.0422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9360300059894676, distance: 0.28943086882694474 entropy 0.03264415264129639
epoch: 59, step: 17
	action: tensor([[ 1.8529, -0.3323, -0.9187,  1.4812, -0.6231,  0.4820, -0.2805]],
       dtype=torch.float64)
	q_value: tensor([[-10.3281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 18
	action: tensor([[ 1.1270,  0.0628, -0.4871,  0.9927, -0.1181,  0.2786,  0.1902]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07451371351931241 entropy 0.03264415264129639
epoch: 59, step: 19
	action: tensor([[ 1.3796, -0.2228, -0.8032,  0.3660, -0.0035,  0.2330, -0.5745]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6898743049778293, distance: 0.6372730732072283 entropy 0.03264415264129639
epoch: 59, step: 20
	action: tensor([[ 1.8691, -0.0483, -0.7491,  1.1645,  0.2648,  0.2559, -0.4269]],
       dtype=torch.float64)
	q_value: tensor([[-8.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 21
	action: tensor([[ 0.9858,  0.1585, -0.6155,  0.8927, -0.1858,  0.0629, -0.0462]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9740253011265356, distance: 0.18443016466019058 entropy 0.03264415264129639
epoch: 59, step: 22
	action: tensor([[ 1.7841, -0.2424, -0.7647,  0.7004, -0.0885,  0.3394, -0.3574]],
       dtype=torch.float64)
	q_value: tensor([[-6.6735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 23
	action: tensor([[ 1.0556,  0.0470, -0.4196,  1.0444,  0.0231,  0.4528, -0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9738951999894093, distance: 0.18489147132405082 entropy 0.03264415264129639
epoch: 59, step: 24
	action: tensor([[ 1.8336, -0.3429, -0.6305,  0.5877, -0.4911,  0.2924, -0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-7.0025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 25
	action: tensor([[ 1.3691, -0.4388, -0.1880,  0.5437, -0.3316,  0.4495, -0.2923]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6860259498422148, distance: 0.6412148488352973 entropy 0.03264415264129639
epoch: 59, step: 26
	action: tensor([[ 1.5068,  0.0627, -0.6567,  1.1330, -0.3977,  0.8507, -0.2828]],
       dtype=torch.float64)
	q_value: tensor([[-7.3557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9709993729269853, distance: 0.19487691126770496 entropy 0.03264415264129639
epoch: 59, step: 27
	action: tensor([[ 1.7529, -0.6009, -0.7535,  1.1487, -0.3631,  0.2199, -0.1231]],
       dtype=torch.float64)
	q_value: tensor([[-11.2052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 28
	action: tensor([[ 1.2196,  0.0558, -0.7652,  0.8454, -0.4275,  0.4150,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9851583894138648, distance: 0.13941105362311307 entropy 0.03264415264129639
epoch: 59, step: 29
	action: tensor([[ 1.9328, -0.1305, -0.5730,  0.8788,  0.3227,  0.6207, -0.3431]],
       dtype=torch.float64)
	q_value: tensor([[-8.2070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 30
	action: tensor([[ 1.7318,  0.0107, -0.7033,  0.6977, -0.1902, -0.0683,  0.2226]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7381146107532492, distance: 0.5856151830936334 entropy 0.03264415264129639
epoch: 59, step: 31
	action: tensor([[ 1.9916, -0.1869, -0.3480,  0.4374, -0.2455, -0.2736, -0.2270]],
       dtype=torch.float64)
	q_value: tensor([[-8.2967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 32
	action: tensor([[ 1.3292, -0.4743, -0.5575,  0.9289, -0.6096, -0.1592, -0.2673]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7628517691354634, distance: 0.5572712496844713 entropy 0.03264415264129639
epoch: 59, step: 33
	action: tensor([[ 1.6822, -0.1464, -0.7432,  0.8638, -0.1091, -0.1647, -0.1594]],
       dtype=torch.float64)
	q_value: tensor([[-7.8724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 34
	action: tensor([[ 1.5334, -0.0096, -0.7763,  0.7069, -0.0967,  0.4735, -0.1571]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9225294519906734, distance: 0.31851141929600996 entropy 0.03264415264129639
epoch: 59, step: 35
	action: tensor([[ 1.7821,  0.1433, -0.8966,  1.2134, -0.3955,  0.4221, -0.4686]],
       dtype=torch.float64)
	q_value: tensor([[-9.3654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 36
	action: tensor([[ 1.4549,  0.2768, -0.4602,  0.7662, -0.0645,  0.0331, -0.2147]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9337194565233451, distance: 0.2946115192612306 entropy 0.03264415264129639
epoch: 59, step: 37
	action: tensor([[ 1.6386, -0.0350, -1.1109,  0.8166, -0.0908,  0.0746, -0.5867]],
       dtype=torch.float64)
	q_value: tensor([[-8.8485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8514513904610106, distance: 0.44105321157289445 entropy 0.03264415264129639
epoch: 59, step: 38
	action: tensor([[ 1.6240, -0.2632, -0.8475,  1.2654, -0.5090, -0.0497,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[-10.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9210400793374237, distance: 0.3215585376199079 entropy 0.03264415264129639
epoch: 59, step: 39
	action: tensor([[ 1.9765, -0.3500, -0.5922,  0.5432,  0.1555,  0.6971,  0.0332]],
       dtype=torch.float64)
	q_value: tensor([[-9.5051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 40
	action: tensor([[ 1.4107,  0.0472, -0.7510,  0.9612, -0.3327,  0.3497, -0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0970800509255109 entropy 0.03264415264129639
epoch: 59, step: 41
	action: tensor([[ 1.6112, -0.0110, -0.3752,  0.7087,  0.1100, -0.2221,  0.2663]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7142365704995728, distance: 0.61173033601637 entropy 0.03264415264129639
epoch: 59, step: 42
	action: tensor([[ 0.8985, -0.0767, -0.7080,  1.0924, -0.4231,  0.5522, -0.5105]],
       dtype=torch.float64)
	q_value: tensor([[-7.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8651492451662909, distance: 0.42022644976419143 entropy 0.03264415264129639
epoch: 59, step: 43
	action: tensor([[ 1.7575, -0.0271, -0.5323,  0.7412, -0.5222,  0.2081, -0.3291]],
       dtype=torch.float64)
	q_value: tensor([[-8.4198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8314753631811324, distance: 0.46977336879089887 entropy 0.03264415264129639
epoch: 59, step: 44
	action: tensor([[ 1.7740,  0.2028, -0.4095,  0.9740, -0.2321,  0.4703, -0.2202]],
       dtype=torch.float64)
	q_value: tensor([[-10.1599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 45
	action: tensor([[ 1.7396, -0.3183, -0.7124,  0.9044, -0.4825,  0.5660,  0.2059]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8343779337700027, distance: 0.4657102455820517 entropy 0.03264415264129639
epoch: 59, step: 46
	action: tensor([[ 1.6019,  0.0896, -0.3539,  1.0465, -0.5480,  0.5542, -0.1667]],
       dtype=torch.float64)
	q_value: tensor([[-9.0078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.914396464762723, distance: 0.33481317825257395 entropy 0.03264415264129639
epoch: 59, step: 47
	action: tensor([[ 1.5808, -0.1442, -0.9885,  1.4080,  0.0592,  0.4967,  0.2277]],
       dtype=torch.float64)
	q_value: tensor([[-10.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9606621208560365, distance: 0.22696671091994755 entropy 0.03264415264129639
epoch: 59, step: 48
	action: tensor([[ 2.0642,  0.3003, -1.1000,  1.1186, -0.1564,  0.1113, -0.1821]],
       dtype=torch.float64)
	q_value: tensor([[-9.1681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 49
	action: tensor([[ 1.2311, -0.0590, -0.7395,  0.3968, -0.1067,  0.1646, -0.3272]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8399964199614014, distance: 0.45774282256710364 entropy 0.03264415264129639
epoch: 59, step: 50
	action: tensor([[ 1.4733, -0.0624, -0.7880,  1.0068,  0.1180, -0.0788, -0.6381]],
       dtype=torch.float64)
	q_value: tensor([[-7.4731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.941305105152129, distance: 0.277240617656462 entropy 0.03264415264129639
epoch: 59, step: 51
	action: tensor([[ 2.0700, -0.3571, -0.8510,  1.5096, -0.3411,  0.0894,  0.2873]],
       dtype=torch.float64)
	q_value: tensor([[-9.4008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 52
	action: tensor([[ 1.6069,  0.2193, -0.6395,  0.6344, -0.0138, -0.0266, -0.1947]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8297537466078181, distance: 0.4721668305308978 entropy 0.03264415264129639
epoch: 59, step: 53
	action: tensor([[ 1.8467, -0.2323, -0.9491,  1.1852, -0.3323,  0.7122, -0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-9.3288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 54
	action: tensor([[ 1.5855,  0.3815, -1.1105,  0.8838, -0.4267,  0.4507,  0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09361591781732717 entropy 0.03264415264129639
epoch: 59, step: 55
	action: tensor([[ 1.8004, -0.2868, -0.6373,  0.9992, -0.1217,  0.2251, -0.4261]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8250847330774596, distance: 0.4785976390056823 entropy 0.03264415264129639
epoch: 59, step: 56
	action: tensor([[ 2.0977,  0.2286, -1.0090,  1.1188, -0.0779,  0.1974,  0.2976]],
       dtype=torch.float64)
	q_value: tensor([[-9.4790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 57
	action: tensor([[ 1.8051,  0.0506, -1.0556,  1.1035, -0.1895,  0.2892, -0.2113]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9620742395131189, distance: 0.22285574912028006 entropy 0.03264415264129639
epoch: 59, step: 58
	action: tensor([[ 2.3014,  0.0818, -1.0451,  1.0829,  0.7556, -0.0989, -0.2609]],
       dtype=torch.float64)
	q_value: tensor([[-10.6941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 59
	action: tensor([[ 1.3348,  0.2204, -0.7031,  0.5154, -0.4004,  0.4008,  0.2261]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9731122841251018, distance: 0.1876435533235687 entropy 0.03264415264129639
epoch: 59, step: 60
	action: tensor([[ 1.3124,  0.5245, -1.0229,  1.0464,  0.0562,  0.2267, -0.3811]],
       dtype=torch.float64)
	q_value: tensor([[-8.3481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9453785893114564, distance: 0.2674472578629389 entropy 0.03264415264129639
epoch: 59, step: 61
	action: tensor([[ 2.2841, -0.0365, -0.8095,  0.9291, -0.1044,  0.3568, -0.3159]],
       dtype=torch.float64)
	q_value: tensor([[-10.4108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 62
	action: tensor([[ 1.3199,  0.0180, -0.4234,  0.7797,  0.1965, -0.1639, -0.2911]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9223563124421914, distance: 0.31886714251630055 entropy 0.03264415264129639
epoch: 59, step: 63
	action: tensor([[ 1.8697,  0.0653, -0.6973,  0.6464, -0.0862,  0.0393, -0.1743]],
       dtype=torch.float64)
	q_value: tensor([[-7.0846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 64
	action: tensor([[ 1.3930, -0.1392, -0.8763,  0.9282, -0.1032,  0.1497,  0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9259364966709266, distance: 0.31142883331838395 entropy 0.03264415264129639
epoch: 59, step: 65
	action: tensor([[ 2.2622,  0.4000, -0.4992,  0.7056, -0.3386,  0.3186, -0.5605]],
       dtype=torch.float64)
	q_value: tensor([[-7.8909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 66
	action: tensor([[ 1.2992,  0.2292, -0.7279,  0.9091, -0.4682,  0.0475,  0.2713]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.024311493859183148 entropy 0.03264415264129639
epoch: 59, step: 67
	action: tensor([[ 1.6065,  0.1029, -1.1200,  0.5799, -0.1595,  0.4911, -0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8769530787562182, distance: 0.4014135877105491 entropy 0.03264415264129639
epoch: 59, step: 68
	action: tensor([[ 1.6569,  0.0453, -0.7976,  1.0234, -0.2944, -0.1124, -0.1156]],
       dtype=torch.float64)
	q_value: tensor([[-10.6515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 69
	action: tensor([[ 1.1259,  0.0860, -0.4118,  1.1446, -0.5252,  0.2404, -0.1272]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9787305785989909, distance: 0.166891620323296 entropy 0.03264415264129639
epoch: 59, step: 70
	action: tensor([[ 1.4362,  0.0657, -0.3063,  0.8450,  0.0126,  0.6523, -0.1687]],
       dtype=torch.float64)
	q_value: tensor([[-8.0992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9419420238561953, distance: 0.2757322975187073 entropy 0.03264415264129639
epoch: 59, step: 71
	action: tensor([[ 1.5044, -0.0290, -1.3133,  1.1492, -0.2665,  0.1087, -0.2487]],
       dtype=torch.float64)
	q_value: tensor([[-8.6628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8986772426349036, distance: 0.36425891805145616 entropy 0.03264415264129639
epoch: 59, step: 72
	action: tensor([[ 1.9649,  0.2263, -0.6431,  0.5294, -0.3566,  0.0327,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-10.5331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 73
	action: tensor([[ 1.1428, -0.1457, -0.3805,  0.6878,  0.3514,  0.7709, -0.2669]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9817662168465736, distance: 0.1545236955510607 entropy 0.03264415264129639
epoch: 59, step: 74
	action: tensor([[ 1.7182,  0.1443, -1.0781,  0.4051,  0.0982, -0.0037,  0.2375]],
       dtype=torch.float64)
	q_value: tensor([[-6.7255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7208901027258153, distance: 0.6045668245834463 entropy 0.03264415264129639
epoch: 59, step: 75
	action: tensor([[ 1.4791,  0.0036, -0.6812,  0.7903, -0.5709,  0.0044,  0.1807]],
       dtype=torch.float64)
	q_value: tensor([[-8.6857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.90670112847845, distance: 0.3495383957833313 entropy 0.03264415264129639
epoch: 59, step: 76
	action: tensor([[ 1.7006, -0.0333, -0.9041,  0.4487,  0.3032,  0.6285, -0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-8.6031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 77
	action: tensor([[ 1.5566, -0.0373, -1.0529,  0.8057, -0.1675,  0.0858,  0.2093]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8583908169716619, distance: 0.4306281359379104 entropy 0.03264415264129639
epoch: 59, step: 78
	action: tensor([[ 1.5420,  0.1476, -0.0328,  0.4527, -0.2493,  0.1708, -0.7458]],
       dtype=torch.float64)
	q_value: tensor([[-8.7457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7186951076201444, distance: 0.6069394069068766 entropy 0.03264415264129639
epoch: 59, step: 79
	action: tensor([[ 2.0772, -0.1379, -1.1573,  0.7865,  0.0698,  0.4956, -0.2103]],
       dtype=torch.float64)
	q_value: tensor([[-9.7939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 80
	action: tensor([[ 1.4421,  0.3912, -0.9094,  1.2257, -0.0890, -0.1086, -0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9824524457685151, distance: 0.15158805823900498 entropy 0.03264415264129639
epoch: 59, step: 81
	action: tensor([[ 1.4813, -0.0896, -1.1238,  1.1745,  0.0752,  0.3078, -0.3385]],
       dtype=torch.float64)
	q_value: tensor([[-9.9141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9471505532881841, distance: 0.26307338669722985 entropy 0.03264415264129639
epoch: 59, step: 82
	action: tensor([[ 2.1539, -0.0651, -0.6530,  1.5779, -0.1618,  0.1997, -0.2668]],
       dtype=torch.float64)
	q_value: tensor([[-9.8818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 83
	action: tensor([[ 1.3156, -0.1475, -0.7950,  0.8940, -0.3178,  0.4084,  0.2524]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9500852074331678, distance: 0.2556650297242179 entropy 0.03264415264129639
epoch: 59, step: 84
	action: tensor([[ 1.6586, -0.2391, -0.7314,  0.5007, -0.3915,  0.4047, -0.0937]],
       dtype=torch.float64)
	q_value: tensor([[-7.5742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6800212070533904, distance: 0.6473174150226143 entropy 0.03264415264129639
epoch: 59, step: 85
	action: tensor([[ 1.2255, -0.4066, -0.4068,  1.1102,  0.1233,  0.6590, -0.0740]],
       dtype=torch.float64)
	q_value: tensor([[-9.1365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9701710411237856, distance: 0.19764040764063237 entropy 0.03264415264129639
epoch: 59, step: 86
	action: tensor([[ 1.8699, -0.1211, -0.9377,  1.5639, -0.1920,  0.1645, -0.2733]],
       dtype=torch.float64)
	q_value: tensor([[-6.7947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 87
	action: tensor([[ 1.2180, -0.1591, -0.2585,  0.8065, -0.2998,  0.5820,  0.2161]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9781756018267401, distance: 0.16905492660705954 entropy 0.03264415264129639
epoch: 59, step: 88
	action: tensor([[ 1.5649,  0.0673, -0.5376,  1.0718, -0.4873,  0.6208,  0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-6.7172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9726347002772688, distance: 0.18930269545253287 entropy 0.03264415264129639
epoch: 59, step: 89
	action: tensor([[ 1.6312,  0.2685, -1.0812,  1.3571, -0.2648, -0.3328, -0.2733]],
       dtype=torch.float64)
	q_value: tensor([[-10.1557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08765562526796439 entropy 0.03264415264129639
epoch: 59, step: 90
	action: tensor([[ 1.6440, -0.0348, -0.5415,  0.8671,  0.2418,  0.6069, -0.1100]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9097874961962253, distance: 0.3437083330852848 entropy 0.03264415264129639
epoch: 59, step: 91
	action: tensor([[ 2.0512, -0.3222, -0.8052,  1.4039, -0.3124,  0.4473, -0.0946]],
       dtype=torch.float64)
	q_value: tensor([[-8.9124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 92
	action: tensor([[ 1.5577, -0.0650, -0.8100,  0.8579, -0.2271,  0.1713, -0.1220]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8957480001600455, distance: 0.3694867684150658 entropy 0.03264415264129639
epoch: 59, step: 93
	action: tensor([[ 1.4917, -0.3481, -0.9051,  1.1788,  0.1076,  0.4247,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-9.2743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9334154738950667, distance: 0.2952863352058701 entropy 0.03264415264129639
epoch: 59, step: 94
	action: tensor([[ 1.7637,  0.1004, -0.5901,  1.0274,  0.1398,  0.2410,  0.2246]],
       dtype=torch.float64)
	q_value: tensor([[-8.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 95
	action: tensor([[ 1.1083, -0.2817, -0.9027,  0.9544, -0.1690,  0.1854, -0.0518]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8659970200206971, distance: 0.41890343488273474 entropy 0.03264415264129639
epoch: 59, step: 96
	action: tensor([[ 1.5304,  0.0198, -0.8293,  1.0448, -0.1486,  0.1746, -0.1988]],
       dtype=torch.float64)
	q_value: tensor([[-6.7012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9765922710498225, distance: 0.17507991746884427 entropy 0.03264415264129639
epoch: 59, step: 97
	action: tensor([[ 1.9014, -0.1324, -1.0850,  1.0365, -0.1465,  0.3348, -0.1904]],
       dtype=torch.float64)
	q_value: tensor([[-9.6423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 98
	action: tensor([[ 0.8485, -0.3026, -0.2964,  0.5790, -0.0351,  0.1277,  0.0532]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8145624282761131, distance: 0.4927828206541482 entropy 0.03264415264129639
epoch: 59, step: 99
	action: tensor([[ 1.6385, -0.4594, -0.4469,  1.1303, -0.4297,  0.0635, -0.4044]],
       dtype=torch.float64)
	q_value: tensor([[-3.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7773106448339804, distance: 0.5400157444798869 entropy 0.03264415264129639
epoch: 59, step: 100
	action: tensor([[ 1.9097, -0.3524, -0.9636,  1.1875, -0.4544,  0.6947, -0.5803]],
       dtype=torch.float64)
	q_value: tensor([[-9.4687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 101
	action: tensor([[ 1.7427,  0.1612, -0.6155,  0.7415, -0.2028,  0.2569, -0.1389]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8748317071297227, distance: 0.40485905568738545 entropy 0.03264415264129639
epoch: 59, step: 102
	action: tensor([[ 1.8582,  0.1185, -0.6169,  0.5540,  0.3098,  0.3111, -0.0061]],
       dtype=torch.float64)
	q_value: tensor([[-9.7993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 103
	action: tensor([[ 1.5328, -0.1781, -0.5960,  0.6796, -0.2719,  0.7975, -0.3224]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8984180026296472, distance: 0.36472460888931896 entropy 0.03264415264129639
epoch: 59, step: 104
	action: tensor([[ 1.7574, -0.3310, -0.9343,  1.2788, -0.3146,  0.3261, -0.3388]],
       dtype=torch.float64)
	q_value: tensor([[-9.8880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 105
	action: tensor([[ 1.1420, -0.2211, -1.1588,  0.1597, -0.2884,  0.2199, -0.0516]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6187117552781415, distance: 0.7066158930556707 entropy 0.03264415264129639
epoch: 59, step: 106
	action: tensor([[ 1.3707, -0.1490, -0.8555,  1.0266,  0.2997,  0.3120, -0.5059]],
       dtype=torch.float64)
	q_value: tensor([[-7.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.964200505619453, distance: 0.21651856480411955 entropy 0.03264415264129639
epoch: 59, step: 107
	action: tensor([[ 2.1607,  0.0408, -0.9598,  1.1933, -0.1792, -0.1714, -0.3167]],
       dtype=torch.float64)
	q_value: tensor([[-8.7054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 108
	action: tensor([[ 1.0335, -0.0431, -1.0552,  0.7929, -0.4192,  0.3580, -0.1633]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8634803244089478, distance: 0.422818827024269 entropy 0.03264415264129639
epoch: 59, step: 109
	action: tensor([[ 1.8503, -0.1523, -0.6780,  0.6666, -0.1220,  0.3077,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-8.0446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 110
	action: tensor([[ 1.4810, -0.1707, -0.4820,  0.6959, -0.1135,  0.2175,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8208107617464939, distance: 0.4844095035392249 entropy 0.03264415264129639
epoch: 59, step: 111
	action: tensor([[ 1.2786, -0.2586, -0.6328,  0.6391, -0.0586,  0.2269, -0.3328]],
       dtype=torch.float64)
	q_value: tensor([[-7.5859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8389154925242571, distance: 0.4592863940789907 entropy 0.03264415264129639
epoch: 59, step: 112
	action: tensor([[ 1.3223, -0.3501, -0.5354,  1.0663, -0.6884,  0.6370, -0.1400]],
       dtype=torch.float64)
	q_value: tensor([[-7.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9505372591340007, distance: 0.2545046855671305 entropy 0.03264415264129639
epoch: 59, step: 113
	action: tensor([[ 1.6611, -0.3906, -0.6780,  0.9340, -0.1532, -0.0262, -0.4249]],
       dtype=torch.float64)
	q_value: tensor([[-8.9342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7236377053347586, distance: 0.60158373852309 entropy 0.03264415264129639
epoch: 59, step: 114
	action: tensor([[ 2.1615, -0.1588, -1.2798,  0.8788,  0.0849,  0.4832,  0.2637]],
       dtype=torch.float64)
	q_value: tensor([[-9.0490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 115
	action: tensor([[ 1.7095,  0.1215, -0.5479,  0.6366, -0.0504, -0.0345, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.767629796861454, distance: 0.5516287744815743 entropy 0.03264415264129639
epoch: 59, step: 116
	action: tensor([[ 1.3782,  0.0860, -0.3010,  0.5503, -0.1086,  0.2285, -0.2680]],
       dtype=torch.float64)
	q_value: tensor([[-8.5817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8659653678563961, distance: 0.41895290548954345 entropy 0.03264415264129639
epoch: 59, step: 117
	action: tensor([[ 1.4362, -0.3189, -0.7261,  1.1147, -0.1262,  0.5731, -0.4497]],
       dtype=torch.float64)
	q_value: tensor([[-7.9905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9552262984589441, distance: 0.24214092283905414 entropy 0.03264415264129639
epoch: 59, step: 118
	action: tensor([[ 1.8932,  0.1439, -1.2944,  1.2939, -0.1615, -0.0083, -0.5948]],
       dtype=torch.float64)
	q_value: tensor([[-9.4483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 119
	action: tensor([[ 1.0640,  0.1897, -0.6662,  0.6636, -0.2369,  0.1505, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9891545579174594, distance: 0.11917367251737751 entropy 0.03264415264129639
epoch: 59, step: 120
	action: tensor([[ 1.4912,  0.1383, -0.4833,  0.7245, -0.6957, -0.1816, -0.3850]],
       dtype=torch.float64)
	q_value: tensor([[-7.0166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.857641039900569, distance: 0.4317666526717915 entropy 0.03264415264129639
epoch: 59, step: 121
	action: tensor([[ 1.4808,  0.0434, -0.8770,  1.2714,  0.1083,  0.4558, -0.4273]],
       dtype=torch.float64)
	q_value: tensor([[-9.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9767911566220733, distance: 0.1743345408007093 entropy 0.03264415264129639
epoch: 59, step: 122
	action: tensor([[ 2.1399, -0.3597, -0.9966,  1.3118, -0.4074,  0.2179, -0.3637]],
       dtype=torch.float64)
	q_value: tensor([[-10.3376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 123
	action: tensor([[ 1.2135, -0.0369, -0.5677,  0.6277,  0.2973,  0.4702,  0.3553]],
       dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9741020230458474, distance: 0.18415758594051618 entropy 0.03264415264129639
epoch: 59, step: 124
	action: tensor([[ 1.2812, -0.3046, -0.6014,  0.5858, -0.0128,  0.3721, -0.3568]],
       dtype=torch.float64)
	q_value: tensor([[-5.8191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8203855205983093, distance: 0.4849839487151666 entropy 0.03264415264129639
epoch: 59, step: 125
	action: tensor([[ 2.0170, -0.0586, -0.8566,  1.1553,  0.1979,  0.4855, -0.1683]],
       dtype=torch.float64)
	q_value: tensor([[-7.2415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 59, step: 126
	action: tensor([[ 1.1719e+00, -6.7269e-04, -6.9313e-01,  8.0360e-01, -2.2763e-01,
          4.2839e-01, -4.6224e-01]], dtype=torch.float64)
	q_value: tensor([[-6.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9855843923487614, distance: 0.13739570918431657 entropy 0.03264415264129639
epoch: 59, step: 127
	action: tensor([[ 1.2032,  0.0383, -0.8649,  0.6571, -0.6686,  0.2957, -0.1248]],
       dtype=torch.float64)
	q_value: tensor([[-8.6671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9482546499785033, distance: 0.2603109024343064 entropy 0.03264415264129639
LOSS epoch 59 actor 517.0725659005085 critic 1995.4173410499282 
epoch: 60, step: 0
	action: tensor([[ 1.7361, -0.0584, -0.5817,  0.8830, -0.2488,  0.2686,  0.2528]],
       dtype=torch.float64)
	q_value: tensor([[-8.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 1
	action: tensor([[ 1.7296, -0.2191, -0.7375,  0.9610,  0.1670,  0.1431,  0.1249]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8271095040875899, distance: 0.47581951861170435 entropy 0.03264415264129639
epoch: 60, step: 2
	action: tensor([[ 1.5992, -0.2892, -1.3352,  1.4176, -0.1735,  0.4252, -0.5240]],
       dtype=torch.float64)
	q_value: tensor([[-7.9062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8375438573918865, distance: 0.4612376617297953 entropy 0.03264415264129639
epoch: 60, step: 3
	action: tensor([[ 2.3701, -0.0781, -0.9382,  1.3572, -0.1272,  0.0251, -0.4716]],
       dtype=torch.float64)
	q_value: tensor([[-11.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 4
	action: tensor([[ 1.6815,  0.0279, -0.9572,  1.4479, -0.4046,  0.5530,  0.0713]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9414614412509728, distance: 0.27687115097230963 entropy 0.03264415264129639
epoch: 60, step: 5
	action: tensor([[ 2.1836, -0.2218, -0.9810,  1.0873,  0.0355,  0.2042, -0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-10.9633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 6
	action: tensor([[ 1.7943, -0.3067, -0.6863,  0.7839,  0.1045,  0.0518, -0.0508]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6543601119406276, distance: 0.6727731042845151 entropy 0.03264415264129639
epoch: 60, step: 7
	action: tensor([[ 2.0804,  0.2557, -0.7728,  1.0613, -0.3347,  0.2291, -0.6525]],
       dtype=torch.float64)
	q_value: tensor([[-7.8294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 8
	action: tensor([[ 1.4819, -0.0308, -0.1760,  1.1976, -0.3665,  0.1417,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9079309545825498, distance: 0.34722702027413505 entropy 0.03264415264129639
epoch: 60, step: 9
	action: tensor([[ 1.8335, -0.1438, -1.0483,  1.0931, -0.4807,  0.4072,  0.2745]],
       dtype=torch.float64)
	q_value: tensor([[-8.5151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 10
	action: tensor([[ 1.8486, -0.1365, -0.5750,  0.8043, -0.8580,  0.6896, -0.5697]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8879153225240451, distance: 0.38311558255169254 entropy 0.03264415264129639
epoch: 60, step: 11
	action: tensor([[ 2.0290, -0.2921, -1.2521,  1.2168, -0.4924,  0.1698,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[-12.1224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 12
	action: tensor([[ 1.3313,  0.0756, -0.8135,  1.0339, -0.3426,  0.0728, -0.2548]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07632960830815305 entropy 0.03264415264129639
epoch: 60, step: 13
	action: tensor([[ 1.7180,  0.0733, -1.3107,  1.0499, -0.2828, -0.2649, -0.2084]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8726274267359534, distance: 0.40840838941815666 entropy 0.03264415264129639
epoch: 60, step: 14
	action: tensor([[ 1.6951, -0.1850, -1.2239,  1.0970, -0.0347,  0.5433, -0.1634]],
       dtype=torch.float64)
	q_value: tensor([[-10.6512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 15
	action: tensor([[ 1.7710,  0.0412, -0.4039,  0.6872,  0.1240,  0.1906,  0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7677759914213746, distance: 0.5514552199268221 entropy 0.03264415264129639
epoch: 60, step: 16
	action: tensor([[ 1.8173,  0.3324, -0.6942,  0.7262, -0.3782,  0.0316,  0.2517]],
       dtype=torch.float64)
	q_value: tensor([[-7.7935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 17
	action: tensor([[ 1.4826,  0.4954, -0.9273,  1.4261,  0.0991,  0.3252, -0.0860]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8683770631950446, distance: 0.41516667091923665 entropy 0.03264415264129639
epoch: 60, step: 18
	action: tensor([[ 2.2015, -0.2654, -1.0381,  1.3573, -0.2239,  0.3351, -0.3358]],
       dtype=torch.float64)
	q_value: tensor([[-10.7628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 19
	action: tensor([[ 1.5138,  0.3423, -1.0512,  1.0494, -0.3751,  0.7188, -0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9639041040824036, distance: 0.2174130487426166 entropy 0.03264415264129639
epoch: 60, step: 20
	action: tensor([[ 1.8379,  0.1386, -0.5623,  1.4500, -0.0647,  0.5437, -0.0944]],
       dtype=torch.float64)
	q_value: tensor([[-11.7056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 21
	action: tensor([[ 1.2626, -0.7428, -0.2845,  0.9491,  0.1388,  0.4940,  0.2999]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7760437038680146, distance: 0.5415497147985512 entropy 0.03264415264129639
epoch: 60, step: 22
	action: tensor([[ 1.7921,  0.0535, -1.0275,  1.0407, -0.4176,  0.2316,  0.1464]],
       dtype=torch.float64)
	q_value: tensor([[-5.1570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 23
	action: tensor([[ 1.5168, -0.4031, -1.0332,  0.8614,  0.0829, -0.0383, -0.6054]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6938667599031845, distance: 0.633157764860339 entropy 0.03264415264129639
epoch: 60, step: 24
	action: tensor([[ 1.8395,  0.0616, -1.0420,  1.6127, -0.2094,  0.0703, -0.3010]],
       dtype=torch.float64)
	q_value: tensor([[-8.9865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 25
	action: tensor([[ 1.6076, -0.1997, -0.5692,  1.1073, -0.0370,  0.2810, -0.3282]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9165171902906468, distance: 0.3306398703114936 entropy 0.03264415264129639
epoch: 60, step: 26
	action: tensor([[ 1.9134,  0.0140, -0.8272,  1.1008, -0.3311,  0.4257, -0.3967]],
       dtype=torch.float64)
	q_value: tensor([[-9.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 27
	action: tensor([[ 1.2630, -0.2469, -0.5244,  1.2847, -0.1033,  0.3393, -0.1274]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9869327310015448, distance: 0.13081245882632775 entropy 0.03264415264129639
epoch: 60, step: 28
	action: tensor([[ 2.3665,  0.0443, -0.6819,  1.1377, -0.4877,  0.5056, -0.9709]],
       dtype=torch.float64)
	q_value: tensor([[-7.6209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 29
	action: tensor([[ 1.5623,  0.2518, -1.1422,  0.3450,  0.1239,  0.6902, -0.1819]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081268859819871, distance: 0.3468573579777086 entropy 0.03264415264129639
epoch: 60, step: 30
	action: tensor([[ 1.9955,  0.0505, -0.7565,  1.2784, -0.3349,  0.5055, -0.4345]],
       dtype=torch.float64)
	q_value: tensor([[-10.3861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 31
	action: tensor([[ 1.6132, -0.0847, -1.0331,  0.4964, -0.0023,  0.0983,  0.0612]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.655788308201166, distance: 0.6713817043194191 entropy 0.03264415264129639
epoch: 60, step: 32
	action: tensor([[ 1.9162, -0.2534, -1.0957,  1.4509,  0.1525,  0.2673, -0.2595]],
       dtype=torch.float64)
	q_value: tensor([[-8.6100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 33
	action: tensor([[ 2.0025, -0.3915, -0.6896,  0.7008,  0.1863, -0.4087, -0.5246]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3844390746019619, distance: 0.8978260770970581 entropy 0.03264415264129639
epoch: 60, step: 34
	action: tensor([[ 2.3841, -0.3534, -0.6203,  1.3015, -0.6304,  0.6188,  0.0516]],
       dtype=torch.float64)
	q_value: tensor([[-8.2323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 35
	action: tensor([[ 1.3365, -0.0424, -0.9194,  1.2600, -0.0472,  0.3589, -0.4284]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9623962863312517, distance: 0.22190754144919883 entropy 0.03264415264129639
epoch: 60, step: 36
	action: tensor([[ 1.9685, -0.2544, -1.4440,  1.4886, -0.3521,  0.5046, -0.3480]],
       dtype=torch.float64)
	q_value: tensor([[-9.6977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 37
	action: tensor([[ 1.4307,  0.0147, -0.2465,  0.5220,  0.1948,  0.2590, -0.3366]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8215598676560022, distance: 0.4833958989514952 entropy 0.03264415264129639
epoch: 60, step: 38
	action: tensor([[ 2.3611,  0.5551, -0.9305,  1.3990,  0.1331,  0.4932, -0.6957]],
       dtype=torch.float64)
	q_value: tensor([[-7.5959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 39
	action: tensor([[ 1.4908,  0.2579, -0.6198,  1.0357, -0.2824,  0.3820,  0.2303]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9753316174887149, distance: 0.17973267178506835 entropy 0.03264415264129639
epoch: 60, step: 40
	action: tensor([[ 1.6600, -0.1120, -0.8267,  0.5528, -0.2389,  0.3179, -0.3312]],
       dtype=torch.float64)
	q_value: tensor([[-9.2652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 41
	action: tensor([[ 1.7828, -0.0536, -0.3273,  0.9819, -0.8207,  0.1389, -0.0990]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8180674055120234, distance: 0.4881035313561017 entropy 0.03264415264129639
epoch: 60, step: 42
	action: tensor([[ 1.0483, -0.4290, -0.5573,  0.5056, -0.0726,  0.5257, -0.3193]],
       dtype=torch.float64)
	q_value: tensor([[-10.0016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7808033619364152, distance: 0.5357641348698462 entropy 0.03264415264129639
epoch: 60, step: 43
	action: tensor([[ 1.4254,  0.0208, -1.1118,  1.2943, -0.3373,  0.3520, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-5.9744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9471655305407326, distance: 0.26303610725545606 entropy 0.03264415264129639
epoch: 60, step: 44
	action: tensor([[ 1.6463,  0.0511, -1.0202,  0.8788,  0.1329,  0.0248, -0.4132]],
       dtype=torch.float64)
	q_value: tensor([[-10.0701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.873229460546577, distance: 0.40744206331793525 entropy 0.03264415264129639
epoch: 60, step: 45
	action: tensor([[ 1.8794, -0.1487, -1.3992,  1.3935, -0.4440,  0.5053, -0.8221]],
       dtype=torch.float64)
	q_value: tensor([[-9.9772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 46
	action: tensor([[ 1.4295, -0.0640, -0.2484,  0.6969, -0.2867,  0.1154, -0.2445]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8332302639038834, distance: 0.4673210179435536 entropy 0.03264415264129639
epoch: 60, step: 47
	action: tensor([[ 1.7795, -0.0991, -1.2016,  1.1437, -0.4601,  0.3969, -0.5124]],
       dtype=torch.float64)
	q_value: tensor([[-8.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 48
	action: tensor([[ 1.9417,  0.0340, -0.6880,  0.5463, -0.4504,  0.2032, -0.1619]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7406002399903755, distance: 0.5828294365388837 entropy 0.03264415264129639
epoch: 60, step: 49
	action: tensor([[ 2.1708, -0.0025, -0.9436,  0.5672,  0.1069,  0.6962,  0.1287]],
       dtype=torch.float64)
	q_value: tensor([[-9.7954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 50
	action: tensor([[ 1.3663, -0.0826, -0.5698,  0.7120, -0.1937,  0.4703, -0.3242]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9469170710761069, distance: 0.26365385886674547 entropy 0.03264415264129639
epoch: 60, step: 51
	action: tensor([[ 1.9002, -0.2714, -0.8648,  1.0322, -0.1092,  0.1868, -0.1469]],
       dtype=torch.float64)
	q_value: tensor([[-8.6620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 52
	action: tensor([[ 1.3119, -0.0203, -0.9261,  0.8695, -0.2934,  0.5623, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9637005543675099, distance: 0.21802519797933415 entropy 0.03264415264129639
epoch: 60, step: 53
	action: tensor([[ 1.9462e+00, -1.6942e-01, -4.5296e-01,  1.3495e+00,  3.5935e-01,
          1.1923e-03, -3.6109e-01]], dtype=torch.float64)
	q_value: tensor([[-8.8650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 54
	action: tensor([[ 1.3637,  0.2021, -0.2256,  1.1431,  0.1610,  0.2572, -0.1934]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9025172108530845, distance: 0.35728983992814567 entropy 0.03264415264129639
epoch: 60, step: 55
	action: tensor([[ 2.0394,  0.1903, -1.2368,  0.7610, -0.0626,  0.4346, -0.2077]],
       dtype=torch.float64)
	q_value: tensor([[-8.2076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 56
	action: tensor([[ 1.3657,  0.0609, -0.8969,  0.2082, -0.1616, -0.3628, -0.4714]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6427679391746748, distance: 0.6839618963442777 entropy 0.03264415264129639
epoch: 60, step: 57
	action: tensor([[ 1.6099, -0.2502, -1.1315,  0.9286, -0.4493,  0.2851, -0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-8.2793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8045911488363179, distance: 0.5058582170847509 entropy 0.03264415264129639
epoch: 60, step: 58
	action: tensor([[ 1.5736, -0.2146, -0.8695,  1.1468,  0.2774,  0.2797, -0.3009]],
       dtype=torch.float64)
	q_value: tensor([[-10.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9415291034636628, distance: 0.2767110929557589 entropy 0.03264415264129639
epoch: 60, step: 59
	action: tensor([[ 2.0735,  0.1244, -0.9754,  1.6572, -0.4162,  0.1267, -0.2113]],
       dtype=torch.float64)
	q_value: tensor([[-9.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 60
	action: tensor([[ 1.4347,  0.1883, -0.7296,  1.0023, -0.4818,  0.0223, -0.1828]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9915686936638191, distance: 0.10507614416888518 entropy 0.03264415264129639
epoch: 60, step: 61
	action: tensor([[ 1.5332, -0.2002, -0.6104,  1.4524, -0.0405, -0.0062, -0.0942]],
       dtype=torch.float64)
	q_value: tensor([[-9.8047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9726285988992613, distance: 0.18932379777813202 entropy 0.03264415264129639
epoch: 60, step: 62
	action: tensor([[ 2.4596,  0.1703, -1.5426,  1.5994, -0.3629,  0.5654, -0.2716]],
       dtype=torch.float64)
	q_value: tensor([[-8.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 63
	action: tensor([[ 1.4689,  0.3628, -0.2724,  0.7407, -0.3903,  0.1496, -0.1610]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8974830974653631, distance: 0.3663991278187586 entropy 0.03264415264129639
epoch: 60, step: 64
	action: tensor([[ 1.6016,  0.2038, -0.5776,  1.0850, -0.3057, -0.0183, -0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-9.4386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9375485088723372, distance: 0.28597502025083227 entropy 0.03264415264129639
epoch: 60, step: 65
	action: tensor([[ 2.1439, -0.1452, -0.8880,  1.1641, -0.0389,  0.5433, -0.5270]],
       dtype=torch.float64)
	q_value: tensor([[-9.9547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 66
	action: tensor([[ 1.7310, -0.0896, -0.6336,  0.9442, -0.2500,  0.3355, -0.5154]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8943872995583521, distance: 0.3718902281827825 entropy 0.03264415264129639
epoch: 60, step: 67
	action: tensor([[ 1.7322,  0.1875, -0.7590,  1.5534, -0.0919,  0.1526,  0.2867]],
       dtype=torch.float64)
	q_value: tensor([[-10.4364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 68
	action: tensor([[ 1.4374,  0.4208, -0.4022,  0.6062, -0.3776,  0.4128,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9413057671968528, distance: 0.2772390540945354 entropy 0.03264415264129639
epoch: 60, step: 69
	action: tensor([[ 1.9166,  0.0778, -0.5750,  0.8349, -0.4210,  0.5142,  0.4077]],
       dtype=torch.float64)
	q_value: tensor([[-9.3907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 70
	action: tensor([[ 1.1675, -0.1984, -1.1252,  0.2702, -0.1292,  0.4153, -0.2756]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7215137357389683, distance: 0.6038910358097676 entropy 0.03264415264129639
epoch: 60, step: 71
	action: tensor([[ 1.7403, -0.2603, -0.9392,  1.2627, -0.5147, -0.1707, -0.3418]],
       dtype=torch.float64)
	q_value: tensor([[-7.6446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 72
	action: tensor([[ 1.4799,  0.1590, -0.5128,  1.1560, -0.5188,  0.3691,  0.6006]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.97311457241704, distance: 0.18763556840643325 entropy 0.03264415264129639
epoch: 60, step: 73
	action: tensor([[ 1.6481, -0.1762, -0.5524,  0.7730, -0.4926,  0.5043, -0.5389]],
       dtype=torch.float64)
	q_value: tensor([[-8.5716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8638893430406024, distance: 0.42218496049122134 entropy 0.03264415264129639
epoch: 60, step: 74
	action: tensor([[ 1.9101,  0.3718, -1.0994,  1.2742, -0.0904,  0.1096, -0.0036]],
       dtype=torch.float64)
	q_value: tensor([[-10.7419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 75
	action: tensor([[ 2.0913, -0.1808, -0.3423,  0.6629, -0.1906,  0.0405,  0.1912]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6115645609667641, distance: 0.7132078527121704 entropy 0.03264415264129639
epoch: 60, step: 76
	action: tensor([[ 1.8878,  0.0601, -0.7608,  0.8453, -0.3168,  0.2376, -0.2138]],
       dtype=torch.float64)
	q_value: tensor([[-7.4920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 77
	action: tensor([[ 1.1353,  0.0328, -0.4471,  0.8061, -0.1177,  0.0660,  0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9863508593632446, distance: 0.13369320897712073 entropy 0.03264415264129639
epoch: 60, step: 78
	action: tensor([[ 1.1658, -0.0227, -0.5590,  1.1349,  0.0471, -0.0800, -0.3169]],
       dtype=torch.float64)
	q_value: tensor([[-6.3198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9838608274098989, distance: 0.14537754352617982 entropy 0.03264415264129639
epoch: 60, step: 79
	action: tensor([[ 1.7258,  0.4522, -0.9852,  1.3522, -0.7225,  0.1537, -0.1171]],
       dtype=torch.float64)
	q_value: tensor([[-7.3350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 80
	action: tensor([[ 1.1848,  0.1722, -0.5631,  0.6700, -0.4497,  0.3060, -0.1151]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06826309369143452 entropy 0.03264415264129639
epoch: 60, step: 81
	action: tensor([[ 1.9109, -0.4427, -0.9792,  1.1133, -0.4828,  0.1549, -0.3348]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7896065573120836, distance: 0.5248954320641445 entropy 0.03264415264129639
epoch: 60, step: 82
	action: tensor([[ 1.4319, -0.0394, -0.8850,  1.3361, -0.1457,  0.1111,  0.2383]],
       dtype=torch.float64)
	q_value: tensor([[-10.0600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9879332336251213, distance: 0.12570488222422058 entropy 0.03264415264129639
epoch: 60, step: 83
	action: tensor([[ 2.1141,  0.0697, -1.3429,  1.4677, -0.1996,  0.0640, -0.3938]],
       dtype=torch.float64)
	q_value: tensor([[-8.4201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 84
	action: tensor([[ 1.5647, -0.2571, -1.3615,  1.0466, -0.0919,  0.2646, -0.1983]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7854715023113074, distance: 0.5300284590684302 entropy 0.03264415264129639
epoch: 60, step: 85
	action: tensor([[ 1.9112, -0.5553, -1.2954,  1.2735, -0.4858,  0.3534, -0.4834]],
       dtype=torch.float64)
	q_value: tensor([[-9.9260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 86
	action: tensor([[ 1.9450, -0.2064, -0.8173,  0.9435, -0.4661,  0.4048, -0.2765]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8741718135492342, distance: 0.40592487150025536 entropy 0.03264415264129639
epoch: 60, step: 87
	action: tensor([[ 1.9261, -0.0820, -1.2842,  1.1451, -0.7697, -0.2421, -0.2995]],
       dtype=torch.float64)
	q_value: tensor([[-10.3376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 88
	action: tensor([[ 1.2674, -0.3718, -0.7688,  1.0232,  0.0522,  0.2882, -0.1861]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9046786727536957, distance: 0.3533065863743357 entropy 0.03264415264129639
epoch: 60, step: 89
	action: tensor([[ 1.7507,  0.0894, -0.7393,  0.8560, -0.4147,  0.0851, -0.1539]],
       dtype=torch.float64)
	q_value: tensor([[-7.1711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 90
	action: tensor([[ 1.6895,  0.1217, -0.5640,  1.4160, -0.2742,  0.5802,  0.1254]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.913203752823376, distance: 0.3371375816620135 entropy 0.03264415264129639
epoch: 60, step: 91
	action: tensor([[ 1.7483, -0.0331, -0.5886,  1.3335, -0.3204,  0.0669, -0.6086]],
       dtype=torch.float64)
	q_value: tensor([[-10.2703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 92
	action: tensor([[ 1.5383, -0.1382, -0.3940,  0.8115, -0.0256,  0.2874,  0.2071]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8538366698540786, distance: 0.4374978347049096 entropy 0.03264415264129639
epoch: 60, step: 93
	action: tensor([[ 1.6147, -0.1129, -0.3796,  1.5129, -0.3830,  0.0860,  0.1901]],
       dtype=torch.float64)
	q_value: tensor([[-7.4133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9229706480712723, distance: 0.31760316035337244 entropy 0.03264415264129639
epoch: 60, step: 94
	action: tensor([[ 2.0631, -0.3706, -0.8428,  0.5295,  0.3249, -0.0774,  0.3075]],
       dtype=torch.float64)
	q_value: tensor([[-9.0824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 95
	action: tensor([[ 1.6461,  0.3136, -0.5708,  0.9462, -0.3483,  0.1744, -0.3647]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9343954696307825, distance: 0.2931052576247405 entropy 0.03264415264129639
epoch: 60, step: 96
	action: tensor([[ 2.2603, -0.2740, -0.8215,  0.6850, -0.0913,  0.5307, -0.3857]],
       dtype=torch.float64)
	q_value: tensor([[-11.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 97
	action: tensor([[ 1.5091,  0.2502, -0.5620,  0.7016,  0.1459,  0.3823,  0.1995]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9447145213616964, distance: 0.26906811127410735 entropy 0.03264415264129639
epoch: 60, step: 98
	action: tensor([[ 1.7623, -0.1338, -0.8822,  1.4508,  0.3044,  0.0877, -0.1204]],
       dtype=torch.float64)
	q_value: tensor([[-8.2373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 99
	action: tensor([[ 1.7472,  0.0085, -0.7319,  0.8447, -0.3295,  0.5079, -0.2190]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9361649471267732, distance: 0.28912543852598527 entropy 0.03264415264129639
epoch: 60, step: 100
	action: tensor([[ 2.2143,  0.0111, -1.1531,  1.0602, -0.0241,  0.0897,  0.0492]],
       dtype=torch.float64)
	q_value: tensor([[-10.4161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 101
	action: tensor([[ 1.4331,  0.0296, -0.7600,  0.5149, -0.1883,  0.0212, -0.2550]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8250727335650985, distance: 0.47861405506785804 entropy 0.03264415264129639
epoch: 60, step: 102
	action: tensor([[ 2.0541,  0.0801, -0.8177,  0.9172, -0.3420,  0.2481, -0.4321]],
       dtype=torch.float64)
	q_value: tensor([[-8.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 103
	action: tensor([[ 1.5852, -0.1655, -0.8491,  0.7441, -0.2206,  0.0496, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.755340342963122, distance: 0.5660279470632732 entropy 0.03264415264129639
epoch: 60, step: 104
	action: tensor([[ 1.7703, -0.3885, -0.9234,  1.1597,  0.2582,  0.3694, -0.1783]],
       dtype=torch.float64)
	q_value: tensor([[-8.6993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 105
	action: tensor([[ 1.4905,  0.0406, -0.8722,  0.6484, -0.2631,  0.2434, -0.0861]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8965518908568797, distance: 0.36805944907920185 entropy 0.03264415264129639
epoch: 60, step: 106
	action: tensor([[ 2.0014, -0.0041, -0.8378,  0.9018, -0.0823,  0.2875, -0.4707]],
       dtype=torch.float64)
	q_value: tensor([[-9.2009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 107
	action: tensor([[ 1.6853,  0.1520, -0.4187,  1.3247,  0.1506,  0.5280, -0.0851]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8667473070243095, distance: 0.4177290618208812 entropy 0.03264415264129639
epoch: 60, step: 108
	action: tensor([[ 2.0710,  0.0399, -1.1994,  1.1833, -0.2585,  0.3777,  0.1605]],
       dtype=torch.float64)
	q_value: tensor([[-9.7377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 109
	action: tensor([[ 1.2123,  0.1543, -0.4708,  0.7072, -0.4293,  0.4493, -0.0796]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.03088481374257228 entropy 0.03264415264129639
epoch: 60, step: 110
	action: tensor([[ 1.6657,  0.0713, -0.5504,  0.7259, -0.2188,  0.3492, -0.3952]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8724186006597673, distance: 0.4087430430698992 entropy 0.03264415264129639
epoch: 60, step: 111
	action: tensor([[ 1.3178,  0.2366, -0.8035,  0.6847, -0.5964,  0.3235, -0.5033]],
       dtype=torch.float64)
	q_value: tensor([[-10.1775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09738695027391937 entropy 0.03264415264129639
epoch: 60, step: 112
	action: tensor([[ 1.2813, -0.1517, -1.0298,  0.8289,  0.1068, -0.2336, -0.2504]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8378918696226646, distance: 0.4607433670807816 entropy 0.03264415264129639
epoch: 60, step: 113
	action: tensor([[ 1.7281,  0.1060, -0.6533,  1.1536, -0.2719,  0.3135, -0.0456]],
       dtype=torch.float64)
	q_value: tensor([[-7.4347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 114
	action: tensor([[ 1.4063,  0.0097, -0.7676,  1.1685,  0.0414,  0.5004, -0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.985315753415121, distance: 0.13867000386477804 entropy 0.03264415264129639
epoch: 60, step: 115
	action: tensor([[ 2.2072, -0.2348, -0.9535,  1.7184,  0.0695,  0.4866, -0.1140]],
       dtype=torch.float64)
	q_value: tensor([[-8.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 116
	action: tensor([[ 1.6096, -0.1785, -1.0655,  0.7505, -0.5963,  0.5811, -0.3335]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8080668289334161, distance: 0.5013392564744132 entropy 0.03264415264129639
epoch: 60, step: 117
	action: tensor([[ 1.7522, -0.0995, -0.8969,  1.6176, -0.2334,  0.4927, -0.5649]],
       dtype=torch.float64)
	q_value: tensor([[-11.3787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 118
	action: tensor([[ 1.6556,  0.2064, -0.0922,  1.3867, -0.2813,  0.0785, -0.6965]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7759609447549565, distance: 0.5416497656459047 entropy 0.03264415264129639
epoch: 60, step: 119
	action: tensor([[ 2.1731, -0.1733, -1.1029,  2.0908, -0.8037,  0.7506, -0.8261]],
       dtype=torch.float64)
	q_value: tensor([[-11.2103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 120
	action: tensor([[ 1.3435,  0.1744, -0.4440,  1.1698, -0.0471,  0.3820,  0.0831]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9480140301192602, distance: 0.26091543333382167 entropy 0.03264415264129639
epoch: 60, step: 121
	action: tensor([[ 1.8150,  0.0524, -0.9617,  1.4669, -0.4634,  0.5487, -0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-8.2145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 122
	action: tensor([[ 1.0993, -0.1177, -0.9116,  1.3001, -0.5776,  0.1518, -0.2570]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.89258968425075, distance: 0.37504181339612225 entropy 0.03264415264129639
epoch: 60, step: 123
	action: tensor([[ 1.7546,  0.1874, -0.9341,  1.1326, -0.6141,  0.2706, -0.4011]],
       dtype=torch.float64)
	q_value: tensor([[-8.7576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 124
	action: tensor([[ 1.6874, -0.0790, -0.6695,  0.9370, -0.3351, -0.1354, -0.2503]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8179023084047702, distance: 0.48832494909524915 entropy 0.03264415264129639
epoch: 60, step: 125
	action: tensor([[ 1.8358, -0.3049, -0.9236,  0.7770, -0.8547,  0.3700, -0.3388]],
       dtype=torch.float64)
	q_value: tensor([[-9.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 60, step: 126
	action: tensor([[ 2.1492, -0.5188, -0.6028,  1.3874, -0.1976,  0.2686,  0.4421]],
       dtype=torch.float64)
	q_value: tensor([[-6.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8945424781416431, distance: 0.3716169153498929 entropy 0.03264415264129639
epoch: 60, step: 127
	action: tensor([[ 1.6564,  0.0399, -0.9521,  1.5531, -0.3702,  0.3936, -0.1871]],
       dtype=torch.float64)
	q_value: tensor([[-7.6480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 60 actor 557.0714652673798 critic 1972.379477137963 
epoch: 61, step: 0
	action: tensor([[ 1.2111,  0.1019, -0.6805,  1.0633, -0.7646,  0.4211,  0.0723]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9746569220850807, distance: 0.18217399094461806 entropy 0.03264415264129639
epoch: 61, step: 1
	action: tensor([[ 1.8162,  0.0850, -0.3376,  0.7118, -0.1758,  0.2251, -0.0655]],
       dtype=torch.float64)
	q_value: tensor([[-10.2612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 2
	action: tensor([[ 1.5605, -0.4864, -0.6095,  0.8100, -0.1887,  0.4523, -0.2639]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7322596794896609, distance: 0.5921252522405294 entropy 0.03264415264129639
epoch: 61, step: 3
	action: tensor([[ 1.5442, -0.3243, -0.8997,  1.3737, -0.3107,  0.5162, -0.1937]],
       dtype=torch.float64)
	q_value: tensor([[-9.7318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9384029334663169, distance: 0.2840120118772341 entropy 0.03264415264129639
epoch: 61, step: 4
	action: tensor([[ 1.8991, -0.1483, -0.5067,  1.8312, -0.4967, -0.1417,  0.1821]],
       dtype=torch.float64)
	q_value: tensor([[-11.5593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 5
	action: tensor([[ 1.7512, -0.1109, -0.5786,  1.2640, -0.3774,  0.4869,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.957147971487663, distance: 0.23688763159117895 entropy 0.03264415264129639
epoch: 61, step: 6
	action: tensor([[ 1.7019e+00,  1.4159e-01, -6.5193e-01,  1.8783e+00, -5.1301e-01,
         -2.8282e-01,  1.8757e-03]], dtype=torch.float64)
	q_value: tensor([[-11.2315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 7
	action: tensor([[ 1.0576, -0.0931, -0.2460,  1.2966, -0.1487, -0.0673,  0.2735]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08814343698457365 entropy 0.03264415264129639
epoch: 61, step: 8
	action: tensor([[ 1.0932,  0.2274, -0.1867,  1.1503, -0.3802,  0.2027, -0.1405]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.927747391774468, distance: 0.30759796494121117 entropy 0.03264415264129639
epoch: 61, step: 9
	action: tensor([[ 1.9472, -0.2494, -0.6938,  1.0127, -0.3648,  0.1144,  0.1029]],
       dtype=torch.float64)
	q_value: tensor([[-8.8528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 10
	action: tensor([[ 1.7613,  0.1920, -0.7928,  0.7882, -0.3109,  0.6050, -0.1578]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.964881529886936, distance: 0.21444922811119546 entropy 0.03264415264129639
epoch: 61, step: 11
	action: tensor([[ 1.6058,  0.1511, -0.6798,  1.3800, -0.1359,  0.4827, -0.2263]],
       dtype=torch.float64)
	q_value: tensor([[-12.3879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.942764597407007, distance: 0.27377202244973076 entropy 0.03264415264129639
epoch: 61, step: 12
	action: tensor([[ 1.9934, -0.1778, -0.8625,  1.0567, -0.4825, -0.1912,  0.0962]],
       dtype=torch.float64)
	q_value: tensor([[-12.4648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 13
	action: tensor([[ 1.7549,  0.4590, -0.2834,  0.9422, -0.1305,  0.0460, -0.3561]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 14
	action: tensor([[ 1.3855, -0.2774, -0.4474,  0.7438, -0.1106,  0.4170, -0.1312]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8709503667076586, distance: 0.4110882660946313 entropy 0.03264415264129639
epoch: 61, step: 15
	action: tensor([[ 1.4043, -0.1617, -0.6808,  1.0339, -0.3315,  0.1995, -0.0868]],
       dtype=torch.float64)
	q_value: tensor([[-8.5028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9619852656370453, distance: 0.22311700589990116 entropy 0.03264415264129639
epoch: 61, step: 16
	action: tensor([[ 1.4735, -0.0505, -0.8389,  0.9120, -0.6105,  0.2455,  0.0511]],
       dtype=torch.float64)
	q_value: tensor([[-9.6980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.947725988453034, distance: 0.2616372694417378 entropy 0.03264415264129639
epoch: 61, step: 17
	action: tensor([[ 1.5425, -0.2419, -0.7613,  0.9896,  0.0978,  0.1128, -0.1380]],
       dtype=torch.float64)
	q_value: tensor([[-10.7238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8762554145907875, distance: 0.4025499673430909 entropy 0.03264415264129639
epoch: 61, step: 18
	action: tensor([[ 2.2598,  0.2655, -0.6645,  1.1918, -0.3054,  0.3579, -0.2901]],
       dtype=torch.float64)
	q_value: tensor([[-9.4982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 19
	action: tensor([[ 1.1268, -0.3633, -0.5827,  1.2159, -0.0465, -0.1677, -0.4871]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9304245887879925, distance: 0.30184541557780775 entropy 0.03264415264129639
epoch: 61, step: 20
	action: tensor([[ 1.9016, -0.4782, -1.4536,  1.2961, -0.0132,  0.2678,  0.1144]],
       dtype=torch.float64)
	q_value: tensor([[-8.2721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 21
	action: tensor([[ 1.4832,  0.2588, -0.2579,  1.1455, -0.0389,  0.5622,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8617865610605391, distance: 0.4254336421282303 entropy 0.03264415264129639
epoch: 61, step: 22
	action: tensor([[ 2.1376,  0.1241, -0.9492,  0.8565, -0.6495,  0.6042,  0.2930]],
       dtype=torch.float64)
	q_value: tensor([[-10.4145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 23
	action: tensor([[ 1.6560,  0.7088, -0.3851,  0.7993, -0.3745,  0.7584,  0.0785]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 24
	action: tensor([[ 1.5038,  0.0214, -0.7748,  1.0830, -0.3680,  0.2512, -0.6060]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9893185369015954, distance: 0.11826931065792144 entropy 0.03264415264129639
epoch: 61, step: 25
	action: tensor([[ 1.7513,  0.3579, -0.8836,  1.3526, -0.0982,  0.0743, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[-12.5372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 26
	action: tensor([[ 1.5964,  0.5005, -0.7627,  0.8794, -0.2630, -0.0399, -0.2454]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9409888109176958, distance: 0.2779866092051316 entropy 0.03264415264129639
epoch: 61, step: 27
	action: tensor([[ 1.8633,  0.0802, -0.5258,  0.8842, -0.1704,  0.3070, -0.2222]],
       dtype=torch.float64)
	q_value: tensor([[-12.4547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 28
	action: tensor([[ 1.3802, -0.0336, -0.9545,  0.8445, -0.4885,  0.1999,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9396262070120668, distance: 0.2811777322320322 entropy 0.03264415264129639
epoch: 61, step: 29
	action: tensor([[ 1.7642, -0.0042, -0.4328,  1.1776, -0.2185,  0.4957, -0.5008]],
       dtype=torch.float64)
	q_value: tensor([[-10.1655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 30
	action: tensor([[ 1.0483, -0.2712, -1.0283,  0.5456, -0.3096,  0.5222,  0.4594]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7473450294909242, distance: 0.5752023034640965 entropy 0.03264415264129639
epoch: 61, step: 31
	action: tensor([[ 1.2387,  0.2392, -0.5996,  0.9045, -0.1263, -0.4281, -0.4504]],
       dtype=torch.float64)
	q_value: tensor([[-7.1927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9853882432548031, distance: 0.13832730317471603 entropy 0.03264415264129639
epoch: 61, step: 32
	action: tensor([[ 1.7998,  0.1248, -0.9750,  1.2735,  0.3960,  0.2820,  0.2501]],
       dtype=torch.float64)
	q_value: tensor([[-9.5463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 33
	action: tensor([[ 1.1411,  0.0876, -0.7439,  1.2027, -0.3318,  0.4953,  0.0742]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9192165600707012, distance: 0.3252504188223468 entropy 0.03264415264129639
epoch: 61, step: 34
	action: tensor([[ 1.9608, -0.2321, -1.1269,  0.7737, -0.3229,  0.2123,  0.1382]],
       dtype=torch.float64)
	q_value: tensor([[-9.4072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 35
	action: tensor([[ 1.3084, -0.0974, -1.0546,  0.9545, -0.3754,  0.2085, -0.2760]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9200263201975996, distance: 0.3236161844046026 entropy 0.03264415264129639
epoch: 61, step: 36
	action: tensor([[ 1.2051, -0.1636, -1.1239,  1.1554, -0.2411, -0.2138, -0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-10.5711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8849355196383974, distance: 0.3881747960745879 entropy 0.03264415264129639
epoch: 61, step: 37
	action: tensor([[ 1.9434e+00, -2.7201e-01, -8.5222e-01,  1.0305e+00, -3.7276e-01,
          6.1996e-01,  6.5602e-05]], dtype=torch.float64)
	q_value: tensor([[-9.3656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 38
	action: tensor([[ 1.4230, -0.3622, -0.5495,  1.1006, -0.1789,  0.0034, -0.1073]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8903499019569145, distance: 0.37893193313915746 entropy 0.03264415264129639
epoch: 61, step: 39
	action: tensor([[ 2.0829, -0.1252, -0.7825,  1.2791,  0.1494,  0.2183, -0.3562]],
       dtype=torch.float64)
	q_value: tensor([[-8.8030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 40
	action: tensor([[ 1.0905, -0.5017, -0.6076,  0.5539,  0.0932,  0.5864, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7495456215465522, distance: 0.5726918563257428 entropy 0.03264415264129639
epoch: 61, step: 41
	action: tensor([[ 1.6094, -0.2184, -0.6366,  1.1146, -0.2089,  0.6428, -0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-6.2332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9526585815193962, distance: 0.2489873749501932 entropy 0.03264415264129639
epoch: 61, step: 42
	action: tensor([[ 1.6494,  0.2759, -0.8293,  1.0460, -0.2059,  0.4341,  0.2916]],
       dtype=torch.float64)
	q_value: tensor([[-10.9871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9842866392315152, distance: 0.1434469220967978 entropy 0.03264415264129639
epoch: 61, step: 43
	action: tensor([[ 2.1946, -0.2284, -0.7534,  0.9434, -0.0942,  0.3285,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[-11.3752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 44
	action: tensor([[ 1.2083,  0.1591, -0.7527,  0.9770,  0.1128, -0.1536, -0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0798856785856174 entropy 0.03264415264129639
epoch: 61, step: 45
	action: tensor([[ 1.9702,  0.6333, -0.8426,  0.3884,  0.2493,  0.0307, -0.3091]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8186653462485797, distance: 0.48730076925214344 entropy 0.03264415264129639
epoch: 61, step: 46
	action: tensor([[ 1.5424e+00, -5.7330e-04, -7.4650e-01,  1.3116e+00, -4.3581e-01,
          3.0799e-01,  2.9469e-01]], dtype=torch.float64)
	q_value: tensor([[-11.7970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9827372395162728, distance: 0.15035290225985756 entropy 0.03264415264129639
epoch: 61, step: 47
	action: tensor([[ 1.5214,  0.2481, -0.8773,  1.1388,  0.1100,  0.0150,  0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-10.6592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9872279313178355, distance: 0.12932643794910845 entropy 0.03264415264129639
epoch: 61, step: 48
	action: tensor([[ 1.9100, -0.2294, -0.5300,  1.2078, -0.1120,  0.2081, -0.1947]],
       dtype=torch.float64)
	q_value: tensor([[-10.3906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 49
	action: tensor([[ 1.3701, -0.1433, -0.7724,  1.2379, -0.0551,  0.1957,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9834786920961887, distance: 0.1470885632501786 entropy 0.03264415264129639
epoch: 61, step: 50
	action: tensor([[ 1.9315,  0.0584, -1.2065,  1.4790, -0.2857, -0.0558, -0.0674]],
       dtype=torch.float64)
	q_value: tensor([[-9.2290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 51
	action: tensor([[ 1.4635,  0.1975, -0.5209,  0.8989, -0.0944,  0.5219, -0.4229]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9704322078284654, distance: 0.19677328762870805 entropy 0.03264415264129639
epoch: 61, step: 52
	action: tensor([[ 1.7398, -0.0320, -0.8060,  1.5791, -0.5604,  0.5513,  0.4168]],
       dtype=torch.float64)
	q_value: tensor([[-11.4853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 53
	action: tensor([[ 1.5235,  0.3477, -0.7141,  0.5848, -0.3031,  0.3532, -0.1216]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9495143444917185, distance: 0.2571228617141511 entropy 0.03264415264129639
epoch: 61, step: 54
	action: tensor([[ 1.4423,  0.0519, -0.9478,  1.3262,  0.1316, -0.3366,  0.3045]],
       dtype=torch.float64)
	q_value: tensor([[-11.5332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.985320157273524, distance: 0.1386492084888763 entropy 0.03264415264129639
epoch: 61, step: 55
	action: tensor([[ 1.5845,  0.1624, -0.5813,  1.5889, -0.2719,  0.0735, -0.1984]],
       dtype=torch.float64)
	q_value: tensor([[-9.1734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9163654726981394, distance: 0.3309401783307208 entropy 0.03264415264129639
epoch: 61, step: 56
	action: tensor([[ 2.2095, -0.0887, -0.7268,  1.5809, -0.6814,  0.3533, -0.0952]],
       dtype=torch.float64)
	q_value: tensor([[-12.1868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 57
	action: tensor([[ 1.8779,  0.6712, -0.6489,  0.8206,  0.3211,  0.2518, -0.5063]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 58
	action: tensor([[ 1.1500, -0.4273, -1.1538,  0.9883, -0.1792, -0.0872, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7002759887510428, distance: 0.6264947866252701 entropy 0.03264415264129639
epoch: 61, step: 59
	action: tensor([[ 1.8088,  0.2213, -0.8022,  1.1363, -0.2815,  0.3195, -0.3658]],
       dtype=torch.float64)
	q_value: tensor([[-7.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 60
	action: tensor([[ 1.3952, -0.3442, -0.5332,  0.5760, -0.0866,  0.7435,  0.0911]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.829764487153507, distance: 0.4721519361960652 entropy 0.03264415264129639
epoch: 61, step: 61
	action: tensor([[ 1.4903,  0.4042, -1.0583,  0.9784, -0.2376,  0.5766, -0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-8.2675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9866980881125892, distance: 0.13198170261490302 entropy 0.03264415264129639
epoch: 61, step: 62
	action: tensor([[ 1.7285,  0.1832, -0.6847,  1.1814, -0.5799,  0.3062, -0.2118]],
       dtype=torch.float64)
	q_value: tensor([[-12.9781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 63
	action: tensor([[ 1.5726, -0.0405, -0.5474,  0.8190, -0.2990,  0.6952,  0.2982]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9421377602168655, distance: 0.2752671038785211 entropy 0.03264415264129639
epoch: 61, step: 64
	action: tensor([[ 1.6636, -0.1126, -0.6947,  0.9627, -0.0073, -0.0941, -0.0538]],
       dtype=torch.float64)
	q_value: tensor([[-10.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 65
	action: tensor([[ 1.3858, -0.1135, -0.2986,  0.7515, -0.5116,  0.2572,  0.1725]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9033332678579276, distance: 0.3557912079703571 entropy 0.03264415264129639
epoch: 61, step: 66
	action: tensor([[ 1.7256, -0.2562, -0.3321,  0.5971, -0.1852,  0.1995, -0.1612]],
       dtype=torch.float64)
	q_value: tensor([[-8.5927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 67
	action: tensor([[ 1.4395, -0.0613, -0.5379,  1.4758, -0.6770,  0.2531,  0.1221]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9690733141401794, distance: 0.20124420566114037 entropy 0.03264415264129639
epoch: 61, step: 68
	action: tensor([[ 1.5012, -0.0047, -0.9452,  0.9333, -0.4736,  0.4069, -0.3997]],
       dtype=torch.float64)
	q_value: tensor([[-10.7916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9598004347458875, distance: 0.2294390688413178 entropy 0.03264415264129639
epoch: 61, step: 69
	action: tensor([[ 2.3802, -0.1296, -0.8955,  1.2266, -0.1570,  0.4014,  0.1837]],
       dtype=torch.float64)
	q_value: tensor([[-12.4644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 70
	action: tensor([[ 1.3861e+00,  4.7581e-04, -2.3741e-01,  9.1871e-01, -1.3361e-01,
         -2.5276e-01, -4.7451e-01]], dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.888592083798375, distance: 0.38195721568972485 entropy 0.03264415264129639
epoch: 61, step: 71
	action: tensor([[ 1.6661,  0.0378, -0.9086,  1.3476, -0.7138, -0.0630, -0.2743]],
       dtype=torch.float64)
	q_value: tensor([[-9.4818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 72
	action: tensor([[ 1.3767, -0.1674, -1.2908,  0.9993, -0.2007,  0.6811,  0.2152]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8310542038298274, distance: 0.47036000702370717 entropy 0.03264415264129639
epoch: 61, step: 73
	action: tensor([[ 1.2977,  0.4862, -0.9033,  0.9754, -0.7576,  0.2755,  0.0689]],
       dtype=torch.float64)
	q_value: tensor([[-10.2863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9598957769463156, distance: 0.22916682445953585 entropy 0.03264415264129639
epoch: 61, step: 74
	action: tensor([[ 1.6763,  0.1542, -0.3775,  0.6853, -0.4691,  0.3063,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[-11.9131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8595423644594056, distance: 0.42887365581481046 entropy 0.03264415264129639
epoch: 61, step: 75
	action: tensor([[ 1.9134, -0.1515, -0.9334,  1.0145, -0.4463,  0.2642,  0.5778]],
       dtype=torch.float64)
	q_value: tensor([[-10.7594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 76
	action: tensor([[ 1.4685, -0.2282, -0.2653,  0.8606,  0.3868, -0.3119, -0.3571]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7609316092502969, distance: 0.559522779464707 entropy 0.03264415264129639
epoch: 61, step: 77
	action: tensor([[ 2.1870,  0.0709, -1.0146,  1.4204, -0.0223,  0.6466, -0.2628]],
       dtype=torch.float64)
	q_value: tensor([[-8.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 78
	action: tensor([[ 0.8237, -0.0416, -0.0613,  1.3857, -0.1233,  0.4963,  0.1489]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8940060439053377, distance: 0.3725608744333537 entropy 0.03264415264129639
epoch: 61, step: 79
	action: tensor([[ 1.5737,  0.0780, -0.9367,  1.0243, -0.0321,  0.4545,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-6.6660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9855124665801076, distance: 0.13773804632747136 entropy 0.03264415264129639
epoch: 61, step: 80
	action: tensor([[ 1.8690,  0.1350, -0.7303,  1.2646, -0.3293,  0.1633, -0.1031]],
       dtype=torch.float64)
	q_value: tensor([[-11.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 81
	action: tensor([[ 1.1920,  0.0060, -0.5937,  0.7585, -0.2166,  0.2435,  0.0515]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9808353142076226, distance: 0.15841910118911431 entropy 0.03264415264129639
epoch: 61, step: 82
	action: tensor([[ 1.2491,  0.2418, -0.3425,  0.8790, -0.3288,  0.4317, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-8.0324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9723965191160848, distance: 0.1901247337884449 entropy 0.03264415264129639
epoch: 61, step: 83
	action: tensor([[ 1.5187,  0.2535, -0.8604,  1.0547,  0.1254,  0.5193,  0.1920]],
       dtype=torch.float64)
	q_value: tensor([[-9.4111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9822124891467425, distance: 0.15262099561020762 entropy 0.03264415264129639
epoch: 61, step: 84
	action: tensor([[ 1.5595,  0.4057, -0.9519,  1.2242, -0.0512,  0.4202, -0.4071]],
       dtype=torch.float64)
	q_value: tensor([[-10.6413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9529380656353322, distance: 0.24825132787352358 entropy 0.03264415264129639
epoch: 61, step: 85
	action: tensor([[ 2.2050, -0.0876, -0.7407,  1.0107,  0.1901, -0.0275, -0.1363]],
       dtype=torch.float64)
	q_value: tensor([[-13.4503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 86
	action: tensor([[ 1.6558, -0.1572, -0.8037,  1.3297, -0.3900,  0.1546, -0.5028]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9710055807916287, distance: 0.19485605250779162 entropy 0.03264415264129639
epoch: 61, step: 87
	action: tensor([[ 1.5834,  0.0424, -1.2004,  1.4648, -0.3368,  0.3795, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[-12.6039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9175161969383487, distance: 0.3286555961586332 entropy 0.03264415264129639
epoch: 61, step: 88
	action: tensor([[ 1.8544, -0.2130, -0.8600,  0.8886, -0.3191,  0.3719, -0.3721]],
       dtype=torch.float64)
	q_value: tensor([[-12.7230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 89
	action: tensor([[ 1.7138, -0.1853, -0.3609,  0.8591, -0.3454,  0.4258, -0.0948]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.808960358762765, distance: 0.5001709223156573 entropy 0.03264415264129639
epoch: 61, step: 90
	action: tensor([[ 1.7684, -0.2411, -1.1964,  0.7974, -0.4102,  0.4520, -0.1305]],
       dtype=torch.float64)
	q_value: tensor([[-10.3021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 91
	action: tensor([[ 1.3325,  0.6189, -0.7630,  0.8904, -0.3080,  0.1702,  0.1173]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9325689845702577, distance: 0.29715739541968267 entropy 0.03264415264129639
epoch: 61, step: 92
	action: tensor([[ 1.0917, -0.1970, -0.9769,  1.0443, -0.2760,  0.1276, -0.1978]],
       dtype=torch.float64)
	q_value: tensor([[-10.9247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8783354999809415, distance: 0.39915229542422553 entropy 0.03264415264129639
epoch: 61, step: 93
	action: tensor([[ 1.9062, -0.2329, -0.8473,  1.2565, -0.8859,  0.2340,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-8.6933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 94
	action: tensor([[ 1.3447, -0.0411, -0.3385,  0.8495, -0.1152,  0.8164, -0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9775706729596452, distance: 0.1713818454263585 entropy 0.03264415264129639
epoch: 61, step: 95
	action: tensor([[ 1.9254,  0.0527, -0.7470,  1.3794, -0.2111,  0.4942, -0.2154]],
       dtype=torch.float64)
	q_value: tensor([[-9.5271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 96
	action: tensor([[ 1.3550, -0.0353, -0.5619,  0.9117, -0.0689,  0.4841, -0.4511]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9891945524006069, distance: 0.11895373258062193 entropy 0.03264415264129639
epoch: 61, step: 97
	action: tensor([[ 1.7755,  0.2301, -0.8277,  1.2251, -0.0058,  0.3135, -1.2171]],
       dtype=torch.float64)
	q_value: tensor([[-10.3257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 98
	action: tensor([[ 1.3025,  0.0766, -0.4689,  0.7079, -0.5833,  0.3328, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.976384024720621, distance: 0.17585699024874765 entropy 0.03264415264129639
epoch: 61, step: 99
	action: tensor([[ 1.6055, -0.0180, -0.7608,  0.8157, -0.1023,  0.5699, -0.2538]],
       dtype=torch.float64)
	q_value: tensor([[-9.5935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9380498987017598, distance: 0.2848247361219967 entropy 0.03264415264129639
epoch: 61, step: 100
	action: tensor([[ 1.5762,  0.0672, -0.8265,  0.9274,  0.0163,  0.5253, -0.7015]],
       dtype=torch.float64)
	q_value: tensor([[-11.5291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9772605804032577, distance: 0.17256248293226797 entropy 0.03264415264129639
epoch: 61, step: 101
	action: tensor([[ 2.0073,  0.2627, -0.9847,  1.3004, -0.1966,  0.6626, -0.1866]],
       dtype=torch.float64)
	q_value: tensor([[-12.8298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 102
	action: tensor([[ 1.7377, -0.0244, -0.4662,  0.9867,  0.1288,  0.0644, -0.0781]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8508834748801613, distance: 0.4418955016654778 entropy 0.03264415264129639
epoch: 61, step: 103
	action: tensor([[ 1.7466,  0.2187, -0.8033,  1.3671, -0.0845,  0.1455, -0.4156]],
       dtype=torch.float64)
	q_value: tensor([[-9.6540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 104
	action: tensor([[ 1.5061,  0.0541, -0.8277,  0.9734, -0.3027,  0.2522, -0.3225]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9803688414923749, distance: 0.16033548805868225 entropy 0.03264415264129639
epoch: 61, step: 105
	action: tensor([[ 1.5278, -0.1554, -0.7146,  0.7930,  0.1319,  0.2410, -0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-11.6383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8659666634050378, distance: 0.41895088073232 entropy 0.03264415264129639
epoch: 61, step: 106
	action: tensor([[ 1.9370,  0.1791, -0.8233,  0.9414, -0.1638,  0.4112, -0.1500]],
       dtype=torch.float64)
	q_value: tensor([[-9.8865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 107
	action: tensor([[ 1.2986, -0.0537, -0.4563,  0.5664, -0.5874,  0.4117, -0.0533]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.921637264231209, distance: 0.3203402336606565 entropy 0.03264415264129639
epoch: 61, step: 108
	action: tensor([[ 2.0390,  0.0088, -0.6059,  0.6622, -0.4163,  0.2332, -0.5468]],
       dtype=torch.float64)
	q_value: tensor([[-9.2418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 109
	action: tensor([[ 1.5372,  0.3830, -0.1759,  0.7676,  0.0507,  0.4874, -0.4371]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8284103923366543, distance: 0.47402602270914146 entropy 0.03264415264129639
epoch: 61, step: 110
	action: tensor([[ 1.9233,  0.0357, -0.6066,  1.3343, -0.2401, -0.2801, -0.2061]],
       dtype=torch.float64)
	q_value: tensor([[-11.3521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 111
	action: tensor([[ 1.4406, -0.1590, -0.4375,  0.9371, -0.4633,  0.2121,  0.0544]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9213139377924252, distance: 0.3210004189395371 entropy 0.03264415264129639
epoch: 61, step: 112
	action: tensor([[ 1.8529e+00,  1.6303e-03, -5.1566e-01,  9.3876e-01,  7.7207e-02,
          2.1273e-01, -1.5665e-01]], dtype=torch.float64)
	q_value: tensor([[-9.2976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 113
	action: tensor([[ 1.4755,  0.2561, -0.5455,  0.8634, -0.3508,  0.3377,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9743439797248833, distance: 0.18329530391291662 entropy 0.03264415264129639
epoch: 61, step: 114
	action: tensor([[ 1.7707,  0.0404, -1.2533,  0.9371, -0.1456,  0.4696,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-10.7109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 115
	action: tensor([[ 1.6232, -0.1528, -0.6926,  0.3969, -0.2887,  0.3218, -0.4180]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6296985468653746, distance: 0.6963609377047769 entropy 0.03264415264129639
epoch: 61, step: 116
	action: tensor([[ 1.6121,  0.1514, -0.8953,  0.9121,  0.0610,  0.5286,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[-10.9824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9728939789644621, distance: 0.18840376562380395 entropy 0.03264415264129639
epoch: 61, step: 117
	action: tensor([[ 1.6294, -0.0145, -1.2733,  1.4274, -0.2144,  0.2107, -0.0976]],
       dtype=torch.float64)
	q_value: tensor([[-11.2550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9210091723057291, distance: 0.32162146478269965 entropy 0.03264415264129639
epoch: 61, step: 118
	action: tensor([[ 2.0966, -0.1233, -1.1774,  1.2148, -0.3481, -0.0771, -0.0260]],
       dtype=torch.float64)
	q_value: tensor([[-12.5313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 119
	action: tensor([[ 1.4166, -0.3103, -0.7093,  0.9171, -0.0602, -0.0229,  0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8290863949134988, distance: 0.473091353505181 entropy 0.03264415264129639
epoch: 61, step: 120
	action: tensor([[ 1.3331,  0.0393, -0.6736,  1.0670, -0.5578, -0.0507, -0.2490]],
       dtype=torch.float64)
	q_value: tensor([[-8.0308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9910089966724599, distance: 0.1085077535870743 entropy 0.03264415264129639
epoch: 61, step: 121
	action: tensor([[ 1.6780,  0.3743, -0.7176,  0.8530, -0.2926, -0.1270, -0.4486]],
       dtype=torch.float64)
	q_value: tensor([[-10.5004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 122
	action: tensor([[ 1.3703,  0.1772, -0.7963,  0.6162, -0.3489,  0.2417,  0.2671]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9616819766477721, distance: 0.2240052732925575 entropy 0.03264415264129639
epoch: 61, step: 123
	action: tensor([[ 1.4179, -0.1274, -0.7406,  1.0663, -0.1278,  0.5865,  0.1395]],
       dtype=torch.float64)
	q_value: tensor([[-9.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9836854210868772, distance: 0.14616541626323007 entropy 0.03264415264129639
epoch: 61, step: 124
	action: tensor([[ 1.3044,  0.2210, -0.5429,  1.0154,  0.0187,  0.2545, -0.3730]],
       dtype=torch.float64)
	q_value: tensor([[-9.5545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9728021262934069, distance: 0.188722712351923 entropy 0.03264415264129639
epoch: 61, step: 125
	action: tensor([[ 1.6638, -0.0865, -0.9859,  1.5269, -0.1517, -0.1394,  0.2161]],
       dtype=torch.float64)
	q_value: tensor([[-10.1824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9745314893391256, distance: 0.18262425944891034 entropy 0.03264415264129639
epoch: 61, step: 126
	action: tensor([[ 1.8135, -0.2260, -0.7633,  1.4616, -0.2798,  0.2503, -0.5601]],
       dtype=torch.float64)
	q_value: tensor([[-10.7629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 61, step: 127
	action: tensor([[ 1.4506, -0.2215, -0.6813,  1.0357,  0.2219,  0.5465, -0.0854]],
       dtype=torch.float64)
	q_value: tensor([[-7.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9677193887553268, distance: 0.2056021103702625 entropy 0.03264415264129639
LOSS epoch 61 actor 410.93796616623933 critic 1574.3206759408688 
epoch: 62, step: 0
	action: tensor([[ 1.9578, -0.6301, -0.6278,  1.3754, -0.2503,  0.3709,  0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-11.5574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 62, step: 1
	action: tensor([[ 0.8874,  0.0669, -0.1587,  0.7117, -0.3790,  0.1187, -0.1640]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.990069328217157, distance: 0.11403705874939976 entropy 0.03264415264129639
epoch: 62, step: 2
	action: tensor([[ 0.7562,  0.1474, -0.6596,  0.7039, -0.1052, -0.0346,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-8.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9270633815827942, distance: 0.3090505389770917 entropy 0.03264415264129639
epoch: 62, step: 3
	action: tensor([[ 1.1481,  0.2423, -0.5165, -0.2264,  0.0564,  0.4614,  0.1444]],
       dtype=torch.float64)
	q_value: tensor([[-7.4395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8362585711673276, distance: 0.4630586285368406 entropy 0.03264415264129639
epoch: 62, step: 4
	action: tensor([[ 1.2222,  0.3517, -0.1772,  0.5949, -0.2924,  0.1212, -0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-8.7369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9475534432677556, distance: 0.26206871767293516 entropy 0.03264415264129639
epoch: 62, step: 5
	action: tensor([[ 1.7611, -0.0216, -0.2305,  1.0997, -0.1859,  0.7718, -0.4826]],
       dtype=torch.float64)
	q_value: tensor([[-11.3088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 62, step: 6
	action: tensor([[ 8.1652e-01,  6.9877e-02, -5.0528e-01,  1.0351e+00,  2.4658e-04,
          4.0178e-01,  1.2495e-01]], dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9052552703491316, distance: 0.3522363918131956 entropy 0.03264415264129639
epoch: 62, step: 7
	action: tensor([[ 1.0640,  0.2455, -0.5758,  1.1023, -0.4029,  0.3009,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[-7.9790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9218524178372727, distance: 0.3199001665208452 entropy 0.03264415264129639
epoch: 62, step: 8
	action: tensor([[ 1.0772, -0.1752, -0.7561,  1.1142,  0.1080,  0.2264,  0.0313]],
       dtype=torch.float64)
	q_value: tensor([[-11.1992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9530388797776347, distance: 0.24798528842474266 entropy 0.03264415264129639
epoch: 62, step: 9
	action: tensor([[ 0.9452, -0.2863, -0.5895,  0.9027, -0.1216,  0.3606, -0.4313]],
       dtype=torch.float64)
	q_value: tensor([[-9.1404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9205011013760762, distance: 0.3226541454391073 entropy 0.03264415264129639
epoch: 62, step: 10
	action: tensor([[ 1.0777,  0.1845, -0.3851,  0.9693, -0.0099,  0.3529, -0.4057]],
       dtype=torch.float64)
	q_value: tensor([[-9.5379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9672290859190017, distance: 0.20715764771831846 entropy 0.03264415264129639
epoch: 62, step: 11
	action: tensor([[ 1.6878,  0.0089, -0.6298,  0.8820, -0.4874,  0.4104, -0.1790]],
       dtype=torch.float64)
	q_value: tensor([[-11.0669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9387034659555665, distance: 0.28331831647764094 entropy 0.03264415264129639
epoch: 62, step: 12
	action: tensor([[ 1.3408,  0.0525, -0.5874,  0.9808, -0.4206,  0.1048, -0.1463]],
       dtype=torch.float64)
	q_value: tensor([[-14.5459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9912565989926242, distance: 0.10700323109298697 entropy 0.03264415264129639
epoch: 62, step: 13
	action: tensor([[ 1.2075,  0.0136, -0.8891,  0.8369,  0.0338,  0.7639, -0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-12.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9720229878519386, distance: 0.19140679838497318 entropy 0.03264415264129639
epoch: 62, step: 14
	action: tensor([[ 2.0180, -0.0132, -0.3444,  0.7973, -0.3341,  0.1254, -0.1719]],
       dtype=torch.float64)
	q_value: tensor([[-12.4259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 62, step: 15
	action: tensor([[ 0.2066, -0.4558, -0.2194,  0.4315, -0.1002,  0.6820,  0.0470]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44610242389768806, distance: 0.8516701491612767 entropy 0.03264415264129639
epoch: 62, step: 16
	action: tensor([[ 1.1885,  0.3801, -0.1649,  0.6632, -0.0219,  0.2695,  0.3743]],
       dtype=torch.float64)
	q_value: tensor([[-3.8629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.940815801310289, distance: 0.27839381295386567 entropy 0.03264415264129639
epoch: 62, step: 17
	action: tensor([[ 1.4885,  0.1247, -0.3771,  0.7098, -0.1585,  0.4486, -0.5209]],
       dtype=torch.float64)
	q_value: tensor([[-8.8317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9206166445772598, distance: 0.32241958842545004 entropy 0.03264415264129639
epoch: 62, step: 18
	action: tensor([[ 1.6095,  0.0528, -0.7021,  0.8591, -0.3504,  0.3302,  0.0937]],
       dtype=torch.float64)
	q_value: tensor([[-13.8012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9335913185464946, distance: 0.2948961630949933 entropy 0.03264415264129639
epoch: 62, step: 19
	action: tensor([[ 1.2977,  0.0453, -0.4463,  0.9325, -0.4457,  0.3974, -0.3663]],
       dtype=torch.float64)
	q_value: tensor([[-13.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09079917257873735 entropy 0.03264415264129639
epoch: 62, step: 20
	action: tensor([[ 0.7969,  0.4782, -0.2017,  0.5206, -0.3452,  0.5657, -0.2723]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9170355010940857, distance: 0.3296118679923228 entropy 0.03264415264129639
epoch: 62, step: 21
	action: tensor([[ 1.3464,  0.3999, -0.5757,  0.5240, -0.2283,  0.0075,  0.3182]],
       dtype=torch.float64)
	q_value: tensor([[-9.9870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9427685375394783, distance: 0.27376259894127464 entropy 0.03264415264129639
epoch: 62, step: 22
	action: tensor([[ 1.0141,  0.2972, -1.0102,  0.1507, -0.3275,  0.0867,  0.0457]],
       dtype=torch.float64)
	q_value: tensor([[-10.8007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9468254830966848, distance: 0.26388121178207735 entropy 0.03264415264129639
epoch: 62, step: 23
	action: tensor([[ 1.3405,  0.3856, -0.7914,  0.2875, -0.4426,  0.0388,  0.0383]],
       dtype=torch.float64)
	q_value: tensor([[-10.1526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9087354548693979, distance: 0.34570665506830706 entropy 0.03264415264129639
epoch: 62, step: 24
	action: tensor([[ 1.3208,  0.1722, -0.2257,  0.7980, -0.4094,  0.6157, -0.1648]],
       dtype=torch.float64)
	q_value: tensor([[-12.2035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9638088210218633, distance: 0.21769981441511058 entropy 0.03264415264129639
epoch: 62, step: 25
	action: tensor([[ 1.0367, -0.1230, -0.6919,  1.0097,  0.5092,  0.3047, -0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-12.4992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9751724247376538, distance: 0.1803116746048077 entropy 0.03264415264129639
epoch: 62, step: 26
	action: tensor([[ 1.5633,  0.3226, -0.8079,  1.3380, -0.2088,  0.2838,  0.2144]],
       dtype=torch.float64)
	q_value: tensor([[-8.6759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9491586590933784, distance: 0.2580270228104688 entropy 0.03264415264129639
epoch: 62, step: 27
	action: tensor([[ 1.4676,  0.4313, -0.9017,  0.9062, -0.7236, -0.1452, -0.1435]],
       dtype=torch.float64)
	q_value: tensor([[-14.3462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9775926813163904, distance: 0.1712977421783812 entropy 0.03264415264129639
epoch: 62, step: 28
	action: tensor([[ 1.1054, -0.0257, -0.3751,  0.5429, -0.0372,  0.5086, -0.2808]],
       dtype=torch.float64)
	q_value: tensor([[-15.2654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9763732716618785, distance: 0.1758970221618026 entropy 0.03264415264129639
epoch: 62, step: 29
	action: tensor([[ 1.3299, -0.3110, -0.5967,  0.6278, -0.0285,  0.0917,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-9.6844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7566270528625092, distance: 0.56453756266185 entropy 0.03264415264129639
epoch: 62, step: 30
	action: tensor([[ 1.2871, -0.2280, -0.7623,  1.2415, -0.5617,  0.4564, -0.1778]],
       dtype=torch.float64)
	q_value: tensor([[-9.2567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9493529333908448, distance: 0.2575335661417337 entropy 0.03264415264129639
epoch: 62, step: 31
	action: tensor([[ 1.3970,  0.3644, -0.5619,  1.1645,  0.0634,  0.0781, -0.3906]],
       dtype=torch.float64)
	q_value: tensor([[-12.9760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9318077899721737, distance: 0.2988299183041451 entropy 0.03264415264129639
epoch: 62, step: 32
	action: tensor([[ 1.4873, -0.5189, -0.6736,  0.9544, -0.4344,  0.0313, -0.0534]],
       dtype=torch.float64)
	q_value: tensor([[-13.6413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.736619838495762, distance: 0.5872840730636167 entropy 0.03264415264129639
epoch: 62, step: 33
	action: tensor([[ 1.5105,  0.1198, -0.7012,  0.5552, -0.0532, -0.1887, -0.2293]],
       dtype=torch.float64)
	q_value: tensor([[-11.3415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7960290093032997, distance: 0.5168218852244401 entropy 0.03264415264129639
epoch: 62, step: 34
	action: tensor([[ 1.1613, -0.1589, -0.5464,  0.5380, -0.0306,  0.0423, -0.4958]],
       dtype=torch.float64)
	q_value: tensor([[-12.1351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8525196017673295, distance: 0.4394645461695184 entropy 0.03264415264129639
epoch: 62, step: 35
	action: tensor([[ 1.4678,  0.4125, -0.6219,  1.0133, -0.2140,  0.5122, -0.5092]],
       dtype=torch.float64)
	q_value: tensor([[-9.8653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9484640461348732, distance: 0.2597836728543611 entropy 0.03264415264129639
epoch: 62, step: 36
	action: tensor([[ 1.7304,  0.1799, -0.1488,  1.5159, -0.2630,  0.2485, -0.5953]],
       dtype=torch.float64)
	q_value: tensor([[-15.8770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 62, step: 37
	action: tensor([[ 1.2309, -0.2050, -0.4886,  0.1244, -0.1523,  0.4175, -0.2358]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6776927702967495, distance: 0.6496683608389703 entropy 0.03264415264129639
epoch: 62, step: 38
	action: tensor([[ 0.9233,  0.3950, -0.3973,  0.6984,  0.0850,  0.0882,  0.3666]],
       dtype=torch.float64)
	q_value: tensor([[-9.4544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9650692011998183, distance: 0.21387545766639277 entropy 0.03264415264129639
epoch: 62, step: 39
	action: tensor([[ 1.3326,  0.2497, -0.2365,  0.6387,  0.2047,  0.0765, -0.2788]],
       dtype=torch.float64)
	q_value: tensor([[-7.4379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9236778454303831, distance: 0.3161418601094091 entropy 0.03264415264129639
epoch: 62, step: 40
	action: tensor([[ 1.6304e+00,  2.3943e-01, -9.0054e-01,  1.0376e+00,  1.3491e-03,
          5.6270e-01, -5.7099e-01]], dtype=torch.float64)
	q_value: tensor([[-10.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9846686081116063, distance: 0.14169270316614754 entropy 0.03264415264129639
epoch: 62, step: 41
	action: tensor([[ 1.9430, -0.0671, -0.5533,  0.8146, -0.5323,  0.3452, -0.4457]],
       dtype=torch.float64)
	q_value: tensor([[-16.5868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 62, step: 42
	action: tensor([[ 0.4010,  0.3087, -0.4392,  1.1151, -0.2502, -0.0564,  0.4021]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6989556031613924, distance: 0.6278732308245439 entropy 0.03264415264129639
epoch: 62, step: 43
	action: tensor([[ 1.3431, -0.1173, -0.6787,  0.5386, -0.0192,  0.1600, -0.2418]],
       dtype=torch.float64)
	q_value: tensor([[-6.1352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8351190281142364, distance: 0.46466714127348824 entropy 0.03264415264129639
epoch: 62, step: 44
	action: tensor([[ 1.3302,  0.0709, -0.1104,  0.8761, -0.1332,  0.3087,  0.3328]],
       dtype=torch.float64)
	q_value: tensor([[-10.7095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9279129643307743, distance: 0.30724532028585305 entropy 0.03264415264129639
epoch: 62, step: 45
	action: tensor([[ 0.9733,  0.4850, -0.3337,  0.7107, -0.0435,  0.5650,  0.2518]],
       dtype=torch.float64)
	q_value: tensor([[-9.4970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8854739644425611, distance: 0.3872654981234027 entropy 0.03264415264129639
epoch: 62, step: 46
	action: tensor([[ 0.7430,  0.1575, -0.2300,  1.2672, -0.4879,  0.8884,  0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-9.2805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7575091976007134, distance: 0.5635135048527732 entropy 0.03264415264129639
epoch: 62, step: 47
	action: tensor([[ 0.9822,  0.4363, -0.5700,  0.3696, -0.3250,  0.6730, -0.2698]],
       dtype=torch.float64)
	q_value: tensor([[-10.2710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9738291935857993, distance: 0.18512507426867245 entropy 0.03264415264129639
epoch: 62, step: 48
	action: tensor([[ 1.5085,  0.0585, -0.7524,  0.1941, -0.1498,  0.4871,  0.1899]],
       dtype=torch.float64)
	q_value: tensor([[-11.8215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.755208479205792, distance: 0.566180462027354 entropy 0.03264415264129639
epoch: 62, step: 49
	action: tensor([[ 1.1380,  0.2586, -0.2978,  0.6791, -0.3253, -0.0085, -0.2627]],
       dtype=torch.float64)
	q_value: tensor([[-11.5778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.978672296817223, distance: 0.1671201194135977 entropy 0.03264415264129639
epoch: 62, step: 50
	action: tensor([[ 1.5180,  0.4358, -0.6862,  1.0326, -0.3562,  0.2618, -0.1661]],
       dtype=torch.float64)
	q_value: tensor([[-10.6057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9480528127551927, distance: 0.26081809095247716 entropy 0.03264415264129639
epoch: 62, step: 51
	action: tensor([[ 1.4000, -0.3176, -0.4816,  0.6652, -0.5585,  0.7612,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[-15.1825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8755315889956452, distance: 0.4037255788667599 entropy 0.03264415264129639
epoch: 62, step: 52
	action: tensor([[ 0.8227, -0.0892, -0.1966,  0.7444, -0.0036,  0.1441,  0.0202]],
       dtype=torch.float64)
	q_value: tensor([[-11.8145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9700442876380488, distance: 0.1980598834571723 entropy 0.03264415264129639
epoch: 62, step: 53
	action: tensor([[ 1.0370,  0.2228, -0.3107,  0.7172,  0.1466,  0.6015,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-6.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9734232363140078, distance: 0.18655536395577554 entropy 0.03264415264129639
epoch: 62, step: 54
	action: tensor([[ 1.9972,  0.2275, -0.4125,  0.6679, -0.0958,  0.3218,  0.2115]],
       dtype=torch.float64)
	q_value: tensor([[-9.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 62, step: 55
	action: tensor([[ 1.4482,  0.0140, -0.1744,  0.5350, -0.3455,  0.2197,  0.1878]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8131295268481353, distance: 0.4946830569306421 entropy 0.03264415264129639
epoch: 62, step: 56
	action: tensor([[ 1.1115,  0.0385, -0.3075,  0.2027, -0.2067, -0.1772,  0.1156]],
       dtype=torch.float64)
	q_value: tensor([[-10.3157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7743953221870143, distance: 0.543539040543028 entropy 0.03264415264129639
epoch: 62, step: 57
	action: tensor([[ 0.9026, -0.1808, -0.7684,  0.8577,  0.4074,  0.3082,  0.0718]],
       dtype=torch.float64)
	q_value: tensor([[-7.4585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9094309180902168, distance: 0.3443869418815671 entropy 0.03264415264129639
epoch: 62, step: 58
	action: tensor([[ 1.4967, -0.0351, -0.4237,  0.6426, -0.2527, -0.0036, -0.1396]],
       dtype=torch.float64)
	q_value: tensor([[-7.2456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7875214035749449, distance: 0.5274900690896082 entropy 0.03264415264129639
epoch: 62, step: 59
	action: tensor([[ 1.5915, -0.1754, -1.0466,  0.7006,  0.0381,  0.1672, -0.5788]],
       dtype=torch.float64)
	q_value: tensor([[-11.5695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7604365129144528, distance: 0.560101848917643 entropy 0.03264415264129639
epoch: 62, step: 60
	action: tensor([[ 1.3692,  0.1247, -0.6716,  1.4885, -0.3242, -0.1284,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-14.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9697635552331999, distance: 0.1989857864087145 entropy 0.03264415264129639
epoch: 62, step: 61
	action: tensor([[ 1.5162,  0.2337, -0.4991,  0.9515, -0.4326,  0.2360,  0.1160]],
       dtype=torch.float64)
	q_value: tensor([[-12.9151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.950405314514264, distance: 0.2548439122123816 entropy 0.03264415264129639
epoch: 62, step: 62
	action: tensor([[ 0.8876, -0.4707, -0.8220,  0.9540, -0.5498,  0.2702,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-13.2213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7255878703630597, distance: 0.5994574271724321 entropy 0.03264415264129639
epoch: 62, step: 63
	action: tensor([[ 1.2140,  0.4135, -0.4609,  0.7402, -0.2517, -0.1240,  0.3652]],
       dtype=torch.float64)
	q_value: tensor([[-8.4221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9716046205737918, distance: 0.19283263303705836 entropy 0.03264415264129639
epoch: 62, step: 64
	action: tensor([[ 1.3461, -0.6631, -0.5789,  1.2326,  0.0157, -0.0161,  0.1068]],
       dtype=torch.float64)
	q_value: tensor([[-9.8069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8134629921706338, distance: 0.49424148555883896 entropy 0.03264415264129639
epoch: 62, step: 65
	action: tensor([[ 1.5654, -0.1171, -0.6217,  0.9279, -0.5486,  0.1629,  0.0741]],
       dtype=torch.float64)
	q_value: tensor([[-9.1041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8929876861782263, distance: 0.3743463220204708 entropy 0.03264415264129639
epoch: 62, step: 66
	action: tensor([[ 1.1212, -0.3282, -0.4827,  0.8045, -0.4769, -0.0714, -0.1378]],
       dtype=torch.float64)
	q_value: tensor([[-12.8597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8566953611155909, distance: 0.4331983739681964 entropy 0.03264415264129639
epoch: 62, step: 67
	action: tensor([[ 1.1515,  0.1092, -0.6708,  0.5647, -0.1415,  0.4816, -0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-9.3004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9907301428395098, distance: 0.11017757626477707 entropy 0.03264415264129639
epoch: 62, step: 68
	action: tensor([[ 1.1042,  0.0363, -0.9393,  0.9831, -0.6148,  0.0773,  0.1069]],
       dtype=torch.float64)
	q_value: tensor([[-10.6274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9492822203791067, distance: 0.2577132865405193 entropy 0.03264415264129639
epoch: 62, step: 69
	action: tensor([[ 1.2165, -0.3157, -0.4878,  0.8085, -0.6455,  0.6050,  0.1634]],
       dtype=torch.float64)
	q_value: tensor([[-11.2060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9222322010857708, distance: 0.31912189105117333 entropy 0.03264415264129639
epoch: 62, step: 70
	action: tensor([[ 1.1817, -0.0565, -0.4709,  0.5776,  0.1560, -0.0589, -0.4108]],
       dtype=torch.float64)
	q_value: tensor([[-10.4748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8859312626235164, distance: 0.38649155643825917 entropy 0.03264415264129639
epoch: 62, step: 71
	action: tensor([[ 1.4219,  0.2371, -0.5991,  0.8181, -0.1243,  0.4237, -0.1741]],
       dtype=torch.float64)
	q_value: tensor([[-9.3771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9763926589825668, distance: 0.17582483967881066 entropy 0.03264415264129639
epoch: 62, step: 72
	action: tensor([[ 1.7756, -0.1901, -0.4447,  0.6968, -0.0307,  0.6574, -0.3982]],
       dtype=torch.float64)
	q_value: tensor([[-13.1650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 62, step: 73
	action: tensor([[ 1.0273,  0.1267, -0.3557,  0.4225, -0.6052, -0.1493, -0.0616]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9304904610493129, distance: 0.3017024918910138 entropy 0.03264415264129639
epoch: 62, step: 74
	action: tensor([[ 1.1446, -0.1966,  0.1332,  0.3749, -0.4305,  0.1385, -0.5078]],
       dtype=torch.float64)
	q_value: tensor([[-9.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7494818310349938, distance: 0.5727647837403393 entropy 0.03264415264129639
epoch: 62, step: 75
	action: tensor([[ 1.5766,  0.6268, -0.5006,  1.2270, -0.6911,  0.6169, -0.3835]],
       dtype=torch.float64)
	q_value: tensor([[-9.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8528109392742064, distance: 0.43903026541361423 entropy 0.03264415264129639
epoch: 62, step: 76
	action: tensor([[ 1.7124, -0.2230, -0.8321,  1.3739, -0.4619, -0.0861, -0.0626]],
       dtype=torch.float64)
	q_value: tensor([[-18.3517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 62, step: 77
	action: tensor([[ 0.9751,  0.0099, -0.0050,  0.4462, -0.2602,  0.3730, -0.3650]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9587464783816377, distance: 0.23242733751443975 entropy 0.03264415264129639
epoch: 62, step: 78
	action: tensor([[ 1.2433, -0.0736, -0.3909,  0.6647, -0.0062,  0.1307, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-8.7158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9209633336264561, distance: 0.32171477033101686 entropy 0.03264415264129639
epoch: 62, step: 79
	action: tensor([[ 1.4202,  0.1485, -0.1926,  0.6726, -0.0921,  0.3443, -0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-9.1377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9005051494075433, distance: 0.36095826906840944 entropy 0.03264415264129639
epoch: 62, step: 80
	action: tensor([[ 1.2161,  0.3343, -0.7966,  1.3950, -0.0919,  0.4024, -0.0786]],
       dtype=torch.float64)
	q_value: tensor([[-11.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8225934846538885, distance: 0.4819938268210468 entropy 0.03264415264129639
epoch: 62, step: 81
	action: tensor([[ 1.3905,  0.2097, -0.8976,  0.7603,  0.4339,  0.5457,  0.1641]],
       dtype=torch.float64)
	q_value: tensor([[-13.2873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.086093591454244 entropy 0.03264415264129639
epoch: 62, step: 82
	action: tensor([[ 1.0518,  0.0494, -0.7871,  0.3406, -0.3446, -0.1943,  0.2057]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8533540511002293, distance: 0.4382195295765585 entropy 0.03264415264129639
epoch: 62, step: 83
	action: tensor([[ 0.8002, -0.1421, -0.6907,  0.5521, -0.0453,  0.5671,  0.4519]],
       dtype=torch.float64)
	q_value: tensor([[-8.3816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.867019182139432, distance: 0.4173026984858558 entropy 0.03264415264129639
epoch: 62, step: 84
	action: tensor([[ 1.2915,  0.1878, -0.4241,  0.6679,  0.3660,  0.0919,  0.3050]],
       dtype=torch.float64)
	q_value: tensor([[-6.7562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9663330717826155, distance: 0.20997057608038966 entropy 0.03264415264129639
epoch: 62, step: 85
	action: tensor([[ 1.4690,  0.2356, -0.7254,  0.8089, -0.6960,  0.0021, -0.2673]],
       dtype=torch.float64)
	q_value: tensor([[-8.6455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9602647866274339, distance: 0.22811007536597233 entropy 0.03264415264129639
epoch: 62, step: 86
	action: tensor([[ 1.3846,  0.1834, -0.3590,  1.0983, -0.0190, -0.2279,  0.1181]],
       dtype=torch.float64)
	q_value: tensor([[-14.6195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9481230710890014, distance: 0.2606416536354362 entropy 0.03264415264129639
epoch: 62, step: 87
	action: tensor([[ 1.5643,  0.4999, -0.9033,  1.5057, -0.1607, -0.2742, -0.2761]],
       dtype=torch.float64)
	q_value: tensor([[-10.6794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.930996739943065, distance: 0.3006017454486712 entropy 0.03264415264129639
epoch: 62, step: 88
	action: tensor([[ 1.5750,  0.0130, -1.1708,  1.0905, -0.3153,  0.3311,  0.3150]],
       dtype=torch.float64)
	q_value: tensor([[-16.2079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9411160735548195, distance: 0.277686696564327 entropy 0.03264415264129639
epoch: 62, step: 89
	action: tensor([[ 0.7625, -0.1511, -0.4481,  1.0479, -0.1017, -0.0101, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-13.8024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9222582024518262, distance: 0.3190685379994893 entropy 0.03264415264129639
epoch: 62, step: 90
	action: tensor([[ 1.0338,  0.4496, -0.5660,  0.7023, -0.0899, -0.1092,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[-7.0121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9775178974297681, distance: 0.17158335511546677 entropy 0.03264415264129639
epoch: 62, step: 91
	action: tensor([[ 0.7521, -0.0659, -0.2730,  0.8672, -0.4292, -0.0069, -0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-9.7586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9640903109203961, distance: 0.2168515424956944 entropy 0.03264415264129639
epoch: 62, step: 92
	action: tensor([[ 1.6008, -0.0249, -0.9013,  0.6435, -0.0561,  0.2624,  0.1092]],
       dtype=torch.float64)
	q_value: tensor([[-7.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8303965873288701, distance: 0.47127454911970695 entropy 0.03264415264129639
epoch: 62, step: 93
	action: tensor([[ 0.8560,  0.3871, -0.7415,  0.9855, -0.0990,  0.4520, -0.1957]],
       dtype=torch.float64)
	q_value: tensor([[-12.5051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8034934324749783, distance: 0.5072770657755783 entropy 0.03264415264129639
epoch: 62, step: 94
	action: tensor([[ 1.5976, -0.1438, -0.7453,  0.5911, -0.6211,  0.0654, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-10.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7078997103940847, distance: 0.6184757658538765 entropy 0.03264415264129639
epoch: 62, step: 95
	action: tensor([[ 0.8680, -0.0401, -0.6346,  0.2293, -0.4746,  0.4057, -0.2837]],
       dtype=torch.float64)
	q_value: tensor([[-13.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8488213171682337, distance: 0.4449405341580729 entropy 0.03264415264129639
epoch: 62, step: 96
	action: tensor([[ 1.0684, -0.0097,  0.2338,  1.1418, -0.1477,  0.3411, -0.0815]],
       dtype=torch.float64)
	q_value: tensor([[-9.2646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9173871029381605, distance: 0.32891268225535486 entropy 0.03264415264129639
epoch: 62, step: 97
	action: tensor([[ 1.3898, -0.4137, -0.6754,  1.1246, -0.1666,  0.1549, -0.5916]],
       dtype=torch.float64)
	q_value: tensor([[-9.1092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9150211953781346, distance: 0.3335892155628276 entropy 0.03264415264129639
epoch: 62, step: 98
	action: tensor([[ 1.4950, -0.0287, -0.6276,  1.2194, -0.4823,  0.1657,  0.1402]],
       dtype=torch.float64)
	q_value: tensor([[-12.7719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9846399465621795, distance: 0.14182508632409546 entropy 0.03264415264129639
epoch: 62, step: 99
	action: tensor([[ 1.1417,  0.3455, -0.3031,  1.0609, -0.2093,  0.5065, -0.3915]],
       dtype=torch.float64)
	q_value: tensor([[-12.8337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8790868073975294, distance: 0.3979179563937518 entropy 0.03264415264129639
epoch: 62, step: 100
	action: tensor([[ 1.7283, -0.3353, -0.9651,  0.9574, -0.0302,  0.4274, -0.2885]],
       dtype=torch.float64)
	q_value: tensor([[-12.6318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 62, step: 101
	action: tensor([[ 0.5835, -0.3921, -0.6411,  0.6497, -0.1255, -0.2973, -0.1228]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5741275251296489, distance: 0.7467865354568198 entropy 0.03264415264129639
epoch: 62, step: 102
	action: tensor([[ 1.1959,  0.0392, -0.7575,  0.6349, -0.1777,  0.1849, -0.2468]],
       dtype=torch.float64)
	q_value: tensor([[-5.2488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9529884941710414, distance: 0.24811828716297066 entropy 0.03264415264129639
epoch: 62, step: 103
	action: tensor([[ 1.3289,  0.1818, -0.8807,  0.7906, -0.2517,  0.3722,  0.1093]],
       dtype=torch.float64)
	q_value: tensor([[-11.0606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0723419266911159 entropy 0.03264415264129639
epoch: 62, step: 104
	action: tensor([[ 1.0293e+00, -6.8852e-04, -2.5871e-01,  6.8324e-01, -2.8549e-01,
          4.6050e-01,  3.9340e-02]], dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08735968030480736 entropy 0.03264415264129639
epoch: 62, step: 105
	action: tensor([[ 0.8715,  0.0077, -0.2009,  0.6476, -0.0063,  0.1186,  0.0678]],
       dtype=torch.float64)
	q_value: tensor([[-8.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9742698342661884, distance: 0.18355997297137067 entropy 0.03264415264129639
epoch: 62, step: 106
	action: tensor([[ 1.1248, -0.4301, -0.6708,  0.9582, -0.7984,  0.3613, -0.1169]],
       dtype=torch.float64)
	q_value: tensor([[-6.4441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8517680286275001, distance: 0.4405828982526792 entropy 0.03264415264129639
epoch: 62, step: 107
	action: tensor([[ 1.2839,  0.2379, -0.3009,  1.1479, -0.4193, -0.0241, -0.1517]],
       dtype=torch.float64)
	q_value: tensor([[-10.9493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9454925848053573, distance: 0.26716802939214046 entropy 0.03264415264129639
epoch: 62, step: 108
	action: tensor([[ 1.1930,  0.1085, -0.9447,  0.6811, -0.0085,  0.3333, -0.0911]],
       dtype=torch.float64)
	q_value: tensor([[-12.1497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9773088301542806, distance: 0.17237930943070878 entropy 0.03264415264129639
epoch: 62, step: 109
	action: tensor([[ 1.6929,  0.3064, -0.6979,  0.8525,  0.0784, -0.1683, -0.1676]],
       dtype=torch.float64)
	q_value: tensor([[-11.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.904141812305845, distance: 0.3543001206519863 entropy 0.03264415264129639
epoch: 62, step: 110
	action: tensor([[ 0.9957,  0.2595, -1.0343,  1.1455, -0.4933,  0.5679,  0.0407]],
       dtype=torch.float64)
	q_value: tensor([[-13.4033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7802695524181481, distance: 0.5364161113040374 entropy 0.03264415264129639
epoch: 62, step: 111
	action: tensor([[ 1.1266,  0.2015, -0.8606,  0.7924, -0.4643,  0.3054, -0.3603]],
       dtype=torch.float64)
	q_value: tensor([[-12.7757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9754442492869437, distance: 0.17932188736953808 entropy 0.03264415264129639
epoch: 62, step: 112
	action: tensor([[ 1.2901, -0.1032, -0.4457,  0.5512, -0.3117,  0.6169, -0.0688]],
       dtype=torch.float64)
	q_value: tensor([[-13.0151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9383173946506614, distance: 0.2842091448265782 entropy 0.03264415264129639
epoch: 62, step: 113
	action: tensor([[ 1.2435, -0.0330, -0.2206,  1.0582, -0.0331,  0.4293,  0.1117]],
       dtype=torch.float64)
	q_value: tensor([[-10.9347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9667381981846045, distance: 0.20870342633890282 entropy 0.03264415264129639
epoch: 62, step: 114
	action: tensor([[ 1.5568,  0.3559, -0.6653,  1.1511, -0.3021, -0.0157,  0.0880]],
       dtype=torch.float64)
	q_value: tensor([[-9.7227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.946878961358106, distance: 0.2637484841226187 entropy 0.03264415264129639
epoch: 62, step: 115
	action: tensor([[ 1.3263,  0.2969, -0.7195,  0.5441,  0.1653,  0.2289, -0.4485]],
       dtype=torch.float64)
	q_value: tensor([[-13.9502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9709518444079065, distance: 0.19503653573319704 entropy 0.03264415264129639
epoch: 62, step: 116
	action: tensor([[ 1.2404, -0.2832, -0.7429,  0.8030, -0.1621, -0.0626,  0.1280]],
       dtype=torch.float64)
	q_value: tensor([[-12.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8369182690239849, distance: 0.46212487861725426 entropy 0.03264415264129639
epoch: 62, step: 117
	action: tensor([[ 1.2957, -0.1377, -0.5426,  1.0879, -0.1600,  0.9126, -0.3444]],
       dtype=torch.float64)
	q_value: tensor([[-9.1159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9681600235284854, distance: 0.20419404026227866 entropy 0.03264415264129639
epoch: 62, step: 118
	action: tensor([[ 1.3919,  0.0869, -0.7483,  1.1161, -0.7273,  0.5778, -0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-13.1340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9789016819990776, distance: 0.16621897895339577 entropy 0.03264415264129639
epoch: 62, step: 119
	action: tensor([[ 1.6357, -0.0747, -0.2833,  0.8792, -0.3245,  0.3010, -0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-14.5663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8318845254673732, distance: 0.4692027389997309 entropy 0.03264415264129639
epoch: 62, step: 120
	action: tensor([[ 1.4929, -0.4037, -0.3791,  0.7932, -0.3106,  0.4321,  0.4079]],
       dtype=torch.float64)
	q_value: tensor([[-12.9858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7837328328075155, distance: 0.5321719613703217 entropy 0.03264415264129639
epoch: 62, step: 121
	action: tensor([[ 0.9661, -0.1470, -0.8599,  0.6877, -0.4545,  0.1356,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[-9.9171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8588515103730768, distance: 0.42992708974983 entropy 0.03264415264129639
epoch: 62, step: 122
	action: tensor([[ 1.7010,  0.1119, -0.7647,  0.6413,  0.0209,  0.2360, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-9.0855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8678937592273968, distance: 0.4159281941354008 entropy 0.03264415264129639
epoch: 62, step: 123
	action: tensor([[ 1.5500e+00,  5.6858e-01, -2.1069e-01,  4.5107e-01, -4.4235e-01,
          5.6925e-01,  8.7163e-04]], dtype=torch.float64)
	q_value: tensor([[-12.9087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.853633209358102, distance: 0.4378022290116394 entropy 0.03264415264129639
epoch: 62, step: 124
	action: tensor([[ 0.9250,  0.1943, -0.5783,  1.0130, -0.4635,  0.0770, -0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-14.1663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9430157881603145, distance: 0.2731706060569802 entropy 0.03264415264129639
epoch: 62, step: 125
	action: tensor([[ 1.4096,  0.0626, -0.7674,  0.3594,  0.1930,  0.0051, -0.2958]],
       dtype=torch.float64)
	q_value: tensor([[-10.1367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7794404169002273, distance: 0.5374272203165679 entropy 0.03264415264129639
epoch: 62, step: 126
	action: tensor([[ 1.2523, -0.2956, -0.3948,  1.1336, -0.0207, -0.0646, -0.2395]],
       dtype=torch.float64)
	q_value: tensor([[-11.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9490708135432178, distance: 0.2582498409227087 entropy 0.03264415264129639
epoch: 62, step: 127
	action: tensor([[ 1.4941, -0.0101, -0.5471,  1.1471,  0.2781,  0.2060,  0.1381]],
       dtype=torch.float64)
	q_value: tensor([[-9.9932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9603614814663894, distance: 0.22783235567684437 entropy 0.03264415264129639
LOSS epoch 62 actor 300.69708836698345 critic 1579.595891736306 
epoch: 63, step: 0
	action: tensor([[ 1.1456, -0.3814, -0.1950,  0.6420, -0.2423,  0.0577, -0.2440]],
       dtype=torch.float64)
	q_value: tensor([[-13.2101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7671222057231868, distance: 0.5522309367446647 entropy 0.03264415264129639
epoch: 63, step: 1
	action: tensor([[ 0.9981,  0.0241, -0.6205,  0.7919, -0.2359,  0.6120,  0.2864]],
       dtype=torch.float64)
	q_value: tensor([[-10.1060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9659630344019795, distance: 0.21112132901693606 entropy 0.03264415264129639
epoch: 63, step: 2
	action: tensor([[ 1.0280,  0.3746, -0.2618,  0.8533, -0.1700,  0.1248,  0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-10.8387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9444296129323889, distance: 0.2697605287389923 entropy 0.03264415264129639
epoch: 63, step: 3
	action: tensor([[ 1.3049,  0.2433, -0.0266,  0.1814, -0.2590,  0.2657, -0.2535]],
       dtype=torch.float64)
	q_value: tensor([[-11.0298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8515810060233174, distance: 0.4408607498731841 entropy 0.03264415264129639
epoch: 63, step: 4
	action: tensor([[ 1.0773, -0.0708, -0.4424,  0.4113, -0.2333,  0.1668, -0.1592]],
       dtype=torch.float64)
	q_value: tensor([[-12.4333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8873125370526828, distance: 0.38414438886684876 entropy 0.03264415264129639
epoch: 63, step: 5
	action: tensor([[ 1.0581,  0.2419, -0.3547,  0.8688, -0.2754,  0.2370, -0.3146]],
       dtype=torch.float64)
	q_value: tensor([[-10.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9774023794250738, distance: 0.17202360688545817 entropy 0.03264415264129639
epoch: 63, step: 6
	action: tensor([[ 0.5328,  0.3375, -0.4081,  0.3853,  0.0241,  0.3306,  0.4519]],
       dtype=torch.float64)
	q_value: tensor([[-12.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8989166837789784, distance: 0.36382826375906724 entropy 0.03264415264129639
epoch: 63, step: 7
	action: tensor([[ 0.6156,  0.2634, -0.4027,  0.8044, -0.0792,  0.3893, -0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-6.2914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8653019359886429, distance: 0.41998847224636027 entropy 0.03264415264129639
epoch: 63, step: 8
	action: tensor([[ 1.1008,  0.3115, -0.4132,  0.6574, -0.1291, -0.0531,  0.3033]],
       dtype=torch.float64)
	q_value: tensor([[-9.4602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.98741600390521, distance: 0.12837072107236763 entropy 0.03264415264129639
epoch: 63, step: 9
	action: tensor([[ 0.6931,  0.0232, -0.1757,  0.9924, -0.6148, -0.0394,  0.0405]],
       dtype=torch.float64)
	q_value: tensor([[-10.1356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9735050639127566, distance: 0.18626794849069492 entropy 0.03264415264129639
epoch: 63, step: 10
	action: tensor([[ 0.9638, -0.2874, -0.7146,  0.6351, -0.1805,  0.4386,  0.4956]],
       dtype=torch.float64)
	q_value: tensor([[-9.0699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8395678686286018, distance: 0.45835541861025036 entropy 0.03264415264129639
epoch: 63, step: 11
	action: tensor([[ 1.0457,  0.1201, -0.5494,  1.1687,  0.0064,  0.5350, -0.0770]],
       dtype=torch.float64)
	q_value: tensor([[-8.6576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9125529111912862, distance: 0.33839923397921595 entropy 0.03264415264129639
epoch: 63, step: 12
	action: tensor([[ 1.3962, -0.1307, -0.5710,  0.6673, -0.3308,  0.3977,  0.0862]],
       dtype=torch.float64)
	q_value: tensor([[-12.6284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9040063327003757, distance: 0.35455040439682145 entropy 0.03264415264129639
epoch: 63, step: 13
	action: tensor([[ 0.8267, -0.2796,  0.2983,  0.7256, -0.1154,  0.3325, -0.5075]],
       dtype=torch.float64)
	q_value: tensor([[-13.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9196652311209962, distance: 0.3243459411463226 entropy 0.03264415264129639
epoch: 63, step: 14
	action: tensor([[ 1.2263,  0.2034, -0.7315,  0.6890, -0.2953,  0.5553, -0.0813]],
       dtype=torch.float64)
	q_value: tensor([[-8.9683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.04961719448661748 entropy 0.03264415264129639
epoch: 63, step: 15
	action: tensor([[ 0.3251, -0.1283, -0.3327,  0.3155, -0.2828,  0.2183, -0.6994]],
       dtype=torch.float64)
	q_value: tensor([[-9.8541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6158352487465056, distance: 0.709276303002845 entropy 0.03264415264129639
epoch: 63, step: 16
	action: tensor([[ 1.0680, -0.2643, -0.6205,  0.3304, -0.6100,  0.5656, -0.2718]],
       dtype=torch.float64)
	q_value: tensor([[-7.2380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7694347959731299, distance: 0.5494821340057142 entropy 0.03264415264129639
epoch: 63, step: 17
	action: tensor([[ 1.3586,  0.1149, -0.7614,  0.4535, -0.1931,  0.2064,  0.1455]],
       dtype=torch.float64)
	q_value: tensor([[-12.4524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9028326647949223, distance: 0.35671127719793005 entropy 0.03264415264129639
epoch: 63, step: 18
	action: tensor([[ 1.1642, -0.0684, -0.2096,  0.3481, -0.2783,  0.7601, -0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-12.9563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9367152225689168, distance: 0.28787657152049384 entropy 0.03264415264129639
epoch: 63, step: 19
	action: tensor([[ 0.8435,  0.2669, -0.7650,  0.6275, -0.0060, -0.0535,  0.3705]],
       dtype=torch.float64)
	q_value: tensor([[-11.6434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9582723382221595, distance: 0.23375920310008774 entropy 0.03264415264129639
epoch: 63, step: 20
	action: tensor([[ 0.8134, -0.2860, -0.1975,  0.5425, -0.0287,  0.0173,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-8.8689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8027821040902088, distance: 0.5081943751077873 entropy 0.03264415264129639
epoch: 63, step: 21
	action: tensor([[ 0.9925, -0.2401, -0.5007,  0.8441,  0.0144, -0.1186, -0.1866]],
       dtype=torch.float64)
	q_value: tensor([[-6.4443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9038399564885297, distance: 0.35485752473690224 entropy 0.03264415264129639
epoch: 63, step: 22
	action: tensor([[ 1.1106,  0.0486, -0.6442,  1.1138, -0.2161,  0.0252, -0.1461]],
       dtype=torch.float64)
	q_value: tensor([[-9.4695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9870920689035596, distance: 0.13001247099326418 entropy 0.03264415264129639
epoch: 63, step: 23
	action: tensor([[ 1.3295,  0.1011, -0.3188,  1.1018,  0.1678,  0.5306,  0.1167]],
       dtype=torch.float64)
	q_value: tensor([[-12.7682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9286451232673859, distance: 0.30568105486835156 entropy 0.03264415264129639
epoch: 63, step: 24
	action: tensor([[ 1.1438, -0.0543, -0.3623,  0.8692, -0.5316,  0.5064,  0.1005]],
       dtype=torch.float64)
	q_value: tensor([[-12.7096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.050254686635314544 entropy 0.03264415264129639
epoch: 63, step: 25
	action: tensor([[ 0.6889,  0.1873, -0.1455,  0.5830,  0.0069,  0.0607, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[-9.8541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.962441815091162, distance: 0.221773163296714 entropy 0.03264415264129639
epoch: 63, step: 26
	action: tensor([[ 1.1749,  0.2027, -0.8147,  0.5516, -0.3001,  0.5401, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-7.0017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.991490332717956, distance: 0.10556330610007437 entropy 0.03264415264129639
epoch: 63, step: 27
	action: tensor([[ 1.3669,  0.2688, -0.6917, -0.2248, -0.0861,  0.5086, -0.1458]],
       dtype=torch.float64)
	q_value: tensor([[-13.8129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7506428688950899, distance: 0.5714359901297094 entropy 0.03264415264129639
epoch: 63, step: 28
	action: tensor([[ 1.2182, -0.2296, -0.2914,  0.7392,  0.1968, -0.2028, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-13.7684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8172017841328583, distance: 0.4892633330790007 entropy 0.03264415264129639
epoch: 63, step: 29
	action: tensor([[ 1.0393,  0.3596, -0.3663,  0.4860,  0.0691,  0.1619, -0.2035]],
       dtype=torch.float64)
	q_value: tensor([[-9.3081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9864986947572123, distance: 0.1329672146203956 entropy 0.03264415264129639
epoch: 63, step: 30
	action: tensor([[ 0.9330,  0.1803, -0.0307,  0.5281, -0.2935, -0.0665, -0.4018]],
       dtype=torch.float64)
	q_value: tensor([[-11.0830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9791420687837153, distance: 0.16526934608520935 entropy 0.03264415264129639
epoch: 63, step: 31
	action: tensor([[ 0.9990, -0.2649, -0.3395,  0.6628,  0.0861, -0.1169, -0.1625]],
       dtype=torch.float64)
	q_value: tensor([[-10.3358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8355729236973282, distance: 0.464027116797461 entropy 0.03264415264129639
epoch: 63, step: 32
	action: tensor([[ 1.0154,  0.0322, -0.7546,  0.6843, -0.1781,  0.7117,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-8.4528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.953924956617827, distance: 0.24563461602418965 entropy 0.03264415264129639
epoch: 63, step: 33
	action: tensor([[ 1.3021e+00,  7.6173e-04, -8.3310e-02,  4.8282e-01, -2.2177e-03,
         -1.0818e-02,  2.0841e-01]], dtype=torch.float64)
	q_value: tensor([[-11.8680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8236763051304348, distance: 0.48052062384367716 entropy 0.03264415264129639
epoch: 63, step: 34
	action: tensor([[ 0.8666, -0.0788, -0.2889,  1.2260,  0.0575,  0.2442, -0.1221]],
       dtype=torch.float64)
	q_value: tensor([[-9.5847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9666507534893084, distance: 0.20897758509656927 entropy 0.03264415264129639
epoch: 63, step: 35
	action: tensor([[ 1.0851,  0.0590, -0.5083,  0.6974,  0.1822,  0.4453, -0.0306]],
       dtype=torch.float64)
	q_value: tensor([[-9.8370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.036640470438077485 entropy 0.03264415264129639
epoch: 63, step: 36
	action: tensor([[ 1.0758, -0.1337, -0.3224,  0.5273, -0.4083, -0.0301, -0.8608]],
       dtype=torch.float64)
	q_value: tensor([[-9.8541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8738553513869436, distance: 0.4064350083676974 entropy 0.03264415264129639
epoch: 63, step: 37
	action: tensor([[ 1.1592, -0.0279, -0.7734,  1.1107, -0.3782,  0.3477, -0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-13.2735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9546082320828494, distance: 0.24380647884195014 entropy 0.03264415264129639
epoch: 63, step: 38
	action: tensor([[ 1.1313, -0.2019, -0.2798,  1.0303, -0.1509,  0.2655, -0.0539]],
       dtype=torch.float64)
	q_value: tensor([[-14.8288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9897903032975175, distance: 0.11562802659116972 entropy 0.03264415264129639
epoch: 63, step: 39
	action: tensor([[ 1.3303, -0.2820, -0.6000,  0.8176,  0.2585,  0.4427, -0.0770]],
       dtype=torch.float64)
	q_value: tensor([[-10.8990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9175635572172707, distance: 0.32856122941592725 entropy 0.03264415264129639
epoch: 63, step: 40
	action: tensor([[ 1.1387,  0.2420, -0.9831,  1.0938,  0.0805, -0.0216,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-11.6615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9579782096332251, distance: 0.23458161354446194 entropy 0.03264415264129639
epoch: 63, step: 41
	action: tensor([[ 1.2412,  0.2487, -0.6273,  0.9998, -0.0169,  0.2537,  0.2387]],
       dtype=torch.float64)
	q_value: tensor([[-13.2260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9779284724570414, distance: 0.17000938207762206 entropy 0.03264415264129639
epoch: 63, step: 42
	action: tensor([[ 1.1710,  0.6020, -0.7868,  0.6842, -0.3224,  0.6121, -0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-12.6051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9158002620609614, distance: 0.33205655903618375 entropy 0.03264415264129639
epoch: 63, step: 43
	action: tensor([[ 1.2918,  0.0318, -0.4695,  0.8418,  0.0955, -0.1873,  0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-15.9760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9473211411707361, distance: 0.2626484683012463 entropy 0.03264415264129639
epoch: 63, step: 44
	action: tensor([[ 1.0887, -0.2316, -0.2657,  0.5316,  0.1901,  0.3245,  0.2674]],
       dtype=torch.float64)
	q_value: tensor([[-10.0135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8724554642521939, distance: 0.40868398733917877 entropy 0.03264415264129639
epoch: 63, step: 45
	action: tensor([[ 0.8539,  0.0058, -0.5395,  0.9745, -0.3861,  0.0311,  0.1297]],
       dtype=torch.float64)
	q_value: tensor([[-7.8888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.954784056547529, distance: 0.24333382997267877 entropy 0.03264415264129639
epoch: 63, step: 46
	action: tensor([[ 1.2501,  0.1820, -0.5238,  0.6556, -0.0219,  0.3113,  0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-9.7720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.985235577111366, distance: 0.13904805912902485 entropy 0.03264415264129639
epoch: 63, step: 47
	action: tensor([[ 0.9914,  0.3385, -0.7592,  0.7052,  0.2093,  0.0430, -0.1911]],
       dtype=torch.float64)
	q_value: tensor([[-12.1474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9801467383950843, distance: 0.16123993927274277 entropy 0.03264415264129639
epoch: 63, step: 48
	action: tensor([[ 0.9351, -0.2757, -0.5730,  0.6564, -0.4146,  0.0109,  0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-11.5591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8320295131382497, distance: 0.4690003683188733 entropy 0.03264415264129639
epoch: 63, step: 49
	action: tensor([[ 0.6888, -0.0321, -0.6024,  0.5283,  0.0423,  0.0897,  0.1109]],
       dtype=torch.float64)
	q_value: tensor([[-8.8571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8603293050965555, distance: 0.42767054514044467 entropy 0.03264415264129639
epoch: 63, step: 50
	action: tensor([[ 1.1524,  0.1314, -0.5988,  0.3234, -0.1028,  0.8962, -0.0344]],
       dtype=torch.float64)
	q_value: tensor([[-6.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9916661555941043, distance: 0.10446706351218818 entropy 0.03264415264129639
epoch: 63, step: 51
	action: tensor([[ 1.0044,  0.0598, -0.2344,  1.0577, -0.4271,  0.3601,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-12.8601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9747988766290832, distance: 0.18166306759169026 entropy 0.03264415264129639
epoch: 63, step: 52
	action: tensor([[ 1.1003,  0.0467, -0.5999,  0.7733, -0.5771, -0.0220, -0.1644]],
       dtype=torch.float64)
	q_value: tensor([[-11.2753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9852938633163995, distance: 0.13877332444132573 entropy 0.03264415264129639
epoch: 63, step: 53
	action: tensor([[ 0.9702,  0.3642, -0.3431,  0.7533, -0.0815, -0.1049, -0.2451]],
       dtype=torch.float64)
	q_value: tensor([[-12.6961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9784000839642737, distance: 0.16818324385831193 entropy 0.03264415264129639
epoch: 63, step: 54
	action: tensor([[ 0.9676,  0.5779, -0.6521,  1.0387, -0.0305,  0.1964, -0.2903]],
       dtype=torch.float64)
	q_value: tensor([[-11.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.782877472071257, distance: 0.5332233225265122 entropy 0.03264415264129639
epoch: 63, step: 55
	action: tensor([[ 0.8907,  0.0899, -0.7865,  0.9915,  0.3079,  0.4597, -0.2987]],
       dtype=torch.float64)
	q_value: tensor([[-14.0072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9048269768771218, distance: 0.35303163626001566 entropy 0.03264415264129639
epoch: 63, step: 56
	action: tensor([[ 1.3683,  0.3934, -0.7619,  0.6639, -0.3485, -0.0669,  0.5309]],
       dtype=torch.float64)
	q_value: tensor([[-11.6773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9641394283905257, distance: 0.2167031863817957 entropy 0.03264415264129639
epoch: 63, step: 57
	action: tensor([[ 0.8555, -0.2127, -0.4251,  0.9370, -0.2020,  0.1650, -0.2566]],
       dtype=torch.float64)
	q_value: tensor([[-13.1741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9474877549206098, distance: 0.26223278443940856 entropy 0.03264415264129639
epoch: 63, step: 58
	action: tensor([[ 1.1551,  0.1384, -0.4059,  0.8796, -0.0456, -0.1520,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-9.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9899800325893793, distance: 0.11454861638402533 entropy 0.03264415264129639
epoch: 63, step: 59
	action: tensor([[ 1.1595,  0.3805, -0.6496,  0.9357, -0.2934,  0.1035, -0.3521]],
       dtype=torch.float64)
	q_value: tensor([[-11.0682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9569562428881754, distance: 0.23741698161592145 entropy 0.03264415264129639
epoch: 63, step: 60
	action: tensor([[ 0.8128, -0.1322, -0.3947,  0.7902, -0.4210, -0.0071,  0.1782]],
       dtype=torch.float64)
	q_value: tensor([[-15.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9352919598257923, distance: 0.2910957176356903 entropy 0.03264415264129639
epoch: 63, step: 61
	action: tensor([[ 0.9288,  0.1199, -0.4112,  0.5950, -0.1641,  0.4751, -0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-8.1558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9911103263919071, distance: 0.1078945731356365 entropy 0.03264415264129639
epoch: 63, step: 62
	action: tensor([[ 0.8982, -0.2903, -0.6030,  0.6937, -0.4754,  0.4537, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[-10.6980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.846483401887393, distance: 0.4483677447957936 entropy 0.03264415264129639
epoch: 63, step: 63
	action: tensor([[ 1.2664,  0.4337, -0.7200,  0.3507, -0.0426,  0.1728,  0.2942]],
       dtype=torch.float64)
	q_value: tensor([[-10.0376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9652513779786523, distance: 0.2133170095643588 entropy 0.03264415264129639
epoch: 63, step: 64
	action: tensor([[1.1355, 0.1849, 0.1392, 0.7158, 0.0846, 0.0158, 0.3789]],
       dtype=torch.float64)
	q_value: tensor([[-12.3763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9365244428513846, distance: 0.28831016458955 entropy 0.03264415264129639
epoch: 63, step: 65
	action: tensor([[ 0.6492,  0.1057, -0.5053,  0.9623, -0.1261,  0.2826, -0.1863]],
       dtype=torch.float64)
	q_value: tensor([[-8.4198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8603169486559518, distance: 0.4276894623829641 entropy 0.03264415264129639
epoch: 63, step: 66
	action: tensor([[ 0.7008, -0.5652, -1.0574,  1.0258, -0.1711, -0.4204, -0.3556]],
       dtype=torch.float64)
	q_value: tensor([[-9.5773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45004645027662393, distance: 0.8486325746005215 entropy 0.03264415264129639
epoch: 63, step: 67
	action: tensor([[ 1.1557,  0.5571, -0.4674,  0.3821, -0.1517,  0.8401, -0.2201]],
       dtype=torch.float64)
	q_value: tensor([[-9.5498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9491186566301065, distance: 0.2581285119412689 entropy 0.03264415264129639
epoch: 63, step: 68
	action: tensor([[ 0.9213,  0.0785, -0.4444,  0.8105, -0.4579,  0.6360, -0.6743]],
       dtype=torch.float64)
	q_value: tensor([[-15.0084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9512518871167427, distance: 0.2526594796249052 entropy 0.03264415264129639
epoch: 63, step: 69
	action: tensor([[ 0.7582, -0.1676, -0.3845,  0.8204, -0.2345,  0.2623, -0.1452]],
       dtype=torch.float64)
	q_value: tensor([[-14.5318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9282087030462342, distance: 0.30661443207954303 entropy 0.03264415264129639
epoch: 63, step: 70
	action: tensor([[ 1.2483,  0.5108, -0.8072,  0.7583, -0.5216,  0.1232, -0.2503]],
       dtype=torch.float64)
	q_value: tensor([[-8.6925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9718440910349008, distance: 0.1920177911095003 entropy 0.03264415264129639
epoch: 63, step: 71
	action: tensor([[ 1.0956, -0.0722, -0.2365,  0.2743, -0.3841,  0.1243, -0.1392]],
       dtype=torch.float64)
	q_value: tensor([[-16.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8192251326581869, distance: 0.4865480306231815 entropy 0.03264415264129639
epoch: 63, step: 72
	action: tensor([[ 0.8966, -0.0552, -0.3874,  0.5651, -0.0533, -0.0235,  0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-10.0266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9216513063857551, distance: 0.32031153080490316 entropy 0.03264415264129639
epoch: 63, step: 73
	action: tensor([[ 1.3849, -0.4057, -0.6161,  0.6104, -0.3873,  0.5946,  0.1477]],
       dtype=torch.float64)
	q_value: tensor([[-7.7399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7842161214874133, distance: 0.5315770108402296 entropy 0.03264415264129639
epoch: 63, step: 74
	action: tensor([[ 1.1014,  0.1299, -0.5384,  0.6048, -0.2127,  0.2403,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-12.4008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9912736719977708, distance: 0.10689870887080932 entropy 0.03264415264129639
epoch: 63, step: 75
	action: tensor([[ 1.2711, -0.1760, -0.5902,  0.6524, -0.2819,  0.1680, -0.3466]],
       dtype=torch.float64)
	q_value: tensor([[-11.4143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8841725020926798, distance: 0.38945970513245765 entropy 0.03264415264129639
epoch: 63, step: 76
	action: tensor([[ 1.0826, -0.0970, -0.4625,  0.7497, -0.2587, -0.0318, -0.0574]],
       dtype=torch.float64)
	q_value: tensor([[-13.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9484320777881587, distance: 0.25986423376768314 entropy 0.03264415264129639
epoch: 63, step: 77
	action: tensor([[ 1.1414e+00,  2.0776e-01, -4.4394e-02,  2.3539e-01, -2.3659e-01,
         -6.9181e-05, -1.6042e-01]], dtype=torch.float64)
	q_value: tensor([[-10.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8853351724861709, distance: 0.3875000869715719 entropy 0.03264415264129639
epoch: 63, step: 78
	action: tensor([[ 1.3033, -0.2166, -0.3254,  0.3999, -0.2296, -0.3542, -0.2265]],
       dtype=torch.float64)
	q_value: tensor([[-10.3379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5758290560817759, distance: 0.7452931868127937 entropy 0.03264415264129639
epoch: 63, step: 79
	action: tensor([[ 0.8912,  0.3406, -0.3725,  0.5387, -0.4253,  0.3200, -0.3939]],
       dtype=torch.float64)
	q_value: tensor([[-10.9501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9772115362133584, distance: 0.17274847336021354 entropy 0.03264415264129639
epoch: 63, step: 80
	action: tensor([[ 1.2227,  0.4646, -0.4565,  1.1157, -0.7036,  0.6602,  0.0548]],
       dtype=torch.float64)
	q_value: tensor([[-12.4827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8539532408715272, distance: 0.43732333902165405 entropy 0.03264415264129639
epoch: 63, step: 81
	action: tensor([[ 1.3667,  0.1507, -0.3533,  0.8183, -0.1763,  0.0662, -0.1661]],
       dtype=torch.float64)
	q_value: tensor([[-16.4669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9390050796832933, distance: 0.2826204134619556 entropy 0.03264415264129639
epoch: 63, step: 82
	action: tensor([[ 1.3726,  0.4804, -0.8374,  0.5398, -0.2953,  0.1589,  0.2008]],
       dtype=torch.float64)
	q_value: tensor([[-13.6104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9683424781769389, distance: 0.20360814664259605 entropy 0.03264415264129639
epoch: 63, step: 83
	action: tensor([[ 0.8735, -0.1058, -0.6380,  0.2405, -0.0380,  0.2997, -0.0698]],
       dtype=torch.float64)
	q_value: tensor([[-14.8306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8257924293620965, distance: 0.4776284695806604 entropy 0.03264415264129639
epoch: 63, step: 84
	action: tensor([[ 0.9626, -0.2290, -0.9272,  0.8308, -0.7044,  0.2714, -0.2973]],
       dtype=torch.float64)
	q_value: tensor([[-8.4165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7951981449359924, distance: 0.517873437904437 entropy 0.03264415264129639
epoch: 63, step: 85
	action: tensor([[ 0.7740, -0.3315, -0.3355,  1.1678, -0.4132,  0.2101, -0.1498]],
       dtype=torch.float64)
	q_value: tensor([[-13.3178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9216941401819554, distance: 0.32022396052173474 entropy 0.03264415264129639
epoch: 63, step: 86
	action: tensor([[ 1.5463, -0.2469, -0.4578,  0.6452, -0.1475,  0.2624, -0.0978]],
       dtype=torch.float64)
	q_value: tensor([[-9.6644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.735849999262113, distance: 0.588141739082522 entropy 0.03264415264129639
epoch: 63, step: 87
	action: tensor([[ 1.3653, -0.0166, -0.8432,  0.5485,  0.1817,  0.3386, -0.4192]],
       dtype=torch.float64)
	q_value: tensor([[-13.3692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9095565022408189, distance: 0.34414809357016696 entropy 0.03264415264129639
epoch: 63, step: 88
	action: tensor([[ 1.1403, -0.3894, -0.7213,  0.5255, -0.2553, -0.0219,  0.3904]],
       dtype=torch.float64)
	q_value: tensor([[-14.1458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6548114253457655, distance: 0.6723337297920763 entropy 0.03264415264129639
epoch: 63, step: 89
	action: tensor([[ 0.6691,  0.0033, -0.8478,  0.6139,  0.1278,  0.1122, -0.0593]],
       dtype=torch.float64)
	q_value: tensor([[-8.9502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8164290685202353, distance: 0.4902963374929879 entropy 0.03264415264129639
epoch: 63, step: 90
	action: tensor([[ 1.1722,  0.1155, -0.7918,  0.6911, -0.2947,  0.4541, -0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-8.0997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9867553013131345, distance: 0.13169756178062536 entropy 0.03264415264129639
epoch: 63, step: 91
	action: tensor([[ 1.2768,  0.4470, -0.4519,  0.2732, -0.6017,  0.1634,  0.1780]],
       dtype=torch.float64)
	q_value: tensor([[-13.7931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9338123410413881, distance: 0.2944050148978677 entropy 0.03264415264129639
epoch: 63, step: 92
	action: tensor([[ 1.3714,  0.2101, -0.8305,  0.9511, -0.1647,  0.2949, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-13.3864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05049089450381928 entropy 0.03264415264129639
epoch: 63, step: 93
	action: tensor([[ 0.6504,  0.2432, -0.3323,  0.3449, -0.7054, -0.0988, -0.2612]],
       dtype=torch.float64)
	q_value: tensor([[-9.8541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9285201608858669, distance: 0.30594860436777754 entropy 0.03264415264129639
epoch: 63, step: 94
	action: tensor([[ 0.7333,  0.5045, -0.5005,  0.8141,  0.4343, -0.0306,  0.0975]],
       dtype=torch.float64)
	q_value: tensor([[-9.4348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8643007349958787, distance: 0.42154645452200057 entropy 0.03264415264129639
epoch: 63, step: 95
	action: tensor([[ 1.1761,  0.2099, -0.7015,  1.0136, -0.1257,  0.1801, -0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-8.7083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9799425115254805, distance: 0.16206714027598654 entropy 0.03264415264129639
epoch: 63, step: 96
	action: tensor([[ 1.1292,  0.0016, -0.5151,  0.5724, -0.4365,  0.3873,  0.2681]],
       dtype=torch.float64)
	q_value: tensor([[-13.4710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.967491684966057, distance: 0.20632598312779724 entropy 0.03264415264129639
epoch: 63, step: 97
	action: tensor([[ 0.4348, -0.0536, -0.5697,  0.3612, -0.1266, -0.2595,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-11.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6736223837142363, distance: 0.6537577892957396 entropy 0.03264415264129639
epoch: 63, step: 98
	action: tensor([[ 0.6495,  0.0196, -0.1134, -0.0205, -0.3665, -0.1753, -0.2514]],
       dtype=torch.float64)
	q_value: tensor([[-5.0718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.707867392483467, distance: 0.6185099789176757 entropy 0.03264415264129639
epoch: 63, step: 99
	action: tensor([[ 0.2715,  0.4928, -0.4012,  0.5462, -0.2816, -0.1593,  0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-6.4256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7594678927809659, distance: 0.5612330284752265 entropy 0.03264415264129639
epoch: 63, step: 100
	action: tensor([[ 0.2386, -0.1046,  0.0613,  0.2571,  0.0798,  0.3137, -0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-6.5297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6449557445135646, distance: 0.6818642772222442 entropy 0.03264415264129639
epoch: 63, step: 101
	action: tensor([[ 0.6165, -0.0245, -0.4376,  0.7287, -0.5909, -0.0897, -0.3198]],
       dtype=torch.float64)
	q_value: tensor([[-3.4800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8777670771420994, distance: 0.4000836389584658 entropy 0.03264415264129639
epoch: 63, step: 102
	action: tensor([[ 0.9588,  0.2735, -0.2152,  0.8387,  0.1129,  0.2442,  0.0795]],
       dtype=torch.float64)
	q_value: tensor([[-9.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9639567537961505, distance: 0.2172544308221042 entropy 0.03264415264129639
epoch: 63, step: 103
	action: tensor([[ 1.7496,  0.0158, -0.6900,  0.5083, -0.3284,  0.4495,  0.0939]],
       dtype=torch.float64)
	q_value: tensor([[-9.6070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8268227978605291, distance: 0.4762138835772156 entropy 0.03264415264129639
epoch: 63, step: 104
	action: tensor([[ 1.3122,  0.4140, -0.6117,  0.4245,  0.1258,  0.2770,  0.3344]],
       dtype=torch.float64)
	q_value: tensor([[-15.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9539228036077902, distance: 0.24564035500539155 entropy 0.03264415264129639
epoch: 63, step: 105
	action: tensor([[ 0.5937,  0.0498, -0.3720,  0.8956, -0.0864,  0.4649, -0.1479]],
       dtype=torch.float64)
	q_value: tensor([[-12.0703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8768752890733952, distance: 0.40154045355045054 entropy 0.03264415264129639
epoch: 63, step: 106
	action: tensor([[ 1.6913, -0.0367, -0.5507,  0.5008,  0.4278,  0.4248,  0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-8.6813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7603370585963696, distance: 0.5602180994529601 entropy 0.03264415264129639
epoch: 63, step: 107
	action: tensor([[ 1.1753,  0.0656, -0.6774,  0.5185, -0.1006,  0.2322, -0.2038]],
       dtype=torch.float64)
	q_value: tensor([[-12.8551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9555145747066449, distance: 0.241360149805594 entropy 0.03264415264129639
epoch: 63, step: 108
	action: tensor([[ 1.2335, -0.0017, -0.4689,  0.7446, -0.1986,  0.1486, -0.2779]],
       dtype=torch.float64)
	q_value: tensor([[-12.3681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9683365097873907, distance: 0.203627338850226 entropy 0.03264415264129639
epoch: 63, step: 109
	action: tensor([[ 1.2704,  0.1804, -0.0680,  0.6194, -0.4003,  0.3795,  0.0397]],
       dtype=torch.float64)
	q_value: tensor([[-12.8130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9358230326998118, distance: 0.2898987138171257 entropy 0.03264415264129639
epoch: 63, step: 110
	action: tensor([[ 0.7010,  0.3506, -0.5282,  1.1084, -0.3847,  0.0966, -0.5465]],
       dtype=torch.float64)
	q_value: tensor([[-12.3878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7893834976636682, distance: 0.525173606071272 entropy 0.03264415264129639
epoch: 63, step: 111
	action: tensor([[ 0.8831,  0.3330, -0.4141,  1.1012,  0.0463,  0.4630, -0.5344]],
       dtype=torch.float64)
	q_value: tensor([[-12.9050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8299264652605782, distance: 0.4719272578184001 entropy 0.03264415264129639
epoch: 63, step: 112
	action: tensor([[ 0.8154,  0.0442, -0.8884,  0.8468,  0.0207,  0.7491, -0.4514]],
       dtype=torch.float64)
	q_value: tensor([[-13.3361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8339059724488276, distance: 0.4663733240757656 entropy 0.03264415264129639
epoch: 63, step: 113
	action: tensor([[ 1.0968, -0.0439, -0.2133,  1.0252, -0.0336,  0.7283, -0.1752]],
       dtype=torch.float64)
	q_value: tensor([[-12.9783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.973976329988126, distance: 0.18460393957458698 entropy 0.03264415264129639
epoch: 63, step: 114
	action: tensor([[ 1.2158,  0.1101, -0.1276,  0.9361, -0.0794,  0.1008, -0.2045]],
       dtype=torch.float64)
	q_value: tensor([[-12.1832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9551680195886055, distance: 0.242298460745096 entropy 0.03264415264129639
epoch: 63, step: 115
	action: tensor([[ 0.9172,  0.3463, -0.2922,  0.7025, -0.1201, -0.1963,  0.1764]],
       dtype=torch.float64)
	q_value: tensor([[-12.1557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9888474181484841, distance: 0.12084937339504655 entropy 0.03264415264129639
epoch: 63, step: 116
	action: tensor([[ 0.0519,  0.1707, -0.8938,  0.6863,  0.4779,  0.5325,  0.2210]],
       dtype=torch.float64)
	q_value: tensor([[-9.1023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3823673678723428, distance: 0.8993356514260081 entropy 0.03264415264129639
epoch: 63, step: 117
	action: tensor([[ 1.1391,  0.0955, -0.2178,  0.8216, -0.3542,  0.4924,  0.0026]],
       dtype=torch.float64)
	q_value: tensor([[-6.2153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9917365162261319, distance: 0.10402513368715405 entropy 0.03264415264129639
epoch: 63, step: 118
	action: tensor([[ 1.2294,  0.0155, -0.6132,  0.5645, -0.1739,  0.2193, -0.5267]],
       dtype=torch.float64)
	q_value: tensor([[-12.1198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.945507870022897, distance: 0.26713056652797396 entropy 0.03264415264129639
epoch: 63, step: 119
	action: tensor([[ 1.5382, -0.0855, -0.8144,  0.6971,  0.2420,  0.0474, -0.2894]],
       dtype=torch.float64)
	q_value: tensor([[-13.8697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8106943627753562, distance: 0.49789580441639425 entropy 0.03264415264129639
epoch: 63, step: 120
	action: tensor([[ 1.2754, -0.1046, -0.5611,  0.6523,  0.1465,  0.5297,  0.3009]],
       dtype=torch.float64)
	q_value: tensor([[-14.2564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9485094427082437, distance: 0.2596692295781714 entropy 0.03264415264129639
epoch: 63, step: 121
	action: tensor([[ 1.1166, -0.0026, -0.5300,  0.3685,  0.4439,  0.3155, -0.7486]],
       dtype=torch.float64)
	q_value: tensor([[-10.6450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9150385798123306, distance: 0.33355509201627564 entropy 0.03264415264129639
epoch: 63, step: 122
	action: tensor([[ 1.0530, -0.0968, -0.5534,  0.6882, -0.3938,  0.0503, -0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-11.8463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9382982657581255, distance: 0.2842532106108149 entropy 0.03264415264129639
epoch: 63, step: 123
	action: tensor([[ 1.1964e+00, -1.1488e-03, -2.8836e-01,  2.3791e-01, -3.8644e-01,
          2.6875e-01,  1.4740e-01]], dtype=torch.float64)
	q_value: tensor([[-12.2280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8346165858586087, distance: 0.4653745934802505 entropy 0.03264415264129639
epoch: 63, step: 124
	action: tensor([[ 0.9811,  0.1081, -0.0366,  0.4512, -0.1965,  0.1876,  0.2907]],
       dtype=torch.float64)
	q_value: tensor([[-10.3839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.96143091133776, distance: 0.2247379328341658 entropy 0.03264415264129639
epoch: 63, step: 125
	action: tensor([[ 0.6616,  0.0670, -0.0621,  0.7717, -0.2219,  0.1002, -0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-8.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.978222050668285, distance: 0.16887493108199675 entropy 0.03264415264129639
epoch: 63, step: 126
	action: tensor([[ 1.4497,  0.5435, -0.6176,  1.0561,  0.7129,  0.1781,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-7.5877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8844524826379061, distance: 0.3889887154433712 entropy 0.03264415264129639
epoch: 63, step: 127
	action: tensor([[ 1.4139,  0.1710, -0.3204,  0.8205, -0.1849,  0.2969, -0.1105]],
       dtype=torch.float64)
	q_value: tensor([[-14.3438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9299995969557182, distance: 0.3027659026582504 entropy 0.03264415264129639
LOSS epoch 63 actor 263.89619013382605 critic 3253.6808775616378 
epoch: 64, step: 0
	action: tensor([[ 0.7328, -0.2118, -0.8289,  0.6276, -0.3444,  0.4422, -0.3042]],
       dtype=torch.float64)
	q_value: tensor([[-13.6698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7353990413533513, distance: 0.5886435639268948 entropy 0.03264415264129639
epoch: 64, step: 1
	action: tensor([[ 0.9077,  0.2476, -0.2380,  0.4214, -0.4031,  0.2376,  0.0409]],
       dtype=torch.float64)
	q_value: tensor([[-10.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9892079745871403, distance: 0.1188798293342023 entropy 0.03264415264129639
epoch: 64, step: 2
	action: tensor([[ 0.3966,  0.1587, -0.4169,  0.8667,  0.4623,  0.1791,  0.3092]],
       dtype=torch.float64)
	q_value: tensor([[-9.3036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7989389889777836, distance: 0.513121986946722 entropy 0.03264415264129639
epoch: 64, step: 3
	action: tensor([[ 0.8959,  0.0283, -0.2232,  0.3386, -0.3804,  0.6800, -0.5308]],
       dtype=torch.float64)
	q_value: tensor([[-5.3107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9707442769160003, distance: 0.1957321252009546 entropy 0.03264415264129639
epoch: 64, step: 4
	action: tensor([[ 0.6073,  0.5310, -0.6574,  0.5010,  0.3722,  0.7582,  0.1590]],
       dtype=torch.float64)
	q_value: tensor([[-11.4776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8195514886686325, distance: 0.48610864541745485 entropy 0.03264415264129639
epoch: 64, step: 5
	action: tensor([[ 1.0976, -0.0649, -0.7926,  0.3659,  0.0401,  0.3479, -0.3729]],
       dtype=torch.float64)
	q_value: tensor([[-8.8898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8835506867974291, distance: 0.39050370527381084 entropy 0.03264415264129639
epoch: 64, step: 6
	action: tensor([[ 0.8516, -0.3049, -0.5607,  1.1319,  0.0271,  0.0116, -0.2986]],
       dtype=torch.float64)
	q_value: tensor([[-11.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9075859695222136, distance: 0.3478769462695585 entropy 0.03264415264129639
epoch: 64, step: 7
	action: tensor([[ 1.4890, -0.0272, -0.5921,  1.0826, -0.1028,  0.7531,  0.2968]],
       dtype=torch.float64)
	q_value: tensor([[-9.4178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9741380765334614, distance: 0.1840293551593244 entropy 0.03264415264129639
epoch: 64, step: 8
	action: tensor([[ 0.8732,  0.4214, -0.1880,  0.4125, -0.1525, -0.1105, -0.2226]],
       dtype=torch.float64)
	q_value: tensor([[-13.8392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9895652354719144, distance: 0.1168955610843796 entropy 0.03264415264129639
epoch: 64, step: 9
	action: tensor([[ 1.2657,  0.5582, -0.6827,  0.4181, -0.3736,  0.3479, -0.7755]],
       dtype=torch.float64)
	q_value: tensor([[-9.2291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9724372939197354, distance: 0.189984259409725 entropy 0.03264415264129639
epoch: 64, step: 10
	action: tensor([[ 0.8069,  0.0373, -0.5015,  1.0741, -0.2385,  0.5749, -0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-17.4764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8717880688795132, distance: 0.409751841500527 entropy 0.03264415264129639
epoch: 64, step: 11
	action: tensor([[ 0.8759,  0.3414,  0.0148,  0.7643, -0.2566,  0.1667,  0.1890]],
       dtype=torch.float64)
	q_value: tensor([[-10.5776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9610101309320455, distance: 0.22596052840818717 entropy 0.03264415264129639
epoch: 64, step: 12
	action: tensor([[ 0.8811, -0.1755, -0.1147,  0.4461, -0.6215, -0.0582,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-8.6527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8217596517703911, distance: 0.4831252146971126 entropy 0.03264415264129639
epoch: 64, step: 13
	action: tensor([[ 1.0916,  0.2279, -0.6866,  0.6993, -0.1392, -0.0473, -0.5594]],
       dtype=torch.float64)
	q_value: tensor([[-7.8405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0964228558499585 entropy 0.03264415264129639
epoch: 64, step: 14
	action: tensor([[ 0.1887,  0.1878,  0.1248,  0.4220, -0.0404,  0.3960,  0.3688]],
       dtype=torch.float64)
	q_value: tensor([[-9.7729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7877240169900797, distance: 0.5272385095045532 entropy 0.03264415264129639
epoch: 64, step: 15
	action: tensor([[ 1.0090,  0.1350,  0.0931,  0.6404, -0.3572,  0.1130,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[-3.7137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9777085253069505, distance: 0.17085437078391752 entropy 0.03264415264129639
epoch: 64, step: 16
	action: tensor([[ 6.5582e-01,  1.8241e-01, -8.3158e-01,  4.6732e-01,  3.1459e-04,
          6.1277e-01, -3.1607e-01]], dtype=torch.float64)
	q_value: tensor([[-9.1937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8501866679342371, distance: 0.44292676556303506 entropy 0.03264415264129639
epoch: 64, step: 17
	action: tensor([[ 0.9537, -0.0827, -0.4548,  0.7443, -0.4048,  0.6024, -0.1860]],
       dtype=torch.float64)
	q_value: tensor([[-10.2956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9671383803804882, distance: 0.2074441420409165 entropy 0.03264415264129639
epoch: 64, step: 18
	action: tensor([[ 1.2812,  0.2837, -0.6256,  0.4817, -0.1456,  0.5255, -0.3279]],
       dtype=torch.float64)
	q_value: tensor([[-11.2220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9876000584912844, distance: 0.12742848258098183 entropy 0.03264415264129639
epoch: 64, step: 19
	action: tensor([[ 1.2686, -0.0071, -0.4877,  0.4985, -0.1206,  0.0338, -0.0978]],
       dtype=torch.float64)
	q_value: tensor([[-14.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8691153094228257, distance: 0.41400074113163376 entropy 0.03264415264129639
epoch: 64, step: 20
	action: tensor([[ 1.0079,  0.4330, -0.4262,  0.7788, -0.0883,  0.3310, -0.0260]],
       dtype=torch.float64)
	q_value: tensor([[-11.0604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9229105242108016, distance: 0.3177270858629785 entropy 0.03264415264129639
epoch: 64, step: 21
	action: tensor([[ 0.5192,  0.0374, -0.3363,  0.6147,  0.3290,  0.1414,  0.3191]],
       dtype=torch.float64)
	q_value: tensor([[-11.4765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.876014526317672, distance: 0.4029415902185862 entropy 0.03264415264129639
epoch: 64, step: 22
	action: tensor([[ 0.3039, -0.1971, -0.1338,  0.3011, -0.2482,  0.2560, -0.0707]],
       dtype=torch.float64)
	q_value: tensor([[-4.8164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6103465850778722, distance: 0.7143251427397198 entropy 0.03264415264129639
epoch: 64, step: 23
	action: tensor([[ 0.8348,  0.0395, -0.6531,  0.7432, -0.2038,  0.2440, -0.5162]],
       dtype=torch.float64)
	q_value: tensor([[-4.2395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9273090713812618, distance: 0.30852957558471206 entropy 0.03264415264129639
epoch: 64, step: 24
	action: tensor([[ 0.5019,  0.1884, -0.2396,  0.6958, -0.2126,  0.2326, -0.0483]],
       dtype=torch.float64)
	q_value: tensor([[-11.3228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8872461795742644, distance: 0.38425747643123503 entropy 0.03264415264129639
epoch: 64, step: 25
	action: tensor([[ 0.7009, -0.0594, -0.5146,  0.7904, -0.0681, -0.0828,  0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-6.9274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8962135091435592, distance: 0.36866092409417994 entropy 0.03264415264129639
epoch: 64, step: 26
	action: tensor([[ 0.8756, -0.0409, -0.7224,  0.7876, -0.0544,  0.1543,  0.4311]],
       dtype=torch.float64)
	q_value: tensor([[-6.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9205661660619248, distance: 0.32252208267997107 entropy 0.03264415264129639
epoch: 64, step: 27
	action: tensor([[ 0.8214,  0.0993, -0.5827,  0.4686,  0.1726,  0.4764, -0.4315]],
       dtype=torch.float64)
	q_value: tensor([[-8.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.963357503283896, distance: 0.21905300843132658 entropy 0.03264415264129639
epoch: 64, step: 28
	action: tensor([[ 7.8839e-01, -1.3567e-01, -3.8848e-04,  8.2886e-01,  1.0081e-01,
          9.6453e-02,  2.3301e-01]], dtype=torch.float64)
	q_value: tensor([[-9.9236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9761556563436412, distance: 0.17670522017670948 entropy 0.03264415264129639
epoch: 64, step: 29
	action: tensor([[ 0.9128,  0.1116, -0.6864,  0.5955, -0.5817,  0.3172, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[-6.1038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9471875632526616, distance: 0.26298125667065797 entropy 0.03264415264129639
epoch: 64, step: 30
	action: tensor([[ 1.0266,  0.0438, -0.5871,  0.1599, -0.4459,  0.0033, -0.4298]],
       dtype=torch.float64)
	q_value: tensor([[-11.1364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8373307849235987, distance: 0.46154003516518355 entropy 0.03264415264129639
epoch: 64, step: 31
	action: tensor([[ 1.2106,  0.2422, -0.6900,  0.3332, -0.4924,  0.4235, -0.3099]],
       dtype=torch.float64)
	q_value: tensor([[-11.2361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9702446801100598, distance: 0.1973962986717535 entropy 0.03264415264129639
epoch: 64, step: 32
	action: tensor([[ 1.6131,  0.2709, -0.6848,  0.7723, -0.0880,  0.0544, -0.4399]],
       dtype=torch.float64)
	q_value: tensor([[-14.4447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9056872540991528, distance: 0.35143247250069853 entropy 0.03264415264129639
epoch: 64, step: 33
	action: tensor([[ 1.3063, -0.1320, -0.3465,  0.6501, -0.0178,  0.1516, -0.2350]],
       dtype=torch.float64)
	q_value: tensor([[-16.6138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8691682166886737, distance: 0.4139170573136849 entropy 0.03264415264129639
epoch: 64, step: 34
	action: tensor([[ 0.9562, -0.2080, -0.9166,  1.0732, -0.3421, -0.0581, -0.2208]],
       dtype=torch.float64)
	q_value: tensor([[-11.3314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8574333673757932, distance: 0.4320814673962751 entropy 0.03264415264129639
epoch: 64, step: 35
	action: tensor([[ 1.2473,  0.4770, -0.3511,  1.1779, -0.3562,  0.3604, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-11.5021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8233233120743457, distance: 0.4810013749848302 entropy 0.03264415264129639
epoch: 64, step: 36
	action: tensor([[ 0.8083,  0.0405, -0.3206,  0.8292, -0.2664,  0.3266, -0.2452]],
       dtype=torch.float64)
	q_value: tensor([[-14.6476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9677047607957334, distance: 0.20564868939413175 entropy 0.03264415264129639
epoch: 64, step: 37
	action: tensor([[ 0.7166, -0.0333, -0.3388,  0.6185,  0.2619,  0.3125, -0.1044]],
       dtype=torch.float64)
	q_value: tensor([[-9.7544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9478056201401135, distance: 0.261437910743106 entropy 0.03264415264129639
epoch: 64, step: 38
	action: tensor([[ 0.6559, -0.5655, -0.4945,  0.4335, -0.0605,  0.1811, -0.0738]],
       dtype=torch.float64)
	q_value: tensor([[-6.8341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48896622536557943, distance: 0.8180531009698284 entropy 0.03264415264129639
epoch: 64, step: 39
	action: tensor([[ 0.4921, -0.4729, -0.2943,  0.3562, -0.1877,  0.3998, -0.4956]],
       dtype=torch.float64)
	q_value: tensor([[-5.4814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5518420657028159, distance: 0.7660766726780455 entropy 0.03264415264129639
epoch: 64, step: 40
	action: tensor([[ 0.6849, -0.2165, -0.8183,  1.1145, -0.0753,  0.4134,  0.1411]],
       dtype=torch.float64)
	q_value: tensor([[-6.4204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.735321517110104, distance: 0.5887297896250614 entropy 0.03264415264129639
epoch: 64, step: 41
	action: tensor([[ 0.8665,  0.2140, -0.1981,  0.5040, -0.0717,  0.2757, -0.3214]],
       dtype=torch.float64)
	q_value: tensor([[-8.6624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09610127206091049 entropy 0.03264415264129639
epoch: 64, step: 42
	action: tensor([[ 0.9367, -0.1859, -0.1559,  0.5084,  0.1539, -0.4611, -0.1734]],
       dtype=torch.float64)
	q_value: tensor([[-9.7729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7469826007411893, distance: 0.5756147140003586 entropy 0.03264415264129639
epoch: 64, step: 43
	action: tensor([[ 0.9381,  0.5557, -0.3323,  0.8935, -0.1406,  0.5203, -0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-6.8868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7904761113913916, distance: 0.523809615093313 entropy 0.03264415264129639
epoch: 64, step: 44
	action: tensor([[ 0.9685,  0.0455, -0.0213,  0.8609, -0.1211,  0.0379, -0.4390]],
       dtype=torch.float64)
	q_value: tensor([[-12.2059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09807276600705428 entropy 0.03264415264129639
epoch: 64, step: 45
	action: tensor([[ 0.4822, -0.0320, -0.2760,  0.5851,  0.0855,  0.3048, -0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-9.7729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8314563898972832, distance: 0.4697998126770347 entropy 0.03264415264129639
epoch: 64, step: 46
	action: tensor([[ 0.9472,  0.5177, -0.5058,  0.3970, -0.1118,  0.0866, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-5.7974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9814388124803526, distance: 0.1559048308657223 entropy 0.03264415264129639
epoch: 64, step: 47
	action: tensor([[ 0.4432,  0.5618, -0.2937,  0.5015, -0.3199,  0.1383,  0.2589]],
       dtype=torch.float64)
	q_value: tensor([[-10.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8179304561035602, distance: 0.4882872062917424 entropy 0.03264415264129639
epoch: 64, step: 48
	action: tensor([[ 0.7337, -0.1608, -0.6196,  0.3539, -0.2307, -0.0740,  0.3067]],
       dtype=torch.float64)
	q_value: tensor([[-7.0326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7434768096604668, distance: 0.5795888333399843 entropy 0.03264415264129639
epoch: 64, step: 49
	action: tensor([[ 0.9695, -0.1748,  0.2332,  0.3577,  0.1260,  0.6010, -0.3783]],
       dtype=torch.float64)
	q_value: tensor([[-6.1602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8739909565173298, distance: 0.40621649139965993 entropy 0.03264415264129639
epoch: 64, step: 50
	action: tensor([[ 0.8450, -0.0297, -0.5878,  0.4389, -0.3922,  0.2246,  0.1246]],
       dtype=torch.float64)
	q_value: tensor([[-8.4560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8959596001329925, distance: 0.3691116048557012 entropy 0.03264415264129639
epoch: 64, step: 51
	action: tensor([[ 1.0042,  0.3146, -0.3891,  0.3323, -0.4653,  0.0150, -0.4550]],
       dtype=torch.float64)
	q_value: tensor([[-8.4839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9813190275178314, distance: 0.15640708917174687 entropy 0.03264415264129639
epoch: 64, step: 52
	action: tensor([[ 0.9633, -0.0031, -0.4521,  0.8185, -0.1705,  0.3598, -0.0916]],
       dtype=torch.float64)
	q_value: tensor([[-12.0571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9881533225570661, distance: 0.12455322447348993 entropy 0.03264415264129639
epoch: 64, step: 53
	action: tensor([[ 1.2871,  0.2043, -0.2432,  0.6648, -0.3441,  0.0159, -0.0430]],
       dtype=torch.float64)
	q_value: tensor([[-10.2001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9302739718734632, distance: 0.3021719565666711 entropy 0.03264415264129639
epoch: 64, step: 54
	action: tensor([[ 0.8396, -0.1292, -0.5523,  0.3373,  0.0508, -0.0199, -0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-12.0677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7934641551159928, distance: 0.520061148881621 entropy 0.03264415264129639
epoch: 64, step: 55
	action: tensor([[ 0.8584,  0.1884, -0.5852,  0.8725, -0.4427, -0.1911, -0.0490]],
       dtype=torch.float64)
	q_value: tensor([[-7.2414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9681186314432653, distance: 0.20432672365157628 entropy 0.03264415264129639
epoch: 64, step: 56
	action: tensor([[ 0.9742,  0.2833, -0.4199,  0.9321, -0.1937,  0.0215,  0.2217]],
       dtype=torch.float64)
	q_value: tensor([[-10.3075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9655359740596101, distance: 0.21244166548995175 entropy 0.03264415264129639
epoch: 64, step: 57
	action: tensor([[ 0.8907, -0.5570, -0.7508,  0.4450, -0.0583, -0.2469,  0.1903]],
       dtype=torch.float64)
	q_value: tensor([[-9.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4022073545554651, distance: 0.8847732363350623 entropy 0.03264415264129639
epoch: 64, step: 58
	action: tensor([[ 0.8005,  0.1248, -0.3224,  0.7949, -0.0474,  0.0552, -0.0518]],
       dtype=torch.float64)
	q_value: tensor([[-6.5685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9758508066540031, distance: 0.17783121984922784 entropy 0.03264415264129639
epoch: 64, step: 59
	action: tensor([[ 1.1944, -0.1011, -0.1873,  0.1899,  0.0135, -0.2120,  0.2300]],
       dtype=torch.float64)
	q_value: tensor([[-8.2746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6431260121083411, distance: 0.6836190244809133 entropy 0.03264415264129639
epoch: 64, step: 60
	action: tensor([[ 0.9588,  0.0495, -0.2514,  0.3667,  0.0121,  0.2954, -0.0465]],
       dtype=torch.float64)
	q_value: tensor([[-7.5939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9517872277617861, distance: 0.2512683253888284 entropy 0.03264415264129639
epoch: 64, step: 61
	action: tensor([[ 0.8445,  0.2818, -0.1536,  0.4423, -0.5874,  0.0542, -0.4704]],
       dtype=torch.float64)
	q_value: tensor([[-8.2489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9911790388317094, distance: 0.10747678046346149 entropy 0.03264415264129639
epoch: 64, step: 62
	action: tensor([[ 0.8882, -0.3025, -0.2949,  0.7294, -0.2102,  0.0345,  0.2173]],
       dtype=torch.float64)
	q_value: tensor([[-10.9954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8718437644856919, distance: 0.40966283318456526 entropy 0.03264415264129639
epoch: 64, step: 63
	action: tensor([[ 0.7928,  0.0682, -0.3287,  0.4510, -0.2562,  0.3277,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-7.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.958764341623387, distance: 0.23237701023195398 entropy 0.03264415264129639
epoch: 64, step: 64
	action: tensor([[ 0.1818,  0.3488, -0.3352,  0.8152, -0.2967,  0.1219,  0.3489]],
       dtype=torch.float64)
	q_value: tensor([[-8.0157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6617969847929442, distance: 0.6654959714841859 entropy 0.03264415264129639
epoch: 64, step: 65
	action: tensor([[ 0.8513,  0.6352, -0.6659,  0.4888, -0.0827,  0.4126, -0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-5.7054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 64, step: 66
	action: tensor([[ 0.7328,  0.2343, -0.4568,  0.4573,  0.1771,  0.3720, -0.1983]],
       dtype=torch.float64)
	q_value: tensor([[-9.7729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9489215336534641, distance: 0.2586280454646554 entropy 0.03264415264129639
epoch: 64, step: 67
	action: tensor([[ 0.9467,  0.2789, -0.7717,  0.5397, -0.2120,  0.2475, -0.1739]],
       dtype=torch.float64)
	q_value: tensor([[-8.3469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9735198883220725, distance: 0.18621583100942904 entropy 0.03264415264129639
epoch: 64, step: 68
	action: tensor([[ 6.5662e-01, -2.6852e-01, -5.1980e-01,  6.6129e-01,  1.5295e-01,
          4.4302e-04, -1.1772e-02]], dtype=torch.float64)
	q_value: tensor([[-11.6067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7652914634397427, distance: 0.5543973376900796 entropy 0.03264415264129639
epoch: 64, step: 69
	action: tensor([[ 0.4225,  0.5541, -0.5127,  0.3863, -0.2302,  0.0919,  0.1508]],
       dtype=torch.float64)
	q_value: tensor([[-5.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.823700927564046, distance: 0.48048707191451734 entropy 0.03264415264129639
epoch: 64, step: 70
	action: tensor([[ 0.8506, -0.1283, -0.2392,  0.0953,  0.0667,  0.3318, -0.1077]],
       dtype=torch.float64)
	q_value: tensor([[-7.1896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7793526060883769, distance: 0.5375341919196581 entropy 0.03264415264129639
epoch: 64, step: 71
	action: tensor([[ 0.9127, -0.1431, -0.2621,  0.8502,  0.0903,  0.2753, -0.1503]],
       dtype=torch.float64)
	q_value: tensor([[-6.6282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9870448650860393, distance: 0.1302499794047218 entropy 0.03264415264129639
epoch: 64, step: 72
	action: tensor([[ 0.7686,  0.1739, -0.8994,  0.7617, -0.4213,  0.5258, -0.3336]],
       dtype=torch.float64)
	q_value: tensor([[-8.5581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8054735410831377, distance: 0.5047147928969602 entropy 0.03264415264129639
epoch: 64, step: 73
	action: tensor([[ 0.9437, -0.2890, -0.6429,  0.5982,  0.0084,  0.0416,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[-12.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7943378150388255, distance: 0.5189600371307727 entropy 0.03264415264129639
epoch: 64, step: 74
	action: tensor([[ 0.9006,  0.2218, -0.0608,  0.3614,  0.5403,  0.1724,  0.3528]],
       dtype=torch.float64)
	q_value: tensor([[-7.9013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9800237096749813, distance: 0.16173876174328133 entropy 0.03264415264129639
epoch: 64, step: 75
	action: tensor([[ 0.6865,  0.1348, -0.3007,  0.3498, -0.2863,  0.5454,  0.2205]],
       dtype=torch.float64)
	q_value: tensor([[-5.8899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9414823181189023, distance: 0.2768217756714597 entropy 0.03264415264129639
epoch: 64, step: 76
	action: tensor([[ 0.5922,  0.1842, -0.4873,  0.4956, -0.5979,  0.3237, -0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-7.6213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8669602386956179, distance: 0.4173951724738023 entropy 0.03264415264129639
epoch: 64, step: 77
	action: tensor([[ 0.9063,  0.0358, -0.2865,  0.4004, -0.0354,  0.2208, -0.1006]],
       dtype=torch.float64)
	q_value: tensor([[-8.9059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9486287837889771, distance: 0.2593681337550253 entropy 0.03264415264129639
epoch: 64, step: 78
	action: tensor([[ 6.6995e-01,  5.6211e-04, -4.3649e-01,  3.8083e-01,  8.8082e-02,
          2.3074e-01, -2.0627e-01]], dtype=torch.float64)
	q_value: tensor([[-8.1418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8863457140582194, distance: 0.3857887883183294 entropy 0.03264415264129639
epoch: 64, step: 79
	action: tensor([[ 0.8562, -0.1618, -0.4288,  0.8445,  0.3170,  0.0527,  0.5185]],
       dtype=torch.float64)
	q_value: tensor([[-6.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9362200647840736, distance: 0.2890005905190681 entropy 0.03264415264129639
epoch: 64, step: 80
	action: tensor([[ 0.6568,  0.4862, -0.4956,  0.6626, -0.0884,  0.2526, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-6.3423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8535540061179288, distance: 0.4379206666205188 entropy 0.03264415264129639
epoch: 64, step: 81
	action: tensor([[ 0.5842, -0.1887, -0.4023,  0.2693, -0.3898,  0.0842,  0.0361]],
       dtype=torch.float64)
	q_value: tensor([[-9.1791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6906007736127013, distance: 0.6365262303222491 entropy 0.03264415264129639
epoch: 64, step: 82
	action: tensor([[ 0.3744, -0.3262, -0.3508,  0.4838, -0.0760,  0.0450, -0.2766]],
       dtype=torch.float64)
	q_value: tensor([[-5.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5655423618622684, distance: 0.7542762126183228 entropy 0.03264415264129639
epoch: 64, step: 83
	action: tensor([[ 0.4682,  0.0349, -0.7966,  0.0887, -0.1847,  0.1726, -0.1685]],
       dtype=torch.float64)
	q_value: tensor([[-4.5855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7096604399278752, distance: 0.6166089163875038 entropy 0.03264415264129639
epoch: 64, step: 84
	action: tensor([[ 0.6249, -0.1142, -0.4943,  0.6129, -0.2488, -0.1314,  0.1553]],
       dtype=torch.float64)
	q_value: tensor([[-6.7849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.821136024264558, distance: 0.48396965619724797 entropy 0.03264415264129639
epoch: 64, step: 85
	action: tensor([[ 0.6728, -0.0385, -0.5519,  0.5664, -0.0462,  0.2980,  0.4846]],
       dtype=torch.float64)
	q_value: tensor([[-6.1243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.872999617011903, distance: 0.40781125601581303 entropy 0.03264415264129639
epoch: 64, step: 86
	action: tensor([[ 0.4281,  0.0724, -0.4720,  0.6889, -0.2753,  0.3016,  0.0794]],
       dtype=torch.float64)
	q_value: tensor([[-6.3123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.77302426119973, distance: 0.5451881560981344 entropy 0.03264415264129639
epoch: 64, step: 87
	action: tensor([[ 0.8975, -0.2345, -0.1053,  0.7449, -0.3836,  0.8390,  0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-6.6491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9782781966649772, distance: 0.16865710133462217 entropy 0.03264415264129639
epoch: 64, step: 88
	action: tensor([[ 0.7528, -0.3132, -0.5002,  0.4258, -0.3511,  0.3106, -0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-9.6819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.734541334439245, distance: 0.5895968393281108 entropy 0.03264415264129639
epoch: 64, step: 89
	action: tensor([[ 1.1704,  0.2055,  0.0956,  0.9160, -0.1180, -0.1598, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-7.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9270445909521232, distance: 0.30909034669426216 entropy 0.03264415264129639
epoch: 64, step: 90
	action: tensor([[ 0.9268, -0.2149, -0.1768,  0.7495, -0.1032,  0.4800,  0.2485]],
       dtype=torch.float64)
	q_value: tensor([[-10.8620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9731699962139683, distance: 0.1874420650537436 entropy 0.03264415264129639
epoch: 64, step: 91
	action: tensor([[ 0.6343, -0.2335, -0.6283,  0.4494, -0.3288,  0.5934,  0.2170]],
       dtype=torch.float64)
	q_value: tensor([[-7.8124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7245651439491236, distance: 0.6005734688971438 entropy 0.03264415264129639
epoch: 64, step: 92
	action: tensor([[ 1.0220,  0.0437, -0.0372,  0.2474, -0.2845,  0.1799, -0.2027]],
       dtype=torch.float64)
	q_value: tensor([[-7.5443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8867674543969728, distance: 0.3850723441031022 entropy 0.03264415264129639
epoch: 64, step: 93
	action: tensor([[ 1.1327, -0.1153, -0.7187,  0.7550, -0.2725,  0.5800,  0.2490]],
       dtype=torch.float64)
	q_value: tensor([[-9.1037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9556234423606055, distance: 0.2410646326887298 entropy 0.03264415264129639
epoch: 64, step: 94
	action: tensor([[ 0.3701,  0.4636, -0.4032,  0.4590, -0.1089,  0.3568,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-11.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7779404970363757, distance: 0.539251516320998 entropy 0.03264415264129639
epoch: 64, step: 95
	action: tensor([[ 0.3093,  0.2070, -0.3006,  0.5448,  0.0064,  0.2161, -0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-6.8703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7881913718481213, distance: 0.5266577955448635 entropy 0.03264415264129639
epoch: 64, step: 96
	action: tensor([[ 1.1091,  0.1007, -0.6978,  0.5821, -0.3641,  0.6968,  0.5356]],
       dtype=torch.float64)
	q_value: tensor([[-5.3393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9809384679100712, distance: 0.1579921814397008 entropy 0.03264415264129639
epoch: 64, step: 97
	action: tensor([[ 0.4599, -0.3915, -0.1773,  0.0625, -0.2973,  0.3814,  0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-11.2706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44332852337481854, distance: 0.8538000536702471 entropy 0.03264415264129639
epoch: 64, step: 98
	action: tensor([[ 0.7585,  0.5132, -0.2427,  0.3754, -0.5194,  0.2969, -0.0544]],
       dtype=torch.float64)
	q_value: tensor([[-4.5423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9486043916575293, distance: 0.2594297031635684 entropy 0.03264415264129639
epoch: 64, step: 99
	action: tensor([[ 0.4849, -0.2166, -0.1878,  0.5096, -0.1885, -0.3503,  0.3206]],
       dtype=torch.float64)
	q_value: tensor([[-10.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6787676606611117, distance: 0.6485841384054428 entropy 0.03264415264129639
epoch: 64, step: 100
	action: tensor([[ 0.6575,  0.1070, -0.7749,  0.5665, -0.2649,  0.4481,  0.3079]],
       dtype=torch.float64)
	q_value: tensor([[-3.9974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8285157260972955, distance: 0.47388050512964663 entropy 0.03264415264129639
epoch: 64, step: 101
	action: tensor([[ 0.7528,  0.3245, -0.1442,  0.6465, -0.5021,  0.3647,  0.2703]],
       dtype=torch.float64)
	q_value: tensor([[-8.4305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9485585745102488, distance: 0.2595453130515954 entropy 0.03264415264129639
epoch: 64, step: 102
	action: tensor([[ 1.0796,  0.1813, -0.5953,  0.9060, -0.3307, -0.2887, -0.2307]],
       dtype=torch.float64)
	q_value: tensor([[-8.8322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0443282259468109 entropy 0.03264415264129639
epoch: 64, step: 103
	action: tensor([[ 0.7041,  0.2971, -0.3035,  0.3849, -0.4485,  0.3519, -0.2790]],
       dtype=torch.float64)
	q_value: tensor([[-9.7729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9469584963792419, distance: 0.26355096257862937 entropy 0.03264415264129639
epoch: 64, step: 104
	action: tensor([[ 0.7697, -0.0905, -0.3667,  0.3902, -0.2153,  0.0226,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-9.8206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.848078662261446, distance: 0.44603206520546446 entropy 0.03264415264129639
epoch: 64, step: 105
	action: tensor([[ 0.6731,  0.0160, -0.1444,  0.6396,  0.4067, -0.0849,  0.0651]],
       dtype=torch.float64)
	q_value: tensor([[-6.6195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.935040760733099, distance: 0.29166019270572296 entropy 0.03264415264129639
epoch: 64, step: 106
	action: tensor([[ 1.0174, -0.0476, -0.2267,  0.4319, -0.3718, -0.0423, -0.4009]],
       dtype=torch.float64)
	q_value: tensor([[-5.1750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8786722417343299, distance: 0.3985995278615552 entropy 0.03264415264129639
epoch: 64, step: 107
	action: tensor([[ 0.7926, -0.3340, -0.0550,  0.6334, -0.2140,  0.1666, -0.2380]],
       dtype=torch.float64)
	q_value: tensor([[-10.1799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8548850594148628, distance: 0.43592598490990925 entropy 0.03264415264129639
epoch: 64, step: 108
	action: tensor([[ 0.8327, -0.0398,  0.1380,  0.5995, -0.0507,  0.0830, -0.0400]],
       dtype=torch.float64)
	q_value: tensor([[-7.3096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9535363882979663, distance: 0.2466682063848599 entropy 0.03264415264129639
epoch: 64, step: 109
	action: tensor([[ 0.7908,  0.1515, -0.3941,  0.4456, -0.3001, -0.0657,  0.0588]],
       dtype=torch.float64)
	q_value: tensor([[-6.8260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.955564413643352, distance: 0.24122490883365785 entropy 0.03264415264129639
epoch: 64, step: 110
	action: tensor([[ 0.4280, -0.0603, -0.3703,  0.4369,  0.1085,  0.2618,  0.2256]],
       dtype=torch.float64)
	q_value: tensor([[-7.7468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7699040211405328, distance: 0.5489227214103067 entropy 0.03264415264129639
epoch: 64, step: 111
	action: tensor([[ 0.8607,  0.0856, -0.1428,  0.7292, -0.1922,  0.5564, -0.1182]],
       dtype=torch.float64)
	q_value: tensor([[-4.4176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9904079904565972, distance: 0.11207570885389267 entropy 0.03264415264129639
epoch: 64, step: 112
	action: tensor([[ 0.7383, -0.0030, -0.6691,  0.9578, -0.3653,  0.7579, -0.0548]],
       dtype=torch.float64)
	q_value: tensor([[-9.5180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7959756785852601, distance: 0.5168894455200038 entropy 0.03264415264129639
epoch: 64, step: 113
	action: tensor([[ 0.9037,  0.2163, -0.7154,  0.5931, -0.1437,  0.1868, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-11.1259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9682840185390094, distance: 0.2037960540952264 entropy 0.03264415264129639
epoch: 64, step: 114
	action: tensor([[ 0.8293, -0.0643, -0.4029,  0.1481, -0.2418,  0.0520,  0.3350]],
       dtype=torch.float64)
	q_value: tensor([[-10.2143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7785874665420178, distance: 0.5384653897023618 entropy 0.03264415264129639
epoch: 64, step: 115
	action: tensor([[ 0.4562,  0.3100, -0.3349,  1.0797, -0.1618,  0.1880,  0.1161]],
       dtype=torch.float64)
	q_value: tensor([[-6.3964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7525481430877521, distance: 0.5692487057242154 entropy 0.03264415264129639
epoch: 64, step: 116
	action: tensor([[ 1.0443,  0.3371, -0.7895,  0.4719, -0.0176,  0.4262,  0.4039]],
       dtype=torch.float64)
	q_value: tensor([[-7.8537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09971653670664202 entropy 0.03264415264129639
epoch: 64, step: 117
	action: tensor([[ 0.6216,  0.0443, -0.3429,  0.2928,  0.1570,  0.2908,  0.2221]],
       dtype=torch.float64)
	q_value: tensor([[-9.7729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8709746082682326, distance: 0.4110496534737093 entropy 0.03264415264129639
epoch: 64, step: 118
	action: tensor([[ 0.7668,  0.3262, -0.2404,  0.2266,  0.0702,  0.0438, -0.2669]],
       dtype=torch.float64)
	q_value: tensor([[-5.2705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9642100183606612, distance: 0.21648979593525852 entropy 0.03264415264129639
epoch: 64, step: 119
	action: tensor([[ 0.6980, -0.1477, -0.0711,  0.7895, -0.0219,  0.1954, -0.2657]],
       dtype=torch.float64)
	q_value: tensor([[-7.8176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9529014838167411, distance: 0.24834779353161857 entropy 0.03264415264129639
epoch: 64, step: 120
	action: tensor([[ 0.8096,  0.0151, -0.7170,  1.0449,  0.1674,  0.6515, -0.2117]],
       dtype=torch.float64)
	q_value: tensor([[-7.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8624844000187182, distance: 0.42435827687222416 entropy 0.03264415264129639
epoch: 64, step: 121
	action: tensor([[ 1.1754,  0.4482, -0.5127,  0.9319, -0.3703, -0.0559,  0.1535]],
       dtype=torch.float64)
	q_value: tensor([[-10.8480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9378531278544837, distance: 0.28527671900988233 entropy 0.03264415264129639
epoch: 64, step: 122
	action: tensor([[ 1.3201,  0.3287, -0.5774,  0.6652, -0.0420, -0.0906,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-12.6198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9556161846322182, distance: 0.2410843447866404 entropy 0.03264415264129639
epoch: 64, step: 123
	action: tensor([[ 1.2024,  0.0242, -0.4269,  0.6676, -0.5739,  0.6553, -0.1191]],
       dtype=torch.float64)
	q_value: tensor([[-12.4702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08859579030677651 entropy 0.03264415264129639
epoch: 64, step: 124
	action: tensor([[ 0.7510, -0.0888, -0.1798,  0.1725, -0.2783,  0.1924, -0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-9.7729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7824409214001771, distance: 0.5337591078075644 entropy 0.03264415264129639
epoch: 64, step: 125
	action: tensor([[ 0.8771,  0.3989,  0.0364,  0.8764,  0.3323,  0.0351, -0.1607]],
       dtype=torch.float64)
	q_value: tensor([[-6.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9250314942340967, distance: 0.3133257736390215 entropy 0.03264415264129639
epoch: 64, step: 126
	action: tensor([[ 8.0964e-01, -1.6830e-01, -7.3897e-01,  5.1212e-01,  1.3552e-01,
          3.6869e-02,  7.4917e-04]], dtype=torch.float64)
	q_value: tensor([[-8.7374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7988978124895038, distance: 0.5131745269188523 entropy 0.03264415264129639
epoch: 64, step: 127
	action: tensor([[ 0.6644,  0.1795, -0.2058,  0.7516, -0.1983,  0.1510,  0.1694]],
       dtype=torch.float64)
	q_value: tensor([[-7.2592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9533979589507743, distance: 0.2470353832186139 entropy 0.03264415264129639
LOSS epoch 64 actor 335.1683757722807 critic 4229.452801451967 
epoch: 65, step: 0
	action: tensor([[ 0.7615,  0.2941, -0.1801,  0.9813,  0.0902,  0.1963,  0.1663]],
       dtype=torch.float64)
	q_value: tensor([[-5.5551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9129488765889708, distance: 0.3376322192218869 entropy 0.03264415264129639
epoch: 65, step: 1
	action: tensor([[ 0.7935,  0.0024, -0.0818,  0.5159, -0.1591,  0.3147,  0.2205]],
       dtype=torch.float64)
	q_value: tensor([[-6.3253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9687335661391818, distance: 0.20234657989663332 entropy 0.03264415264129639
epoch: 65, step: 2
	action: tensor([[ 0.7262,  0.0312, -0.2965, -0.0912, -0.4978,  0.4352,  0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-5.2350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7950519080349215, distance: 0.5180582963192534 entropy 0.03264415264129639
epoch: 65, step: 3
	action: tensor([[ 0.2531,  0.0430, -0.5066,  0.2014, -0.0234,  0.3831, -0.4697]],
       dtype=torch.float64)
	q_value: tensor([[-6.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6344002653804495, distance: 0.6919259667030272 entropy 0.03264415264129639
epoch: 65, step: 4
	action: tensor([[ 0.3301,  0.1316, -0.5383,  0.4836,  0.0405,  0.3816, -0.0580]],
       dtype=torch.float64)
	q_value: tensor([[-4.6075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7262756543216262, distance: 0.598705718653731 entropy 0.03264415264129639
epoch: 65, step: 5
	action: tensor([[ 0.8813, -0.1049, -0.5202,  0.6621, -0.3491,  0.4333, -0.0499]],
       dtype=torch.float64)
	q_value: tensor([[-4.5860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9323018465706904, distance: 0.29774543021406324 entropy 0.03264415264129639
epoch: 65, step: 6
	action: tensor([[ 0.5539,  0.0317, -0.6549,  0.3927,  0.1189,  0.1280, -0.1936]],
       dtype=torch.float64)
	q_value: tensor([[-7.5705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8062990910795216, distance: 0.5036426757375317 entropy 0.03264415264129639
epoch: 65, step: 7
	action: tensor([[ 0.6908, -0.1327, -0.0047,  0.4793, -0.0440,  0.2372, -0.0348]],
       dtype=torch.float64)
	q_value: tensor([[-5.0573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8948875047857971, distance: 0.3710085055383402 entropy 0.03264415264129639
epoch: 65, step: 8
	action: tensor([[ 0.8740,  0.1349, -0.1515,  0.7144, -0.4918, -0.0835, -0.4313]],
       dtype=torch.float64)
	q_value: tensor([[-4.5495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09146560968869047 entropy 0.03264415264129639
epoch: 65, step: 9
	action: tensor([[ 0.3830, -0.1861, -0.1196,  0.2620, -0.1829,  0.3346,  0.3059]],
       dtype=torch.float64)
	q_value: tensor([[-8.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6366395265814864, distance: 0.6898037239854526 entropy 0.03264415264129639
epoch: 65, step: 10
	action: tensor([[ 0.8108, -0.0443, -0.6364,  0.1300, -0.5991,  0.0202,  0.2930]],
       dtype=torch.float64)
	q_value: tensor([[-2.9717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7465417936626286, distance: 0.5761159139633334 entropy 0.03264415264129639
epoch: 65, step: 11
	action: tensor([[ 0.4398, -0.0012, -0.0651,  0.3784, -0.1880,  0.1746, -0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-6.2702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8079614440035188, distance: 0.5014768729993369 entropy 0.03264415264129639
epoch: 65, step: 12
	action: tensor([[ 1.1066, -0.1245, -0.4335,  0.5929, -0.2968,  0.3441,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-4.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9335272570181492, distance: 0.2950383654753501 entropy 0.03264415264129639
epoch: 65, step: 13
	action: tensor([[ 0.4306, -0.0056, -0.4910,  0.4773, -0.2031, -0.2263,  0.2604]],
       dtype=torch.float64)
	q_value: tensor([[-8.0682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7459911524547458, distance: 0.5767413840476393 entropy 0.03264415264129639
epoch: 65, step: 14
	action: tensor([[ 0.7254, -0.1835, -0.1231,  0.5548,  0.1064,  0.2503,  0.1685]],
       dtype=torch.float64)
	q_value: tensor([[-3.6313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8978716637654007, distance: 0.36570409357782796 entropy 0.03264415264129639
epoch: 65, step: 15
	action: tensor([[ 0.4574,  0.1049, -0.2495,  1.1307,  0.2878,  0.3290, -0.2028]],
       dtype=torch.float64)
	q_value: tensor([[-4.2237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8437237103834287, distance: 0.45237983619980565 entropy 0.03264415264129639
epoch: 65, step: 16
	action: tensor([[ 0.9192,  0.1319, -0.4603,  0.9163, -0.0572,  0.2856, -0.2962]],
       dtype=torch.float64)
	q_value: tensor([[-5.8339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9668301978380195, distance: 0.2084145974553274 entropy 0.03264415264129639
epoch: 65, step: 17
	action: tensor([[ 0.7216, -0.0310, -0.5085,  0.2818, -0.2303,  0.2137,  0.2822]],
       dtype=torch.float64)
	q_value: tensor([[-8.7219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8430430020431362, distance: 0.4533640038129905 entropy 0.03264415264129639
epoch: 65, step: 18
	action: tensor([[ 0.4626,  0.0732,  0.0491,  0.2252, -0.1250, -0.3350, -0.6088]],
       dtype=torch.float64)
	q_value: tensor([[-5.1722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7574564146342782, distance: 0.5635748315004337 entropy 0.03264415264129639
epoch: 65, step: 19
	action: tensor([[ 0.9263,  0.2087, -0.4279,  0.4163,  0.4093,  0.3752,  0.1903]],
       dtype=torch.float64)
	q_value: tensor([[-4.7436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09189702720007238 entropy 0.03264415264129639
epoch: 65, step: 20
	action: tensor([[ 0.3592, -0.0707, -0.4287,  0.5668, -0.0244,  0.1198, -0.0294]],
       dtype=torch.float64)
	q_value: tensor([[-8.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6844336422053439, distance: 0.6428387411078738 entropy 0.03264415264129639
epoch: 65, step: 21
	action: tensor([[ 0.3944,  0.2519, -0.2036,  0.3476, -0.2975,  0.4626, -0.3494]],
       dtype=torch.float64)
	q_value: tensor([[-3.6785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8207062017330423, distance: 0.4845508135812988 entropy 0.03264415264129639
epoch: 65, step: 22
	action: tensor([[ 0.7565, -0.0495, -0.3745,  0.1981, -0.3705,  0.7572, -0.3210]],
       dtype=torch.float64)
	q_value: tensor([[-5.9595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8833345229679994, distance: 0.39086598151513136 entropy 0.03264415264129639
epoch: 65, step: 23
	action: tensor([[ 0.5821,  0.1562, -0.4261,  0.4467, -0.0074, -0.0104,  0.0862]],
       dtype=torch.float64)
	q_value: tensor([[-8.0368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8954563004661735, distance: 0.3700033239271245 entropy 0.03264415264129639
epoch: 65, step: 24
	action: tensor([[ 0.8793, -0.0072, -0.6124,  0.6809, -0.1954,  0.2455,  0.1593]],
       dtype=torch.float64)
	q_value: tensor([[-4.4984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9444437885042792, distance: 0.2697261196420796 entropy 0.03264415264129639
epoch: 65, step: 25
	action: tensor([[ 0.7875,  0.0574, -0.4844,  0.5191, -0.2141,  0.2825,  0.1326]],
       dtype=torch.float64)
	q_value: tensor([[-6.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9434081195387662, distance: 0.27222860383708036 entropy 0.03264415264129639
epoch: 65, step: 26
	action: tensor([[ 0.2414,  0.3622, -0.6054,  0.6881,  0.3408,  0.5850, -0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-6.2051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6404432852550668, distance: 0.6861836964104264 entropy 0.03264415264129639
epoch: 65, step: 27
	action: tensor([[ 0.1714,  0.3171, -0.4221,  0.4344, -0.3532,  0.0968,  0.2135]],
       dtype=torch.float64)
	q_value: tensor([[-5.8338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6682631327627551, distance: 0.6591034164561157 entropy 0.03264415264129639
epoch: 65, step: 28
	action: tensor([[ 0.9078,  0.1017, -0.1883,  0.5477, -0.0494,  0.2798, -0.3854]],
       dtype=torch.float64)
	q_value: tensor([[-4.0303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9909671793351068, distance: 0.1087597967350288 entropy 0.03264415264129639
epoch: 65, step: 29
	action: tensor([[ 0.9668, -0.1382, -0.5320,  0.6652, -0.3041,  0.1677,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-7.6683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9258366078182724, distance: 0.3116387733404246 entropy 0.03264415264129639
epoch: 65, step: 30
	action: tensor([[ 0.6807, -0.1362, -0.7532,  0.6463, -0.7947,  0.6318, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-7.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7009227694606618, distance: 0.6258184584640691 entropy 0.03264415264129639
epoch: 65, step: 31
	action: tensor([[ 0.9133,  0.2859, -0.0028,  0.1760,  0.0933,  0.7561, -0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-8.8383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07488663500396903 entropy 0.03264415264129639
epoch: 65, step: 32
	action: tensor([[ 0.3553,  0.0022, -0.4838,  0.2075, -0.1060, -0.2059, -0.2074]],
       dtype=torch.float64)
	q_value: tensor([[-8.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6427929248842308, distance: 0.6839379769176771 entropy 0.03264415264129639
epoch: 65, step: 33
	action: tensor([[ 0.8416, -0.0999, -0.7185,  0.5551, -0.2529, -0.0070, -0.4412]],
       dtype=torch.float64)
	q_value: tensor([[-3.5083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8532102060165347, distance: 0.43843440177389725 entropy 0.03264415264129639
epoch: 65, step: 34
	action: tensor([[ 0.5824, -0.1084, -0.4091,  0.3378,  0.1057, -0.1872, -0.5948]],
       dtype=torch.float64)
	q_value: tensor([[-8.0015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.727666135589423, distance: 0.5971831120137684 entropy 0.03264415264129639
epoch: 65, step: 35
	action: tensor([[ 0.8630,  0.0556, -0.4606,  0.7014,  0.1799,  0.3305,  0.1177]],
       dtype=torch.float64)
	q_value: tensor([[-5.1763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9770194015338066, distance: 0.17347518548345256 entropy 0.03264415264129639
epoch: 65, step: 36
	action: tensor([[ 0.7001,  0.0172, -0.3834,  0.8839, -0.0160,  0.5991, -0.1378]],
       dtype=torch.float64)
	q_value: tensor([[-6.1758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9201813778726592, distance: 0.32330230914288943 entropy 0.03264415264129639
epoch: 65, step: 37
	action: tensor([[ 0.6933,  0.1006, -0.7502,  0.2725, -0.1290, -0.1129,  0.1318]],
       dtype=torch.float64)
	q_value: tensor([[-7.1297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8586883867185428, distance: 0.430175448862544 entropy 0.03264415264129639
epoch: 65, step: 38
	action: tensor([[ 0.7763,  0.3596, -0.3751,  0.3557, -0.4385,  0.0895,  0.1883]],
       dtype=torch.float64)
	q_value: tensor([[-5.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9772613922241824, distance: 0.1725594025747368 entropy 0.03264415264129639
epoch: 65, step: 39
	action: tensor([[ 1.5764,  0.1886, -0.4657,  0.0313, -0.4094,  0.0407,  0.0571]],
       dtype=torch.float64)
	q_value: tensor([[-6.6203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5509989013506356, distance: 0.7667969823780373 entropy 0.03264415264129639
epoch: 65, step: 40
	action: tensor([[ 0.7000, -0.3327, -0.1944,  0.4633, -0.1341,  0.0364, -0.2533]],
       dtype=torch.float64)
	q_value: tensor([[-10.7416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7266487720984255, distance: 0.5982975271136693 entropy 0.03264415264129639
epoch: 65, step: 41
	action: tensor([[ 0.7245, -0.3091,  0.2439,  0.6430, -0.2208,  0.4732, -0.5864]],
       dtype=torch.float64)
	q_value: tensor([[-4.7926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9123616988686333, distance: 0.3387690048390924 entropy 0.03264415264129639
epoch: 65, step: 42
	action: tensor([[ 0.5064, -0.1921, -0.5568,  0.6961, -0.7242,  0.1727, -0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-6.8831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7020086813589116, distance: 0.6246812911474573 entropy 0.03264415264129639
epoch: 65, step: 43
	action: tensor([[ 0.5497, -0.1838, -0.7019,  0.7871, -0.2231,  0.3199,  0.2842]],
       dtype=torch.float64)
	q_value: tensor([[-6.6137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7081240638725235, distance: 0.6182382038787868 entropy 0.03264415264129639
epoch: 65, step: 44
	action: tensor([[ 0.4111, -0.1886, -0.4784,  0.2535, -0.0245,  0.3228, -0.2764]],
       dtype=torch.float64)
	q_value: tensor([[-5.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6278607610747502, distance: 0.698086799575184 entropy 0.03264415264129639
epoch: 65, step: 45
	action: tensor([[ 0.9716,  0.2506, -0.4757, -0.0379,  0.2098,  0.2565,  0.1329]],
       dtype=torch.float64)
	q_value: tensor([[-4.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9173505384615488, distance: 0.3289854625963487 entropy 0.03264415264129639
epoch: 65, step: 46
	action: tensor([[ 0.6859,  0.2047, -0.1839,  0.3170,  0.0358,  0.4074,  0.1245]],
       dtype=torch.float64)
	q_value: tensor([[-6.3876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9654775498756633, distance: 0.2126216571609367 entropy 0.03264415264129639
epoch: 65, step: 47
	action: tensor([[0.7816, 0.2097, 0.1992, 0.3060, 0.4408, 0.2214, 0.1073]],
       dtype=torch.float64)
	q_value: tensor([[-5.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.978342739521081, distance: 0.16840634615785144 entropy 0.03264415264129639
epoch: 65, step: 48
	action: tensor([[ 1.0116, -0.3680, -0.2965,  0.7015,  0.0832, -0.5521,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-4.1146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6793674242319581, distance: 0.6479783792208887 entropy 0.03264415264129639
epoch: 65, step: 49
	action: tensor([[ 0.4570,  0.0779, -0.3745,  0.5619,  0.1697,  0.5573,  0.2465]],
       dtype=torch.float64)
	q_value: tensor([[-5.1509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8610948618001499, distance: 0.4264968704208888 entropy 0.03264415264129639
epoch: 65, step: 50
	action: tensor([[ 0.8015,  0.2731, -0.5072,  0.2507, -0.3858,  0.1321, -0.1417]],
       dtype=torch.float64)
	q_value: tensor([[-4.4528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9631672420807585, distance: 0.21962097365950406 entropy 0.03264415264129639
epoch: 65, step: 51
	action: tensor([[ 0.3433, -0.2397,  0.1720,  0.6593,  0.3259,  0.2397,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[-7.5203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8179957296432265, distance: 0.48819967080728444 entropy 0.03264415264129639
epoch: 65, step: 52
	action: tensor([[ 1.0424,  0.0664, -0.0620,  0.6230,  0.0186,  0.0673, -0.1772]],
       dtype=torch.float64)
	q_value: tensor([[-2.6948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9687464710372814, distance: 0.20230481735498132 entropy 0.03264415264129639
epoch: 65, step: 53
	action: tensor([[ 1.0342,  0.0739, -0.2878,  0.2961, -0.4994,  0.5922,  0.1821]],
       dtype=torch.float64)
	q_value: tensor([[-7.2041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9645871656455195, distance: 0.21534611305790932 entropy 0.03264415264129639
epoch: 65, step: 54
	action: tensor([[ 0.5902,  0.1749, -0.5562,  0.3307, -0.3477,  0.1812,  0.2072]],
       dtype=torch.float64)
	q_value: tensor([[-8.1879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8745595386515718, distance: 0.4052989835468595 entropy 0.03264415264129639
epoch: 65, step: 55
	action: tensor([[ 1.0268,  0.1552, -0.0385,  0.1486,  0.0825,  0.5401, -0.3710]],
       dtype=torch.float64)
	q_value: tensor([[-5.5606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9480778715613304, distance: 0.2607551753384156 entropy 0.03264415264129639
epoch: 65, step: 56
	action: tensor([[ 0.5921, -0.1215, -0.0703,  0.4852, -0.1581, -0.2567,  0.0871]],
       dtype=torch.float64)
	q_value: tensor([[-7.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7975172038488942, distance: 0.5149330392513469 entropy 0.03264415264129639
epoch: 65, step: 57
	action: tensor([[ 0.9750, -0.3698, -0.3213,  0.6595, -0.2845, -0.0202,  0.2035]],
       dtype=torch.float64)
	q_value: tensor([[-3.6130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7901091317156141, distance: 0.524268138907327 entropy 0.03264415264129639
epoch: 65, step: 58
	action: tensor([[ 1.0647,  0.1902, -0.2867,  0.7397, -0.4079,  0.0202, -0.5594]],
       dtype=torch.float64)
	q_value: tensor([[-5.7854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08955415853844156 entropy 0.03264415264129639
epoch: 65, step: 59
	action: tensor([[ 0.8735,  0.1623, -0.1457,  1.0113, -0.2485, -0.1871, -0.0780]],
       dtype=torch.float64)
	q_value: tensor([[-8.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9889635217270338, distance: 0.12021867847339465 entropy 0.03264415264129639
epoch: 65, step: 60
	action: tensor([[ 1.1418,  0.2647,  0.1831,  0.3558,  0.2123,  0.6265, -0.1701]],
       dtype=torch.float64)
	q_value: tensor([[-7.5217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9174653915438645, distance: 0.32875679728059204 entropy 0.03264415264129639
epoch: 65, step: 61
	action: tensor([[ 0.9530,  0.1431, -0.4041,  0.8419,  0.3960,  0.0957,  0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-8.0367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08520845581062961 entropy 0.03264415264129639
epoch: 65, step: 62
	action: tensor([[ 0.5618,  0.2729, -0.1378,  0.6040, -0.2951,  0.1045,  0.1676]],
       dtype=torch.float64)
	q_value: tensor([[-8.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9203026715530035, distance: 0.32305656801767874 entropy 0.03264415264129639
epoch: 65, step: 63
	action: tensor([[ 0.4713, -0.0578, -0.0447,  0.9704,  0.0837,  0.1856, -0.2755]],
       dtype=torch.float64)
	q_value: tensor([[-5.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9122446981237528, distance: 0.3389950647080166 entropy 0.03264415264129639
epoch: 65, step: 64
	action: tensor([[ 1.2695,  0.0710, -0.5112,  0.2076,  0.0155, -0.2032, -0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-5.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7334905276988011, distance: 0.5907626336793256 entropy 0.03264415264129639
epoch: 65, step: 65
	action: tensor([[ 0.7034,  0.3203, -0.6018,  0.4478,  0.0768, -0.2200,  0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-8.0179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9486629352621496, distance: 0.2592819057350667 entropy 0.03264415264129639
epoch: 65, step: 66
	action: tensor([[ 0.4998, -0.0928, -0.3088,  0.1704,  0.0178,  0.3440, -0.3995]],
       dtype=torch.float64)
	q_value: tensor([[-5.7062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7499272073783354, distance: 0.5722554207556059 entropy 0.03264415264129639
epoch: 65, step: 67
	action: tensor([[ 0.5787, -0.0840, -0.6212,  0.5612, -0.5579,  0.4462, -0.2151]],
       dtype=torch.float64)
	q_value: tensor([[-4.5514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7463980990540108, distance: 0.5762792012797718 entropy 0.03264415264129639
epoch: 65, step: 68
	action: tensor([[ 0.3309,  0.2183, -0.1303,  0.8467, -0.2541,  0.4756, -0.0532]],
       dtype=torch.float64)
	q_value: tensor([[-7.3501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7730872993069849, distance: 0.545112443125006 entropy 0.03264415264129639
epoch: 65, step: 69
	action: tensor([[ 0.9252, -0.0427, -0.5568,  0.7786,  0.2141,  0.4694, -0.3807]],
       dtype=torch.float64)
	q_value: tensor([[-5.6504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9730918723237288, distance: 0.18771476458450256 entropy 0.03264415264129639
epoch: 65, step: 70
	action: tensor([[ 1.0583, -0.1151, -0.4237,  0.2831,  0.0022,  0.0435, -0.3388]],
       dtype=torch.float64)
	q_value: tensor([[-8.2919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.789352574105697, distance: 0.5252121587017681 entropy 0.03264415264129639
epoch: 65, step: 71
	action: tensor([[ 1.1615, -0.0819, -0.4502,  0.1586,  0.0595, -0.0048,  0.4844]],
       dtype=torch.float64)
	q_value: tensor([[-7.3222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6907688546204769, distance: 0.6363533105225475 entropy 0.03264415264129639
epoch: 65, step: 72
	action: tensor([[ 0.8201, -0.0355, -0.3262,  0.1328, -0.0637,  0.0021,  0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-5.9732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7833777431653441, distance: 0.5326086694715265 entropy 0.03264415264129639
epoch: 65, step: 73
	action: tensor([[ 0.5188,  0.7300, -0.5378,  0.4032,  0.1924,  0.7371, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[-4.6344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 65, step: 74
	action: tensor([[ 0.4637,  0.1853, -0.0758,  0.2144, -0.0668,  0.2263,  0.1775]],
       dtype=torch.float64)
	q_value: tensor([[-8.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8502810688824616, distance: 0.44278719422129215 entropy 0.03264415264129639
epoch: 65, step: 75
	action: tensor([[ 0.5277, -0.3098, -0.2880,  0.6751,  0.0696,  0.2554, -0.5389]],
       dtype=torch.float64)
	q_value: tensor([[-3.4201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7681324559399806, distance: 0.5510318147109755 entropy 0.03264415264129639
epoch: 65, step: 76
	action: tensor([[ 0.8342,  0.3625, -0.2513,  0.5960,  0.2904,  0.2490,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-5.3432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9672954674072253, distance: 0.20694772980479845 entropy 0.03264415264129639
epoch: 65, step: 77
	action: tensor([[ 0.9151,  0.1384, -0.6491,  0.6192, -0.1533, -0.0680,  0.3168]],
       dtype=torch.float64)
	q_value: tensor([[-6.1305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9615819387906681, distance: 0.22429749087945064 entropy 0.03264415264129639
epoch: 65, step: 78
	action: tensor([[ 0.9324,  0.2286, -0.4466,  0.2625,  0.0790,  0.2830, -0.3966]],
       dtype=torch.float64)
	q_value: tensor([[-6.6446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9760908897879538, distance: 0.17694504282963316 entropy 0.03264415264129639
epoch: 65, step: 79
	action: tensor([[ 0.5459,  0.1297, -0.7635,  0.3714, -0.3695,  0.3391, -0.1393]],
       dtype=torch.float64)
	q_value: tensor([[-8.0343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7876906437163964, distance: 0.5272799531556356 entropy 0.03264415264129639
epoch: 65, step: 80
	action: tensor([[ 0.2316, -0.1154, -0.2777,  0.0865, -0.0013,  0.2612, -0.3499]],
       dtype=torch.float64)
	q_value: tensor([[-6.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5243120599593625, distance: 0.7892557667679923 entropy 0.03264415264129639
epoch: 65, step: 81
	action: tensor([[ 1.0663,  0.6121, -0.4705,  0.5269, -0.2151,  0.5792,  0.1771]],
       dtype=torch.float64)
	q_value: tensor([[-2.9530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9057668558761572, distance: 0.3512841333005241 entropy 0.03264415264129639
epoch: 65, step: 82
	action: tensor([[ 0.6596, -0.3132, -0.1984,  0.1801, -0.1107,  0.6217, -0.1884]],
       dtype=torch.float64)
	q_value: tensor([[-9.8530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7310091765044695, distance: 0.5935064260195959 entropy 0.03264415264129639
epoch: 65, step: 83
	action: tensor([[ 0.5047,  0.3347, -0.2656,  0.3277, -0.2846, -0.0967,  0.0794]],
       dtype=torch.float64)
	q_value: tensor([[-5.1928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9120689265183948, distance: 0.33933439405988874 entropy 0.03264415264129639
epoch: 65, step: 84
	action: tensor([[ 0.3418, -0.3920, -0.7205,  0.1602, -0.1111,  0.5426, -0.3291]],
       dtype=torch.float64)
	q_value: tensor([[-4.5979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3622057326253759, distance: 0.9138964681829886 entropy 0.03264415264129639
epoch: 65, step: 85
	action: tensor([[ 0.8428,  0.1779, -0.5234,  0.2140, -0.1740,  0.0495, -0.0287]],
       dtype=torch.float64)
	q_value: tensor([[-4.7777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9191079038507688, distance: 0.32546908124590324 entropy 0.03264415264129639
epoch: 65, step: 86
	action: tensor([[ 0.5354,  0.3839, -0.2795,  0.6440, -0.2004, -0.0048,  0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-6.5255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.89024968473375, distance: 0.37910506035747243 entropy 0.03264415264129639
epoch: 65, step: 87
	action: tensor([[ 0.8307,  0.2356, -0.5302,  0.2885, -0.0996,  0.3501, -0.3123]],
       dtype=torch.float64)
	q_value: tensor([[-5.5702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9722888660079265, distance: 0.19049511464169816 entropy 0.03264415264129639
epoch: 65, step: 88
	action: tensor([[ 0.5854,  0.4466, -0.2331,  0.5445, -0.2921,  0.3713,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[-7.9668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8850874554953048, distance: 0.38791843046078617 entropy 0.03264415264129639
epoch: 65, step: 89
	action: tensor([[ 0.8349,  0.0880,  0.1298,  0.6130, -0.4585, -0.0804, -0.5820]],
       dtype=torch.float64)
	q_value: tensor([[-6.5714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9782622965655854, distance: 0.16871881753000284 entropy 0.03264415264129639
epoch: 65, step: 90
	action: tensor([[ 0.3911, -0.1594, -0.3516,  0.4197, -0.2479,  0.4575, -0.2657]],
       dtype=torch.float64)
	q_value: tensor([[-8.0595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6990798812283854, distance: 0.6277436171742629 entropy 0.03264415264129639
epoch: 65, step: 91
	action: tensor([[ 0.6945,  0.1815, -0.3748,  0.3666, -0.3141,  0.1396, -0.3171]],
       dtype=torch.float64)
	q_value: tensor([[-4.9021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9400058997498809, distance: 0.2802921697747783 entropy 0.03264415264129639
epoch: 65, step: 92
	action: tensor([[ 1.3642,  0.1458, -0.1240,  0.1286, -0.1296,  0.0892, -0.1545]],
       dtype=torch.float64)
	q_value: tensor([[-6.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7323176320811402, distance: 0.5920611657968616 entropy 0.03264415264129639
epoch: 65, step: 93
	action: tensor([[ 1.1025,  0.0336, -0.5907,  0.3913, -0.1533,  0.1343,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-8.9148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9126966436338075, distance: 0.33812101459016014 entropy 0.03264415264129639
epoch: 65, step: 94
	action: tensor([[ 0.8417,  0.3889, -0.2581,  0.4938, -0.1720,  0.0986,  0.2004]],
       dtype=torch.float64)
	q_value: tensor([[-7.9339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9845157037855328, distance: 0.1423975209767128 entropy 0.03264415264129639
epoch: 65, step: 95
	action: tensor([[ 0.2543,  0.8284, -0.3099, -0.0065, -0.5387,  0.0807,  0.1833]],
       dtype=torch.float64)
	q_value: tensor([[-6.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8014928657047371, distance: 0.5098527348208597 entropy 0.03264415264129639
epoch: 65, step: 96
	action: tensor([[ 0.6491,  0.2087, -0.2254,  0.6796,  0.1615, -0.1134, -0.0790]],
       dtype=torch.float64)
	q_value: tensor([[-5.9182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9575660427774508, distance: 0.2357292423706724 entropy 0.03264415264129639
epoch: 65, step: 97
	action: tensor([[ 0.5818,  0.0368, -1.0426,  0.7818, -0.1708,  0.2895,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[-5.0997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6612788870927853, distance: 0.6660055174817117 entropy 0.03264415264129639
epoch: 65, step: 98
	action: tensor([[ 0.6068,  0.3168, -0.2834,  0.7368, -0.0523,  0.1107, -0.1974]],
       dtype=torch.float64)
	q_value: tensor([[-7.2293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8998451895069131, distance: 0.36215342768926234 entropy 0.03264415264129639
epoch: 65, step: 99
	action: tensor([[ 0.5901,  0.0999, -0.4601,  0.7089, -0.2221, -0.0544, -0.3373]],
       dtype=torch.float64)
	q_value: tensor([[-6.3729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8894014861008778, distance: 0.3805671860020001 entropy 0.03264415264129639
epoch: 65, step: 100
	action: tensor([[ 0.4814,  0.3922, -0.1119,  0.8815, -0.2262,  0.4838,  0.0422]],
       dtype=torch.float64)
	q_value: tensor([[-6.5306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7905386768867841, distance: 0.523731402400794 entropy 0.03264415264129639
epoch: 65, step: 101
	action: tensor([[ 1.2281, -0.3665, -0.4727,  0.5669, -0.0940,  0.4927, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-6.4580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8232212981824519, distance: 0.4811402210933648 entropy 0.03264415264129639
epoch: 65, step: 102
	action: tensor([[ 0.9569,  0.3486, -0.3920,  0.8419, -0.1587,  0.3277, -0.1346]],
       dtype=torch.float64)
	q_value: tensor([[-8.0831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9316953892007915, distance: 0.29907609659707035 entropy 0.03264415264129639
epoch: 65, step: 103
	action: tensor([[ 0.7908, -0.3333, -0.5449,  0.4310,  0.3084,  0.1419,  0.0608]],
       dtype=torch.float64)
	q_value: tensor([[-9.0710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7064222248650182, distance: 0.6200379628467435 entropy 0.03264415264129639
epoch: 65, step: 104
	action: tensor([[ 0.7221,  0.3805, -0.3274,  0.5782, -0.1323,  0.3388, -0.3213]],
       dtype=torch.float64)
	q_value: tensor([[-4.4960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9319319759222511, distance: 0.2985576922806197 entropy 0.03264415264129639
epoch: 65, step: 105
	action: tensor([[ 0.7692,  0.3211, -0.8153,  0.3734,  0.0011,  0.0424, -0.1821]],
       dtype=torch.float64)
	q_value: tensor([[-7.8555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9504559655033027, distance: 0.25471374308723066 entropy 0.03264415264129639
epoch: 65, step: 106
	action: tensor([[ 0.6021,  0.1036, -0.3375,  0.2579,  0.2528,  0.1934,  0.1885]],
       dtype=torch.float64)
	q_value: tensor([[-7.5513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8859439787740199, distance: 0.38647001319209706 entropy 0.03264415264129639
epoch: 65, step: 107
	action: tensor([[ 0.4838, -0.0086,  0.1852,  0.7191, -0.1312,  0.5162, -0.4864]],
       dtype=torch.float64)
	q_value: tensor([[-3.7814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9656582390479592, distance: 0.21206450039424138 entropy 0.03264415264129639
epoch: 65, step: 108
	action: tensor([[ 0.8179,  0.1591, -0.3102,  0.5669, -0.2573,  0.3804, -0.3887]],
       dtype=torch.float64)
	q_value: tensor([[-6.2002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9809321451988987, distance: 0.1580183822745224 entropy 0.03264415264129639
epoch: 65, step: 109
	action: tensor([[ 0.8558, -0.1699, -0.5533,  1.1627, -0.6170, -0.2612,  0.1631]],
       dtype=torch.float64)
	q_value: tensor([[-8.2215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.954085229917933, distance: 0.24520702048044926 entropy 0.03264415264129639
epoch: 65, step: 110
	action: tensor([[ 0.7664,  0.0098, -0.3338,  0.6371,  0.3053, -0.2157, -0.1885]],
       dtype=torch.float64)
	q_value: tensor([[-7.5584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9242554209195596, distance: 0.3149433709104397 entropy 0.03264415264129639
epoch: 65, step: 111
	action: tensor([[ 0.8829,  0.3231, -0.8049, -0.0701,  0.0055,  0.2808,  0.3810]],
       dtype=torch.float64)
	q_value: tensor([[-5.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9445234524451641, distance: 0.2695326655395229 entropy 0.03264415264129639
epoch: 65, step: 112
	action: tensor([[ 6.9590e-01, -1.4197e-01, -2.3415e-04,  5.3944e-01, -2.2618e-01,
          6.1411e-01,  4.1246e-01]], dtype=torch.float64)
	q_value: tensor([[-6.8685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9509679764159438, distance: 0.2533941602643075 entropy 0.03264415264129639
epoch: 65, step: 113
	action: tensor([[ 0.6140,  0.1883, -0.3694,  0.1856, -0.3084,  0.1958,  0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-4.9561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8972818324303315, distance: 0.3667586157326443 entropy 0.03264415264129639
epoch: 65, step: 114
	action: tensor([[ 0.8621, -0.0369, -0.4302,  0.3294, -0.3646,  0.3802, -0.2995]],
       dtype=torch.float64)
	q_value: tensor([[-5.1968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9053253028434141, distance: 0.35210618640053354 entropy 0.03264415264129639
epoch: 65, step: 115
	action: tensor([[ 0.4857, -0.1303, -1.0108,  0.6309, -0.2803,  0.0983,  0.4548]],
       dtype=torch.float64)
	q_value: tensor([[-7.7391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5706047961092507, distance: 0.749868806480799 entropy 0.03264415264129639
epoch: 65, step: 116
	action: tensor([[ 0.8117, -0.0061, -0.7278,  0.3841,  0.2775,  0.3058,  0.3778]],
       dtype=torch.float64)
	q_value: tensor([[-5.2493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8842870283587307, distance: 0.3892671153000051 entropy 0.03264415264129639
epoch: 65, step: 117
	action: tensor([[ 0.5418,  0.0917, -0.3106,  0.5146, -0.3937, -0.0842,  0.1077]],
       dtype=torch.float64)
	q_value: tensor([[-5.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8703858178465003, distance: 0.41198647147041395 entropy 0.03264415264129639
epoch: 65, step: 118
	action: tensor([[ 0.5540,  0.4208, -0.2787,  1.0775, -0.3646, -0.0758,  0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-4.7434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7918996568391269, distance: 0.5220271508842743 entropy 0.03264415264129639
epoch: 65, step: 119
	action: tensor([[ 1.0862, -0.2683, -0.4054,  0.3230,  0.0957,  0.1645,  0.0928]],
       dtype=torch.float64)
	q_value: tensor([[-6.8620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7166214224639935, distance: 0.6091723759851517 entropy 0.03264415264129639
epoch: 65, step: 120
	action: tensor([[ 0.4630,  0.3073, -0.8517,  0.2593,  0.0689,  0.2668, -0.0904]],
       dtype=torch.float64)
	q_value: tensor([[-5.9987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8132142006175271, distance: 0.4945709701351163 entropy 0.03264415264129639
epoch: 65, step: 121
	action: tensor([[ 1.0772,  0.1665, -0.6680,  0.7095, -0.0153,  0.4080, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-5.9546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09503922706515114 entropy 0.03264415264129639
epoch: 65, step: 122
	action: tensor([[ 0.3921, -0.1394, -0.2652,  0.0566,  0.1340,  0.4184, -0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-8.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6128017934682003, distance: 0.7120711030498292 entropy 0.03264415264129639
epoch: 65, step: 123
	action: tensor([[ 0.6478,  0.1672,  0.0918,  0.4029,  0.1173,  0.2927, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[-3.0803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9699184854493786, distance: 0.19847533446667762 entropy 0.03264415264129639
epoch: 65, step: 124
	action: tensor([[ 0.7885,  0.4447, -0.4113,  0.5738, -0.4884,  0.4451, -0.3955]],
       dtype=torch.float64)
	q_value: tensor([[-4.4192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9096951269947327, distance: 0.34388425072601014 entropy 0.03264415264129639
epoch: 65, step: 125
	action: tensor([[ 0.4705,  0.0373, -0.1982,  0.5914, -0.0293,  0.5172, -0.1456]],
       dtype=torch.float64)
	q_value: tensor([[-9.9267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8837392943660186, distance: 0.39018733672172357 entropy 0.03264415264129639
epoch: 65, step: 126
	action: tensor([[ 0.9419,  0.4653, -0.5487,  0.2320, -0.0147,  0.0056, -0.0633]],
       dtype=torch.float64)
	q_value: tensor([[-5.1248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9887880706264033, distance: 0.121170491626177 entropy 0.03264415264129639
epoch: 65, step: 127
	action: tensor([[ 0.8509,  0.1081, -0.4152,  0.2888,  0.1536,  0.2512,  0.2165]],
       dtype=torch.float64)
	q_value: tensor([[-7.8313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9446298574281543, distance: 0.2692740573062901 entropy 0.03264415264129639
LOSS epoch 65 actor 296.45348563685445 critic 3995.9197206450226 
epoch: 66, step: 0
	action: tensor([[ 0.7300,  0.2099, -0.6060,  0.7883,  0.0510, -0.1219, -0.3024]],
       dtype=torch.float64)
	q_value: tensor([[-3.7639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9248206515258848, distance: 0.31376606566832016 entropy 0.03264415264129639
epoch: 66, step: 1
	action: tensor([[ 1.0706, -0.0843, -0.3099,  0.4183, -0.5946,  0.5305,  0.2361]],
       dtype=torch.float64)
	q_value: tensor([[-5.2597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9279416506082026, distance: 0.3071841816815177 entropy 0.03264415264129639
epoch: 66, step: 2
	action: tensor([[ 0.9308, -0.2979, -0.7050,  0.1786,  0.0579,  0.1773,  0.2975]],
       dtype=torch.float64)
	q_value: tensor([[-5.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6213924485458248, distance: 0.7041275372830518 entropy 0.03264415264129639
epoch: 66, step: 3
	action: tensor([[ 0.6346,  0.5070, -0.3078,  0.6481,  0.0233,  0.2404,  0.5237]],
       dtype=torch.float64)
	q_value: tensor([[-3.7769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8743713466914662, distance: 0.4056028943554417 entropy 0.03264415264129639
epoch: 66, step: 4
	action: tensor([[ 0.6112,  0.3750, -0.0461,  0.0161,  0.1803,  0.4848, -0.0009]],
       dtype=torch.float64)
	q_value: tensor([[-3.8403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9600364637437989, distance: 0.2287645093260844 entropy 0.03264415264129639
epoch: 66, step: 5
	action: tensor([[1.1163, 0.3120, 0.1498, 0.6485, 0.0872, 0.0016, 0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-3.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9390486740744306, distance: 0.2825193979426921 entropy 0.03264415264129639
epoch: 66, step: 6
	action: tensor([[ 0.4024, -0.2464, -0.1596,  0.8155, -0.0305,  0.3127, -0.2754]],
       dtype=torch.float64)
	q_value: tensor([[-4.9853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8255331563528832, distance: 0.4779837644971629 entropy 0.03264415264129639
epoch: 66, step: 7
	action: tensor([[ 0.7071, -0.0474, -0.6666,  0.2818, -0.7364,  0.0668,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[-3.2612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7945242286314952, distance: 0.5187247893708291 entropy 0.03264415264129639
epoch: 66, step: 8
	action: tensor([[ 0.6741,  0.7804, -0.2257,  0.6655, -0.1148,  0.2923, -0.1835]],
       dtype=torch.float64)
	q_value: tensor([[-4.8814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 66, step: 9
	action: tensor([[ 0.7893,  0.1874,  0.3769,  0.6803, -0.1806, -0.0042,  0.1181]],
       dtype=torch.float64)
	q_value: tensor([[-6.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9915682178201559, distance: 0.10507910925578272 entropy 0.03264415264129639
epoch: 66, step: 10
	action: tensor([[ 0.6421,  0.0713, -0.5092,  0.4965, -0.2052,  0.1941,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-3.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.883329428415272, distance: 0.39087451559763997 entropy 0.03264415264129639
epoch: 66, step: 11
	action: tensor([[ 0.6851,  0.4383, -0.0894,  0.7310,  0.1621, -0.1128,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-3.9194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.93986531978592, distance: 0.2806203721073709 entropy 0.03264415264129639
epoch: 66, step: 12
	action: tensor([[ 0.9467,  0.1920, -0.3675,  0.3564, -0.2624,  0.3556, -0.0984]],
       dtype=torch.float64)
	q_value: tensor([[-3.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9844433082439767, distance: 0.14273001668515073 entropy 0.03264415264129639
epoch: 66, step: 13
	action: tensor([[ 0.6189,  0.0867, -0.4349,  0.6003, -0.1444,  0.2500,  0.0319]],
       dtype=torch.float64)
	q_value: tensor([[-5.7285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.896880241170827, distance: 0.3674748637865434 entropy 0.03264415264129639
epoch: 66, step: 14
	action: tensor([[ 0.7600, -0.0790, -0.8081,  0.5464, -0.1743,  0.2889,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-3.8571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.821689228844508, distance: 0.4832206468805376 entropy 0.03264415264129639
epoch: 66, step: 15
	action: tensor([[ 1.0714,  0.2147, -0.4045,  0.4691, -0.1114,  0.5577, -0.1443]],
       dtype=torch.float64)
	q_value: tensor([[-4.7029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.04353164745173115 entropy 0.03264415264129639
epoch: 66, step: 16
	action: tensor([[ 0.4928,  0.1253, -0.0121,  0.2252, -0.3725,  0.0355,  0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-6.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8209034972540821, distance: 0.4842841394960756 entropy 0.03264415264129639
epoch: 66, step: 17
	action: tensor([[ 0.7199,  0.2318, -0.4706,  0.2835,  0.0679,  0.1488,  0.3588]],
       dtype=torch.float64)
	q_value: tensor([[-2.6627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9334694075708269, distance: 0.29516671954814344 entropy 0.03264415264129639
epoch: 66, step: 18
	action: tensor([[ 1.1511, -0.1757, -0.2973,  0.3699,  0.1867, -0.0128,  0.1073]],
       dtype=torch.float64)
	q_value: tensor([[-3.4533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7524345964353216, distance: 0.5693792945034487 entropy 0.03264415264129639
epoch: 66, step: 19
	action: tensor([[ 0.5547,  0.1402, -0.0081,  0.3344, -0.3718,  0.6283,  0.2632]],
       dtype=torch.float64)
	q_value: tensor([[-4.2647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9323763930603611, distance: 0.2975814524009177 entropy 0.03264415264129639
epoch: 66, step: 20
	action: tensor([[ 0.8281,  0.0080, -0.8065,  0.2122, -0.0346,  0.2112,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[-3.7690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8493443054851311, distance: 0.44417025260995613 entropy 0.03264415264129639
epoch: 66, step: 21
	action: tensor([[ 0.6601,  0.0562, -0.5454,  0.6060, -0.4055,  0.4705, -0.0814]],
       dtype=torch.float64)
	q_value: tensor([[-4.6475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8657134013971919, distance: 0.41934650718244926 entropy 0.03264415264129639
epoch: 66, step: 22
	action: tensor([[ 0.3624,  0.0523, -0.4871,  0.1622, -0.0371, -0.1974,  0.0191]],
       dtype=torch.float64)
	q_value: tensor([[-5.2152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6719742847087984, distance: 0.6554063410515094 entropy 0.03264415264129639
epoch: 66, step: 23
	action: tensor([[ 0.5158,  0.0241, -0.3167,  0.3387, -0.0021, -0.2516, -0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-2.1936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7818835722577803, distance: 0.5344423702501625 entropy 0.03264415264129639
epoch: 66, step: 24
	action: tensor([[ 0.7677,  0.2214, -0.5624,  0.4034, -0.0771,  0.3925, -0.3551]],
       dtype=torch.float64)
	q_value: tensor([[-2.4668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9506696665886355, distance: 0.25416381384106257 entropy 0.03264415264129639
epoch: 66, step: 25
	action: tensor([[ 0.9399,  0.1624, -0.0141,  0.4706, -0.3754, -0.0072, -0.0973]],
       dtype=torch.float64)
	q_value: tensor([[-5.7854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9702737893552218, distance: 0.19729971992071696 entropy 0.03264415264129639
epoch: 66, step: 26
	action: tensor([[ 1.2236,  0.4220, -0.8654,  1.1391,  0.1788,  0.3634, -0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-4.9966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8937544379185958, distance: 0.37300280049790013 entropy 0.03264415264129639
epoch: 66, step: 27
	action: tensor([[ 0.7505,  0.7664, -0.0286,  0.4535, -0.1416,  0.2730, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-8.7012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 66, step: 28
	action: tensor([[ 0.5769, -0.0759,  0.0534,  0.0585,  0.1331, -0.1520,  0.1044]],
       dtype=torch.float64)
	q_value: tensor([[-6.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6293214944340615, distance: 0.6967153756614275 entropy 0.03264415264129639
epoch: 66, step: 29
	action: tensor([[ 0.8074,  0.4025, -0.8303,  0.0337, -0.0982,  0.1099, -0.2017]],
       dtype=torch.float64)
	q_value: tensor([[-1.7317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9566201779510006, distance: 0.23834199836299835 entropy 0.03264415264129639
epoch: 66, step: 30
	action: tensor([[ 0.5436,  0.2633, -0.0800,  0.1417, -0.0893, -0.2259, -0.1853]],
       dtype=torch.float64)
	q_value: tensor([[-5.9261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8714199478166973, distance: 0.4103396579611582 entropy 0.03264415264129639
epoch: 66, step: 31
	action: tensor([[ 0.9533,  0.0490, -0.4308,  0.6903, -0.0232,  0.1987, -0.0749]],
       dtype=torch.float64)
	q_value: tensor([[-2.9478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9876346180763924, distance: 0.1272507822162751 entropy 0.03264415264129639
epoch: 66, step: 32
	action: tensor([[ 0.5367,  0.0624,  0.0463,  0.7692,  0.1340,  0.2804, -0.0910]],
       dtype=torch.float64)
	q_value: tensor([[-5.2382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9655945975247164, distance: 0.21226090646892673 entropy 0.03264415264129639
epoch: 66, step: 33
	action: tensor([[ 0.8298,  0.0534, -0.2720,  0.4813, -0.2269,  0.2934,  0.4479]],
       dtype=torch.float64)
	q_value: tensor([[-3.2487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9652190607699627, distance: 0.21341618196025738 entropy 0.03264415264129639
epoch: 66, step: 34
	action: tensor([[ 0.6473,  0.0270, -0.1315,  0.8092, -0.2797,  0.2766,  0.1371]],
       dtype=torch.float64)
	q_value: tensor([[-3.8307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9623922541013967, distance: 0.22191943865483224 entropy 0.03264415264129639
epoch: 66, step: 35
	action: tensor([[ 0.4522, -0.4875, -0.2593,  0.1572, -0.4343,  0.4954, -0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-3.8778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4037220144217202, distance: 0.8836516262837836 entropy 0.03264415264129639
epoch: 66, step: 36
	action: tensor([[ 0.7953,  0.2545, -0.1171,  0.8216, -0.7206,  0.3325,  0.1973]],
       dtype=torch.float64)
	q_value: tensor([[-3.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9507047371196683, distance: 0.25407345113600904 entropy 0.03264415264129639
epoch: 66, step: 37
	action: tensor([[ 0.9683, -0.0086, -0.3872,  0.6746, -0.2830,  0.5743,  0.2995]],
       dtype=torch.float64)
	q_value: tensor([[-5.6095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9856732334632071, distance: 0.1369716808358343 entropy 0.03264415264129639
epoch: 66, step: 38
	action: tensor([[ 0.9600,  0.4252, -0.3463,  0.4280, -0.6200,  0.2195, -0.6648]],
       dtype=torch.float64)
	q_value: tensor([[-5.2817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9852951938036113, distance: 0.13876704677936208 entropy 0.03264415264129639
epoch: 66, step: 39
	action: tensor([[ 0.8852,  0.3249, -0.4502,  0.0702, -0.0140,  0.5462,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[-8.4148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9878059926202842, distance: 0.1263659073309314 entropy 0.03264415264129639
epoch: 66, step: 40
	action: tensor([[ 0.7943, -0.0513, -0.0845,  0.1821, -0.2132,  0.3322, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-5.3345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8629606711120786, distance: 0.4236227789336309 entropy 0.03264415264129639
epoch: 66, step: 41
	action: tensor([[ 0.2884,  0.2548, -0.4313,  0.7729, -0.1401,  0.2138,  0.1721]],
       dtype=torch.float64)
	q_value: tensor([[-3.6891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7045403108822743, distance: 0.6220220948388951 entropy 0.03264415264129639
epoch: 66, step: 42
	action: tensor([[ 0.8246,  0.1020, -0.1265,  0.3419, -0.4885,  0.1043,  0.0746]],
       dtype=torch.float64)
	q_value: tensor([[-3.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9433360722492473, distance: 0.2724018362371939 entropy 0.03264415264129639
epoch: 66, step: 43
	action: tensor([[ 0.4104,  0.0582, -0.7701,  0.2895, -0.4457,  0.4310, -0.1319]],
       dtype=torch.float64)
	q_value: tensor([[-4.3480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6535772287523276, distance: 0.6735345974737579 entropy 0.03264415264129639
epoch: 66, step: 44
	action: tensor([[ 1.2032,  0.3450, -0.6206, -0.0771, -0.4188, -0.0383, -0.1324]],
       dtype=torch.float64)
	q_value: tensor([[-4.7481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8231397332271239, distance: 0.4812512063379257 entropy 0.03264415264129639
epoch: 66, step: 45
	action: tensor([[ 0.6975,  0.0519, -0.1096,  0.3600,  0.3006, -0.0810,  0.0464]],
       dtype=torch.float64)
	q_value: tensor([[-7.2200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8884956027708216, distance: 0.3821225704097345 entropy 0.03264415264129639
epoch: 66, step: 46
	action: tensor([[ 0.6132, -0.0996, -0.5604,  0.2779,  0.0769, -0.1987, -0.0549]],
       dtype=torch.float64)
	q_value: tensor([[-2.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7091039712110109, distance: 0.6171995340094582 entropy 0.03264415264129639
epoch: 66, step: 47
	action: tensor([[ 0.5387,  0.4971,  0.0799,  0.6281, -0.0989, -0.0249,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-2.8603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9245600433487433, distance: 0.31430942806100026 entropy 0.03264415264129639
epoch: 66, step: 48
	action: tensor([[ 0.8817,  0.3101, -0.0477,  0.4523,  0.0754,  0.3826, -0.0807]],
       dtype=torch.float64)
	q_value: tensor([[-3.4942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9911236330047959, distance: 0.10781379126170731 entropy 0.03264415264129639
epoch: 66, step: 49
	action: tensor([[ 0.9510,  0.0837, -0.3539,  0.7452,  0.0550,  0.3078,  0.0911]],
       dtype=torch.float64)
	q_value: tensor([[-4.8013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.056085555879555576 entropy 0.03264415264129639
epoch: 66, step: 50
	action: tensor([[ 0.4879,  0.7090,  0.0350,  0.4400, -0.1763,  0.6124, -0.3005]],
       dtype=torch.float64)
	q_value: tensor([[-6.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 66, step: 51
	action: tensor([[ 0.5327,  0.2323, -0.4327,  0.5911, -0.0039,  0.1716,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-6.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8785300650595884, distance: 0.398833006842218 entropy 0.03264415264129639
epoch: 66, step: 52
	action: tensor([[ 0.7528, -0.0401, -0.4432,  0.7322,  0.3134,  0.0118,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[-3.5804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9253265223449452, distance: 0.31270864091359307 entropy 0.03264415264129639
epoch: 66, step: 53
	action: tensor([[ 0.6702,  0.2955, -0.5260,  0.3282, -0.4180, -0.0884,  0.3897]],
       dtype=torch.float64)
	q_value: tensor([[-3.4375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9421265270353621, distance: 0.27529382229879157 entropy 0.03264415264129639
epoch: 66, step: 54
	action: tensor([[ 0.2989, -0.1235, -0.4266,  0.2308, -0.1489,  0.3731, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[-4.0003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5846926404812981, distance: 0.7374651583880121 entropy 0.03264415264129639
epoch: 66, step: 55
	action: tensor([[ 0.8025, -0.1058, -0.4006,  0.2163, -0.1145,  0.4715, -0.1218]],
       dtype=torch.float64)
	q_value: tensor([[-2.3598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8442207086348802, distance: 0.4516599220380446 entropy 0.03264415264129639
epoch: 66, step: 56
	action: tensor([[ 0.5908, -0.0792, -0.3165,  0.5935, -0.3316, -0.1439, -0.1589]],
       dtype=torch.float64)
	q_value: tensor([[-4.3560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8508131730932775, distance: 0.4419996563950011 entropy 0.03264415264129639
epoch: 66, step: 57
	action: tensor([[ 7.8280e-01, -3.5823e-04, -3.8202e-01,  5.6924e-01, -3.2410e-01,
          3.1152e-01,  1.4266e-01]], dtype=torch.float64)
	q_value: tensor([[-3.6369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9429899729471811, distance: 0.2732324754651369 entropy 0.03264415264129639
epoch: 66, step: 58
	action: tensor([[ 0.7170,  0.4646, -0.3021,  0.1257,  0.0198, -0.2244, -0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-4.3945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9685034257922968, distance: 0.20308991272569238 entropy 0.03264415264129639
epoch: 66, step: 59
	action: tensor([[ 0.5318,  0.1666, -0.4039,  0.6845, -0.6262,  0.2226,  0.2435]],
       dtype=torch.float64)
	q_value: tensor([[-4.2123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8515394475085235, distance: 0.44092246783508765 entropy 0.03264415264129639
epoch: 66, step: 60
	action: tensor([[ 0.4876,  0.0874, -0.1800,  0.7610, -0.3751,  0.6433, -0.1349]],
       dtype=torch.float64)
	q_value: tensor([[-4.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.857668018598398, distance: 0.4317257383029968 entropy 0.03264415264129639
epoch: 66, step: 61
	action: tensor([[ 0.7237, -0.2341, -0.2649, -0.1035, -0.4036,  0.3804, -0.0090]],
       dtype=torch.float64)
	q_value: tensor([[-4.7137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5777517249814856, distance: 0.7436021474420061 entropy 0.03264415264129639
epoch: 66, step: 62
	action: tensor([[ 0.9915, -0.3264, -0.0235,  0.4296, -0.4882,  0.6500, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-3.7264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8506468740494865, distance: 0.4422459370128211 entropy 0.03264415264129639
epoch: 66, step: 63
	action: tensor([[ 0.8724,  0.2035, -0.1044,  0.5730, -0.3041,  0.0676,  0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-5.2670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09389087032607327 entropy 0.03264415264129639
epoch: 66, step: 64
	action: tensor([[ 0.4979,  0.2743, -0.3338,  0.6478,  0.0287, -0.3920, -0.3235]],
       dtype=torch.float64)
	q_value: tensor([[-6.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8743829218176955, distance: 0.40558420828054226 entropy 0.03264415264129639
epoch: 66, step: 65
	action: tensor([[ 0.7352,  0.3487, -0.0664,  0.3801,  0.2712,  0.5007,  0.0756]],
       dtype=torch.float64)
	q_value: tensor([[-3.9482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9818386591653977, distance: 0.1542164308631943 entropy 0.03264415264129639
epoch: 66, step: 66
	action: tensor([[ 0.8451,  0.4419, -0.5525, -0.0115,  0.2818,  0.1090,  0.1580]],
       dtype=torch.float64)
	q_value: tensor([[-3.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.972047133830123, distance: 0.19132418230731646 entropy 0.03264415264129639
epoch: 66, step: 67
	action: tensor([[ 0.7191,  0.0433, -0.4457,  0.4361, -0.0683,  0.0550,  0.1509]],
       dtype=torch.float64)
	q_value: tensor([[-4.4321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8984705241957484, distance: 0.36463030879252517 entropy 0.03264415264129639
epoch: 66, step: 68
	action: tensor([[ 0.5810, -0.0201, -0.6419,  1.0897,  0.0487,  0.3214, -0.1259]],
       dtype=torch.float64)
	q_value: tensor([[-3.4493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7731119392969117, distance: 0.5450828459942307 entropy 0.03264415264129639
epoch: 66, step: 69
	action: tensor([[ 1.3310,  0.0911, -0.5489,  0.8260, -0.2882,  0.4419, -0.4033]],
       dtype=torch.float64)
	q_value: tensor([[-4.7441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0917765534810137 entropy 0.03264415264129639
epoch: 66, step: 70
	action: tensor([[ 0.6924,  0.2314, -0.4463,  0.7497, -0.0744,  0.0570, -0.1304]],
       dtype=torch.float64)
	q_value: tensor([[-6.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.917209984387351, distance: 0.3292650809015428 entropy 0.03264415264129639
epoch: 66, step: 71
	action: tensor([[ 1.6157,  0.4001, -0.4597,  0.6600, -0.2196, -0.0526,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-4.7638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8434840082798678, distance: 0.45272664136639196 entropy 0.03264415264129639
epoch: 66, step: 72
	action: tensor([[ 0.5130,  0.0241, -0.0703,  0.6513, -0.4030,  0.2544, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[-8.8921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9190654234210652, distance: 0.3255545299632301 entropy 0.03264415264129639
epoch: 66, step: 73
	action: tensor([[ 1.0457, -0.2252, -0.2536,  0.6395,  0.2590,  0.1216, -0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-3.6309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8855243431415207, distance: 0.3871803119128014 entropy 0.03264415264129639
epoch: 66, step: 74
	action: tensor([[ 0.9956,  0.2026, -0.5246,  0.3437, -0.0415,  0.0809, -0.0880]],
       dtype=torch.float64)
	q_value: tensor([[-4.4519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9652054619042498, distance: 0.21345789926918224 entropy 0.03264415264129639
epoch: 66, step: 75
	action: tensor([[ 1.0686,  0.4500, -0.4431,  0.6759, -0.0785, -0.2746,  0.4652]],
       dtype=torch.float64)
	q_value: tensor([[-5.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9789182597442505, distance: 0.166153663856458 entropy 0.03264415264129639
epoch: 66, step: 76
	action: tensor([[ 0.3664,  0.0236, -0.2668,  0.5162, -0.2864,  0.1471, -0.1539]],
       dtype=torch.float64)
	q_value: tensor([[-5.3220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7824551780451261, distance: 0.5337416189056858 entropy 0.03264415264129639
epoch: 66, step: 77
	action: tensor([[ 1.0728,  0.6082, -0.6375,  0.5141, -0.2978,  0.6171, -0.1114]],
       dtype=torch.float64)
	q_value: tensor([[-3.0459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9185042994501177, distance: 0.3266811245823445 entropy 0.03264415264129639
epoch: 66, step: 78
	action: tensor([[ 0.9139, -0.0552, -0.3721,  0.7172,  0.0534,  0.3886, -0.0305]],
       dtype=torch.float64)
	q_value: tensor([[-8.4680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9862469144209206, distance: 0.13420131333007687 entropy 0.03264415264129639
epoch: 66, step: 79
	action: tensor([[ 0.8107,  0.2818, -0.7924,  0.7368,  0.0643, -0.0291, -0.3247]],
       dtype=torch.float64)
	q_value: tensor([[-4.8089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9298664977733346, distance: 0.30305360640616935 entropy 0.03264415264129639
epoch: 66, step: 80
	action: tensor([[ 0.7240,  0.1737, -0.4181,  0.1945, -0.3029, -0.1687, -0.2130]],
       dtype=torch.float64)
	q_value: tensor([[-6.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9006917553028385, distance: 0.3606196155966018 entropy 0.03264415264129639
epoch: 66, step: 81
	action: tensor([[ 0.6919, -0.2073, -0.3304,  0.5295, -0.1539,  0.1351, -0.3990]],
       dtype=torch.float64)
	q_value: tensor([[-4.3632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8350499129835293, distance: 0.4647645211135619 entropy 0.03264415264129639
epoch: 66, step: 82
	action: tensor([[ 0.2328, -0.1309, -0.3151,  0.6549, -0.5382,  0.3293, -0.1197]],
       dtype=torch.float64)
	q_value: tensor([[-4.1971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6159799698384685, distance: 0.709142692475356 entropy 0.03264415264129639
epoch: 66, step: 83
	action: tensor([[ 0.5490,  0.8449, -0.6620,  0.0653, -0.1009,  0.1554, -0.1925]],
       dtype=torch.float64)
	q_value: tensor([[-3.3885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 66, step: 84
	action: tensor([[ 0.4124, -0.0504,  0.0249,  0.6172, -0.1995,  0.4641, -0.2065]],
       dtype=torch.float64)
	q_value: tensor([[-6.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.865387477083826, distance: 0.4198550925286431 entropy 0.03264415264129639
epoch: 66, step: 85
	action: tensor([[ 0.7015,  0.1881, -0.4795,  0.2412, -0.0919,  0.3302,  0.0724]],
       dtype=torch.float64)
	q_value: tensor([[-3.5439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9235723559646358, distance: 0.316360264060601 entropy 0.03264415264129639
epoch: 66, step: 86
	action: tensor([[ 0.6333,  0.0348, -0.5926,  0.4586,  0.2050, -0.0485, -0.4233]],
       dtype=torch.float64)
	q_value: tensor([[-4.1530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8489099345700749, distance: 0.444810108183905 entropy 0.03264415264129639
epoch: 66, step: 87
	action: tensor([[ 0.6381,  0.1091, -0.4164,  0.2937, -0.3351,  0.0300, -0.3067]],
       dtype=torch.float64)
	q_value: tensor([[-4.0842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.883457260773838, distance: 0.390660322315128 entropy 0.03264415264129639
epoch: 66, step: 88
	action: tensor([[ 0.8838, -0.3337, -0.5420,  0.9386, -0.0511,  0.4849,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[-4.4082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9160537800184649, distance: 0.33155628572997414 entropy 0.03264415264129639
epoch: 66, step: 89
	action: tensor([[ 0.8837, -0.0540, -0.9244,  0.2634,  0.0790,  0.6555, -0.1451]],
       dtype=torch.float64)
	q_value: tensor([[-4.8562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8719286306546257, distance: 0.4095271695836263 entropy 0.03264415264129639
epoch: 66, step: 90
	action: tensor([[ 0.7360, -0.2332, -0.0276,  0.6386, -0.3399,  0.0324, -0.0442]],
       dtype=torch.float64)
	q_value: tensor([[-5.8015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8768144843063687, distance: 0.40163959108304204 entropy 0.03264415264129639
epoch: 66, step: 91
	action: tensor([[ 0.8792, -0.0903, -0.0488,  0.2098,  0.0174,  0.3023, -0.1130]],
       dtype=torch.float64)
	q_value: tensor([[-3.6014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8459421154390568, distance: 0.4491575024917663 entropy 0.03264415264129639
epoch: 66, step: 92
	action: tensor([[ 0.8522,  0.2878, -0.1864,  0.7842, -0.2857,  0.2036, -0.2165]],
       dtype=torch.float64)
	q_value: tensor([[-3.7950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.963929299554555, distance: 0.21733715672622292 entropy 0.03264415264129639
epoch: 66, step: 93
	action: tensor([[ 0.9149,  0.0754, -0.3114,  0.7146, -0.1732,  0.7819,  0.0603]],
       dtype=torch.float64)
	q_value: tensor([[-5.8376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9808274772216361, distance: 0.15845148892015606 entropy 0.03264415264129639
epoch: 66, step: 94
	action: tensor([[ 9.4052e-01,  3.0890e-01, -6.5926e-01,  4.5636e-01, -4.3007e-04,
          4.5016e-01, -3.4013e-01]], dtype=torch.float64)
	q_value: tensor([[-5.8912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9836157620508873, distance: 0.14647712816321562 entropy 0.03264415264129639
epoch: 66, step: 95
	action: tensor([[ 0.4650,  0.1053, -0.1878,  0.7594, -0.6116, -0.0154,  0.1985]],
       dtype=torch.float64)
	q_value: tensor([[-6.8845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.894959925508119, distance: 0.37088067423950694 entropy 0.03264415264129639
epoch: 66, step: 96
	action: tensor([[ 0.2287,  0.0927, -0.2289,  0.5366,  0.1072,  0.3857, -0.1987]],
       dtype=torch.float64)
	q_value: tensor([[-3.6157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.754919389732328, distance: 0.5665146821743869 entropy 0.03264415264129639
epoch: 66, step: 97
	action: tensor([[ 0.9102,  0.1772, -0.0317,  0.5914,  0.1374,  0.0263, -0.4605]],
       dtype=torch.float64)
	q_value: tensor([[-2.6642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9909353354414308, distance: 0.10895133648375223 entropy 0.03264415264129639
epoch: 66, step: 98
	action: tensor([[ 0.8085,  0.0469, -0.2917,  0.6231, -0.2463, -0.0550,  0.0370]],
       dtype=torch.float64)
	q_value: tensor([[-5.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9642818600387353, distance: 0.21627240544936785 entropy 0.03264415264129639
epoch: 66, step: 99
	action: tensor([[ 0.8606,  0.0150, -0.1845,  0.8817,  0.1014,  0.6091, -0.1384]],
       dtype=torch.float64)
	q_value: tensor([[-4.1750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9841377550031961, distance: 0.14412490033042244 entropy 0.03264415264129639
epoch: 66, step: 100
	action: tensor([[ 0.4943,  0.2601, -0.8301,  0.2106, -0.1254,  0.1911, -0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-5.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8235800832302356, distance: 0.4806517188201683 entropy 0.03264415264129639
epoch: 66, step: 101
	action: tensor([[ 0.4600,  0.1828, -0.6573,  0.4516, -0.1808,  0.0041, -0.3120]],
       dtype=torch.float64)
	q_value: tensor([[-4.4526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7975284884480894, distance: 0.5149186901460554 entropy 0.03264415264129639
epoch: 66, step: 102
	action: tensor([[ 1.0245,  0.1969, -0.3374,  0.4297,  0.4525, -0.1800,  0.1806]],
       dtype=torch.float64)
	q_value: tensor([[-4.1840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.955006826093398, distance: 0.24273366225815163 entropy 0.03264415264129639
epoch: 66, step: 103
	action: tensor([[ 0.5396, -0.0721,  0.0803,  0.2290, -0.3321,  0.0824,  0.2056]],
       dtype=torch.float64)
	q_value: tensor([[-4.0999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7664026708792125, distance: 0.5530834073452339 entropy 0.03264415264129639
epoch: 66, step: 104
	action: tensor([[ 0.9775, -0.1671, -0.4909,  0.7456,  0.1814,  0.2531, -0.5097]],
       dtype=torch.float64)
	q_value: tensor([[-2.3186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9499121317083812, distance: 0.2561078956217301 entropy 0.03264415264129639
epoch: 66, step: 105
	action: tensor([[ 1.3681,  0.1747, -0.5936,  0.4420, -0.5362, -0.0806, -0.0716]],
       dtype=torch.float64)
	q_value: tensor([[-5.9006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8655777562034355, distance: 0.419558248270554 entropy 0.03264415264129639
epoch: 66, step: 106
	action: tensor([[ 0.5169,  0.5753, -0.4392,  0.2353, -0.3279,  0.6159, -0.1095]],
       dtype=torch.float64)
	q_value: tensor([[-7.9163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.83470940415867, distance: 0.4652439838262637 entropy 0.03264415264129639
epoch: 66, step: 107
	action: tensor([[ 0.8356,  0.1754, -0.2729,  0.7205,  0.3907,  0.4775, -0.1251]],
       dtype=torch.float64)
	q_value: tensor([[-5.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9806281020459379, distance: 0.15927322710697697 entropy 0.03264415264129639
epoch: 66, step: 108
	action: tensor([[ 0.6195,  0.1165, -0.7387,  0.5038,  0.0363, -0.2801,  0.5137]],
       dtype=torch.float64)
	q_value: tensor([[-4.8178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8414786410907095, distance: 0.45561769918212036 entropy 0.03264415264129639
epoch: 66, step: 109
	action: tensor([[ 0.5150,  0.4113,  0.0276,  0.8839, -0.4623,  0.6811, -0.1352]],
       dtype=torch.float64)
	q_value: tensor([[-3.3105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7876980212033583, distance: 0.5272707919136594 entropy 0.03264415264129639
epoch: 66, step: 110
	action: tensor([[ 0.2829, -0.1332, -0.0974,  0.4897, -0.2885,  0.1975,  0.1183]],
       dtype=torch.float64)
	q_value: tensor([[-5.6709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6980007373560606, distance: 0.6288682003861231 entropy 0.03264415264129639
epoch: 66, step: 111
	action: tensor([[ 0.4362, -0.0284, -0.2788,  0.8760, -0.4702,  0.1260, -0.1490]],
       dtype=torch.float64)
	q_value: tensor([[-2.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8335884351970505, distance: 0.46681891562903666 entropy 0.03264415264129639
epoch: 66, step: 112
	action: tensor([[ 0.6718, -0.3880, -0.2749,  0.6519, -0.5243, -0.1187, -0.0680]],
       dtype=torch.float64)
	q_value: tensor([[-4.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7388113086380617, distance: 0.5848357036193675 entropy 0.03264415264129639
epoch: 66, step: 113
	action: tensor([[ 0.3670, -0.3888, -0.5405,  0.5490, -0.4483,  0.2535,  0.0362]],
       dtype=torch.float64)
	q_value: tensor([[-3.6045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4770958284847263, distance: 0.827499512326685 entropy 0.03264415264129639
epoch: 66, step: 114
	action: tensor([[ 0.9897,  0.0917, -0.2192,  0.3209, -0.4842,  0.0457,  0.4863]],
       dtype=torch.float64)
	q_value: tensor([[-2.9081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.919710934122221, distance: 0.32425366670487077 entropy 0.03264415264129639
epoch: 66, step: 115
	action: tensor([[ 0.4688,  0.1952, -0.3981,  0.3827, -0.1347, -0.2068,  0.1290]],
       dtype=torch.float64)
	q_value: tensor([[-4.4177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8469112438029524, distance: 0.4477425213382432 entropy 0.03264415264129639
epoch: 66, step: 116
	action: tensor([[ 0.5681,  0.1239, -0.2770,  0.4820, -0.1847,  0.5948,  0.2307]],
       dtype=torch.float64)
	q_value: tensor([[-2.7271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9098635109644623, distance: 0.34356349498493205 entropy 0.03264415264129639
epoch: 66, step: 117
	action: tensor([[ 0.5386,  0.1971, -0.4411,  0.3728, -0.4079,  0.7016,  0.0607]],
       dtype=torch.float64)
	q_value: tensor([[-3.8526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8407617566461794, distance: 0.4566467617641197 entropy 0.03264415264129639
epoch: 66, step: 118
	action: tensor([[ 0.6433,  0.1706, -0.2516,  0.8178, -0.0027,  0.3545,  0.2627]],
       dtype=torch.float64)
	q_value: tensor([[-5.0095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9228975329438128, distance: 0.31775385672549483 entropy 0.03264415264129639
epoch: 66, step: 119
	action: tensor([[ 1.0843,  0.0809, -0.1564,  0.8063,  0.0329,  0.2512,  0.2755]],
       dtype=torch.float64)
	q_value: tensor([[-3.8360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9907233336075608, distance: 0.11021803465261294 entropy 0.03264415264129639
epoch: 66, step: 120
	action: tensor([[ 0.8120,  0.2893, -0.3081,  0.5926, -0.0956, -0.0588, -0.5267]],
       dtype=torch.float64)
	q_value: tensor([[-4.8670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9841259763897523, distance: 0.14417840084269262 entropy 0.03264415264129639
epoch: 66, step: 121
	action: tensor([[ 0.7922,  0.3765,  0.3151,  0.6397, -0.1305,  0.3922,  0.2492]],
       dtype=torch.float64)
	q_value: tensor([[-5.8712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.952412702911812, distance: 0.24963312433682117 entropy 0.03264415264129639
epoch: 66, step: 122
	action: tensor([[ 0.6612,  0.2857, -0.4718, -0.0826, -0.1334,  0.2138, -0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-4.1763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9082554535553223, distance: 0.34661457627994924 entropy 0.03264415264129639
epoch: 66, step: 123
	action: tensor([[ 0.8688,  0.1091, -0.3948,  0.3026, -0.2450,  0.3316,  0.3497]],
       dtype=torch.float64)
	q_value: tensor([[-5.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.952085630556844, distance: 0.250489532296356 entropy 0.03264415264129639
epoch: 66, step: 124
	action: tensor([[ 0.6669,  0.0080, -0.0812,  0.6249,  0.0114,  0.1776,  0.0319]],
       dtype=torch.float64)
	q_value: tensor([[-4.3674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.961616799543141, distance: 0.22419570341725084 entropy 0.03264415264129639
epoch: 66, step: 125
	action: tensor([[ 0.5766,  0.2289, -0.0164,  0.4029, -0.3982,  0.2822, -0.0483]],
       dtype=torch.float64)
	q_value: tensor([[-3.1899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9499421450827978, distance: 0.25603115234776525 entropy 0.03264415264129639
epoch: 66, step: 126
	action: tensor([[ 0.9753,  0.0222, -0.0109,  0.5407, -0.1409,  0.1543, -0.2645]],
       dtype=torch.float64)
	q_value: tensor([[-3.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9592840153725204, distance: 0.2309080980744123 entropy 0.03264415264129639
epoch: 66, step: 127
	action: tensor([[ 0.9958,  0.2669, -0.1467,  1.0995, -0.2666,  0.2293,  0.2793]],
       dtype=torch.float64)
	q_value: tensor([[-5.1455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9054415284756807, distance: 0.3518899917207046 entropy 0.03264415264129639
LOSS epoch 66 actor 215.2437365755575 critic 2616.0355287219986 
epoch: 67, step: 0
	action: tensor([[ 1.1004, -0.3720, -0.4335,  0.6452, -0.1266,  0.3508, -0.2820]],
       dtype=torch.float64)
	q_value: tensor([[-4.0930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8515656426479115, distance: 0.44088356681107227 entropy 0.03264415264129639
epoch: 67, step: 1
	action: tensor([[ 0.9721,  0.3287, -0.5077,  0.6750, -0.0788,  0.5433,  0.2240]],
       dtype=torch.float64)
	q_value: tensor([[-4.1807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.957612208411973, distance: 0.23560097774162658 entropy 0.03264415264129639
epoch: 67, step: 2
	action: tensor([[ 9.9129e-01,  1.0237e-04, -9.5655e-01,  7.3965e-01, -2.5976e-02,
          1.7831e-01, -9.8027e-02]], dtype=torch.float64)
	q_value: tensor([[-4.3400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9144123170358823, distance: 0.3347821760534045 entropy 0.03264415264129639
epoch: 67, step: 3
	action: tensor([[ 0.9231, -0.1666, -0.8584,  0.8767, -0.0693,  0.2986, -0.0601]],
       dtype=torch.float64)
	q_value: tensor([[-4.5715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8702875238027374, distance: 0.4121426586287356 entropy 0.03264415264129639
epoch: 67, step: 4
	action: tensor([[ 0.5502, -0.2448, -0.7134,  0.4302,  0.0648, -0.2925, -0.2159]],
       dtype=torch.float64)
	q_value: tensor([[-4.0711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5862688588443887, distance: 0.7360643751876986 entropy 0.03264415264129639
epoch: 67, step: 5
	action: tensor([[ 1.0509,  0.1780, -0.2934,  0.7103,  0.0805,  0.0442,  0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-2.1846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07063449930610893 entropy 0.03264415264129639
epoch: 67, step: 6
	action: tensor([[ 1.0877, -0.2024, -0.7080, -0.0767, -0.2926,  0.3992,  0.3012]],
       dtype=torch.float64)
	q_value: tensor([[-5.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5916001861583828, distance: 0.7313065462364375 entropy 0.03264415264129639
epoch: 67, step: 7
	action: tensor([[ 0.4870, -0.2340, -0.5080,  0.4899, -0.2982, -0.0088, -0.1948]],
       dtype=torch.float64)
	q_value: tensor([[-3.9279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6578441150945302, distance: 0.6693737856370231 entropy 0.03264415264129639
epoch: 67, step: 8
	action: tensor([[ 0.7303, -0.0906, -0.5093,  0.7206, -0.2984, -0.0724,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[-2.1424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8885327751505723, distance: 0.3820588707275486 entropy 0.03264415264129639
epoch: 67, step: 9
	action: tensor([[ 0.9832,  0.2147, -0.2798,  0.8442, -0.0124,  0.3416, -0.3574]],
       dtype=torch.float64)
	q_value: tensor([[-2.8952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9743250535536163, distance: 0.1833628989400777 entropy 0.03264415264129639
epoch: 67, step: 10
	action: tensor([[ 0.8235, -0.5785, -0.1469,  1.0790, -0.1333,  0.0318,  0.4147]],
       dtype=torch.float64)
	q_value: tensor([[-4.8360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8615066118744965, distance: 0.4258642788124777 entropy 0.03264415264129639
epoch: 67, step: 11
	action: tensor([[ 0.9557,  0.2899, -0.2224,  0.5605, -0.4049,  0.2551,  0.0656]],
       dtype=torch.float64)
	q_value: tensor([[-2.0409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09688726541550526 entropy 0.03264415264129639
epoch: 67, step: 12
	action: tensor([[ 0.2418, -0.0348,  0.0697,  0.1582, -0.1077,  0.2458, -0.1934]],
       dtype=torch.float64)
	q_value: tensor([[-5.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6000812657339889, distance: 0.7236733304845815 entropy 0.03264415264129639
epoch: 67, step: 13
	action: tensor([[ 0.7654,  0.3068, -1.1026,  0.3885, -0.1205, -0.0481,  0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-1.3039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9087763698611608, distance: 0.3456291541583064 entropy 0.03264415264129639
epoch: 67, step: 14
	action: tensor([[ 1.2169,  0.1721, -0.8233,  0.3559, -0.2447,  0.2381,  0.2559]],
       dtype=torch.float64)
	q_value: tensor([[-4.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9334081349233734, distance: 0.29530260804335184 entropy 0.03264415264129639
epoch: 67, step: 15
	action: tensor([[ 1.1318,  0.0679, -0.4818,  0.4750,  0.2663,  0.1660, -0.2340]],
       dtype=torch.float64)
	q_value: tensor([[-4.9585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.945621339267898, distance: 0.26685229791005133 entropy 0.03264415264129639
epoch: 67, step: 16
	action: tensor([[ 1.2020,  0.6219, -0.5846,  0.9023, -0.4619,  0.1470,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[-4.2665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 67, step: 17
	action: tensor([[ 0.7684,  0.4411, -0.1472,  0.2745,  0.1440,  0.0831, -0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-5.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9815181091622209, distance: 0.15557144788600802 entropy 0.03264415264129639
epoch: 67, step: 18
	action: tensor([[ 0.4068,  0.2114, -0.2679,  0.2172, -0.3744,  0.1182, -0.2299]],
       dtype=torch.float64)
	q_value: tensor([[-2.9625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8184964654091607, distance: 0.48752763325531445 entropy 0.03264415264129639
epoch: 67, step: 19
	action: tensor([[ 0.7968,  0.2297,  0.2197,  0.6076, -0.3691,  0.0417,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-2.4614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08193214004679245 entropy 0.03264415264129639
epoch: 67, step: 20
	action: tensor([[ 0.6048,  0.5104,  0.1976, -0.0750,  0.1116,  0.2631, -0.6902]],
       dtype=torch.float64)
	q_value: tensor([[-5.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9488247145587628, distance: 0.2588730438012621 entropy 0.03264415264129639
epoch: 67, step: 21
	action: tensor([[ 0.8640,  0.2118, -0.6291,  0.8064, -0.4543,  0.3481, -0.2952]],
       dtype=torch.float64)
	q_value: tensor([[-3.4289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9197105265362077, distance: 0.32425448973780907 entropy 0.03264415264129639
epoch: 67, step: 22
	action: tensor([[ 0.8403, -0.2053, -0.4037,  0.8351, -0.1463,  0.2049, -0.2555]],
       dtype=torch.float64)
	q_value: tensor([[-5.2854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9397292518317988, distance: 0.28093767540535136 entropy 0.03264415264129639
epoch: 67, step: 23
	action: tensor([[ 1.0894,  0.3015, -0.1995,  0.8154,  0.0815, -0.0349, -0.6899]],
       dtype=torch.float64)
	q_value: tensor([[-3.5571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9707479639971018, distance: 0.19571979081158772 entropy 0.03264415264129639
epoch: 67, step: 24
	action: tensor([[ 1.4297,  0.1850, -0.6175,  0.6901, -0.3354,  0.4357, -0.0471]],
       dtype=torch.float64)
	q_value: tensor([[-5.4867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9747355570671996, distance: 0.18189114491453617 entropy 0.03264415264129639
epoch: 67, step: 25
	action: tensor([[ 0.6275, -0.4725,  0.0253,  0.7061,  0.0705,  0.6070, -0.3306]],
       dtype=torch.float64)
	q_value: tensor([[-6.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8705763244217066, distance: 0.41168359183449543 entropy 0.03264415264129639
epoch: 67, step: 26
	action: tensor([[ 1.2237,  0.4159, -0.6605,  0.7242, -0.3541,  0.2380, -0.1740]],
       dtype=torch.float64)
	q_value: tensor([[-2.7013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9784863094354944, distance: 0.16784721984715167 entropy 0.03264415264129639
epoch: 67, step: 27
	action: tensor([[ 0.9400,  0.2414, -0.3173,  0.8439,  0.0107,  0.2328, -0.5067]],
       dtype=torch.float64)
	q_value: tensor([[-6.4114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9696869481785797, distance: 0.19923770213440198 entropy 0.03264415264129639
epoch: 67, step: 28
	action: tensor([[ 1.1841,  0.3630, -0.5218,  0.7713, -0.0820, -0.0286, -0.0638]],
       dtype=torch.float64)
	q_value: tensor([[-4.9415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9777148974387598, distance: 0.17082994924160075 entropy 0.03264415264129639
epoch: 67, step: 29
	action: tensor([[ 1.2624,  0.2217, -0.7164,  0.7225, -0.3646,  0.3846, -0.0910]],
       dtype=torch.float64)
	q_value: tensor([[-5.2259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.010315905195407824 entropy 0.03264415264129639
epoch: 67, step: 30
	action: tensor([[ 0.7041,  0.0196, -0.5676,  0.3446,  0.0008,  0.3610, -0.3160]],
       dtype=torch.float64)
	q_value: tensor([[-5.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8758865399683907, distance: 0.40314950861276655 entropy 0.03264415264129639
epoch: 67, step: 31
	action: tensor([[ 0.5500, -0.0644, -0.8410,  0.4939,  0.1172,  0.6045, -0.3360]],
       dtype=torch.float64)
	q_value: tensor([[-3.3559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7410058603374394, distance: 0.582373576503504 entropy 0.03264415264129639
epoch: 67, step: 32
	action: tensor([[ 0.9351, -0.0971, -0.7994,  0.5752,  0.2729,  0.0882,  0.1183]],
       dtype=torch.float64)
	q_value: tensor([[-3.5112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8721488427305633, distance: 0.4091749377453716 entropy 0.03264415264129639
epoch: 67, step: 33
	action: tensor([[ 0.6948,  0.1098, -0.4610,  0.6189, -0.4853,  0.3358, -0.1603]],
       dtype=torch.float64)
	q_value: tensor([[-3.1898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9109147126617629, distance: 0.3415542438249507 entropy 0.03264415264129639
epoch: 67, step: 34
	action: tensor([[ 0.9183,  0.0369, -0.3953,  0.5435, -0.3983,  0.6596, -0.1055]],
       dtype=torch.float64)
	q_value: tensor([[-3.9098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9761038013716988, distance: 0.1768972587581186 entropy 0.03264415264129639
epoch: 67, step: 35
	action: tensor([[ 0.5101,  0.2207, -0.4741,  0.6636,  0.2987,  0.5475,  0.6019]],
       dtype=torch.float64)
	q_value: tensor([[-4.6186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8554529610542031, distance: 0.43507215909891545 entropy 0.03264415264129639
epoch: 67, step: 36
	action: tensor([[ 0.7639, -0.0303, -0.7096,  0.2281,  0.1419,  0.1687, -0.3152]],
       dtype=torch.float64)
	q_value: tensor([[-2.3428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8255373872931251, distance: 0.47797796874548093 entropy 0.03264415264129639
epoch: 67, step: 37
	action: tensor([[ 1.2105, -0.1469, -0.8826,  0.5364, -0.2279,  0.6587, -0.0301]],
       dtype=torch.float64)
	q_value: tensor([[-3.2116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8948651595310031, distance: 0.371047938714298 entropy 0.03264415264129639
epoch: 67, step: 38
	action: tensor([[ 0.9604,  0.0739, -0.6746,  0.3819, -0.0485,  0.5921, -0.4863]],
       dtype=torch.float64)
	q_value: tensor([[-5.4159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9641035133327205, distance: 0.21681167544664168 entropy 0.03264415264129639
epoch: 67, step: 39
	action: tensor([[ 0.8424,  0.1536, -0.7242,  0.5673,  0.0546,  0.1313, -0.1308]],
       dtype=torch.float64)
	q_value: tensor([[-5.2961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.948986403768156, distance: 0.25846376330853094 entropy 0.03264415264129639
epoch: 67, step: 40
	action: tensor([[ 0.6941,  0.4863,  0.0764,  0.8202,  0.3392,  0.3980, -0.1294]],
       dtype=torch.float64)
	q_value: tensor([[-3.8283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 67, step: 41
	action: tensor([[ 0.2234, -0.1022, -0.3710, -0.0112, -0.1271,  0.1549, -0.6622]],
       dtype=torch.float64)
	q_value: tensor([[-5.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45865473782176336, distance: 0.8419646604813296 entropy 0.03264415264129639
epoch: 67, step: 42
	action: tensor([[ 0.8403, -0.2039, -0.3639,  0.4228, -0.2129, -0.0943, -0.4025]],
       dtype=torch.float64)
	q_value: tensor([[-1.9910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7798644975728974, distance: 0.5369103029512736 entropy 0.03264415264129639
epoch: 67, step: 43
	action: tensor([[ 1.2571,  0.0682, -0.9694,  0.3793, -0.2356,  0.3904, -0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-3.3660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9094830503232637, distance: 0.3442878117797565 entropy 0.03264415264129639
epoch: 67, step: 44
	action: tensor([[0.7228, 0.2619, 0.0800, 0.7022, 0.3030, 0.4206, 0.0561]],
       dtype=torch.float64)
	q_value: tensor([[-6.1852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9715909864718573, distance: 0.19287892198110584 entropy 0.03264415264129639
epoch: 67, step: 45
	action: tensor([[ 0.6992,  0.2807, -0.2086,  0.5365, -0.2371, -0.0237,  0.6863]],
       dtype=torch.float64)
	q_value: tensor([[-2.6171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.963394734883596, distance: 0.218941692781322 entropy 0.03264415264129639
epoch: 67, step: 46
	action: tensor([[ 0.3816,  0.3280, -0.3358,  0.7634, -0.3227,  0.2226, -0.4858]],
       dtype=torch.float64)
	q_value: tensor([[-2.2199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7611248564432829, distance: 0.5592965930049001 entropy 0.03264415264129639
epoch: 67, step: 47
	action: tensor([[ 0.8544,  0.0684, -0.6434,  0.8258, -0.3893,  0.1065, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-3.7813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9410230454278276, distance: 0.2779059624990761 entropy 0.03264415264129639
epoch: 67, step: 48
	action: tensor([[ 0.8793, -0.1552,  0.0605,  0.2941, -0.2532,  0.1843,  0.1035]],
       dtype=torch.float64)
	q_value: tensor([[-4.1353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8146490449147327, distance: 0.49266771944644855 entropy 0.03264415264129639
epoch: 67, step: 49
	action: tensor([[ 0.5475, -0.2342, -0.6105,  0.5082, -0.2115,  0.5500,  0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-2.4297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7009889139491399, distance: 0.6257492510377598 entropy 0.03264415264129639
epoch: 67, step: 50
	action: tensor([[ 0.9021, -0.0124, -0.3055,  0.7950,  0.1484,  0.5049,  0.2437]],
       dtype=torch.float64)
	q_value: tensor([[-2.4558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9918864945971316, distance: 0.103076807268031 entropy 0.03264415264129639
epoch: 67, step: 51
	action: tensor([[ 0.6434, -0.1927, -0.3308,  0.4215, -0.6775,  0.3955, -0.3007]],
       dtype=torch.float64)
	q_value: tensor([[-3.1157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.782548503834839, distance: 0.5336271201990431 entropy 0.03264415264129639
epoch: 67, step: 52
	action: tensor([[ 0.4196, -0.1076, -0.2269, -0.0443, -0.0569,  0.1797,  0.0459]],
       dtype=torch.float64)
	q_value: tensor([[-3.7475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5785866618633264, distance: 0.7428665990511046 entropy 0.03264415264129639
epoch: 67, step: 53
	action: tensor([[ 0.3164, -0.1760, -0.1723,  0.3220, -0.0815,  0.0261, -0.3127]],
       dtype=torch.float64)
	q_value: tensor([[-1.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5991736925639628, distance: 0.7244940150859213 entropy 0.03264415264129639
epoch: 67, step: 54
	action: tensor([[ 1.0835,  0.0616, -0.4000,  0.4712, -0.6109, -0.0875, -0.1168]],
       dtype=torch.float64)
	q_value: tensor([[-1.3808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9173838300173813, distance: 0.3289191975485396 entropy 0.03264415264129639
epoch: 67, step: 55
	action: tensor([[ 0.8610, -0.1089, -0.7016,  0.4760, -0.7804,  0.2574, -0.0776]],
       dtype=torch.float64)
	q_value: tensor([[-4.6165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8326301357963273, distance: 0.4681611003006198 entropy 0.03264415264129639
epoch: 67, step: 56
	action: tensor([[ 0.6061,  0.0209, -0.6647,  0.4109, -0.1495,  0.8380, -0.3191]],
       dtype=torch.float64)
	q_value: tensor([[-4.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8059919599507238, distance: 0.5040418041573922 entropy 0.03264415264129639
epoch: 67, step: 57
	action: tensor([[ 0.6942, -0.2476, -0.1906,  0.5265,  0.1018, -0.0333, -0.1941]],
       dtype=torch.float64)
	q_value: tensor([[-4.3933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7892086760623465, distance: 0.525391520255737 entropy 0.03264415264129639
epoch: 67, step: 58
	action: tensor([[ 0.6125,  0.4035, -0.9458,  0.3178, -0.4465, -0.2549,  0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-1.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9280451720047019, distance: 0.30696344692121236 entropy 0.03264415264129639
epoch: 67, step: 59
	action: tensor([[ 0.1827, -0.1225, -0.0853,  0.5642,  0.0500,  0.6220, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[-3.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7277525057903571, distance: 0.597088406723127 entropy 0.03264415264129639
epoch: 67, step: 60
	action: tensor([[ 0.6873,  0.0451, -0.5267,  0.6684, -0.0809,  0.0387, -0.0292]],
       dtype=torch.float64)
	q_value: tensor([[-1.7236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9078136604722713, distance: 0.34744812999085245 entropy 0.03264415264129639
epoch: 67, step: 61
	action: tensor([[ 1.0312, -0.0500, -0.3849,  0.6134, -0.1257,  0.1216,  0.3935]],
       dtype=torch.float64)
	q_value: tensor([[-2.8089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9492855280934503, distance: 0.2577048826253645 entropy 0.03264415264129639
epoch: 67, step: 62
	action: tensor([[ 0.6942, -0.0282, -0.5706,  0.4156,  0.2726,  0.1425,  0.3344]],
       dtype=torch.float64)
	q_value: tensor([[-3.1043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.856136820487732, distance: 0.43404176462445415 entropy 0.03264415264129639
epoch: 67, step: 63
	action: tensor([[ 0.5840,  0.1078,  0.0668,  0.3894, -0.1306,  0.3069,  0.0813]],
       dtype=torch.float64)
	q_value: tensor([[-2.0927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.944322498007467, distance: 0.2700203925743864 entropy 0.03264415264129639
epoch: 67, step: 64
	action: tensor([[ 1.4017,  0.1079, -0.5754,  0.3173, -0.1422,  0.0882, -0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-2.0327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8006580677441948, distance: 0.5109236723603396 entropy 0.03264415264129639
epoch: 67, step: 65
	action: tensor([[ 0.4081,  0.4038, -0.3750,  0.5624,  0.3481,  0.5594, -0.1272]],
       dtype=torch.float64)
	q_value: tensor([[-5.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8217671014664885, distance: 0.4831151182916967 entropy 0.03264415264129639
epoch: 67, step: 66
	action: tensor([[ 0.8716,  0.1442,  0.0774,  0.9458, -0.2822,  0.2054,  0.0391]],
       dtype=torch.float64)
	q_value: tensor([[-2.8052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9766327951954655, distance: 0.1749283000799226 entropy 0.03264415264129639
epoch: 67, step: 67
	action: tensor([[ 0.7680,  0.0569, -0.1337,  0.6137, -0.4088,  0.3610, -0.5239]],
       dtype=torch.float64)
	q_value: tensor([[-3.5879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9811881798717228, distance: 0.1569538965627299 entropy 0.03264415264129639
epoch: 67, step: 68
	action: tensor([[ 0.8801, -0.2094, -0.3986,  0.7693,  0.1626,  0.4203, -0.1631]],
       dtype=torch.float64)
	q_value: tensor([[-4.3351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.959645798176217, distance: 0.22987993947868274 entropy 0.03264415264129639
epoch: 67, step: 69
	action: tensor([[ 0.8981, -0.0036, -0.2499,  0.5695,  0.0433,  0.3273, -0.3323]],
       dtype=torch.float64)
	q_value: tensor([[-3.3317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9822934301909216, distance: 0.15227335315016607 entropy 0.03264415264129639
epoch: 67, step: 70
	action: tensor([[ 1.2080,  0.3979, -0.1745,  0.5249, -0.0235,  0.4071, -0.0931]],
       dtype=torch.float64)
	q_value: tensor([[-3.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9332835290795105, distance: 0.29557876214611917 entropy 0.03264415264129639
epoch: 67, step: 71
	action: tensor([[ 0.7270,  0.4933, -0.3179,  0.5531, -0.0306,  0.0353, -0.3475]],
       dtype=torch.float64)
	q_value: tensor([[-5.1568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9383157918812326, distance: 0.28421283726764063 entropy 0.03264415264129639
epoch: 67, step: 72
	action: tensor([[ 0.9081,  0.3360, -0.3922, -0.0140, -0.0105,  0.6265,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-4.0183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9868063891829548, distance: 0.13144332238264667 entropy 0.03264415264129639
epoch: 67, step: 73
	action: tensor([[-0.0335,  0.0100, -0.4917,  0.1481,  0.1332,  0.1279, -0.1451]],
       dtype=torch.float64)
	q_value: tensor([[-4.0055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3338649590742352, distance: 0.9339805518477738 entropy 0.03264415264129639
epoch: 67, step: 74
	action: tensor([[ 0.5841,  0.0958, -0.2304,  0.5860,  0.0328,  0.0309, -0.2141]],
       dtype=torch.float64)
	q_value: tensor([[-1.1160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9040390215633484, distance: 0.35449003147453706 entropy 0.03264415264129639
epoch: 67, step: 75
	action: tensor([[ 1.3639, -0.0110, -0.0472,  0.7573,  0.0724,  0.5084, -0.0523]],
       dtype=torch.float64)
	q_value: tensor([[-2.4136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8892340398921017, distance: 0.3808551665034203 entropy 0.03264415264129639
epoch: 67, step: 76
	action: tensor([[ 0.8581, -0.1834, -0.2720,  0.7073, -0.2774,  0.1833, -0.2798]],
       dtype=torch.float64)
	q_value: tensor([[-4.8799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9398892869711776, distance: 0.28056444472321274 entropy 0.03264415264129639
epoch: 67, step: 77
	action: tensor([[ 0.4568, -0.2769, -0.1534,  0.8719, -0.6668,  0.3455,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[-3.6134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8168134721092458, distance: 0.48978272011302737 entropy 0.03264415264129639
epoch: 67, step: 78
	action: tensor([[ 1.0947,  0.2027, -0.6129,  0.6727, -0.2613,  0.2876, -0.7313]],
       dtype=torch.float64)
	q_value: tensor([[-2.7232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06938872815199147 entropy 0.03264415264129639
epoch: 67, step: 79
	action: tensor([[ 0.4213,  0.2381, -0.2925,  0.8304, -0.3822,  0.4321, -0.0976]],
       dtype=torch.float64)
	q_value: tensor([[-5.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7670070871313879, distance: 0.5523674121727411 entropy 0.03264415264129639
epoch: 67, step: 80
	action: tensor([[ 0.4681,  0.0409, -0.5036,  0.7872, -0.2636,  0.5919,  0.2067]],
       dtype=torch.float64)
	q_value: tensor([[-3.2905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7564735585489553, distance: 0.5647155603563587 entropy 0.03264415264129639
epoch: 67, step: 81
	action: tensor([[ 1.3679,  0.1002, -0.5725,  0.2552, -0.2807, -0.1642, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-2.9696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7261659769011577, distance: 0.5988256530393455 entropy 0.03264415264129639
epoch: 67, step: 82
	action: tensor([[ 0.6398,  0.4013, -0.0373,  0.1783, -0.4748,  0.6948, -0.3656]],
       dtype=torch.float64)
	q_value: tensor([[-5.1670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.946765400370996, distance: 0.26403025146587517 entropy 0.03264415264129639
epoch: 67, step: 83
	action: tensor([[ 0.4831, -0.2768, -0.4321,  0.6837, -0.4575,  0.3427,  0.1515]],
       dtype=torch.float64)
	q_value: tensor([[-4.5530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7068974456763848, distance: 0.6195359250454053 entropy 0.03264415264129639
epoch: 67, step: 84
	action: tensor([[ 0.9843,  0.2779, -0.5186,  1.2017,  0.0051,  0.0240,  0.2871]],
       dtype=torch.float64)
	q_value: tensor([[-2.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9046745619340802, distance: 0.35331420462740165 entropy 0.03264415264129639
epoch: 67, step: 85
	action: tensor([[ 0.7593,  0.3548, -0.2480,  0.1678,  0.2273,  0.1449,  0.2593]],
       dtype=torch.float64)
	q_value: tensor([[-4.1219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9719208910438812, distance: 0.1917557317660878 entropy 0.03264415264129639
epoch: 67, step: 86
	action: tensor([[ 0.6463,  0.4699, -0.3825,  0.2902, -0.1287,  0.7404, -0.1097]],
       dtype=torch.float64)
	q_value: tensor([[-2.2876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9049802461761951, distance: 0.35274725570841975 entropy 0.03264415264129639
epoch: 67, step: 87
	action: tensor([[ 1.1230,  0.0225, -0.3750,  0.5139, -0.0110,  0.3164, -0.1233]],
       dtype=torch.float64)
	q_value: tensor([[-4.2046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9598754670637406, distance: 0.22922484530892212 entropy 0.03264415264129639
epoch: 67, step: 88
	action: tensor([[ 0.4774, -0.2103, -0.5567,  0.2449, -0.6791,  0.3006, -0.1309]],
       dtype=torch.float64)
	q_value: tensor([[-4.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.575528795313391, distance: 0.7455569280232514 entropy 0.03264415264129639
epoch: 67, step: 89
	action: tensor([[ 0.4173,  0.1835, -0.2129,  0.6660,  0.0817, -0.1604,  0.2639]],
       dtype=torch.float64)
	q_value: tensor([[-3.0362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8495412451320882, distance: 0.4438798442758119 entropy 0.03264415264129639
epoch: 67, step: 90
	action: tensor([[ 0.8491,  0.2565, -0.4548,  0.7054, -0.4746,  0.4148, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[-1.5501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9453371555712744, distance: 0.2675486763303502 entropy 0.03264415264129639
epoch: 67, step: 91
	action: tensor([[ 0.4495, -0.0547, -0.6337,  0.9760,  0.0031,  0.7272, -0.4127]],
       dtype=torch.float64)
	q_value: tensor([[-4.5344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6842914915522305, distance: 0.642983512019216 entropy 0.03264415264129639
epoch: 67, step: 92
	action: tensor([[ 0.9470,  0.2932, -0.7152,  0.7137, -0.2857,  0.3812, -0.1201]],
       dtype=torch.float64)
	q_value: tensor([[-3.9516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9485014957521579, distance: 0.25968926723610997 entropy 0.03264415264129639
epoch: 67, step: 93
	action: tensor([[ 1.2961, -0.0113, -0.5401,  0.3042,  0.0285,  0.1503, -0.2936]],
       dtype=torch.float64)
	q_value: tensor([[-5.2067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8024558604370767, distance: 0.5086145364717027 entropy 0.03264415264129639
epoch: 67, step: 94
	action: tensor([[ 0.7346,  0.2022, -0.4681,  0.2200, -0.1767,  0.2452, -0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-5.0354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9393596182135023, distance: 0.28179783794792956 entropy 0.03264415264129639
epoch: 67, step: 95
	action: tensor([[ 1.0402,  0.4731, -0.6638,  0.7530, -0.5137, -0.3627, -0.1596]],
       dtype=torch.float64)
	q_value: tensor([[-3.2171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9767544648362645, distance: 0.1744722925909315 entropy 0.03264415264129639
epoch: 67, step: 96
	action: tensor([[ 0.7002, -0.1538, -0.1804,  0.7029, -0.5056,  0.6735, -0.1566]],
       dtype=torch.float64)
	q_value: tensor([[-5.5702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.929629731505484, distance: 0.30356472030172366 entropy 0.03264415264129639
epoch: 67, step: 97
	action: tensor([[ 1.0742, -0.2893, -0.4074,  1.2168, -0.1465,  0.2229,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[-3.8530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9897115747992346, distance: 0.11607298296062335 entropy 0.03264415264129639
epoch: 67, step: 98
	action: tensor([[ 1.3004, -0.2908, -0.8018,  0.6179, -0.0203,  0.2003, -0.0753]],
       dtype=torch.float64)
	q_value: tensor([[-4.0605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7889573210694776, distance: 0.5257046745986976 entropy 0.03264415264129639
epoch: 67, step: 99
	action: tensor([[ 0.8486,  0.3263, -0.7743,  0.7440, -0.3629, -0.1232,  0.1548]],
       dtype=torch.float64)
	q_value: tensor([[-4.7618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9508275246462307, distance: 0.25375682333144245 entropy 0.03264415264129639
epoch: 67, step: 100
	action: tensor([[ 0.7204, -0.3059, -0.4777,  0.5722,  0.0718, -0.0577,  0.0302]],
       dtype=torch.float64)
	q_value: tensor([[-4.2037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7347317897526131, distance: 0.5893852960376625 entropy 0.03264415264129639
epoch: 67, step: 101
	action: tensor([[ 0.8187, -0.0900, -0.4439,  0.7070, -0.5906,  0.4422, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-2.0110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9228365161772388, distance: 0.31787956268093887 entropy 0.03264415264129639
epoch: 67, step: 102
	action: tensor([[ 0.5447,  0.1030, -0.2003,  0.8788, -0.1250,  0.4449, -0.0260]],
       dtype=torch.float64)
	q_value: tensor([[-4.0624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8886424590779264, distance: 0.3818708511936599 entropy 0.03264415264129639
epoch: 67, step: 103
	action: tensor([[ 0.8168, -0.1829, -0.4820,  0.7804, -0.4733,  0.3746, -0.0499]],
       dtype=torch.float64)
	q_value: tensor([[-2.9715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9058298263910767, distance: 0.35116674233546025 entropy 0.03264415264129639
epoch: 67, step: 104
	action: tensor([[ 0.9161, -0.3391, -0.2444,  0.1803, -0.5220,  0.8574,  0.1686]],
       dtype=torch.float64)
	q_value: tensor([[-3.7661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7587700569606286, distance: 0.5620465682191503 entropy 0.03264415264129639
epoch: 67, step: 105
	action: tensor([[ 0.4026,  0.2755, -0.1687,  0.6143, -0.0316,  0.4721,  0.1463]],
       dtype=torch.float64)
	q_value: tensor([[-3.8591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8562852208330821, distance: 0.4338178415629665 entropy 0.03264415264129639
epoch: 67, step: 106
	action: tensor([[ 1.2459,  0.2962, -0.2955,  0.5381,  0.1754, -0.4401, -0.0956]],
       dtype=torch.float64)
	q_value: tensor([[-2.2915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9267521310495648, distance: 0.3097092598135188 entropy 0.03264415264129639
epoch: 67, step: 107
	action: tensor([[ 1.1044, -0.1074, -0.5166,  0.3699, -0.2685,  0.1138, -0.0388]],
       dtype=torch.float64)
	q_value: tensor([[-4.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8304271372282902, distance: 0.4712321029264354 entropy 0.03264415264129639
epoch: 67, step: 108
	action: tensor([[ 0.6279,  0.0702, -0.1835,  0.6405,  0.0950, -0.2240, -0.5128]],
       dtype=torch.float64)
	q_value: tensor([[-4.0193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9293061739035533, distance: 0.3042618046733669 entropy 0.03264415264129639
epoch: 67, step: 109
	action: tensor([[ 0.6606, -0.0798, -0.3828,  0.6516, -0.2947,  0.3260, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[-2.9638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8954000257923158, distance: 0.37010289480126995 entropy 0.03264415264129639
epoch: 67, step: 110
	action: tensor([[ 0.6947,  0.0983, -0.6596,  0.3826,  0.1056,  0.6258, -0.0802]],
       dtype=torch.float64)
	q_value: tensor([[-2.8890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9123240589171386, distance: 0.3388417463225503 entropy 0.03264415264129639
epoch: 67, step: 111
	action: tensor([[ 0.5956, -0.3072, -0.5109,  0.8781, -0.0087,  0.4027, -0.0637]],
       dtype=torch.float64)
	q_value: tensor([[-3.5028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8056770553397457, distance: 0.5044507066226799 entropy 0.03264415264129639
epoch: 67, step: 112
	action: tensor([[ 0.7446, -0.0905, -0.4214,  0.6104, -0.3877,  0.6291, -0.2010]],
       dtype=torch.float64)
	q_value: tensor([[-2.6236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9053046799201107, distance: 0.35214453383495004 entropy 0.03264415264129639
epoch: 67, step: 113
	action: tensor([[ 0.6713,  0.4552, -0.8355,  0.7217,  0.0239,  0.2247,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-4.0779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.813583895712621, distance: 0.4940812888433415 entropy 0.03264415264129639
epoch: 67, step: 114
	action: tensor([[ 1.0167, -0.0156, -0.6425,  0.3103, -0.3910,  0.4123, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[-4.1272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9008734686899774, distance: 0.3602895351699145 entropy 0.03264415264129639
epoch: 67, step: 115
	action: tensor([[ 0.7095, -0.0844, -0.5261,  0.7627, -0.0385,  0.7602, -0.5294]],
       dtype=torch.float64)
	q_value: tensor([[-4.4569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8828485377480334, distance: 0.3916792370090727 entropy 0.03264415264129639
epoch: 67, step: 116
	action: tensor([[ 0.9903,  0.2809, -0.3741,  0.3052, -0.0099,  0.0201,  0.0435]],
       dtype=torch.float64)
	q_value: tensor([[-4.6023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9707104164337661, distance: 0.19584536231958072 entropy 0.03264415264129639
epoch: 67, step: 117
	action: tensor([[ 0.7887, -0.1904, -0.1441,  0.4686,  0.0451,  0.2127,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-3.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8687436133022602, distance: 0.41458817947845217 entropy 0.03264415264129639
epoch: 67, step: 118
	action: tensor([[ 0.8561, -0.1310, -0.1903,  0.3808, -0.0280,  0.1964, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[-2.1584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8693397713067245, distance: 0.4136455916418599 entropy 0.03264415264129639
epoch: 67, step: 119
	action: tensor([[ 0.5732, -0.1466, -0.3414,  0.7469, -0.1669, -0.0079,  0.0747]],
       dtype=torch.float64)
	q_value: tensor([[-2.5877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8516675013966369, distance: 0.44073226910591135 entropy 0.03264415264129639
epoch: 67, step: 120
	action: tensor([[ 0.8198,  0.4167, -0.5261,  0.4270, -0.7256,  0.1759,  0.0626]],
       dtype=torch.float64)
	q_value: tensor([[-2.0688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9669194952525524, distance: 0.20813386880896456 entropy 0.03264415264129639
epoch: 67, step: 121
	action: tensor([[ 0.3411,  0.0313, -0.2104,  0.7367, -0.3063,  0.5414, -0.3667]],
       dtype=torch.float64)
	q_value: tensor([[-4.5937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7834434458744104, distance: 0.5325278917923877 entropy 0.03264415264129639
epoch: 67, step: 122
	action: tensor([[ 1.1916, -0.0241, -0.3028,  0.0946,  0.2212,  0.4065,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[-3.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7975244441331623, distance: 0.5149238328027425 entropy 0.03264415264129639
epoch: 67, step: 123
	action: tensor([[ 0.9298,  0.1005, -0.5072,  0.4278,  0.1849,  0.2861, -0.2309]],
       dtype=torch.float64)
	q_value: tensor([[-3.5173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.971374145457705, distance: 0.19361362815488334 entropy 0.03264415264129639
epoch: 67, step: 124
	action: tensor([[ 0.6426,  0.0905, -0.6669,  0.3193,  0.1361,  0.1832,  0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-3.8170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8675929769023272, distance: 0.41640142204549474 entropy 0.03264415264129639
epoch: 67, step: 125
	action: tensor([[ 0.5545,  0.1849, -0.2349,  0.4850, -0.2415,  0.0234, -0.2603]],
       dtype=torch.float64)
	q_value: tensor([[-2.4974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9185524506605995, distance: 0.3265846016021347 entropy 0.03264415264129639
epoch: 67, step: 126
	action: tensor([[ 0.6936, -0.4231, -0.3060,  0.4935,  0.0140,  0.5033,  0.1924]],
       dtype=torch.float64)
	q_value: tensor([[-2.7866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7615013564642796, distance: 0.5588556550839958 entropy 0.03264415264129639
epoch: 67, step: 127
	action: tensor([[ 0.9877, -0.0137, -0.7256,  0.2143, -0.2805,  0.7921,  0.2123]],
       dtype=torch.float64)
	q_value: tensor([[-2.1085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9053479523120536, distance: 0.3520640658799593 entropy 0.03264415264129639
LOSS epoch 67 actor 232.5389645118994 critic 2695.5500785585295 
epoch: 68, step: 0
	action: tensor([[ 0.6717,  0.3363, -0.6147,  0.4116, -0.2372,  0.2086,  0.4243]],
       dtype=torch.float64)
	q_value: tensor([[-3.5355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.924124455224892, distance: 0.31521552875908626 entropy 0.03264415264129639
epoch: 68, step: 1
	action: tensor([[ 1.1330, -0.0509, -0.2792,  0.6626,  0.2223,  0.1172, -0.4268]],
       dtype=torch.float64)
	q_value: tensor([[-2.2694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9452249898466067, distance: 0.26782303464963697 entropy 0.03264415264129639
epoch: 68, step: 2
	action: tensor([[ 1.2434, -0.0123, -0.6393,  0.8594, -0.3396,  0.5521, -0.3033]],
       dtype=torch.float64)
	q_value: tensor([[-3.2721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0979685750439712 entropy 0.03264415264129639
epoch: 68, step: 3
	action: tensor([[ 0.3338, -0.2954, -0.2215,  0.3350,  0.2179,  0.5036, -0.2547]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6084434859871752, distance: 0.71606742912188 entropy 0.03264415264129639
epoch: 68, step: 4
	action: tensor([[ 1.0820, -0.1820, -0.2392,  0.8034, -0.2425,  0.4468, -0.4834]],
       dtype=torch.float64)
	q_value: tensor([[-1.0341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9759117549480973, distance: 0.17760667080125586 entropy 0.03264415264129639
epoch: 68, step: 5
	action: tensor([[ 1.1476,  0.1604, -0.5826,  0.9803, -0.5350,  0.3665,  0.0916]],
       dtype=torch.float64)
	q_value: tensor([[-3.8049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9796356065572897, distance: 0.16330234936682209 entropy 0.03264415264129639
epoch: 68, step: 6
	action: tensor([[ 1.8807, -0.1610, -0.5717,  0.3604, -0.2741,  0.3057,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-4.1541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 68, step: 7
	action: tensor([[ 0.5063,  0.5103, -0.2107,  0.4244,  0.0172, -0.0049, -0.4026]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.891455737963369, distance: 0.3770163012584019 entropy 0.03264415264129639
epoch: 68, step: 8
	action: tensor([[ 0.9778,  0.2540, -0.5246,  0.6567, -0.0427,  0.4593,  0.0921]],
       dtype=torch.float64)
	q_value: tensor([[-2.3227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.978720702445016, distance: 0.16693036270414705 entropy 0.03264415264129639
epoch: 68, step: 9
	action: tensor([[ 1.4762, -0.0625, -0.2172,  1.0408,  0.0458, -0.0726, -0.1456]],
       dtype=torch.float64)
	q_value: tensor([[-3.1921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8824085327476608, distance: 0.39241409469035093 entropy 0.03264415264129639
epoch: 68, step: 10
	action: tensor([[ 1.7522,  0.6022, -0.9172,  1.1731,  0.2797,  0.2139,  0.1849]],
       dtype=torch.float64)
	q_value: tensor([[-4.0110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 68, step: 11
	action: tensor([[ 1.2875,  0.0484,  0.0285,  0.2967, -0.0535,  0.3663,  0.1318]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8362959788944896, distance: 0.46300573123284045 entropy 0.03264415264129639
epoch: 68, step: 12
	action: tensor([[ 1.3701,  0.0565, -0.4976,  0.5343,  0.0243,  0.2673, -0.1669]],
       dtype=torch.float64)
	q_value: tensor([[-2.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9036208202039242, distance: 0.3552616318100309 entropy 0.03264415264129639
epoch: 68, step: 13
	action: tensor([[ 1.4460,  0.3613, -0.3407,  0.7574, -0.3296,  0.7131,  0.5909]],
       dtype=torch.float64)
	q_value: tensor([[-4.1180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9183186833506274, distance: 0.3270529404440071 entropy 0.03264415264129639
epoch: 68, step: 14
	action: tensor([[ 0.8654,  0.4561,  0.1664,  0.8330, -0.2469,  0.3381,  0.1895]],
       dtype=torch.float64)
	q_value: tensor([[-4.3270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 68, step: 15
	action: tensor([[ 0.9383,  0.0631,  0.0880,  0.5623, -0.1837,  0.5919,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07114296331747963 entropy 0.03264415264129639
epoch: 68, step: 16
	action: tensor([[ 0.8578,  0.1959, -0.2089,  0.5344, -0.3695,  0.4297, -0.2610]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9859747574167884, distance: 0.13552264964223942 entropy 0.03264415264129639
epoch: 68, step: 17
	action: tensor([[ 0.7758,  0.2805, -0.7830,  0.3922, -0.6374, -0.0855,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-3.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9530544238784677, distance: 0.24794424354187536 entropy 0.03264415264129639
epoch: 68, step: 18
	action: tensor([[ 0.9868,  0.0879, -0.2589,  0.7484, -0.3921,  0.8703, -0.3855]],
       dtype=torch.float64)
	q_value: tensor([[-3.1214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9777316301888431, distance: 0.17076580343818426 entropy 0.03264415264129639
epoch: 68, step: 19
	action: tensor([[ 1.5294,  0.0024, -0.8063,  0.4765, -0.0261,  0.3739,  0.2614]],
       dtype=torch.float64)
	q_value: tensor([[-4.4262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8216860500110399, distance: 0.4832249541669364 entropy 0.03264415264129639
epoch: 68, step: 20
	action: tensor([[ 1.4138,  0.1866, -0.6268,  1.0171, -0.1559,  0.3798, -0.1386]],
       dtype=torch.float64)
	q_value: tensor([[-4.3088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9813611623538241, distance: 0.15623060191214494 entropy 0.03264415264129639
epoch: 68, step: 21
	action: tensor([[ 1.4197,  0.0846, -0.5743,  0.9878,  0.0698,  0.3677,  0.1466]],
       dtype=torch.float64)
	q_value: tensor([[-5.0769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9774417661720347, distance: 0.17187362637836232 entropy 0.03264415264129639
epoch: 68, step: 22
	action: tensor([[ 1.1126,  0.5069, -0.5201,  1.2606, -0.4756,  0.8093, -0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-4.1317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.70333956805382, distance: 0.6232847565481462 entropy 0.03264415264129639
epoch: 68, step: 23
	action: tensor([[ 1.1716, -0.0444, -0.5198,  1.1205, -0.2912,  0.1154, -0.2772]],
       dtype=torch.float64)
	q_value: tensor([[-5.2896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.049950193371449 entropy 0.03264415264129639
epoch: 68, step: 24
	action: tensor([[ 1.3462,  0.3743,  0.1188,  0.5653, -0.0368,  0.2116, -0.0677]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.847674167188182, distance: 0.4466254572966161 entropy 0.03264415264129639
epoch: 68, step: 25
	action: tensor([[ 1.4820,  0.3960, -0.4941,  1.0036, -0.5862,  0.7295,  0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-3.8163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.929120130786293, distance: 0.3046619005844032 entropy 0.03264415264129639
epoch: 68, step: 26
	action: tensor([[ 0.9793,  0.3079, -0.5247,  0.8254, -0.2849,  0.4508,  0.1241]],
       dtype=torch.float64)
	q_value: tensor([[-5.6052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9385263325809393, distance: 0.2837273846785923 entropy 0.03264415264129639
epoch: 68, step: 27
	action: tensor([[ 1.0730,  0.0494, -0.5110,  0.9110, -0.3604,  0.0117, -0.0985]],
       dtype=torch.float64)
	q_value: tensor([[-3.5629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05260746412401799 entropy 0.03264415264129639
epoch: 68, step: 28
	action: tensor([[ 0.7359,  0.0982, -0.2881,  0.6078, -0.1306,  0.0603, -0.6060]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.953029310036825, distance: 0.24801055436852215 entropy 0.03264415264129639
epoch: 68, step: 29
	action: tensor([[ 1.1648,  0.1745, -0.8645,  0.8835,  0.0315,  0.4986, -0.2474]],
       dtype=torch.float64)
	q_value: tensor([[-3.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.973317700638354, distance: 0.18692540035184912 entropy 0.03264415264129639
epoch: 68, step: 30
	action: tensor([[ 1.5233, -0.1784, -0.8613,  1.0005, -0.2689,  0.1556, -0.0732]],
       dtype=torch.float64)
	q_value: tensor([[-4.6326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9163889791061481, distance: 0.3308936678655175 entropy 0.03264415264129639
epoch: 68, step: 31
	action: tensor([[ 1.2957,  0.2002, -0.6273,  1.1872, -0.1491,  0.1964,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-4.8532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9681135835868766, distance: 0.20434289879054884 entropy 0.03264415264129639
epoch: 68, step: 32
	action: tensor([[ 1.3190,  0.2464, -1.0925,  0.9442, -0.0833,  0.3580, -0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-4.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9816233756121592, distance: 0.15512777468524527 entropy 0.03264415264129639
epoch: 68, step: 33
	action: tensor([[ 1.3644,  0.5288, -0.7825,  0.8563, -0.2346,  0.6800,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-5.1008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9329915642603827, distance: 0.2962248124363811 entropy 0.03264415264129639
epoch: 68, step: 34
	action: tensor([[ 1.1452,  0.1429, -0.3338,  1.2439, -0.6648, -0.0963, -0.5393]],
       dtype=torch.float64)
	q_value: tensor([[-5.6259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9528520579189327, distance: 0.24847806933437677 entropy 0.03264415264129639
epoch: 68, step: 35
	action: tensor([[ 1.2876,  0.1438, -0.2635,  0.9385, -0.6159,  0.2729,  0.2418]],
       dtype=torch.float64)
	q_value: tensor([[-5.0587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9631210735469123, distance: 0.21975857403915558 entropy 0.03264415264129639
epoch: 68, step: 36
	action: tensor([[ 0.5997,  0.3093, -0.3309,  0.5788, -0.5824,  0.4030, -0.0631]],
       dtype=torch.float64)
	q_value: tensor([[-4.0156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8851687067839569, distance: 0.3877812633395167 entropy 0.03264415264129639
epoch: 68, step: 37
	action: tensor([[ 1.1341,  0.3172, -0.6499,  0.9242, -0.2895,  0.0964, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[-2.9107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9740775150323889, distance: 0.18424470219156563 entropy 0.03264415264129639
epoch: 68, step: 38
	action: tensor([[ 1.0466,  0.2781, -0.6049,  0.6770, -0.2152,  0.1045, -0.1570]],
       dtype=torch.float64)
	q_value: tensor([[-4.1400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08702150330531792 entropy 0.03264415264129639
epoch: 68, step: 39
	action: tensor([[ 0.7412, -0.8229, -0.1878,  0.6769, -0.3043,  0.1379,  0.0370]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4299008907729247, distance: 0.8640360731686216 entropy 0.03264415264129639
epoch: 68, step: 40
	action: tensor([[ 1.0294, -0.1605, -0.6275,  0.8542, -0.6129,  0.4990, -0.2859]],
       dtype=torch.float64)
	q_value: tensor([[-1.3988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.911052376133427, distance: 0.3412902399515698 entropy 0.03264415264129639
epoch: 68, step: 41
	action: tensor([[ 0.8247,  0.2014, -0.9328,  0.6859,  0.0471,  0.3062,  0.2878]],
       dtype=torch.float64)
	q_value: tensor([[-4.2025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8852790913694601, distance: 0.38759483608331247 entropy 0.03264415264129639
epoch: 68, step: 42
	action: tensor([[ 0.9239, -0.0061, -0.5900,  1.0053, -0.0914,  0.0436,  0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-2.8081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9560295052661254, distance: 0.23995918008232878 entropy 0.03264415264129639
epoch: 68, step: 43
	action: tensor([[ 1.1557,  0.3010, -0.9625,  0.6908, -0.1819,  0.5655, -0.2278]],
       dtype=torch.float64)
	q_value: tensor([[-2.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9745110301255057, distance: 0.18269759704453611 entropy 0.03264415264129639
epoch: 68, step: 44
	action: tensor([[ 1.2018,  0.0047, -0.4728,  1.2529, -0.0403,  0.2553,  0.1282]],
       dtype=torch.float64)
	q_value: tensor([[-5.0925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9758931550946205, distance: 0.1776752274893241 entropy 0.03264415264129639
epoch: 68, step: 45
	action: tensor([[ 1.1476,  0.3816,  0.0895,  0.8391, -0.4063,  0.4069, -0.2383]],
       dtype=torch.float64)
	q_value: tensor([[-3.5113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8853283165608701, distance: 0.3875116713082768 entropy 0.03264415264129639
epoch: 68, step: 46
	action: tensor([[ 1.4029,  0.1733, -0.9515,  0.4084,  0.2465,  0.2166, -0.3153]],
       dtype=torch.float64)
	q_value: tensor([[-4.2302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8948925996049676, distance: 0.37099951400908415 entropy 0.03264415264129639
epoch: 68, step: 47
	action: tensor([[ 1.4315,  0.0262, -0.4805,  0.9230, -0.2380,  0.4192, -0.1168]],
       dtype=torch.float64)
	q_value: tensor([[-4.8180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9730427839986451, distance: 0.18788590998694796 entropy 0.03264415264129639
epoch: 68, step: 48
	action: tensor([[ 1.3880,  0.1099, -0.6176,  0.7238, -0.2807,  0.2375, -0.1854]],
       dtype=torch.float64)
	q_value: tensor([[-4.7336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.966545843912536, distance: 0.20930602632856163 entropy 0.03264415264129639
epoch: 68, step: 49
	action: tensor([[ 1.1022,  0.0389, -0.6829,  1.0605, -0.5574,  0.4347, -0.3307]],
       dtype=torch.float64)
	q_value: tensor([[-4.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9495902412682391, distance: 0.2569295183178223 entropy 0.03264415264129639
epoch: 68, step: 50
	action: tensor([[ 1.1170,  0.3435, -0.7570,  0.7984,  0.0355,  0.3393, -0.4908]],
       dtype=torch.float64)
	q_value: tensor([[-4.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9678562481439912, distance: 0.20516580408507232 entropy 0.03264415264129639
epoch: 68, step: 51
	action: tensor([[ 1.0426,  0.1885, -1.1866,  0.8122,  0.1672,  0.0063, -0.0025]],
       dtype=torch.float64)
	q_value: tensor([[-4.8663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9296323753893847, distance: 0.30355901762762066 entropy 0.03264415264129639
epoch: 68, step: 52
	action: tensor([[ 1.1944,  0.4200, -0.8127,  0.9618, -0.0926,  0.1321, -0.1520]],
       dtype=torch.float64)
	q_value: tensor([[-3.8595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9570350688957276, distance: 0.23719949120708447 entropy 0.03264415264129639
epoch: 68, step: 53
	action: tensor([[ 1.5964, -0.0736, -0.6857,  0.8055, -0.2121,  0.0239, -0.5023]],
       dtype=torch.float64)
	q_value: tensor([[-4.7722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8383006291417205, distance: 0.4601621127405413 entropy 0.03264415264129639
epoch: 68, step: 54
	action: tensor([[ 1.3350,  0.5221, -0.7637,  1.1962, -0.4828, -0.2323,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[-5.5428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9279872318073583, distance: 0.307087010164071 entropy 0.03264415264129639
epoch: 68, step: 55
	action: tensor([[ 1.4920,  0.2949, -0.8665,  1.0575,  0.0090,  0.5002,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-5.2510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9794003773559574, distance: 0.16424279448322046 entropy 0.03264415264129639
epoch: 68, step: 56
	action: tensor([[ 0.8214,  0.4209, -0.7651,  1.1355, -0.2101,  0.4185, -0.4697]],
       dtype=torch.float64)
	q_value: tensor([[-5.2046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7134795472605371, distance: 0.6125400753664361 entropy 0.03264415264129639
epoch: 68, step: 57
	action: tensor([[ 1.2265,  0.0249, -0.2701,  0.5423, -0.5315,  0.9667,  0.0659]],
       dtype=torch.float64)
	q_value: tensor([[-4.7646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9899872582641696, distance: 0.11450730685240303 entropy 0.03264415264129639
epoch: 68, step: 58
	action: tensor([[ 1.3866,  0.2770, -0.7206,  0.8242, -0.0234,  0.4522, -0.1883]],
       dtype=torch.float64)
	q_value: tensor([[-4.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9872720190538389, distance: 0.12910303487629343 entropy 0.03264415264129639
epoch: 68, step: 59
	action: tensor([[ 1.3211,  0.1368, -0.4807,  0.8826,  0.2806,  0.4458, -0.7066]],
       dtype=torch.float64)
	q_value: tensor([[-5.1511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9720938695839061, distance: 0.19116417330398952 entropy 0.03264415264129639
epoch: 68, step: 60
	action: tensor([[ 1.6551,  0.1807, -0.1497,  1.3522, -0.5690,  0.5427, -0.0198]],
       dtype=torch.float64)
	q_value: tensor([[-5.0979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8243908434988925, distance: 0.4795459987945433 entropy 0.03264415264129639
epoch: 68, step: 61
	action: tensor([[ 1.4656, -0.0344, -0.8121,  0.7104, -0.3365,  0.7249, -0.3207]],
       dtype=torch.float64)
	q_value: tensor([[-5.6664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9559062091281428, distance: 0.2402953751727774 entropy 0.03264415264129639
epoch: 68, step: 62
	action: tensor([[ 1.3722,  0.0238, -0.6506,  0.9139, -0.1783,  0.3503, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[-5.7753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9914930220874546, distance: 0.1055466238279973 entropy 0.03264415264129639
epoch: 68, step: 63
	action: tensor([[ 1.1784, -0.2210, -0.5651,  0.8941, -0.1491,  0.4624, -0.0592]],
       dtype=torch.float64)
	q_value: tensor([[-4.4496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9725503759062246, distance: 0.18959413301578515 entropy 0.03264415264129639
epoch: 68, step: 64
	action: tensor([[ 1.6323, -0.0634, -0.7117,  1.5460, -0.2260,  0.3594, -0.1033]],
       dtype=torch.float64)
	q_value: tensor([[-3.5313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9509772878255447, distance: 0.2533700987564107 entropy 0.03264415264129639
epoch: 68, step: 65
	action: tensor([[ 1.0924,  0.3946, -0.6872,  1.0433, -0.2515, -0.1243,  0.1729]],
       dtype=torch.float64)
	q_value: tensor([[-5.6231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9452693614886003, distance: 0.26771453484815194 entropy 0.03264415264129639
epoch: 68, step: 66
	action: tensor([[ 0.7546,  0.0549, -0.8274,  1.2897, -0.1546,  0.2498, -0.1242]],
       dtype=torch.float64)
	q_value: tensor([[-3.8655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7404525084668563, distance: 0.5829953773728443 entropy 0.03264415264129639
epoch: 68, step: 67
	action: tensor([[ 1.1833, -0.1889, -1.0906,  0.6920, -0.4091,  0.4872,  0.2615]],
       dtype=torch.float64)
	q_value: tensor([[-3.4361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8182593338666677, distance: 0.4878460029347724 entropy 0.03264415264129639
epoch: 68, step: 68
	action: tensor([[ 1.1140, -0.0602, -0.7655,  0.9584,  0.1471,  0.4500,  0.0443]],
       dtype=torch.float64)
	q_value: tensor([[-3.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9681325744680951, distance: 0.20428203855386795 entropy 0.03264415264129639
epoch: 68, step: 69
	action: tensor([[ 1.5442,  0.2753, -1.0189,  1.2489, -0.1627,  0.5098,  0.2960]],
       dtype=torch.float64)
	q_value: tensor([[-3.3821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9633861898069814, distance: 0.21896724599562395 entropy 0.03264415264129639
epoch: 68, step: 70
	action: tensor([[ 1.4361,  0.2044, -0.1937,  1.0395, -0.1991,  0.2329, -0.3045]],
       dtype=torch.float64)
	q_value: tensor([[-5.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9041483594136382, distance: 0.35428802110813895 entropy 0.03264415264129639
epoch: 68, step: 71
	action: tensor([[ 1.2628,  0.1967, -0.4089,  0.9204, -0.6332, -0.0295, -0.0459]],
       dtype=torch.float64)
	q_value: tensor([[-4.9600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9757777606466195, distance: 0.17809996697405203 entropy 0.03264415264129639
epoch: 68, step: 72
	action: tensor([[ 1.4763,  0.3872, -0.7772,  0.5927,  0.2177,  0.2473,  0.3095]],
       dtype=torch.float64)
	q_value: tensor([[-4.3714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9416645111170479, distance: 0.2763905016888204 entropy 0.03264415264129639
epoch: 68, step: 73
	action: tensor([[ 1.1821, -0.2943, -0.3430,  1.1716, -0.4303, -0.1375, -0.3749]],
       dtype=torch.float64)
	q_value: tensor([[-4.3953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9644361131892027, distance: 0.21580490075868236 entropy 0.03264415264129639
epoch: 68, step: 74
	action: tensor([[ 1.4840,  0.2156, -0.8369,  0.5917, -0.3235,  0.7460,  0.2520]],
       dtype=torch.float64)
	q_value: tensor([[-3.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9813244100057139, distance: 0.1563845550153401 entropy 0.03264415264129639
epoch: 68, step: 75
	action: tensor([[ 0.9556,  0.0916, -0.7142,  0.6960, -0.6035,  0.0705, -0.1023]],
       dtype=torch.float64)
	q_value: tensor([[-5.1398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9607934614888705, distance: 0.22658749785665103 entropy 0.03264415264129639
epoch: 68, step: 76
	action: tensor([[ 1.1373, -0.2717, -0.8118,  0.7371, -0.0803,  0.0447, -0.0472]],
       dtype=torch.float64)
	q_value: tensor([[-3.6311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8358208717073983, distance: 0.46367711967493197 entropy 0.03264415264129639
epoch: 68, step: 77
	action: tensor([[ 0.6265,  0.3573, -0.8695,  0.9748, -0.4810,  0.2994, -0.3515]],
       dtype=torch.float64)
	q_value: tensor([[-3.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6744830609538326, distance: 0.6528952211188808 entropy 0.03264415264129639
epoch: 68, step: 78
	action: tensor([[ 1.2760,  0.1351, -0.1765,  0.9632, -0.0244,  0.7715, -0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-4.2138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9229470008743493, distance: 0.3176519070192303 entropy 0.03264415264129639
epoch: 68, step: 79
	action: tensor([[ 1.8377,  0.5875, -0.3074,  1.3231, -0.2700,  0.1854, -0.2683]],
       dtype=torch.float64)
	q_value: tensor([[-4.2645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 68, step: 80
	action: tensor([[ 1.1320,  0.0419, -0.2165,  0.5978, -0.1678,  0.1401, -0.3186]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9560733010514266, distance: 0.23983964743935246 entropy 0.03264415264129639
epoch: 68, step: 81
	action: tensor([[ 1.3856, -0.1498, -0.4792,  0.9051,  0.1225,  0.2648,  0.3240]],
       dtype=torch.float64)
	q_value: tensor([[-3.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9442388610093247, distance: 0.27022312455318037 entropy 0.03264415264129639
epoch: 68, step: 82
	action: tensor([[ 0.9197,  0.2863, -0.1785,  0.7835, -0.2335,  0.6000, -0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-3.1668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9381445686995378, distance: 0.28460702326965065 entropy 0.03264415264129639
epoch: 68, step: 83
	action: tensor([[ 1.4889, -0.0130, -0.4222,  0.9395, -0.4679,  0.5781,  0.2138]],
       dtype=torch.float64)
	q_value: tensor([[-3.4464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9629238370980556, distance: 0.2203454485169575 entropy 0.03264415264129639
epoch: 68, step: 84
	action: tensor([[ 1.1655,  0.4575, -0.6403,  0.8226, -0.3789,  0.1791,  0.0533]],
       dtype=torch.float64)
	q_value: tensor([[-4.6290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.952054941341584, distance: 0.25056973888912426 entropy 0.03264415264129639
epoch: 68, step: 85
	action: tensor([[ 1.1947,  0.0998, -0.8251,  0.9631, -0.0781,  0.4831,  0.1901]],
       dtype=torch.float64)
	q_value: tensor([[-4.4400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9773042626726547, distance: 0.17239665758237252 entropy 0.03264415264129639
epoch: 68, step: 86
	action: tensor([[ 1.0223,  0.3389, -0.3486,  1.2727, -0.2706,  0.3414, -0.0676]],
       dtype=torch.float64)
	q_value: tensor([[-3.9044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8128524710469821, distance: 0.4950496318268827 entropy 0.03264415264129639
epoch: 68, step: 87
	action: tensor([[ 1.5077, -0.2432, -0.3711,  1.1292, -0.5204,  0.2025,  0.4065]],
       dtype=torch.float64)
	q_value: tensor([[-4.0799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9087869877138746, distance: 0.34560903905085266 entropy 0.03264415264129639
epoch: 68, step: 88
	action: tensor([[ 0.6688,  0.2628, -0.3141,  0.9387, -0.0107,  0.8230, -0.1197]],
       dtype=torch.float64)
	q_value: tensor([[-3.8976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.817855378920991, distance: 0.4883878696113145 entropy 0.03264415264129639
epoch: 68, step: 89
	action: tensor([[ 1.0510,  0.9128, -0.4763,  1.2677,  0.1450,  0.2724, -0.3201]],
       dtype=torch.float64)
	q_value: tensor([[-3.2082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42546835427187435, distance: 0.867388521744172 entropy 0.03264415264129639
epoch: 68, step: 90
	action: tensor([[ 1.5082, -0.1149, -0.7204,  0.9615, -0.0341,  0.4770,  0.6211]],
       dtype=torch.float64)
	q_value: tensor([[-5.2588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9596475690372184, distance: 0.22987489551959606 entropy 0.03264415264129639
epoch: 68, step: 91
	action: tensor([[ 1.0771,  0.3435, -0.3081,  1.2126, -0.0574, -0.0804, -0.4918]],
       dtype=torch.float64)
	q_value: tensor([[-3.7382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8955924649543933, distance: 0.3697622872643188 entropy 0.03264415264129639
epoch: 68, step: 92
	action: tensor([[ 0.9537, -0.0960, -0.4493,  1.2268, -0.4896,  0.1625,  0.2162]],
       dtype=torch.float64)
	q_value: tensor([[-4.4156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9666256529925643, distance: 0.209056214377762 entropy 0.03264415264129639
epoch: 68, step: 93
	action: tensor([[ 1.0495,  0.5290, -0.3266,  0.8791, -0.2811,  0.2105, -0.3242]],
       dtype=torch.float64)
	q_value: tensor([[-2.9836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 68, step: 94
	action: tensor([[ 0.8400,  0.0071, -0.2722,  0.5724,  0.3818,  0.0585, -0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9146709213316955, distance: 0.33427601907739557 entropy 0.03264415264129639
epoch: 68, step: 95
	action: tensor([[ 1.5479, -0.0361, -0.6443,  0.8530, -0.1115,  0.5858,  0.0302]],
       dtype=torch.float64)
	q_value: tensor([[-1.7579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.985942714617417, distance: 0.13567737237115332 entropy 0.03264415264129639
epoch: 68, step: 96
	action: tensor([[ 1.5406, -0.0309, -0.3289,  0.7762, -0.1815,  0.6187,  0.3565]],
       dtype=torch.float64)
	q_value: tensor([[-4.8574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9577890562771034, distance: 0.23510898372766273 entropy 0.03264415264129639
epoch: 68, step: 97
	action: tensor([[ 1.2270e+00, -2.1579e-01, -5.4872e-01,  4.3041e-01,  9.7483e-04,
          3.9755e-01,  3.8962e-02]], dtype=torch.float64)
	q_value: tensor([[-4.1010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8505327849588168, distance: 0.4424148179940038 entropy 0.03264415264129639
epoch: 68, step: 98
	action: tensor([[ 1.2365,  0.3751, -1.1047,  0.7853, -0.2462,  0.1820, -0.2778]],
       dtype=torch.float64)
	q_value: tensor([[-3.0959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9530986029651763, distance: 0.24782754960112385 entropy 0.03264415264129639
epoch: 68, step: 99
	action: tensor([[ 1.7557, -0.2113, -0.4556,  0.7372, -0.3558,  0.6021, -0.1184]],
       dtype=torch.float64)
	q_value: tensor([[-5.4540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 68, step: 100
	action: tensor([[ 0.6746, -0.4754, -0.1255,  0.2862, -0.2938,  0.0133, -0.1245]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47066322148029327, distance: 0.8325737773236503 entropy 0.03264415264129639
epoch: 68, step: 101
	action: tensor([[ 1.4425,  0.0651, -0.5710,  0.5249,  0.1747,  0.3314, -0.0539]],
       dtype=torch.float64)
	q_value: tensor([[-1.2571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8924359329188566, distance: 0.3753101421750965 entropy 0.03264415264129639
epoch: 68, step: 102
	action: tensor([[ 1.0323,  0.6256, -0.7074,  0.7447, -0.2331,  0.1773, -0.3506]],
       dtype=torch.float64)
	q_value: tensor([[-4.1400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9001985305955496, distance: 0.36151403379676295 entropy 0.03264415264129639
epoch: 68, step: 103
	action: tensor([[ 1.2703,  0.3229, -0.8175,  0.6306, -0.5765,  0.6136,  0.1294]],
       dtype=torch.float64)
	q_value: tensor([[-4.9336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05588624451607026 entropy 0.03264415264129639
epoch: 68, step: 104
	action: tensor([[ 0.6553,  0.5064, -0.2971,  0.0192, -0.0502,  0.2696,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9557104127432365, distance: 0.24082829432993613 entropy 0.03264415264129639
epoch: 68, step: 105
	action: tensor([[ 0.8902, -0.0048, -0.0462,  0.4539, -0.3504,  0.4326,  0.2125]],
       dtype=torch.float64)
	q_value: tensor([[-2.2144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.966861233351208, distance: 0.20831707248850262 entropy 0.03264415264129639
epoch: 68, step: 106
	action: tensor([[ 0.9589, -0.2375, -0.3177,  0.7863,  0.1127,  0.3718,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-2.2561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9585080955249681, distance: 0.2330979091318005 entropy 0.03264415264129639
epoch: 68, step: 107
	action: tensor([[ 0.9202,  0.0243,  0.0846,  0.8898, -0.1327,  0.3262, -0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-2.2558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09490991394320995 entropy 0.03264415264129639
epoch: 68, step: 108
	action: tensor([[ 0.9445,  0.3760, -0.2539,  0.4471, -0.1183,  0.1624,  0.5679]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9905013985335929, distance: 0.11152867083316015 entropy 0.03264415264129639
epoch: 68, step: 109
	action: tensor([[ 0.9143, -0.0319, -0.0625,  0.8765, -0.3529,  0.3663, -0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-2.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0235398067365998 entropy 0.03264415264129639
epoch: 68, step: 110
	action: tensor([[ 0.7921,  0.4639, -0.5192,  0.3252, -0.3351,  0.4516, -0.4966]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9498204525733281, distance: 0.25634217406916976 entropy 0.03264415264129639
epoch: 68, step: 111
	action: tensor([[ 1.6421,  0.4430, -0.9153,  0.5071, -0.4576,  0.3968, -0.2336]],
       dtype=torch.float64)
	q_value: tensor([[-4.2469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9344754102600812, distance: 0.29292662544206716 entropy 0.03264415264129639
epoch: 68, step: 112
	action: tensor([[ 0.9143, -0.1522, -0.4186,  0.5798, -0.3144,  0.2684, -0.1056]],
       dtype=torch.float64)
	q_value: tensor([[-6.6391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9180346382661774, distance: 0.3276211068343773 entropy 0.03264415264129639
epoch: 68, step: 113
	action: tensor([[ 1.1152, -0.1212, -0.4475,  0.8786,  0.1017,  0.7790,  0.2531]],
       dtype=torch.float64)
	q_value: tensor([[-2.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08979621272522478 entropy 0.03264415264129639
epoch: 68, step: 114
	action: tensor([[ 0.3938, -0.2275, -0.6949,  1.0362,  0.2503,  0.4467, -0.0624]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6075203356947882, distance: 0.7169110476913554 entropy 0.03264415264129639
epoch: 68, step: 115
	action: tensor([[ 1.3731,  0.8160, -0.2078,  0.8666, -0.3792,  0.4146,  0.2024]],
       dtype=torch.float64)
	q_value: tensor([[-1.7470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 68, step: 116
	action: tensor([[ 0.9667, -0.3771, -0.8509,  0.3357,  0.0539,  0.1367, -0.5926]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.613107710620936, distance: 0.7117897512713524 entropy 0.03264415264129639
epoch: 68, step: 117
	action: tensor([[ 1.0250,  0.0808, -0.8630,  1.2537,  0.0615, -0.2464, -0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-3.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9311532286787532, distance: 0.300260692811108 entropy 0.03264415264129639
epoch: 68, step: 118
	action: tensor([[ 1.4642,  0.0757, -0.7677,  0.8903, -0.7306,  0.2435, -0.1439]],
       dtype=torch.float64)
	q_value: tensor([[-3.5028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.977703011204098, distance: 0.17087550106651664 entropy 0.03264415264129639
epoch: 68, step: 119
	action: tensor([[ 1.4398,  0.3917, -0.6585,  0.9341, -0.2526,  0.5552, -0.3517]],
       dtype=torch.float64)
	q_value: tensor([[-5.5847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9630729870504205, distance: 0.21990179916038632 entropy 0.03264415264129639
epoch: 68, step: 120
	action: tensor([[ 1.2416,  0.1518, -0.5516,  0.8364, -0.2973,  0.1464,  0.4308]],
       dtype=torch.float64)
	q_value: tensor([[-6.0879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07360173889321843 entropy 0.03264415264129639
epoch: 68, step: 121
	action: tensor([[ 0.7624,  0.0869, -0.1639,  0.9412, -0.2701, -0.1077, -0.5297]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9824271359781253, distance: 0.15169734069140164 entropy 0.03264415264129639
epoch: 68, step: 122
	action: tensor([[ 1.2305,  0.2175, -0.5198,  0.8469, -0.5069,  0.3717, -0.0724]],
       dtype=torch.float64)
	q_value: tensor([[-3.2031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08498097954962507 entropy 0.03264415264129639
epoch: 68, step: 123
	action: tensor([[ 0.8660, -0.0902, -0.6343,  0.1885, -0.0538,  0.6685,  0.0641]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.863186067108897, distance: 0.423274257917465 entropy 0.03264415264129639
epoch: 68, step: 124
	action: tensor([[ 0.9382,  0.3351, -0.4043,  0.7174, -0.4535,  0.1964, -0.1630]],
       dtype=torch.float64)
	q_value: tensor([[-2.7418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9716250992272382, distance: 0.192763085367778 entropy 0.03264415264129639
epoch: 68, step: 125
	action: tensor([[ 0.9261,  0.7093,  0.0013,  1.0392, -0.3674,  0.2528, -0.3216]],
       dtype=torch.float64)
	q_value: tensor([[-3.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 68, step: 126
	action: tensor([[ 0.3939,  0.1963, -0.4419,  0.5334, -0.2491, -0.0042,  0.3352]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7812944613304156, distance: 0.535163621598238 entropy 0.03264415264129639
epoch: 68, step: 127
	action: tensor([[ 0.8528,  0.2028, -0.6743,  0.1284, -0.4722,  0.4573,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-1.3125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9294983619183509, distance: 0.3038479406036644 entropy 0.03264415264129639
LOSS epoch 68 actor 538.2166748983451 critic 3728.8117645534467 
epoch: 69, step: 0
	action: tensor([[ 0.9318,  0.4608,  0.2464,  0.8673, -0.3008,  0.1381, -0.5038]],
       dtype=torch.float64)
	q_value: tensor([[-2.6714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 1
	action: tensor([[ 0.7326, -0.1184, -0.3378,  0.8189, -0.1339,  0.3708,  0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388263084029782, distance: 0.28303427945510395 entropy 0.03264415264129639
epoch: 69, step: 2
	action: tensor([[ 1.5081,  0.3089, -0.2874,  0.8102,  0.0312,  0.7178, -0.2038]],
       dtype=torch.float64)
	q_value: tensor([[-1.3619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8703154778584092, distance: 0.41209824624856867 entropy 0.03264415264129639
epoch: 69, step: 3
	action: tensor([[ 1.6731,  0.1486, -0.8268,  0.7519, -0.5499,  0.0477,  0.1648]],
       dtype=torch.float64)
	q_value: tensor([[-4.1578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 4
	action: tensor([[ 0.9316, -0.0213, -0.5680,  0.8600,  0.1278,  0.2634, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9599793504605301, distance: 0.22892791863231207 entropy 0.03264415264129639
epoch: 69, step: 5
	action: tensor([[ 1.5326,  0.4237, -0.2914,  1.2427, -0.3555,  0.1277,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-1.9592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8347263798200318, distance: 0.46522009248788865 entropy 0.03264415264129639
epoch: 69, step: 6
	action: tensor([[ 1.8188,  0.4833, -0.4773,  0.9062, -0.2566,  0.4390,  0.1096]],
       dtype=torch.float64)
	q_value: tensor([[-4.0875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 7
	action: tensor([[ 0.7335, -0.2984, -0.3954,  0.4949,  0.0148,  0.2453, -0.2255]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7657301758848125, distance: 0.5538789612019082 entropy 0.03264415264129639
epoch: 69, step: 8
	action: tensor([[ 1.4899,  0.1012, -0.6548,  1.3446, -0.3418, -0.0706,  0.1391]],
       dtype=torch.float64)
	q_value: tensor([[-1.3031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9870846781922005, distance: 0.13004968637404946 entropy 0.03264415264129639
epoch: 69, step: 9
	action: tensor([[ 1.7135,  0.1744, -0.7543,  1.0853, -0.0813,  0.5709,  0.1237]],
       dtype=torch.float64)
	q_value: tensor([[-3.6667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 10
	action: tensor([[ 0.5008,  0.0619, -0.4657,  0.5324, -0.3876,  0.3568, -0.1221]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.79759543568593, distance: 0.5148335541340813 entropy 0.03264415264129639
epoch: 69, step: 11
	action: tensor([[ 1.6707, -0.0851, -0.6155,  1.2703, -0.0605,  0.3818, -0.0570]],
       dtype=torch.float64)
	q_value: tensor([[-1.6071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9630980047253482, distance: 0.21982729592556396 entropy 0.03264415264129639
epoch: 69, step: 12
	action: tensor([[ 1.5103,  0.2513, -0.9286,  1.2886, -0.6041,  0.7160, -0.4477]],
       dtype=torch.float64)
	q_value: tensor([[-3.9362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9254664952677478, distance: 0.3124154225262052 entropy 0.03264415264129639
epoch: 69, step: 13
	action: tensor([[ 1.8417,  0.1042, -0.7822,  1.6577, -0.4532,  0.3445,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-5.8330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 14
	action: tensor([[ 1.0109,  0.2629, -0.4212,  0.3533, -0.1362,  0.2089, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9839513787241341, distance: 0.14496913831281483 entropy 0.03264415264129639
epoch: 69, step: 15
	action: tensor([[ 1.3544, -0.2970, -0.6814,  1.0743,  0.1416,  0.7281, -0.2209]],
       dtype=torch.float64)
	q_value: tensor([[-2.3704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9709444707690648, distance: 0.19506128838432668 entropy 0.03264415264129639
epoch: 69, step: 16
	action: tensor([[ 2.0921,  0.2640, -0.7466,  1.4845, -0.3816,  0.4082, -0.2576]],
       dtype=torch.float64)
	q_value: tensor([[-3.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 17
	action: tensor([[ 1.0814,  0.0959, -0.4122,  0.3792, -0.6446,  0.0141, -0.0410]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9179069178446455, distance: 0.32787626106735024 entropy 0.03264415264129639
epoch: 69, step: 18
	action: tensor([[ 1.3701,  0.0762, -0.6464,  0.4922, -0.5894,  0.1412, -0.1049]],
       dtype=torch.float64)
	q_value: tensor([[-2.6171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.88971922511697, distance: 0.380020126133766 entropy 0.03264415264129639
epoch: 69, step: 19
	action: tensor([[ 1.1903,  0.3412, -0.4378,  0.5202, -0.3606,  0.3635,  0.1033]],
       dtype=torch.float64)
	q_value: tensor([[-3.6643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.985665584622111, distance: 0.1370082394976111 entropy 0.03264415264129639
epoch: 69, step: 20
	action: tensor([[ 1.2547,  0.0215, -0.6682,  0.6375, -0.4775,  0.6714, -0.3027]],
       dtype=torch.float64)
	q_value: tensor([[-3.0678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.979340987602004, distance: 0.16447938422262073 entropy 0.03264415264129639
epoch: 69, step: 21
	action: tensor([[ 1.2372,  0.5325, -0.7345,  1.2235, -0.0368,  0.0931, -0.3369]],
       dtype=torch.float64)
	q_value: tensor([[-4.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8592875788257051, distance: 0.4292624610840896 entropy 0.03264415264129639
epoch: 69, step: 22
	action: tensor([[ 2.1313, -0.0351, -0.5067,  1.4866, -0.4959,  0.6578, -0.1970]],
       dtype=torch.float64)
	q_value: tensor([[-4.2449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 23
	action: tensor([[ 0.8007, -0.1807, -0.7957,  0.6329, -0.5487,  0.1314, -0.2229]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7822374294695098, distance: 0.5340086728584823 entropy 0.03264415264129639
epoch: 69, step: 24
	action: tensor([[ 1.2020,  0.2510, -0.5735,  1.2729, -0.2079,  0.7871, -0.2866]],
       dtype=torch.float64)
	q_value: tensor([[-2.3140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8381623724848389, distance: 0.4603587952680123 entropy 0.03264415264129639
epoch: 69, step: 25
	action: tensor([[ 1.9054,  0.4925, -1.1970,  1.2227, -0.3327,  0.7346, -0.1786]],
       dtype=torch.float64)
	q_value: tensor([[-4.1737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 26
	action: tensor([[ 0.8821,  0.1166, -0.6766,  0.7008, -0.3109, -0.0823,  0.2844]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9475754890517243, distance: 0.26201363190598426 entropy 0.03264415264129639
epoch: 69, step: 27
	action: tensor([[ 1.6442, -0.4182, -0.7856,  1.1171, -0.2092,  0.6521, -0.2422]],
       dtype=torch.float64)
	q_value: tensor([[-1.8766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9054660139842136, distance: 0.3518444285727892 entropy 0.03264415264129639
epoch: 69, step: 28
	action: tensor([[ 1.5827, -0.0266, -0.6443,  1.3432, -0.4918,  0.4718,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-4.1385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9716104430135507, distance: 0.1928128619691425 entropy 0.03264415264129639
epoch: 69, step: 29
	action: tensor([[ 1.4023,  0.6332, -1.0809,  1.0694, -0.2160,  0.3678, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-4.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.927304403779259, distance: 0.308539481017036 entropy 0.03264415264129639
epoch: 69, step: 30
	action: tensor([[ 1.2849,  0.4530, -0.7186,  1.6559, -0.6367,  0.5943, -0.1871]],
       dtype=torch.float64)
	q_value: tensor([[-4.8974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6958226742384628, distance: 0.6311318712325954 entropy 0.03264415264129639
epoch: 69, step: 31
	action: tensor([[ 1.9632,  0.6479, -0.8308,  1.7612, -0.2403,  0.5758, -0.1813]],
       dtype=torch.float64)
	q_value: tensor([[-5.0205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 32
	action: tensor([[ 0.8036, -0.0056, -0.0347,  0.3493, -0.2676,  0.2332,  0.1572]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9151283337256507, distance: 0.3333788603479436 entropy 0.03264415264129639
epoch: 69, step: 33
	action: tensor([[ 1.2245,  0.4285, -0.6031,  0.9816, -0.1254,  0.4904, -0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-1.2295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9126847739724286, distance: 0.3381439990761576 entropy 0.03264415264129639
epoch: 69, step: 34
	action: tensor([[ 1.9868,  0.3151, -0.4545,  0.8219, -0.5176,  0.2843,  0.2133]],
       dtype=torch.float64)
	q_value: tensor([[-3.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 35
	action: tensor([[ 0.4981, -0.0767, -0.9424,  0.7344,  0.1215,  0.0252,  0.1615]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6300041557635729, distance: 0.696073525889366 entropy 0.03264415264129639
epoch: 69, step: 36
	action: tensor([[ 1.6743, -0.2064, -0.8983,  0.8063, -0.0069,  0.4706,  0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-1.2259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.858265516002519, distance: 0.4308186115513865 entropy 0.03264415264129639
epoch: 69, step: 37
	action: tensor([[ 1.5385,  0.2277, -0.8254,  1.3124, -0.3291,  0.5096, -0.5096]],
       dtype=torch.float64)
	q_value: tensor([[-3.6508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9530047263113743, distance: 0.24807544830364694 entropy 0.03264415264129639
epoch: 69, step: 38
	action: tensor([[ 1.5469,  0.1377, -0.6687,  1.7362, -0.7120,  0.5617,  0.2771]],
       dtype=torch.float64)
	q_value: tensor([[-5.3986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8471047385061575, distance: 0.4474594724673835 entropy 0.03264415264129639
epoch: 69, step: 39
	action: tensor([[ 1.4941,  0.3783, -0.7147,  1.2069, -0.5398,  0.2690, -0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-4.4928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9492717371927042, distance: 0.2577399193786492 entropy 0.03264415264129639
epoch: 69, step: 40
	action: tensor([[ 1.5359, -0.2271, -0.7969,  1.5941, -0.4818,  0.4956, -0.0486]],
       dtype=torch.float64)
	q_value: tensor([[-4.6141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9346581995430661, distance: 0.292517761822429 entropy 0.03264415264129639
epoch: 69, step: 41
	action: tensor([[ 1.7111,  0.6904, -0.2635,  1.1906, -0.6033,  0.4941, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[-4.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 42
	action: tensor([[ 1.0033,  0.0754, -0.3229,  0.8522, -0.0811,  0.1831,  0.5017]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.060810946766952825 entropy 0.03264415264129639
epoch: 69, step: 43
	action: tensor([[ 0.7215,  0.0849, -0.5620,  0.8853, -0.0980,  0.5165,  0.0516]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8680943090944487, distance: 0.41561236490078973 entropy 0.03264415264129639
epoch: 69, step: 44
	action: tensor([[ 1.4571,  0.3736, -0.5198,  0.9351, -0.0129,  0.6612, -0.1562]],
       dtype=torch.float64)
	q_value: tensor([[-2.0146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9082967056001982, distance: 0.34653664157825237 entropy 0.03264415264129639
epoch: 69, step: 45
	action: tensor([[ 2.1010,  0.5879, -0.5603,  1.5438, -0.4183,  0.6212,  0.2704]],
       dtype=torch.float64)
	q_value: tensor([[-4.2992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 46
	action: tensor([[ 1.3194,  0.2840, -0.2908,  0.6605, -0.0202,  0.3457, -0.1906]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9503931329709063, distance: 0.2548752079201008 entropy 0.03264415264129639
epoch: 69, step: 47
	action: tensor([[ 1.6843,  0.1617, -0.7054,  0.9931, -0.5071,  0.6993, -0.1371]],
       dtype=torch.float64)
	q_value: tensor([[-3.3734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 48
	action: tensor([[ 1.1948, -0.4226, -0.4353,  0.7141,  0.2555,  0.5805,  0.1826]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8795896339391048, distance: 0.3970897086444873 entropy 0.03264415264129639
epoch: 69, step: 49
	action: tensor([[ 1.1331,  0.6713, -0.6243,  1.5172, -0.5048,  0.5764, -0.1962]],
       dtype=torch.float64)
	q_value: tensor([[-1.9492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5471650917462711, distance: 0.7700636837281761 entropy 0.03264415264129639
epoch: 69, step: 50
	action: tensor([[ 1.6510,  0.4721, -0.6744,  1.2335, -0.2681,  0.2757,  0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-4.7501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9075349082610555, distance: 0.34797303872732854 entropy 0.03264415264129639
epoch: 69, step: 51
	action: tensor([[ 1.4367e+00,  3.2510e-01, -7.4416e-01,  1.3654e+00,  7.1312e-03,
          4.3854e-01,  1.0339e-04]], dtype=torch.float64)
	q_value: tensor([[-4.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8886024211416224, distance: 0.3819394947118309 entropy 0.03264415264129639
epoch: 69, step: 52
	action: tensor([[ 2.0792,  0.4402, -0.7807,  1.0372, -0.4692,  0.3448,  0.0540]],
       dtype=torch.float64)
	q_value: tensor([[-4.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 53
	action: tensor([[ 1.0387, -0.1932, -0.7417,  1.0645, -0.4158,  0.0987, -0.4660]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9292829083432791, distance: 0.3043118673126867 entropy 0.03264415264129639
epoch: 69, step: 54
	action: tensor([[ 1.7303, -0.0693, -0.5239,  1.5117,  0.1090,  0.5289,  0.2947]],
       dtype=torch.float64)
	q_value: tensor([[-3.2771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9093063205049503, distance: 0.34462375025284614 entropy 0.03264415264129639
epoch: 69, step: 55
	action: tensor([[ 2.0128,  0.1594, -0.7016,  1.7460, -0.4870,  0.1650,  0.1126]],
       dtype=torch.float64)
	q_value: tensor([[-3.4392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 56
	action: tensor([[ 1.2766,  0.0480, -0.0756,  0.6190, -0.1778,  0.1952,  0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9102074089832412, distance: 0.34290746923279875 entropy 0.03264415264129639
epoch: 69, step: 57
	action: tensor([[ 1.3589,  0.2869, -0.4831,  1.1436, -0.2430,  0.3318, -0.0642]],
       dtype=torch.float64)
	q_value: tensor([[-2.2664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9318240755340822, distance: 0.2987942331157146 entropy 0.03264415264129639
epoch: 69, step: 58
	action: tensor([[ 1.7291,  0.1136, -0.7701,  0.9190, -0.7770,  0.7038, -0.3448]],
       dtype=torch.float64)
	q_value: tensor([[-3.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 59
	action: tensor([[ 0.8370,  0.3247, -0.5103,  0.3863, -0.0421, -0.1417, -0.0630]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9703077716832604, distance: 0.19718691339541355 entropy 0.03264415264129639
epoch: 69, step: 60
	action: tensor([[ 1.6313,  0.2229, -0.7234,  0.8858, -0.2288,  0.5463,  0.1415]],
       dtype=torch.float64)
	q_value: tensor([[-1.9283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9765769972942193, distance: 0.17513702877159426 entropy 0.03264415264129639
epoch: 69, step: 61
	action: tensor([[ 1.0124,  0.2642, -1.1361,  1.2750, -0.3110,  0.3480,  0.1969]],
       dtype=torch.float64)
	q_value: tensor([[-4.2567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7672698232846393, distance: 0.552055883758205 entropy 0.03264415264129639
epoch: 69, step: 62
	action: tensor([[ 1.3384,  0.1723, -0.1812,  0.9416, -0.3634,  0.4562,  0.3219]],
       dtype=torch.float64)
	q_value: tensor([[-3.4034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9448709713905734, distance: 0.2686871293395416 entropy 0.03264415264129639
epoch: 69, step: 63
	action: tensor([[ 1.1107,  0.2931, -0.5367,  1.3365, -0.4779,  0.5329, -0.0274]],
       dtype=torch.float64)
	q_value: tensor([[-2.9743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8140124209174747, distance: 0.49351307585775767 entropy 0.03264415264129639
epoch: 69, step: 64
	action: tensor([[ 2.1344,  0.3183, -0.7098,  1.1161, -0.6649,  0.5795, -0.4238]],
       dtype=torch.float64)
	q_value: tensor([[-3.6284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 65
	action: tensor([[ 0.5517, -0.3040, -0.7650,  0.6913, -0.1796,  0.0206, -0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6142695842608075, distance: 0.7107201621340523 entropy 0.03264415264129639
epoch: 69, step: 66
	action: tensor([[ 1.9801,  0.3001, -0.5776,  0.8444, -0.5775,  0.8374, -0.2054]],
       dtype=torch.float64)
	q_value: tensor([[-1.3183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 67
	action: tensor([[ 0.8613, -0.0657, -0.1312,  0.5629, -0.3471,  0.0948,  0.2502]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9337860201531694, distance: 0.2944635471935828 entropy 0.03264415264129639
epoch: 69, step: 68
	action: tensor([[ 1.2607, -0.2827, -1.0482,  0.6324, -0.4687,  0.1498, -0.0793]],
       dtype=torch.float64)
	q_value: tensor([[-1.3229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7444156408187431, distance: 0.578527262981671 entropy 0.03264415264129639
epoch: 69, step: 69
	action: tensor([[ 1.4645,  0.3172, -0.2792,  0.8136, -0.5440,  0.4199, -0.1915]],
       dtype=torch.float64)
	q_value: tensor([[-3.2017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9315450076664213, distance: 0.29940514315859207 entropy 0.03264415264129639
epoch: 69, step: 70
	action: tensor([[ 1.4723,  0.2627, -0.8365,  1.0740, -0.3335,  0.2442,  0.2602]],
       dtype=torch.float64)
	q_value: tensor([[-4.2968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09297415008082216 entropy 0.03264415264129639
epoch: 69, step: 71
	action: tensor([[ 0.7165,  0.3326,  0.2574,  1.0360, -0.1713,  0.1885,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9019471640384646, distance: 0.35833297304404627 entropy 0.03264415264129639
epoch: 69, step: 72
	action: tensor([[ 1.3548,  0.6091, -0.5958,  1.3525, -0.3053,  0.4787,  0.2692]],
       dtype=torch.float64)
	q_value: tensor([[-1.6998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 73
	action: tensor([[ 1.2798, -0.0740, -0.2261,  0.7316, -0.3945,  0.3629,  0.2093]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9492867797033749, distance: 0.25770170258654435 entropy 0.03264415264129639
epoch: 69, step: 74
	action: tensor([[ 1.3785,  0.0943, -1.1210,  1.1238, -0.2103,  0.3839,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-2.5990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.953150604253386, distance: 0.2476901237801577 entropy 0.03264415264129639
epoch: 69, step: 75
	action: tensor([[ 1.4407, -0.0440, -1.0775,  1.6611, -0.4717,  0.4198,  0.1017]],
       dtype=torch.float64)
	q_value: tensor([[-4.0170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8440896856274871, distance: 0.45184982341285523 entropy 0.03264415264129639
epoch: 69, step: 76
	action: tensor([[ 2.3233,  0.2014, -0.8891,  1.4255, -0.1876,  0.1682, -0.0602]],
       dtype=torch.float64)
	q_value: tensor([[-4.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 77
	action: tensor([[ 0.7446, -0.0518, -0.4935,  0.8543, -0.0886, -0.0710, -0.2756]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9087473275180381, distance: 0.345684167764291 entropy 0.03264415264129639
epoch: 69, step: 78
	action: tensor([[ 1.2037,  0.0488, -0.7789,  1.3461, -0.1026,  0.4701, -0.1849]],
       dtype=torch.float64)
	q_value: tensor([[-1.8368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9070099667288375, distance: 0.34895939462393544 entropy 0.03264415264129639
epoch: 69, step: 79
	action: tensor([[ 1.7697, -0.0171, -0.5004,  1.4704, -0.3787, -0.0171, -0.2213]],
       dtype=torch.float64)
	q_value: tensor([[-3.6642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 80
	action: tensor([[ 0.9045,  0.8413, -0.4079,  0.4061,  0.2422,  0.1149, -0.1763]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 81
	action: tensor([[ 0.8853,  0.1496, -0.2871,  0.3067, -0.2692,  0.4225,  0.4632]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9692210920651261, distance: 0.20076282426420167 entropy 0.03264415264129639
epoch: 69, step: 82
	action: tensor([[ 1.0984, -0.3130, -0.9913,  0.5651, -0.3674,  0.4056, -0.0737]],
       dtype=torch.float64)
	q_value: tensor([[-1.6721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7396308902603332, distance: 0.5839174073537953 entropy 0.03264415264129639
epoch: 69, step: 83
	action: tensor([[ 1.4099,  0.2357, -0.7608,  1.0755, -0.2865,  0.0078, -0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-2.8022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06308839013711508 entropy 0.03264415264129639
epoch: 69, step: 84
	action: tensor([[ 0.6032,  0.1724, -0.3941,  0.3800, -0.1902,  0.1299,  0.1349]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8884450549146021, distance: 0.3822091736678604 entropy 0.03264415264129639
epoch: 69, step: 85
	action: tensor([[ 1.3256,  0.1834, -0.5412,  1.2366,  0.1084,  0.4061, -0.0553]],
       dtype=torch.float64)
	q_value: tensor([[-1.2078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9357363307490377, distance: 0.2900944717474151 entropy 0.03264415264129639
epoch: 69, step: 86
	action: tensor([[ 1.6532e+00, -1.6360e-04, -1.0705e+00,  1.3066e+00, -4.4070e-01,
          3.3962e-01,  1.2390e-02]], dtype=torch.float64)
	q_value: tensor([[-3.4579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.959302180598014, distance: 0.2308565831024555 entropy 0.03264415264129639
epoch: 69, step: 87
	action: tensor([[ 0.8651,  0.4902, -0.5669,  1.4604, -0.3929,  0.6628, -0.2862]],
       dtype=torch.float64)
	q_value: tensor([[-4.6230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5491982745513306, distance: 0.7683329849626703 entropy 0.03264415264129639
epoch: 69, step: 88
	action: tensor([[ 1.9277,  0.0693, -0.9437,  0.9611, -0.4225,  0.3060, -0.4857]],
       dtype=torch.float64)
	q_value: tensor([[-3.9775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 89
	action: tensor([[ 0.9887,  0.0560, -0.0419,  0.6809,  0.3537,  0.7119,  0.1471]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9798872544409988, distance: 0.16229022898165457 entropy 0.03264415264129639
epoch: 69, step: 90
	action: tensor([[ 1.6680,  0.1424, -0.3392,  0.9726, -0.0420,  0.5824,  0.3836]],
       dtype=torch.float64)
	q_value: tensor([[-1.7997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8747001488568141, distance: 0.4050717635898783 entropy 0.03264415264129639
epoch: 69, step: 91
	action: tensor([[ 1.5197,  0.2385, -0.1100,  0.8706, -0.3765,  0.0145,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[-3.3966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8082575569453494, distance: 0.5010900989011546 entropy 0.03264415264129639
epoch: 69, step: 92
	action: tensor([[ 1.2536,  0.1193, -0.6958,  1.1854,  0.0891, -0.0077,  0.1622]],
       dtype=torch.float64)
	q_value: tensor([[-3.5192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09542409791221212 entropy 0.03264415264129639
epoch: 69, step: 93
	action: tensor([[ 1.2644,  0.0773, -0.1989,  0.7732,  0.4614,  0.3321, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9469187338642683, distance: 0.2636497294417369 entropy 0.03264415264129639
epoch: 69, step: 94
	action: tensor([[ 1.7095,  0.3805, -0.5454,  1.6979, -0.3392,  0.3878, -0.8344]],
       dtype=torch.float64)
	q_value: tensor([[-2.3488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 95
	action: tensor([[ 1.0357,  0.2847, -0.6477,  0.9188, -0.4125,  0.1793, -0.3152]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9590165547392491, distance: 0.2316652667397404 entropy 0.03264415264129639
epoch: 69, step: 96
	action: tensor([[ 1.5529,  0.3218, -0.7665,  1.3587, -0.7004,  0.6467, -0.0528]],
       dtype=torch.float64)
	q_value: tensor([[-3.5056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9226848218067409, distance: 0.31819186626062484 entropy 0.03264415264129639
epoch: 69, step: 97
	action: tensor([[ 1.8839, -0.1899, -0.3178,  1.3170,  0.0463,  0.5328,  0.1016]],
       dtype=torch.float64)
	q_value: tensor([[-5.2411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 98
	action: tensor([[ 0.7489, -0.0200, -0.1500,  0.7898, -0.3814, -0.0428, -0.2520]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9624030246389869, distance: 0.22188765846226755 entropy 0.03264415264129639
epoch: 69, step: 99
	action: tensor([[ 1.7021,  0.2050, -0.3749,  0.8761, -0.0129,  0.5077, -0.5792]],
       dtype=torch.float64)
	q_value: tensor([[-1.8483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8981492358892932, distance: 0.36520678630741876 entropy 0.03264415264129639
epoch: 69, step: 100
	action: tensor([[ 1.8302,  0.2896, -1.0934,  1.6330, -0.4062,  0.7440,  0.1147]],
       dtype=torch.float64)
	q_value: tensor([[-4.7573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 101
	action: tensor([[ 0.8465, -0.1186, -0.4794,  0.7908, -0.3653,  0.3092, -0.1117]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.92579914071046, distance: 0.31171748285500506 entropy 0.03264415264129639
epoch: 69, step: 102
	action: tensor([[ 1.3240, -0.0817, -0.4809,  1.0704, -0.1121,  0.4952, -0.6156]],
       dtype=torch.float64)
	q_value: tensor([[-2.1005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9918773551468993, distance: 0.10313484631253378 entropy 0.03264415264129639
epoch: 69, step: 103
	action: tensor([[ 1.7769, -0.4445, -1.1166,  1.5899, -0.4254,  0.4526,  0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-4.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 104
	action: tensor([[ 0.8994, -0.1169, -0.1450,  0.4053, -0.4405, -0.1557, -0.2790]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8161582333909563, distance: 0.4906578885527371 entropy 0.03264415264129639
epoch: 69, step: 105
	action: tensor([[ 1.2118,  0.2441, -1.0171,  0.4767, -0.0722, -0.0944, -0.1103]],
       dtype=torch.float64)
	q_value: tensor([[-1.8654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9438727908163173, distance: 0.2711086768495692 entropy 0.03264415264129639
epoch: 69, step: 106
	action: tensor([[ 0.9309,  0.4704, -0.8198,  0.1977, -0.0534,  0.4734, -0.4500]],
       dtype=torch.float64)
	q_value: tensor([[-3.3437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9860525437379217, distance: 0.13514631156424087 entropy 0.03264415264129639
epoch: 69, step: 107
	action: tensor([[ 0.9971,  0.4042, -0.0186,  0.8540, -0.3462,  0.1155,  0.5011]],
       dtype=torch.float64)
	q_value: tensor([[-3.5535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9191309715305642, distance: 0.3254226715703796 entropy 0.03264415264129639
epoch: 69, step: 108
	action: tensor([[ 1.4948, -0.0749, -0.7960,  0.6472, -0.0830,  0.3965,  0.1217]],
       dtype=torch.float64)
	q_value: tensor([[-1.8929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8842874465880017, distance: 0.3892664118220516 entropy 0.03264415264129639
epoch: 69, step: 109
	action: tensor([[ 1.0827,  0.3330, -1.0216,  1.1183,  0.1423,  0.0995,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-3.3760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081234783932544, distance: 0.3468637904159582 entropy 0.03264415264129639
epoch: 69, step: 110
	action: tensor([[ 1.4173,  0.2635, -0.3886,  0.8325, -0.6047,  0.3102,  0.2764]],
       dtype=torch.float64)
	q_value: tensor([[-3.2166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9542695193876795, distance: 0.2447144283678412 entropy 0.03264415264129639
epoch: 69, step: 111
	action: tensor([[ 1.0999, -0.2644, -0.6654,  1.3517, -0.5661,  0.2857, -0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-3.5101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9349118152278549, distance: 0.29194952511265654 entropy 0.03264415264129639
epoch: 69, step: 112
	action: tensor([[ 1.9621,  0.3581, -0.7538,  1.1263, -1.1096,  0.6737, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-3.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 113
	action: tensor([[ 0.5503,  0.0310, -0.1718,  1.1315, -0.0160,  0.1739, -0.1559]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8891847238467423, distance: 0.38093994065478437 entropy 0.03264415264129639
epoch: 69, step: 114
	action: tensor([[ 1.5667, -0.0146, -1.1692,  1.0601, -0.5293,  0.5499, -0.1716]],
       dtype=torch.float64)
	q_value: tensor([[-1.5234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9125684257084316, distance: 0.3383692139198134 entropy 0.03264415264129639
epoch: 69, step: 115
	action: tensor([[ 1.2386,  0.4176, -0.6420,  1.0100, -0.6638,  0.5708, -0.4968]],
       dtype=torch.float64)
	q_value: tensor([[-5.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9333334828084425, distance: 0.2954680846323548 entropy 0.03264415264129639
epoch: 69, step: 116
	action: tensor([[ 1.7281, -0.2314, -0.5439,  0.8301, -0.6218,  0.3852, -0.0255]],
       dtype=torch.float64)
	q_value: tensor([[-5.0610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 117
	action: tensor([[ 0.5311,  0.0735, -0.5760,  0.7519, -0.5901,  0.3712, -0.1483]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7636560548154713, distance: 0.5563254573608096 entropy 0.03264415264129639
epoch: 69, step: 118
	action: tensor([[ 1.2915,  0.2545, -0.5706,  0.4008, -0.3664,  0.1259, -0.3252]],
       dtype=torch.float64)
	q_value: tensor([[-2.1376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9368207373698798, distance: 0.28763648289163624 entropy 0.03264415264129639
epoch: 69, step: 119
	action: tensor([[ 1.2721,  0.1464, -0.4376,  0.9178, -0.4891,  0.2328, -0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-3.7517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9889101637136891, distance: 0.12050893842687774 entropy 0.03264415264129639
epoch: 69, step: 120
	action: tensor([[ 1.7434,  0.1966, -0.4534,  1.1336,  0.1235, -0.0084, -0.0909]],
       dtype=torch.float64)
	q_value: tensor([[-3.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 121
	action: tensor([[ 1.0883,  0.6584, -0.2993,  0.9669, -0.7149,  0.0743, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 122
	action: tensor([[ 1.0578,  0.1483, -0.2893,  1.0142,  0.1001,  0.6059, -0.3914]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9421966743865602, distance: 0.2751269324729324 entropy 0.03264415264129639
epoch: 69, step: 123
	action: tensor([[ 1.8067,  0.0364, -0.3209,  1.1824, -0.3295,  0.7214, -0.1151]],
       dtype=torch.float64)
	q_value: tensor([[-3.1316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 69, step: 124
	action: tensor([[ 0.9332,  0.0436, -0.3938,  0.8405,  0.3457,  0.0061, -0.0630]],
       dtype=torch.float64)
	q_value: tensor([[-3.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9842987429119361, distance: 0.1433916643386533 entropy 0.03264415264129639
epoch: 69, step: 125
	action: tensor([[ 1.5988,  0.3643, -0.6487,  1.2022, -0.3828,  0.1565,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-1.6701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9506174680017055, distance: 0.2542982492206076 entropy 0.03264415264129639
epoch: 69, step: 126
	action: tensor([[ 1.6150,  0.4074, -1.0709,  1.1915, -0.2904,  0.3315, -0.4631]],
       dtype=torch.float64)
	q_value: tensor([[-4.4988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9811486914078176, distance: 0.1571185435536315 entropy 0.03264415264129639
epoch: 69, step: 127
	action: tensor([[ 1.7444,  0.1864, -0.4324,  1.6184, -0.2572,  0.5215,  0.0380]],
       dtype=torch.float64)
	q_value: tensor([[-5.7196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 69 actor 447.31108050227357 critic 2408.1004472971963 
epoch: 70, step: 0
	action: tensor([[ 0.4999,  0.1425, -0.2047,  0.7978,  0.0748,  0.1098, -0.5120]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8833516421159487, distance: 0.3908373032024922 entropy 0.03264415264129639
epoch: 70, step: 1
	action: tensor([[ 1.4813,  0.2944, -0.6378,  1.0364, -0.1684,  0.9460,  0.0682]],
       dtype=torch.float64)
	q_value: tensor([[-1.4091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9235954237847144, distance: 0.3163125176431371 entropy 0.03264415264129639
epoch: 70, step: 2
	action: tensor([[ 1.2744,  0.0318, -0.4555,  1.0953, -0.1593,  0.2829, -0.1410]],
       dtype=torch.float64)
	q_value: tensor([[-3.8740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.986851593437532, distance: 0.13121795209893797 entropy 0.03264415264129639
epoch: 70, step: 3
	action: tensor([[ 1.2499,  0.3730, -0.5537,  0.9062, -0.0682,  0.0154, -0.2941]],
       dtype=torch.float64)
	q_value: tensor([[-2.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9634845689530457, distance: 0.2186728720909413 entropy 0.03264415264129639
epoch: 70, step: 4
	action: tensor([[ 1.3501,  0.2127, -0.5241,  0.8250, -0.1006,  0.2523, -0.1942]],
       dtype=torch.float64)
	q_value: tensor([[-3.2058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9755029904501963, distance: 0.17910727606693988 entropy 0.03264415264129639
epoch: 70, step: 5
	action: tensor([[ 1.7779, -0.1397, -0.7876,  1.4412,  0.0057,  0.4264, -0.1807]],
       dtype=torch.float64)
	q_value: tensor([[-3.1850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 6
	action: tensor([[ 0.9280,  0.1748, -0.1793,  0.1609,  0.0180,  0.0668, -0.4423]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9131673732869312, distance: 0.3372082277120284 entropy 0.03264415264129639
epoch: 70, step: 7
	action: tensor([[ 1.5506,  0.1464, -0.4604,  0.8643, -0.0072,  0.2007,  0.0652]],
       dtype=torch.float64)
	q_value: tensor([[-1.8364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9210236832584371, distance: 0.3215919218068897 entropy 0.03264415264129639
epoch: 70, step: 8
	action: tensor([[ 1.2539, -0.1355, -0.6362,  0.7110, -0.4647,  0.0217,  0.3215]],
       dtype=torch.float64)
	q_value: tensor([[-3.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8951664878996324, distance: 0.37051582444253695 entropy 0.03264415264129639
epoch: 70, step: 9
	action: tensor([[ 1.1194,  0.3488, -0.5786,  1.1237,  0.1614,  0.6975,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-2.1416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8364541998144714, distance: 0.462781928232474 entropy 0.03264415264129639
epoch: 70, step: 10
	action: tensor([[ 1.9379,  0.2556, -0.6973,  1.0483, -0.4236,  0.2931, -0.2784]],
       dtype=torch.float64)
	q_value: tensor([[-2.8189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 11
	action: tensor([[ 0.0292, -0.2521, -0.4368,  0.6002,  0.0296, -0.0744, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28139867113536954, distance: 0.9700647001601094 entropy 0.03264415264129639
epoch: 70, step: 12
	action: tensor([[ 1.0276e+00,  2.0235e-01, -7.1081e-01,  7.5867e-01, -1.2870e-01,
          5.6713e-01, -4.4879e-04]], dtype=torch.float64)
	q_value: tensor([[-0.3836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9585939951737915, distance: 0.23285649574256095 entropy 0.03264415264129639
epoch: 70, step: 13
	action: tensor([[ 1.6324,  0.0683, -1.0632,  0.9032, -0.6489,  0.1383,  0.2446]],
       dtype=torch.float64)
	q_value: tensor([[-2.5685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.92215442721397, distance: 0.31928142458028197 entropy 0.03264415264129639
epoch: 70, step: 14
	action: tensor([[ 1.3187,  0.3262, -0.4717,  0.7875, -0.2035, -0.3971, -0.0785]],
       dtype=torch.float64)
	q_value: tensor([[-3.7848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9395769762730297, distance: 0.28129234956929405 entropy 0.03264415264129639
epoch: 70, step: 15
	action: tensor([[ 1.5229, -0.5131, -0.5697,  0.8015, -0.6556,  0.1071, -0.0831]],
       dtype=torch.float64)
	q_value: tensor([[-2.8090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6519349107198278, distance: 0.675129253179216 entropy 0.03264415264129639
epoch: 70, step: 16
	action: tensor([[ 1.9071,  0.1773, -0.4717,  0.9297, -0.5197,  0.4908,  0.2804]],
       dtype=torch.float64)
	q_value: tensor([[-2.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 17
	action: tensor([[ 1.2525,  0.0799, -0.4680,  0.2839, -0.2852,  0.4341,  0.1922]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9057084419722019, distance: 0.3513929946719562 entropy 0.03264415264129639
epoch: 70, step: 18
	action: tensor([[ 1.2144, -0.1046, -0.1642,  0.7183, -0.1871,  0.3145,  0.6875]],
       dtype=torch.float64)
	q_value: tensor([[-2.3905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9477344084012956, distance: 0.26161619720216817 entropy 0.03264415264129639
epoch: 70, step: 19
	action: tensor([[ 1.4046,  0.0057, -0.4668,  0.8236, -0.1527,  0.2982, -0.4215]],
       dtype=torch.float64)
	q_value: tensor([[-1.4672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9559160768710828, distance: 0.2402684858350617 entropy 0.03264415264129639
epoch: 70, step: 20
	action: tensor([[ 1.4898,  0.5798,  0.1568,  1.1941, -0.7818,  0.3719, -0.0231]],
       dtype=torch.float64)
	q_value: tensor([[-3.3666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 21
	action: tensor([[ 0.9926,  0.1605, -0.6284,  0.7939,  0.0278, -0.0966, -0.2890]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9810707195216597, distance: 0.15744314137763993 entropy 0.03264415264129639
epoch: 70, step: 22
	action: tensor([[ 1.7741, -0.0720, -0.6146,  1.4124, -0.2214,  0.2168, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-2.2820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 23
	action: tensor([[ 1.0375, -0.3574,  0.1163,  0.5732,  0.0802,  0.1746, -0.1414]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.778554295948714, distance: 0.5385057228923086 entropy 0.03264415264129639
epoch: 70, step: 24
	action: tensor([[ 1.4488,  0.2160, -0.5535,  0.9083,  0.2002,  0.2894,  0.0503]],
       dtype=torch.float64)
	q_value: tensor([[-1.2858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9619406298352371, distance: 0.2232479562398537 entropy 0.03264415264129639
epoch: 70, step: 25
	action: tensor([[ 1.4934,  0.6333, -0.9397,  0.8835, -0.6039,  0.3400, -0.2356]],
       dtype=torch.float64)
	q_value: tensor([[-2.9455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9683281481318422, distance: 0.203654223903989 entropy 0.03264415264129639
epoch: 70, step: 26
	action: tensor([[ 1.7128, -0.0772, -0.4975,  0.9514,  0.1447,  0.2322, -0.4142]],
       dtype=torch.float64)
	q_value: tensor([[-4.9039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 27
	action: tensor([[ 0.7833, -0.2370, -0.4636,  0.8238, -0.1480,  0.3688, -0.0301]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8858735616796353, distance: 0.3865892962249832 entropy 0.03264415264129639
epoch: 70, step: 28
	action: tensor([[ 1.4698,  0.0414, -0.3919,  0.7367, -0.3876,  0.5507, -0.2801]],
       dtype=torch.float64)
	q_value: tensor([[-1.4124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9465878494006974, distance: 0.26447018906754155 entropy 0.03264415264129639
epoch: 70, step: 29
	action: tensor([[ 1.4813,  0.1496, -1.0523,  1.1893,  0.0674,  0.4552,  0.0974]],
       dtype=torch.float64)
	q_value: tensor([[-3.6257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9772497898585569, distance: 0.1726034211438007 entropy 0.03264415264129639
epoch: 70, step: 30
	action: tensor([[ 1.2326,  0.5555, -0.4491,  1.5145,  0.0692,  0.4451,  0.2495]],
       dtype=torch.float64)
	q_value: tensor([[-3.5550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.610769755205925, distance: 0.7139371528418859 entropy 0.03264415264129639
epoch: 70, step: 31
	action: tensor([[ 1.4926, -0.5079, -0.0824,  1.5859, -0.2750,  0.2743, -0.2596]],
       dtype=torch.float64)
	q_value: tensor([[-3.0731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8543647858355315, distance: 0.43670673791410264 entropy 0.03264415264129639
epoch: 70, step: 32
	action: tensor([[ 1.8515, -0.2901, -1.0081,  1.2694, -0.3384,  0.6585,  0.4960]],
       dtype=torch.float64)
	q_value: tensor([[-2.9781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 33
	action: tensor([[ 0.8178,  0.2574, -0.3327,  0.4230, -0.3738,  0.5652,  0.1602]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9663535572784957, distance: 0.2099066854002063 entropy 0.03264415264129639
epoch: 70, step: 34
	action: tensor([[ 0.8393,  0.2416, -0.4665,  0.7919, -0.2834,  0.4197, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-1.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9295253283060153, distance: 0.3037898251808404 entropy 0.03264415264129639
epoch: 70, step: 35
	action: tensor([[ 1.2876,  0.1099, -0.5564,  1.1522, -0.4548,  0.6166,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-2.1158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9563848810527958, distance: 0.23898752073124063 entropy 0.03264415264129639
epoch: 70, step: 36
	action: tensor([[ 1.0659,  0.6367, -0.8043,  1.1616, -0.2062,  0.1165, -0.4409]],
       dtype=torch.float64)
	q_value: tensor([[-3.2548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7637680386325353, distance: 0.5561936434707135 entropy 0.03264415264129639
epoch: 70, step: 37
	action: tensor([[ 1.6198,  0.3163, -0.5977,  1.4656, -0.5222,  0.1892, -0.1940]],
       dtype=torch.float64)
	q_value: tensor([[-3.8400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8966213849078897, distance: 0.36793580139026827 entropy 0.03264415264129639
epoch: 70, step: 38
	action: tensor([[ 1.5058,  0.3458, -0.9004,  1.1188, -0.5322,  0.7384, -0.0489]],
       dtype=torch.float64)
	q_value: tensor([[-4.4136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9560809108781565, distance: 0.23981887172370722 entropy 0.03264415264129639
epoch: 70, step: 39
	action: tensor([[ 1.3753,  0.6350, -0.4039,  1.0039, -0.3572,  0.0081, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-4.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 40
	action: tensor([[ 1.0187, -0.2115, -0.7053,  0.6062, -0.3384,  0.3397,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8605680632393722, distance: 0.4273048510338124 entropy 0.03264415264129639
epoch: 70, step: 41
	action: tensor([[ 1.1013,  0.5524, -0.4873,  0.8100, -0.2587, -0.3390,  0.1326]],
       dtype=torch.float64)
	q_value: tensor([[-1.9288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9499380541006921, distance: 0.2560416142169892 entropy 0.03264415264129639
epoch: 70, step: 42
	action: tensor([[ 1.2963,  0.1639, -0.5507,  1.0362, -0.2389,  0.5970, -0.3138]],
       dtype=torch.float64)
	q_value: tensor([[-2.4873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9701443084841478, distance: 0.1977289502309673 entropy 0.03264415264129639
epoch: 70, step: 43
	action: tensor([[ 1.1938,  0.5517, -0.4868,  0.4430, -0.3723,  0.0564,  0.0904]],
       dtype=torch.float64)
	q_value: tensor([[-3.5975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9514087054216788, distance: 0.252252760829653 entropy 0.03264415264129639
epoch: 70, step: 44
	action: tensor([[ 1.6710, -0.3021, -0.3390,  0.8921, -0.3731,  0.3985,  0.4824]],
       dtype=torch.float64)
	q_value: tensor([[-2.8363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7932909484091982, distance: 0.5202791720567242 entropy 0.03264415264129639
epoch: 70, step: 45
	action: tensor([[ 1.2679,  0.2770, -0.5623,  1.2294, -0.3282,  0.2739,  0.3481]],
       dtype=torch.float64)
	q_value: tensor([[-2.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9219031869326593, distance: 0.31979623700536236 entropy 0.03264415264129639
epoch: 70, step: 46
	action: tensor([[ 1.3311,  0.0604, -0.8820,  1.3987, -0.6415,  0.4117, -0.3559]],
       dtype=torch.float64)
	q_value: tensor([[-2.7563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9130822253266981, distance: 0.3373735201741847 entropy 0.03264415264129639
epoch: 70, step: 47
	action: tensor([[ 1.2924e+00,  7.9656e-04, -6.8109e-01,  1.2490e+00, -4.0317e-01,
          3.6412e-01,  1.5583e-01]], dtype=torch.float64)
	q_value: tensor([[-4.2091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9757671708848027, distance: 0.1781388946327942 entropy 0.03264415264129639
epoch: 70, step: 48
	action: tensor([[ 1.8643,  0.0619, -0.7454,  1.0822, -0.4800,  0.2376, -0.1917]],
       dtype=torch.float64)
	q_value: tensor([[-2.8812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 49
	action: tensor([[ 0.6853, -0.0303, -0.3275,  0.5178,  0.0427,  0.9246, -0.5205]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9327230380486586, distance: 0.29681775715165076 entropy 0.03264415264129639
epoch: 70, step: 50
	action: tensor([[ 1.5468,  0.0745, -0.5118,  1.2035, -0.9258,  0.3105, -0.3076]],
       dtype=torch.float64)
	q_value: tensor([[-2.2490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9743796902906329, distance: 0.183167695300727 entropy 0.03264415264129639
epoch: 70, step: 51
	action: tensor([[ 1.4956,  0.2884, -0.8767,  0.6435, -0.4622,  0.2964, -0.1441]],
       dtype=torch.float64)
	q_value: tensor([[-4.3471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9666177783689082, distance: 0.20908087616238716 entropy 0.03264415264129639
epoch: 70, step: 52
	action: tensor([[ 0.9580,  0.2951, -0.4205,  0.9681, -0.0376,  0.4343, -0.1045]],
       dtype=torch.float64)
	q_value: tensor([[-4.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9065794969912582, distance: 0.34976616395907295 entropy 0.03264415264129639
epoch: 70, step: 53
	action: tensor([[ 1.2436,  0.3529, -0.8769,  0.6825, -0.1182,  0.0296,  0.1406]],
       dtype=torch.float64)
	q_value: tensor([[-2.3692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07782807670583025 entropy 0.03264415264129639
epoch: 70, step: 54
	action: tensor([[ 0.6703,  0.4128, -0.8533,  0.6561, -0.2142,  0.2108, -0.0269]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8258061078244069, distance: 0.47760971795222995 entropy 0.03264415264129639
epoch: 70, step: 55
	action: tensor([[ 1.0336,  0.2383, -0.5428,  1.1230, -0.1957,  0.5256, -0.5235]],
       dtype=torch.float64)
	q_value: tensor([[-2.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8803081219481932, distance: 0.3959032199715076 entropy 0.03264415264129639
epoch: 70, step: 56
	action: tensor([[ 1.4918,  0.1116, -0.2491,  0.8289, -0.4703,  0.1853, -0.1978]],
       dtype=torch.float64)
	q_value: tensor([[-3.3805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8858393817918102, distance: 0.38664718197776565 entropy 0.03264415264129639
epoch: 70, step: 57
	action: tensor([[ 1.2046,  0.2931, -0.6864,  1.0937, -0.0841,  0.2975, -0.0377]],
       dtype=torch.float64)
	q_value: tensor([[-3.4026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9402962274439259, distance: 0.2796131424309827 entropy 0.03264415264129639
epoch: 70, step: 58
	action: tensor([[ 1.6332,  0.2925, -0.9223,  0.8989, -0.6340,  0.4720, -0.2338]],
       dtype=torch.float64)
	q_value: tensor([[-3.0222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9906114673805773, distance: 0.1108805963465466 entropy 0.03264415264129639
epoch: 70, step: 59
	action: tensor([[ 1.2575, -0.0540, -0.7972,  0.9467, -0.2849,  0.7159, -0.3126]],
       dtype=torch.float64)
	q_value: tensor([[-4.8161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9601439233752082, distance: 0.22845673504053168 entropy 0.03264415264129639
epoch: 70, step: 60
	action: tensor([[ 1.5091,  0.2251, -0.7627,  0.8650, -0.2255,  0.2523,  0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-3.5620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9796243372862847, distance: 0.16334752733663854 entropy 0.03264415264129639
epoch: 70, step: 61
	action: tensor([[ 1.4447,  0.3526, -0.7553,  1.0151, -0.4009,  0.1323, -0.2582]],
       dtype=torch.float64)
	q_value: tensor([[-3.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9802319503439166, distance: 0.16089353916193194 entropy 0.03264415264129639
epoch: 70, step: 62
	action: tensor([[ 1.5208, -0.1956, -1.0508,  0.9151, -0.4130,  0.4437, -0.1307]],
       dtype=torch.float64)
	q_value: tensor([[-4.0468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8789304717103574, distance: 0.3981751189103744 entropy 0.03264415264129639
epoch: 70, step: 63
	action: tensor([[ 1.8636,  0.1197, -0.6874,  1.2579, -0.6677,  0.5036, -0.1542]],
       dtype=torch.float64)
	q_value: tensor([[-3.7523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 64
	action: tensor([[ 0.9686,  0.0560,  0.2631, -0.0123, -0.3066,  0.1733, -0.0874]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8085941451691223, distance: 0.5006500943138595 entropy 0.03264415264129639
epoch: 70, step: 65
	action: tensor([[ 1.0925,  0.5335, -0.3453,  0.8241,  0.1012,  0.2736,  0.2860]],
       dtype=torch.float64)
	q_value: tensor([[-1.4207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 66
	action: tensor([[ 1.2610,  0.2343, -0.4304,  0.8193, -0.4209,  0.1059, -0.2809]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9854082693061628, distance: 0.13823247886543083 entropy 0.03264415264129639
epoch: 70, step: 67
	action: tensor([[ 1.7913,  0.2547, -0.9008,  0.7206, -0.6395, -0.0740,  0.0937]],
       dtype=torch.float64)
	q_value: tensor([[-3.2000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 68
	action: tensor([[ 1.1784,  0.0509, -0.6278,  0.8287, -0.5306,  0.2243,  0.1205]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9900925950568777, distance: 0.11390339015371022 entropy 0.03264415264129639
epoch: 70, step: 69
	action: tensor([[ 0.9071,  0.3271, -0.1612,  1.0208,  0.0229,  0.1888, -0.2329]],
       dtype=torch.float64)
	q_value: tensor([[-2.5586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9056327492581451, distance: 0.3515340070504198 entropy 0.03264415264129639
epoch: 70, step: 70
	action: tensor([[ 1.1449, -0.1308, -0.6626,  1.0651, -0.2366,  0.1995,  0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-2.1327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9805655302745365, distance: 0.1595302482137972 entropy 0.03264415264129639
epoch: 70, step: 71
	action: tensor([[ 1.0651,  0.3253, -1.1034,  0.8331,  0.0475,  0.1244, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[-2.1779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9534537054514082, distance: 0.24688758412322054 entropy 0.03264415264129639
epoch: 70, step: 72
	action: tensor([[ 1.2048,  0.3052, -0.5911,  1.1264, -0.3860,  0.3626,  0.1967]],
       dtype=torch.float64)
	q_value: tensor([[-2.8862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9246054092537751, distance: 0.3142149086803027 entropy 0.03264415264129639
epoch: 70, step: 73
	action: tensor([[ 1.5181, -0.0948, -0.3892,  0.6227,  0.0039,  0.2365, -0.1261]],
       dtype=torch.float64)
	q_value: tensor([[-2.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7878263894392697, distance: 0.5271113608610661 entropy 0.03264415264129639
epoch: 70, step: 74
	action: tensor([[ 1.5871,  0.3967, -0.6672,  1.4406, -0.7643,  0.5808,  0.1762]],
       dtype=torch.float64)
	q_value: tensor([[-2.8367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.889525011675081, distance: 0.38035460216179 entropy 0.03264415264129639
epoch: 70, step: 75
	action: tensor([[ 1.3161,  0.3003, -0.4094,  1.2331, -0.7321, -0.0074,  0.3182]],
       dtype=torch.float64)
	q_value: tensor([[-4.4339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9190880615891001, distance: 0.3255089964354153 entropy 0.03264415264129639
epoch: 70, step: 76
	action: tensor([[ 1.3017,  0.2469, -0.4735,  1.0992, -0.0872,  0.6104,  0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-2.9389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9154371216071611, distance: 0.3327718429429607 entropy 0.03264415264129639
epoch: 70, step: 77
	action: tensor([[ 1.6302, -0.0042, -0.4274,  1.0046, -0.2173,  0.2200,  0.5152]],
       dtype=torch.float64)
	q_value: tensor([[-2.8347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8929383598443659, distance: 0.37443258781389904 entropy 0.03264415264129639
epoch: 70, step: 78
	action: tensor([[ 1.2846,  0.5237, -0.5074,  0.8058, -1.0287,  0.1085, -0.2688]],
       dtype=torch.float64)
	q_value: tensor([[-2.6601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9328626446658194, distance: 0.29650963353099163 entropy 0.03264415264129639
epoch: 70, step: 79
	action: tensor([[ 1.3032,  0.2277, -0.4880,  0.9932, -0.5778,  0.2259, -0.0405]],
       dtype=torch.float64)
	q_value: tensor([[-4.1146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9762355953609118, distance: 0.17640876620593487 entropy 0.03264415264129639
epoch: 70, step: 80
	action: tensor([[ 1.3167,  0.2640, -0.4209,  0.7521, -0.4335,  0.5389, -0.1406]],
       dtype=torch.float64)
	q_value: tensor([[-3.2378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9756850842926194, distance: 0.17844035453000137 entropy 0.03264415264129639
epoch: 70, step: 81
	action: tensor([[ 1.2309,  0.4026, -0.8508,  1.2277, -0.6973, -0.1329,  0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-3.4216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9297442369230364, distance: 0.30331764183046567 entropy 0.03264415264129639
epoch: 70, step: 82
	action: tensor([[ 1.3363,  0.3297, -0.6434,  0.9851, -0.3832,  0.5365,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-3.4865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9571449047188743, distance: 0.23689610804635255 entropy 0.03264415264129639
epoch: 70, step: 83
	action: tensor([[ 1.2152,  0.1892, -0.4109,  1.4360, -0.1832,  0.1547, -0.2193]],
       dtype=torch.float64)
	q_value: tensor([[-3.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8855308261629049, distance: 0.38716934829900895 entropy 0.03264415264129639
epoch: 70, step: 84
	action: tensor([[ 1.6600,  0.2104, -0.4501,  0.9946,  0.0209,  0.3261,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-3.0707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8973528295048318, distance: 0.36663184512170915 entropy 0.03264415264129639
epoch: 70, step: 85
	action: tensor([[ 0.9634,  0.6955, -0.3371,  1.3345, -0.0271,  0.7357, -0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-3.3345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4754406994354877, distance: 0.8288081042895569 entropy 0.03264415264129639
epoch: 70, step: 86
	action: tensor([[ 1.3654,  0.2042, -0.8100,  1.0673, -0.4094,  0.1973,  0.0916]],
       dtype=torch.float64)
	q_value: tensor([[-3.4205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08086132267631782 entropy 0.03264415264129639
epoch: 70, step: 87
	action: tensor([[ 0.9713, -0.0978, -1.2201,  0.3089,  0.1070,  0.4619, -0.2603]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7984499403702285, distance: 0.5137456513346307 entropy 0.03264415264129639
epoch: 70, step: 88
	action: tensor([[ 1.2102, -0.4499, -0.6524,  1.3075, -0.4371,  0.3281,  0.1260]],
       dtype=torch.float64)
	q_value: tensor([[-2.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9364218545133585, distance: 0.28854305202689257 entropy 0.03264415264129639
epoch: 70, step: 89
	action: tensor([[ 1.5533,  0.0577, -0.2610,  0.8870, -0.3158,  0.3845, -0.3348]],
       dtype=torch.float64)
	q_value: tensor([[-2.3202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8920047858319565, distance: 0.37606156434667715 entropy 0.03264415264129639
epoch: 70, step: 90
	action: tensor([[ 1.3612,  0.5251, -1.3087,  1.1840,  0.0760,  0.4555, -0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-3.6801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9056046421700052, distance: 0.35158635498693047 entropy 0.03264415264129639
epoch: 70, step: 91
	action: tensor([[ 1.5472,  0.5140, -0.9502,  0.9046, -0.2544,  0.3880,  0.1228]],
       dtype=torch.float64)
	q_value: tensor([[-4.2388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9818272214285465, distance: 0.15426498480369105 entropy 0.03264415264129639
epoch: 70, step: 92
	action: tensor([[ 1.6254,  0.2419, -0.5051,  1.3044, -0.4027, -0.2054,  0.0422]],
       dtype=torch.float64)
	q_value: tensor([[-4.1470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9109528673488649, distance: 0.34148109318829584 entropy 0.03264415264129639
epoch: 70, step: 93
	action: tensor([[ 1.4960,  0.1936, -0.7703,  1.0344, -0.0580, -0.1009, -0.1463]],
       dtype=torch.float64)
	q_value: tensor([[-3.5872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.977977894915778, distance: 0.16981893330793615 entropy 0.03264415264129639
epoch: 70, step: 94
	action: tensor([[ 1.6452,  0.1641, -0.7207,  1.4374,  0.0137,  0.5402,  0.2271]],
       dtype=torch.float64)
	q_value: tensor([[-3.4608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9187553579758729, distance: 0.3261775437562037 entropy 0.03264415264129639
epoch: 70, step: 95
	action: tensor([[ 1.6473, -0.2778, -0.8950,  1.0529, -0.1867,  0.3361,  0.4178]],
       dtype=torch.float64)
	q_value: tensor([[-3.5771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8923225658492911, distance: 0.37550786901476624 entropy 0.03264415264129639
epoch: 70, step: 96
	action: tensor([[ 0.8702,  0.3151, -0.5647,  1.0528, -0.4351,  0.3238, -0.0846]],
       dtype=torch.float64)
	q_value: tensor([[-2.8500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.849042933987731, distance: 0.4446142895179362 entropy 0.03264415264129639
epoch: 70, step: 97
	action: tensor([[ 1.4246,  0.5156, -0.8425,  0.7291,  0.0698,  0.6677, -0.1524]],
       dtype=torch.float64)
	q_value: tensor([[-2.5772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9538138627080935, distance: 0.24593056887063464 entropy 0.03264415264129639
epoch: 70, step: 98
	action: tensor([[ 1.9425,  0.2010, -0.5901,  0.8001, -0.3008,  0.4785, -0.1441]],
       dtype=torch.float64)
	q_value: tensor([[-4.0268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 99
	action: tensor([[ 1.0330,  0.6738, -0.7706,  0.3211, -0.2859,  0.8673, -0.2964]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9281577911916316, distance: 0.3067231328793481 entropy 0.03264415264129639
epoch: 70, step: 100
	action: tensor([[ 1.3901,  0.0148, -0.3526,  0.5782, -0.4618,  0.3357, -0.2296]],
       dtype=torch.float64)
	q_value: tensor([[-3.9468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.902374794930204, distance: 0.3575507331167916 entropy 0.03264415264129639
epoch: 70, step: 101
	action: tensor([[ 1.2299,  0.4371, -0.4741,  0.8543, -0.8109,  0.2603,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[-3.1856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9495246295673782, distance: 0.25709666949419013 entropy 0.03264415264129639
epoch: 70, step: 102
	action: tensor([[ 1.7366, -0.0074, -0.0934,  1.0173, -0.6829,  0.6865,  0.3907]],
       dtype=torch.float64)
	q_value: tensor([[-3.3690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 70, step: 103
	action: tensor([[ 1.1711, -0.1229, -0.4769,  0.6399, -0.1386, -0.0380,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8829361697262337, distance: 0.3915327170817251 entropy 0.03264415264129639
epoch: 70, step: 104
	action: tensor([[ 1.4077, -0.1549, -0.2315,  0.8787, -0.4608,  0.2537,  0.2143]],
       dtype=torch.float64)
	q_value: tensor([[-1.8283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9052370345406171, distance: 0.35227028819121864 entropy 0.03264415264129639
epoch: 70, step: 105
	action: tensor([[ 1.3873,  0.4261, -0.6469,  1.0234, -0.1558,  0.1700,  0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-2.4606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9499910818907219, distance: 0.255905973082378 entropy 0.03264415264129639
epoch: 70, step: 106
	action: tensor([[ 1.1398,  0.1752, -0.7127,  1.2148,  0.0229,  0.3004,  0.0780]],
       dtype=torch.float64)
	q_value: tensor([[-3.3847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9303609248576757, distance: 0.30198348354750587 entropy 0.03264415264129639
epoch: 70, step: 107
	action: tensor([[ 1.4491, -0.0577, -0.4977,  0.8535, -0.2666,  0.4686, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[-2.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9567130229876857, distance: 0.23808680224037507 entropy 0.03264415264129639
epoch: 70, step: 108
	action: tensor([[ 1.0383,  0.3203, -0.7705,  0.8746, -0.5040,  0.0833, -0.2676]],
       dtype=torch.float64)
	q_value: tensor([[-3.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.960083822994528, distance: 0.22862891913003106 entropy 0.03264415264129639
epoch: 70, step: 109
	action: tensor([[ 1.5701,  0.0692, -0.6026,  1.4098, -0.5189,  0.4009, -0.1695]],
       dtype=torch.float64)
	q_value: tensor([[-3.1747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.952356160371333, distance: 0.24978138554950494 entropy 0.03264415264129639
epoch: 70, step: 110
	action: tensor([[ 1.4069, -0.0149, -0.7776,  1.2389, -0.4142,  0.2760, -0.0717]],
       dtype=torch.float64)
	q_value: tensor([[-4.1025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9885603128182717, distance: 0.12239502565406661 entropy 0.03264415264129639
epoch: 70, step: 111
	action: tensor([[ 1.5078, -0.0986, -0.8358,  1.0763, -0.4904,  0.2591,  0.2000]],
       dtype=torch.float64)
	q_value: tensor([[-3.4338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9654742268259872, distance: 0.2126318901444657 entropy 0.03264415264129639
epoch: 70, step: 112
	action: tensor([[ 1.3453,  0.0686, -0.5616,  0.7476, -0.1960,  0.6249, -0.3608]],
       dtype=torch.float64)
	q_value: tensor([[-3.2083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.991674956699794, distance: 0.10441188678374679 entropy 0.03264415264129639
epoch: 70, step: 113
	action: tensor([[ 0.7321,  0.1732, -0.4647,  1.1451, -0.7927,  0.8161, -0.3836]],
       dtype=torch.float64)
	q_value: tensor([[-3.5464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7430479898423072, distance: 0.5800730690853042 entropy 0.03264415264129639
epoch: 70, step: 114
	action: tensor([[ 1.4373,  0.2269, -0.7474,  0.9576, -0.2331,  0.5352,  0.0613]],
       dtype=torch.float64)
	q_value: tensor([[-3.2874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9891958821564998, distance: 0.11894641292577898 entropy 0.03264415264129639
epoch: 70, step: 115
	action: tensor([[ 1.5467,  0.2696, -0.5233,  1.0005, -0.3880,  0.1672,  0.3279]],
       dtype=torch.float64)
	q_value: tensor([[-3.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9380919649795643, distance: 0.2847280167391237 entropy 0.03264415264129639
epoch: 70, step: 116
	action: tensor([[ 1.3433, -0.0297, -0.5312,  0.9226, -0.4638,  0.5461,  0.4462]],
       dtype=torch.float64)
	q_value: tensor([[-3.2074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09550009521231606 entropy 0.03264415264129639
epoch: 70, step: 117
	action: tensor([[ 1.0306,  0.1197, -0.3766,  0.5303, -0.7393,  0.3452,  0.1060]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9824085838507752, distance: 0.1517773949724495 entropy 0.03264415264129639
epoch: 70, step: 118
	action: tensor([[ 0.8083, -0.0542, -0.5333,  0.7011, -0.3142,  0.5672,  0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-2.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9192105091174555, distance: 0.3252625997732116 entropy 0.03264415264129639
epoch: 70, step: 119
	action: tensor([[ 1.5743,  0.2796, -0.9591,  0.6393,  0.1538, -0.4363, -0.2181]],
       dtype=torch.float64)
	q_value: tensor([[-1.7924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8127836425813937, distance: 0.4951406572720394 entropy 0.03264415264129639
epoch: 70, step: 120
	action: tensor([[ 1.3017,  0.1266, -0.2425,  0.4074, -0.4804,  0.3162, -0.0368]],
       dtype=torch.float64)
	q_value: tensor([[-3.5861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9063329398311768, distance: 0.35022741456494433 entropy 0.03264415264129639
epoch: 70, step: 121
	action: tensor([[ 0.9475,  0.4726, -0.3415,  1.0087, -0.7146,  0.3244, -0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-2.7401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8411212589893664, distance: 0.45613099890852055 entropy 0.03264415264129639
epoch: 70, step: 122
	action: tensor([[ 1.5512,  0.1267, -0.5688,  1.3075, -0.1769,  0.2164,  0.1809]],
       dtype=torch.float64)
	q_value: tensor([[-3.1297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9515367204431108, distance: 0.25192025847971616 entropy 0.03264415264129639
epoch: 70, step: 123
	action: tensor([[ 1.1879,  0.2493, -0.7652,  0.8621, -0.4377,  0.1291, -0.3131]],
       dtype=torch.float64)
	q_value: tensor([[-3.2505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08866281757643975 entropy 0.03264415264129639
epoch: 70, step: 124
	action: tensor([[ 0.6359,  0.2689,  0.0419,  0.7638, -0.6256,  0.2184,  0.0869]],
       dtype=torch.float64)
	q_value: tensor([[-3.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9493297725856262, distance: 0.2575924442111672 entropy 0.03264415264129639
epoch: 70, step: 125
	action: tensor([[ 1.4424, -0.1590, -0.9308,  0.2492, -0.0415,  0.4271,  0.0457]],
       dtype=torch.float64)
	q_value: tensor([[-1.5346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.679244953227985, distance: 0.6481021205248231 entropy 0.03264415264129639
epoch: 70, step: 126
	action: tensor([[ 1.3896,  0.1003, -0.5571,  1.2138, -0.1452,  0.5143,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-2.9119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9571360452114243, distance: 0.23692059375056473 entropy 0.03264415264129639
epoch: 70, step: 127
	action: tensor([[ 1.9314,  0.1523, -0.0383,  0.9131, -0.2653,  0.2428, -0.1481]],
       dtype=torch.float64)
	q_value: tensor([[-3.0383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 70 actor 299.6146338313188 critic 3119.5626875682547 
epoch: 71, step: 0
	action: tensor([[ 0.7282, -0.2240, -0.4033,  0.3918, -0.1728,  0.4587, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.816748627287693, distance: 0.4898693996940498 entropy 0.03264415264129639
epoch: 71, step: 1
	action: tensor([[ 0.7645, -0.2166, -0.5720,  0.7825, -0.4751, -0.5366, -0.3961]],
       dtype=torch.float64)
	q_value: tensor([[-1.1329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8409552371496439, distance: 0.45636925587841304 entropy 0.03264415264129639
epoch: 71, step: 2
	action: tensor([[ 0.7221, -0.1091, -0.2191,  0.9379, -0.3292,  0.2705, -0.4273]],
       dtype=torch.float64)
	q_value: tensor([[-1.7379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9528343645140925, distance: 0.24852468866166816 entropy 0.03264415264129639
epoch: 71, step: 3
	action: tensor([[ 1.1802,  0.4173, -0.7313,  0.6325, -0.0265, -0.2154, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[-1.7428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9859986708809341, distance: 0.13540706523771806 entropy 0.03264415264129639
epoch: 71, step: 4
	action: tensor([[ 1.2054,  0.1089, -0.7747,  0.6625, -0.1020,  0.1742, -0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-2.5377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9793153509899469, distance: 0.1645814071733688 entropy 0.03264415264129639
epoch: 71, step: 5
	action: tensor([[ 0.7210,  0.3582,  0.1923,  0.7519, -0.7921,  0.6309, -0.0218]],
       dtype=torch.float64)
	q_value: tensor([[-2.5357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.921756177202201, distance: 0.32009708830211314 entropy 0.03264415264129639
epoch: 71, step: 6
	action: tensor([[ 1.0702, -0.2097, -1.1658,  0.2139, -0.0788,  0.0436, -0.2391]],
       dtype=torch.float64)
	q_value: tensor([[-2.1088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6394847984757606, distance: 0.687097683333165 entropy 0.03264415264129639
epoch: 71, step: 7
	action: tensor([[ 0.7769, -0.0920, -0.3388,  0.6388, -0.5646,  0.1947,  0.3633]],
       dtype=torch.float64)
	q_value: tensor([[-2.2914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9191287003954085, distance: 0.32542724114228405 entropy 0.03264415264129639
epoch: 71, step: 8
	action: tensor([[ 1.0550, -0.2623, -0.4573,  0.6129, -0.1596, -0.1331, -0.1497]],
       dtype=torch.float64)
	q_value: tensor([[-1.1497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8046800716738381, distance: 0.5057431059565582 entropy 0.03264415264129639
epoch: 71, step: 9
	action: tensor([[ 1.0437,  0.0974, -0.0810,  0.2352, -0.0316,  0.3093,  0.0797]],
       dtype=torch.float64)
	q_value: tensor([[-1.5981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9170658033396797, distance: 0.3295516681955175 entropy 0.03264415264129639
epoch: 71, step: 10
	action: tensor([[ 0.9128,  0.2964, -0.1701,  0.8684,  0.1672,  0.2515, -0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-1.4308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.947907231944903, distance: 0.2611833036391625 entropy 0.03264415264129639
epoch: 71, step: 11
	action: tensor([[ 0.9516, -0.0697, -0.8104,  0.8326, -0.6405,  0.7208,  0.3902]],
       dtype=torch.float64)
	q_value: tensor([[-1.6742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8527209607786888, distance: 0.4391644372304836 entropy 0.03264415264129639
epoch: 71, step: 12
	action: tensor([[ 1.3484,  0.1970, -0.3831,  0.9247,  0.0027,  0.5338,  0.1065]],
       dtype=torch.float64)
	q_value: tensor([[-2.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9587252143868721, distance: 0.23248723175838584 entropy 0.03264415264129639
epoch: 71, step: 13
	action: tensor([[ 1.0551, -0.1793, -0.5424,  0.7681, -0.3787,  0.2163, -0.1127]],
       dtype=torch.float64)
	q_value: tensor([[-2.6266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9411206508779313, distance: 0.27767590341148285 entropy 0.03264415264129639
epoch: 71, step: 14
	action: tensor([[ 0.8701,  0.2391, -0.4784,  0.6664, -0.0446,  0.4619, -0.2167]],
       dtype=torch.float64)
	q_value: tensor([[-2.0086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.963613008438788, distance: 0.21828795300863807 entropy 0.03264415264129639
epoch: 71, step: 15
	action: tensor([[ 1.0041,  0.5292, -0.2347,  0.6489, -0.0817,  0.2046, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-2.0842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 71, step: 16
	action: tensor([[ 0.8250,  0.4186, -0.5686,  0.2638,  0.2302,  0.0729, -0.1781]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9771527067568355, distance: 0.17297130866150917 entropy 0.03264415264129639
epoch: 71, step: 17
	action: tensor([[ 1.2159,  0.3405, -0.1525,  0.7915, -0.2592,  0.0733, -0.0713]],
       dtype=torch.float64)
	q_value: tensor([[-1.7164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9428585657317066, distance: 0.2735471924937973 entropy 0.03264415264129639
epoch: 71, step: 18
	action: tensor([[ 0.8318, -0.2976, -0.3707,  0.7374,  0.0538, -0.0842, -0.1054]],
       dtype=torch.float64)
	q_value: tensor([[-2.4474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8430489392512335, distance: 0.45335542903797293 entropy 0.03264415264129639
epoch: 71, step: 19
	action: tensor([[ 1.0208, -0.1099, -0.5918,  0.6790,  0.3897,  0.0184, -0.3053]],
       dtype=torch.float64)
	q_value: tensor([[-0.9680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9221777043191689, distance: 0.31923368581561196 entropy 0.03264415264129639
epoch: 71, step: 20
	action: tensor([[ 0.9627,  0.0666, -0.2010,  0.7643, -0.0848, -0.0370, -0.5564]],
       dtype=torch.float64)
	q_value: tensor([[-1.6669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09526182852853753 entropy 0.03264415264129639
epoch: 71, step: 21
	action: tensor([[ 0.6600,  0.2363, -0.1884,  0.3591,  0.3161, -0.1424,  0.2868]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.928143281493412, distance: 0.3067541051690432 entropy 0.03264415264129639
epoch: 71, step: 22
	action: tensor([[ 0.1235,  0.4604, -0.0545,  0.6567,  0.1280,  0.0185, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[-0.6605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7337481514847295, distance: 0.5904770316277458 entropy 0.03264415264129639
epoch: 71, step: 23
	action: tensor([[ 1.0258,  0.2151,  0.3510,  0.6094, -0.5569, -0.0071,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[-0.6382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.943649379060815, distance: 0.27164770874760413 entropy 0.03264415264129639
epoch: 71, step: 24
	action: tensor([[ 1.1212, -0.3065, -0.4522,  0.4352, -0.6761, -0.2810,  0.1844]],
       dtype=torch.float64)
	q_value: tensor([[-1.7076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6546021640743265, distance: 0.6725374910185772 entropy 0.03264415264129639
epoch: 71, step: 25
	action: tensor([[ 1.0912,  0.1253, -0.5648,  1.0546,  0.0775, -0.1264, -0.4786]],
       dtype=torch.float64)
	q_value: tensor([[-1.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08820328506921922 entropy 0.03264415264129639
epoch: 71, step: 26
	action: tensor([[ 0.6770, -0.3205, -0.0517,  0.9982, -0.1969,  0.3485, -0.3026]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9482898647863264, distance: 0.2602223112997885 entropy 0.03264415264129639
epoch: 71, step: 27
	action: tensor([[ 0.7668,  0.2594, -0.5115,  0.5841, -0.2813,  0.6193,  0.1421]],
       dtype=torch.float64)
	q_value: tensor([[-1.3668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9186747486950635, distance: 0.32633931700036833 entropy 0.03264415264129639
epoch: 71, step: 28
	action: tensor([[ 0.8181,  0.0903, -0.4798,  0.9466, -0.1831,  0.0523,  0.2298]],
       dtype=torch.float64)
	q_value: tensor([[-1.8627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9545809131897617, distance: 0.2438798348884635 entropy 0.03264415264129639
epoch: 71, step: 29
	action: tensor([[ 0.7748, -0.0623, -0.5673,  1.0289, -0.3057,  0.5144,  0.0694]],
       dtype=torch.float64)
	q_value: tensor([[-1.3350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8702110954285684, distance: 0.4122640608085628 entropy 0.03264415264129639
epoch: 71, step: 30
	action: tensor([[ 0.4911,  0.4551, -0.3313,  0.7750, -0.3865,  0.0783, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[-1.7538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8069191060022026, distance: 0.502835977774089 entropy 0.03264415264129639
epoch: 71, step: 31
	action: tensor([[ 0.7147, -0.1900, -0.0754,  0.9098,  0.0034,  0.3624, -0.1184]],
       dtype=torch.float64)
	q_value: tensor([[-1.4476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.977578535907244, distance: 0.17135180251444165 entropy 0.03264415264129639
epoch: 71, step: 32
	action: tensor([[ 0.9622,  0.4609, -0.1573,  0.8003, -0.3486,  0.6008,  0.2028]],
       dtype=torch.float64)
	q_value: tensor([[-1.1140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8612161542141661, distance: 0.42631062053312246 entropy 0.03264415264129639
epoch: 71, step: 33
	action: tensor([[ 0.8029,  0.1507, -0.3417,  0.2281, -0.2630,  0.0824, -0.4074]],
       dtype=torch.float64)
	q_value: tensor([[-2.2069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9304037171276964, distance: 0.3018906869061238 entropy 0.03264415264129639
epoch: 71, step: 34
	action: tensor([[ 1.3645,  0.2696, -0.6006,  0.2908, -0.1925, -0.0648, -0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-1.7816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.843724719663158, distance: 0.45237837539427694 entropy 0.03264415264129639
epoch: 71, step: 35
	action: tensor([[ 0.9494, -0.2678, -0.6990,  0.6942, -0.3636,  0.1268, -0.1531]],
       dtype=torch.float64)
	q_value: tensor([[-2.8054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8357509168874557, distance: 0.4637758928725148 entropy 0.03264415264129639
epoch: 71, step: 36
	action: tensor([[ 0.3670, -0.0951, -0.2142,  0.7365, -0.3229, -0.2131, -0.1620]],
       dtype=torch.float64)
	q_value: tensor([[-1.7862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7815789002202618, distance: 0.5348155031357105 entropy 0.03264415264129639
epoch: 71, step: 37
	action: tensor([[ 1.0556,  0.3028, -0.5090,  0.4625, -0.1137,  0.4902, -0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-0.7018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07157577390279167 entropy 0.03264415264129639
epoch: 71, step: 38
	action: tensor([[ 0.6926, -0.0420, -0.1442,  0.5823, -0.0887,  0.1848,  0.2405]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9323210071158032, distance: 0.2977032919268012 entropy 0.03264415264129639
epoch: 71, step: 39
	action: tensor([[ 1.0143, -0.0586, -0.2237,  0.5374, -0.0890, -0.1142,  0.2308]],
       dtype=torch.float64)
	q_value: tensor([[-0.7370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8945025413948264, distance: 0.37168727432276827 entropy 0.03264415264129639
epoch: 71, step: 40
	action: tensor([[ 1.1719, -0.0567, -0.2992,  0.4011, -0.4372,  0.6004,  0.2535]],
       dtype=torch.float64)
	q_value: tensor([[-1.1219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9288069167506346, distance: 0.3053343002235645 entropy 0.03264415264129639
epoch: 71, step: 41
	action: tensor([[ 0.6888,  0.0172, -0.6076,  0.8366, -0.0466, -0.1842,  0.2513]],
       dtype=torch.float64)
	q_value: tensor([[-2.0573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8908340824805068, distance: 0.3780943851039827 entropy 0.03264415264129639
epoch: 71, step: 42
	action: tensor([[ 1.3695,  0.3614, -0.4403,  0.4065, -0.5661,  0.0317, -0.4316]],
       dtype=torch.float64)
	q_value: tensor([[-0.9829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8977768890345151, distance: 0.3658737402747406 entropy 0.03264415264129639
epoch: 71, step: 43
	action: tensor([[ 0.9869, -0.0147, -0.3686,  1.0844, -0.8771,  0.4105,  0.4656]],
       dtype=torch.float64)
	q_value: tensor([[-3.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9711105318286614, distance: 0.19450307278051987 entropy 0.03264415264129639
epoch: 71, step: 44
	action: tensor([[ 0.5864,  0.1449, -0.5843,  0.3422, -0.5241,  0.4615,  0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-2.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.836985722780889, distance: 0.4620292968404257 entropy 0.03264415264129639
epoch: 71, step: 45
	action: tensor([[ 0.6512, -0.1937, -0.1702,  0.7913, -0.3017,  0.2512, -0.2245]],
       dtype=torch.float64)
	q_value: tensor([[-1.6373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9199214533221326, distance: 0.32382828856490875 entropy 0.03264415264129639
epoch: 71, step: 46
	action: tensor([[ 1.4387,  0.3217, -0.5351,  0.6516,  0.0576,  0.2473,  0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-1.2359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9396348862077221, distance: 0.28115752077815537 entropy 0.03264415264129639
epoch: 71, step: 47
	action: tensor([[ 0.8707,  0.3166, -0.5112,  0.7278, -0.3237,  0.0250, -0.1186]],
       dtype=torch.float64)
	q_value: tensor([[-2.9051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9684031140300378, distance: 0.20341306076106555 entropy 0.03264415264129639
epoch: 71, step: 48
	action: tensor([[ 0.4966, -0.1328, -0.2761,  0.7205, -0.2912,  0.1658, -0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-2.0422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8398299949840098, distance: 0.45798081736661383 entropy 0.03264415264129639
epoch: 71, step: 49
	action: tensor([[ 8.6824e-01,  3.2093e-01, -5.7359e-01,  3.2763e-02, -3.9092e-01,
          3.3108e-01,  1.2937e-04]], dtype=torch.float64)
	q_value: tensor([[-0.9407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9609119280715122, distance: 0.22624491017218676 entropy 0.03264415264129639
epoch: 71, step: 50
	action: tensor([[ 0.3521,  0.4880, -0.3144,  0.5859,  0.2048,  0.1850, -0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-2.0732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7963290625937886, distance: 0.5164416076675885 entropy 0.03264415264129639
epoch: 71, step: 51
	action: tensor([[ 1.0116e+00, -8.8556e-02,  1.8525e-01,  3.3921e-01, -3.5101e-02,
          1.7637e-01, -3.4328e-04]], dtype=torch.float64)
	q_value: tensor([[-1.0091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8446251606682882, distance: 0.4510732153688217 entropy 0.03264415264129639
epoch: 71, step: 52
	action: tensor([[ 1.0720,  0.0265, -0.3855,  0.5909, -0.3736,  0.4123,  0.2396]],
       dtype=torch.float64)
	q_value: tensor([[-1.1416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9843435959397654, distance: 0.14318670781757467 entropy 0.03264415264129639
epoch: 71, step: 53
	action: tensor([[ 1.3016,  0.0098, -0.8256,  0.5527, -0.6584,  0.3546, -0.3966]],
       dtype=torch.float64)
	q_value: tensor([[-1.8618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9166924424443957, distance: 0.33029263839545686 entropy 0.03264415264129639
epoch: 71, step: 54
	action: tensor([[ 1.2843,  0.5049,  0.1064,  0.7221,  0.0873,  0.5183, -0.4932]],
       dtype=torch.float64)
	q_value: tensor([[-3.6296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 71, step: 55
	action: tensor([[ 0.9170,  0.1030, -0.3895,  0.2962,  0.0528,  0.2562,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9433235982315975, distance: 0.27243181790685816 entropy 0.03264415264129639
epoch: 71, step: 56
	action: tensor([[ 0.8884,  0.3124, -0.4388,  0.9820, -0.5340,  0.3050,  0.0695]],
       dtype=torch.float64)
	q_value: tensor([[-1.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8964691098546871, distance: 0.36820668346283303 entropy 0.03264415264129639
epoch: 71, step: 57
	action: tensor([[ 1.1897,  0.0581, -0.1303,  0.4944, -0.1255, -0.0668,  0.4895]],
       dtype=torch.float64)
	q_value: tensor([[-2.2187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8937118504941297, distance: 0.37307755014740385 entropy 0.03264415264129639
epoch: 71, step: 58
	action: tensor([[ 0.8129,  0.0811, -0.3694,  0.4291, -0.4212,  0.6431,  0.0717]],
       dtype=torch.float64)
	q_value: tensor([[-1.3258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9572729443827848, distance: 0.23654195193221553 entropy 0.03264415264129639
epoch: 71, step: 59
	action: tensor([[ 0.5230, -0.1168, -0.7905,  0.1954, -0.0762,  0.0123,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-1.8376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6471271199693133, distance: 0.6797760107383206 entropy 0.03264415264129639
epoch: 71, step: 60
	action: tensor([[ 0.5499,  0.0730, -0.2235,  0.6730, -0.2329, -0.2147,  0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-0.8789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9043910857010824, distance: 0.35383915280435774 entropy 0.03264415264129639
epoch: 71, step: 61
	action: tensor([[ 0.7715,  0.0858, -0.4800,  0.5276, -0.4289, -0.0298, -0.1684]],
       dtype=torch.float64)
	q_value: tensor([[-0.7684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9399738569720367, distance: 0.28036701164072436 entropy 0.03264415264129639
epoch: 71, step: 62
	action: tensor([[ 0.8994, -0.1122,  0.0285,  0.5494, -0.4461, -0.0164,  0.1071]],
       dtype=torch.float64)
	q_value: tensor([[-1.5857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8989219221973141, distance: 0.36381883634111184 entropy 0.03264415264129639
epoch: 71, step: 63
	action: tensor([[ 0.5523,  0.2574, -0.5319,  0.6951,  0.0850,  0.4314,  0.2143]],
       dtype=torch.float64)
	q_value: tensor([[-1.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.84324955828848, distance: 0.45306559088048404 entropy 0.03264415264129639
epoch: 71, step: 64
	action: tensor([[ 1.0106,  0.1483, -0.3455,  0.6890, -0.5676, -0.0050,  0.2216]],
       dtype=torch.float64)
	q_value: tensor([[-1.2220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08856894361536778 entropy 0.03264415264129639
epoch: 71, step: 65
	action: tensor([[ 0.7291, -0.2249, -0.0539,  0.4990, -0.0645,  0.3285,  0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8717346499031697, distance: 0.4098371933249261 entropy 0.03264415264129639
epoch: 71, step: 66
	action: tensor([[ 1.1662, -0.0808, -0.0939,  0.6374,  0.1229,  0.5673, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[-0.7613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9518876162374031, distance: 0.2510065940338714 entropy 0.03264415264129639
epoch: 71, step: 67
	action: tensor([[ 1.3789,  0.0360, -0.5020,  0.7889, -0.1556, -0.0092,  0.0398]],
       dtype=torch.float64)
	q_value: tensor([[-1.8657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9302355004589733, distance: 0.3022553069296455 entropy 0.03264415264129639
epoch: 71, step: 68
	action: tensor([[ 1.2379,  0.4221, -0.4318,  0.7632, -0.0552, -0.1816,  0.5943]],
       dtype=torch.float64)
	q_value: tensor([[-2.4556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9572165208424321, distance: 0.23669808404648268 entropy 0.03264415264129639
epoch: 71, step: 69
	action: tensor([[ 0.9736,  0.3012, -0.4161,  0.5770, -0.5613, -0.3287, -0.3062]],
       dtype=torch.float64)
	q_value: tensor([[-1.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9902162378796769, distance: 0.11319041069303212 entropy 0.03264415264129639
epoch: 71, step: 70
	action: tensor([[ 1.0892,  0.0791, -0.5658,  0.7764, -0.4298,  0.0737, -0.2712]],
       dtype=torch.float64)
	q_value: tensor([[-2.3692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09193243827518614 entropy 0.03264415264129639
epoch: 71, step: 71
	action: tensor([[ 0.4910,  0.4036, -0.0707,  0.4730,  0.2293, -0.1653,  0.3979]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9039040433366768, distance: 0.3547392558081238 entropy 0.03264415264129639
epoch: 71, step: 72
	action: tensor([[ 0.4548,  0.4001, -0.4862,  1.0375, -0.3369,  0.3495, -0.0512]],
       dtype=torch.float64)
	q_value: tensor([[-0.5964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6392717177117027, distance: 0.6873007062178973 entropy 0.03264415264129639
epoch: 71, step: 73
	action: tensor([[ 0.6958,  0.1093, -0.1534,  0.6999, -0.1770,  0.3964, -0.2548]],
       dtype=torch.float64)
	q_value: tensor([[-1.7685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9708406101995586, distance: 0.19540960598409787 entropy 0.03264415264129639
epoch: 71, step: 74
	action: tensor([[ 0.7837,  0.1635, -0.2174,  0.6694,  0.3441,  0.3387, -0.2911]],
       dtype=torch.float64)
	q_value: tensor([[-1.5043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9900068872781448, distance: 0.11439501152661999 entropy 0.03264415264129639
epoch: 71, step: 75
	action: tensor([[ 1.4225, -0.1916, -0.3319,  0.6540, -0.2526,  0.4923, -0.2575]],
       dtype=torch.float64)
	q_value: tensor([[-1.4333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8667383649398048, distance: 0.417743077694868 entropy 0.03264415264129639
epoch: 71, step: 76
	action: tensor([[ 0.4678, -0.1156, -1.0101,  0.8986, -0.5970,  0.5074, -0.0471]],
       dtype=torch.float64)
	q_value: tensor([[-2.8252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4378606143521212, distance: 0.8579830350850934 entropy 0.03264415264129639
epoch: 71, step: 77
	action: tensor([[ 0.6016,  0.4812, -0.1792,  0.3763, -0.0134,  0.3463, -0.0345]],
       dtype=torch.float64)
	q_value: tensor([[-1.9800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9173683405700767, distance: 0.32895003011921214 entropy 0.03264415264129639
epoch: 71, step: 78
	action: tensor([[ 1.0187,  0.1059,  0.2743,  0.4924, -0.5919, -0.0656, -0.0840]],
       dtype=torch.float64)
	q_value: tensor([[-1.2674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9180046420842352, distance: 0.3276810497369278 entropy 0.03264415264129639
epoch: 71, step: 79
	action: tensor([[ 1.1202, -0.3711,  0.0219,  0.3555, -0.1321, -0.0321, -0.3494]],
       dtype=torch.float64)
	q_value: tensor([[-1.7019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5742947421422362, distance: 0.7466399097869003 entropy 0.03264415264129639
epoch: 71, step: 80
	action: tensor([[ 0.6080,  0.5366, -0.9672,  0.9309,  0.1302,  0.2991,  0.3109]],
       dtype=torch.float64)
	q_value: tensor([[-1.5268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6439119808330744, distance: 0.6828658183738688 entropy 0.03264415264129639
epoch: 71, step: 81
	action: tensor([[ 0.8099,  0.3590,  0.0718,  0.7987, -0.0957,  0.3930,  0.3398]],
       dtype=torch.float64)
	q_value: tensor([[-1.9029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9199571309431611, distance: 0.3237561424622312 entropy 0.03264415264129639
epoch: 71, step: 82
	action: tensor([[ 0.9874, -0.0987, -0.7326,  0.8713, -0.5328,  0.2536, -0.1062]],
       dtype=torch.float64)
	q_value: tensor([[-1.2652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9248200968537781, distance: 0.3137672231468509 entropy 0.03264415264129639
epoch: 71, step: 83
	action: tensor([[ 1.4848,  0.2130, -0.3042,  0.5319, -0.3327,  0.1961, -0.2410]],
       dtype=torch.float64)
	q_value: tensor([[-2.2363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8556803194109001, distance: 0.43472986146595805 entropy 0.03264415264129639
epoch: 71, step: 84
	action: tensor([[ 1.1431, -0.1991, -0.3075,  0.6191,  0.0313,  0.1555, -0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-3.2441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8808578244019712, distance: 0.39499305203160756 entropy 0.03264415264129639
epoch: 71, step: 85
	action: tensor([[ 0.8806,  0.4347, -0.3873,  1.0826,  0.0345,  0.5463,  0.2737]],
       dtype=torch.float64)
	q_value: tensor([[-1.7020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7703083770857282, distance: 0.5484401884964765 entropy 0.03264415264129639
epoch: 71, step: 86
	action: tensor([[ 0.9624, -0.4547, -0.6163,  1.1211, -0.1332,  0.0519,  0.3159]],
       dtype=torch.float64)
	q_value: tensor([[-1.9864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8725230310114549, distance: 0.40857572276142834 entropy 0.03264415264129639
epoch: 71, step: 87
	action: tensor([[ 1.1656,  0.1213, -0.1238,  0.5500, -0.3123,  0.2063, -0.0570]],
       dtype=torch.float64)
	q_value: tensor([[-1.2107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9562114840818103, distance: 0.23946211082487545 entropy 0.03264415264129639
epoch: 71, step: 88
	action: tensor([[ 1.3220,  0.0977, -0.5147,  0.8183, -0.4869,  0.3987, -0.1384]],
       dtype=torch.float64)
	q_value: tensor([[-2.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09275211649534439 entropy 0.03264415264129639
epoch: 71, step: 89
	action: tensor([[ 0.4860,  0.5702, -0.4284,  0.3920, -0.4312, -0.0633, -0.3318]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8703761893924152, distance: 0.4120017734963661 entropy 0.03264415264129639
epoch: 71, step: 90
	action: tensor([[ 0.7833,  0.2275, -0.5828,  0.3437, -0.2531,  0.4201, -0.1811]],
       dtype=torch.float64)
	q_value: tensor([[-1.7995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.948541990182878, distance: 0.25958714740522076 entropy 0.03264415264129639
epoch: 71, step: 91
	action: tensor([[ 0.5842,  0.1837, -0.3822,  0.7429, -0.3558, -0.0299,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[-1.9993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9093442982050433, distance: 0.3445515876284294 entropy 0.03264415264129639
epoch: 71, step: 92
	action: tensor([[ 1.0798,  0.2635, -0.5385,  0.5949, -0.2477,  0.1836, -0.1938]],
       dtype=torch.float64)
	q_value: tensor([[-1.0836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06541242890138656 entropy 0.03264415264129639
epoch: 71, step: 93
	action: tensor([[ 0.6451,  0.4240, -0.4000,  0.0977,  0.0879,  0.0487, -0.4581]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9401479572240197, distance: 0.27996012715693286 entropy 0.03264415264129639
epoch: 71, step: 94
	action: tensor([[ 0.9203, -0.1658, -0.4001,  0.9911, -0.5036, -0.0861, -0.1125]],
       dtype=torch.float64)
	q_value: tensor([[-1.5760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9640761909869565, distance: 0.21689417205461148 entropy 0.03264415264129639
epoch: 71, step: 95
	action: tensor([[ 1.1760,  0.2531,  0.0149,  0.5332, -0.1990,  0.3175, -0.0678]],
       dtype=torch.float64)
	q_value: tensor([[-1.7877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9536419761151886, distance: 0.24638777218943542 entropy 0.03264415264129639
epoch: 71, step: 96
	action: tensor([[ 0.9317,  0.0232, -0.4867,  0.7742, -0.3403,  0.4983, -0.4229]],
       dtype=torch.float64)
	q_value: tensor([[-2.1573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9655490835026955, distance: 0.21240125731452467 entropy 0.03264415264129639
epoch: 71, step: 97
	action: tensor([[ 0.9912,  0.7401, -0.4473,  1.2928, -0.1062,  0.4771,  0.2737]],
       dtype=torch.float64)
	q_value: tensor([[-2.5286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5149377180997627, distance: 0.7969947232481348 entropy 0.03264415264129639
epoch: 71, step: 98
	action: tensor([[ 1.5267,  0.1170, -0.5218,  0.7225, -0.3513,  0.0278, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[-2.6820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8807599827028504, distance: 0.39515520644944413 entropy 0.03264415264129639
epoch: 71, step: 99
	action: tensor([[ 1.0376, -0.2184, -0.5865,  0.6313, -0.3519,  0.0416,  0.0669]],
       dtype=torch.float64)
	q_value: tensor([[-3.1000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8600363272791864, distance: 0.42811885802559535 entropy 0.03264415264129639
epoch: 71, step: 100
	action: tensor([[ 0.8528,  0.1349, -0.5133,  0.9491, -0.2111, -0.1910, -0.1336]],
       dtype=torch.float64)
	q_value: tensor([[-1.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9669285223824554, distance: 0.2081054687032261 entropy 0.03264415264129639
epoch: 71, step: 101
	action: tensor([[ 1.2986, -0.0325, -0.2782,  0.7581,  0.0521,  0.4068,  0.0953]],
       dtype=torch.float64)
	q_value: tensor([[-1.8036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9519915521694784, distance: 0.2507353259409255 entropy 0.03264415264129639
epoch: 71, step: 102
	action: tensor([[ 1.2009,  0.1432, -0.5402,  1.0326, -0.6006,  0.6671,  0.0912]],
       dtype=torch.float64)
	q_value: tensor([[-2.0742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9593999511869417, distance: 0.23057911668529435 entropy 0.03264415264129639
epoch: 71, step: 103
	action: tensor([[ 1.2029,  0.3033, -0.6062,  0.9587, -0.0512,  0.1341, -0.3351]],
       dtype=torch.float64)
	q_value: tensor([[-2.9878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9724246885441963, distance: 0.1900276976190074 entropy 0.03264415264129639
epoch: 71, step: 104
	action: tensor([[ 0.7653,  0.1160, -0.3273,  0.6106, -0.7894,  0.2363,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[-3.0194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9520448006621959, distance: 0.25059623602058245 entropy 0.03264415264129639
epoch: 71, step: 105
	action: tensor([[ 1.1722, -0.2732, -0.4414,  0.4323, -0.2889, -0.3599,  0.0505]],
       dtype=torch.float64)
	q_value: tensor([[-1.7987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6219876705771157, distance: 0.7035738279391173 entropy 0.03264415264129639
epoch: 71, step: 106
	action: tensor([[ 0.9871,  0.2441, -0.4937,  1.2048, -0.4878,  0.0956, -0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-1.5800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9062021892470308, distance: 0.35047177191743034 entropy 0.03264415264129639
epoch: 71, step: 107
	action: tensor([[ 1.2381,  0.1286, -0.2110,  0.9950, -0.4007,  0.3611, -0.0930]],
       dtype=torch.float64)
	q_value: tensor([[-2.5079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9664238896285757, distance: 0.20968718294486793 entropy 0.03264415264129639
epoch: 71, step: 108
	action: tensor([[ 1.0287, -0.2215, -0.4588,  0.7173, -0.0703,  0.2209, -0.2935]],
       dtype=torch.float64)
	q_value: tensor([[-2.6424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9264491566502935, distance: 0.3103491224287393 entropy 0.03264415264129639
epoch: 71, step: 109
	action: tensor([[ 0.8416, -0.0030, -0.7618,  0.7206, -0.1070,  0.0085, -0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-1.8626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9008179953529227, distance: 0.3603903339526736 entropy 0.03264415264129639
epoch: 71, step: 110
	action: tensor([[ 1.2194, -0.0927, -0.4787,  0.6742, -0.3141,  0.3312, -0.0789]],
       dtype=torch.float64)
	q_value: tensor([[-1.6170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9537834993943479, distance: 0.24601139442345 entropy 0.03264415264129639
epoch: 71, step: 111
	action: tensor([[ 0.9215, -0.0753, -0.1590,  0.5096, -0.2594, -0.0293, -0.5813]],
       dtype=torch.float64)
	q_value: tensor([[-2.3432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9090555425742664, distance: 0.34509988249777074 entropy 0.03264415264129639
epoch: 71, step: 112
	action: tensor([[ 0.9339,  0.5554, -0.6034,  0.6009, -0.1055,  0.3510,  0.4018]],
       dtype=torch.float64)
	q_value: tensor([[-1.9510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9117702110569454, distance: 0.33991029109122756 entropy 0.03264415264129639
epoch: 71, step: 113
	action: tensor([[ 0.5212,  0.6369, -0.3365,  0.6149, -0.2849,  0.2161,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-2.0187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 71, step: 114
	action: tensor([[ 1.0570,  0.7182, -0.2264,  0.7319,  0.0850,  0.1228,  0.1608]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 71, step: 115
	action: tensor([[ 0.6143,  0.2489, -0.3965,  0.7391,  0.1469,  0.0891,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[-3.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8905599965782748, distance: 0.3785687335895677 entropy 0.03264415264129639
epoch: 71, step: 116
	action: tensor([[ 1.2736,  0.0854,  0.1118,  0.6265, -0.1065, -0.2073, -0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-1.0083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8459747982753573, distance: 0.44910985637545325 entropy 0.03264415264129639
epoch: 71, step: 117
	action: tensor([[ 0.8713, -0.2973, -0.6150,  0.7158,  0.0301,  0.2332, -0.1773]],
       dtype=torch.float64)
	q_value: tensor([[-1.8665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8505852524431464, distance: 0.4423371607300234 entropy 0.03264415264129639
epoch: 71, step: 118
	action: tensor([[ 0.8882,  0.2867, -0.0836,  0.9512, -0.2874,  0.7936,  0.3759]],
       dtype=torch.float64)
	q_value: tensor([[-1.3739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8682996818191214, distance: 0.4152886916271825 entropy 0.03264415264129639
epoch: 71, step: 119
	action: tensor([[ 0.5936,  0.0688, -0.2537,  0.8075, -0.1028,  0.1721,  0.1249]],
       dtype=torch.float64)
	q_value: tensor([[-1.9095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9230899829991603, distance: 0.31735704741125703 entropy 0.03264415264129639
epoch: 71, step: 120
	action: tensor([[ 0.9615,  0.2323, -0.7355,  0.6032, -0.4885,  0.3840, -0.5228]],
       dtype=torch.float64)
	q_value: tensor([[-0.9239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9604187041590159, distance: 0.2276678453734459 entropy 0.03264415264129639
epoch: 71, step: 121
	action: tensor([[ 1.3409,  0.2760, -0.2872,  0.6545, -0.0051, -0.2139, -0.2583]],
       dtype=torch.float64)
	q_value: tensor([[-3.2266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8993139130892132, distance: 0.36311268813604375 entropy 0.03264415264129639
epoch: 71, step: 122
	action: tensor([[ 1.1288,  0.3718, -0.2229,  0.7626, -0.1090,  0.3378, -0.2765]],
       dtype=torch.float64)
	q_value: tensor([[-2.6174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9316459681702793, distance: 0.2991842736007625 entropy 0.03264415264129639
epoch: 71, step: 123
	action: tensor([[ 0.9596,  0.1400, -0.8285,  1.1817, -0.0247,  0.0739, -0.4956]],
       dtype=torch.float64)
	q_value: tensor([[-2.6592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8910094024752522, distance: 0.37779065412790697 entropy 0.03264415264129639
epoch: 71, step: 124
	action: tensor([[ 0.8537,  0.0359, -0.8506,  1.1178,  0.0513, -0.1857, -0.2183]],
       dtype=torch.float64)
	q_value: tensor([[-2.8386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8819031069778613, distance: 0.3932565177743545 entropy 0.03264415264129639
epoch: 71, step: 125
	action: tensor([[ 0.6814,  0.5410, -0.0823,  0.5757,  0.0675, -0.0352, -0.2601]],
       dtype=torch.float64)
	q_value: tensor([[-1.9760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9294555394443133, distance: 0.30394020473902716 entropy 0.03264415264129639
epoch: 71, step: 126
	action: tensor([[ 0.7139, -0.1857,  0.0282,  1.0289, -0.8133,  0.2117,  0.2364]],
       dtype=torch.float64)
	q_value: tensor([[-1.4968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9822303601571443, distance: 0.15254430762275697 entropy 0.03264415264129639
epoch: 71, step: 127
	action: tensor([[ 1.2210, -0.0096, -0.7193,  0.7065, -0.2359,  0.4670, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-1.2947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9793667026010153, distance: 0.16437698571198786 entropy 0.03264415264129639
LOSS epoch 71 actor 319.202665746807 critic 3497.7087040661127 
epoch: 72, step: 0
	action: tensor([[ 0.6715,  0.4686, -0.2768,  0.5085, -0.1414,  0.2004,  0.1590]],
       dtype=torch.float64)
	q_value: tensor([[-2.7401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9260833159793028, distance: 0.3111200007193237 entropy 0.03264415264129639
epoch: 72, step: 1
	action: tensor([[ 0.9234,  0.2962, -0.6431,  0.9643, -0.1260, -0.1329,  0.2434]],
       dtype=torch.float64)
	q_value: tensor([[-1.1566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9533397159356205, distance: 0.24718970685456268 entropy 0.03264415264129639
epoch: 72, step: 2
	action: tensor([[ 1.0650, -0.2600, -0.3209,  0.3703, -0.2438,  0.3306,  0.1603]],
       dtype=torch.float64)
	q_value: tensor([[-1.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.79406823973676, distance: 0.5193000437175804 entropy 0.03264415264129639
epoch: 72, step: 3
	action: tensor([[ 1.0326, -0.3845, -0.8727,  0.5945,  0.2642,  0.3705,  0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-1.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7600056904060648, distance: 0.5606052563765913 entropy 0.03264415264129639
epoch: 72, step: 4
	action: tensor([[ 0.5392, -0.1167,  0.0138,  0.2254, -0.4569,  0.0671,  0.5643]],
       dtype=torch.float64)
	q_value: tensor([[-1.3576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7258344235093744, distance: 0.5991880671259353 entropy 0.03264415264129639
epoch: 72, step: 5
	action: tensor([[ 0.8862,  0.5964, -0.3739,  0.6052,  0.1036,  0.0448,  0.1282]],
       dtype=torch.float64)
	q_value: tensor([[-0.3938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 72, step: 6
	action: tensor([[ 0.4889,  0.0505, -0.2992,  0.0501, -0.1850,  0.4543, -0.2344]],
       dtype=torch.float64)
	q_value: tensor([[-3.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7678695685791002, distance: 0.5513441013305697 entropy 0.03264415264129639
epoch: 72, step: 7
	action: tensor([[ 0.8598,  0.0346, -0.1624,  0.4306,  0.2681, -0.3906,  0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-0.9925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8719416050040589, distance: 0.40950642535639353 entropy 0.03264415264129639
epoch: 72, step: 8
	action: tensor([[ 0.9097, -0.0859, -0.3543,  0.3503,  0.1095,  0.5016, -0.2400]],
       dtype=torch.float64)
	q_value: tensor([[-0.7384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9175056460463361, distance: 0.3286766154296489 entropy 0.03264415264129639
epoch: 72, step: 9
	action: tensor([[ 0.8729,  0.4067, -0.5831,  0.5922, -0.6450, -0.2560, -0.0839]],
       dtype=torch.float64)
	q_value: tensor([[-1.4222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.984745956700629, distance: 0.14133482343180745 entropy 0.03264415264129639
epoch: 72, step: 10
	action: tensor([[ 0.8603, -0.3020, -0.5545,  0.3403,  0.1715,  0.5187, -0.4591]],
       dtype=torch.float64)
	q_value: tensor([[-1.9997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7988542724389732, distance: 0.5132300768754469 entropy 0.03264415264129639
epoch: 72, step: 11
	action: tensor([[ 1.1124,  0.1883, -0.1976,  0.8344,  0.0595,  0.5590, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[-1.4840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9594665148521442, distance: 0.23039002224371383 entropy 0.03264415264129639
epoch: 72, step: 12
	action: tensor([[ 0.5499,  0.2127, -0.0524,  0.9544, -0.5018,  0.0641, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[-1.9398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9214743836404441, distance: 0.3206729818792238 entropy 0.03264415264129639
epoch: 72, step: 13
	action: tensor([[ 0.7504,  0.2612, -0.2769,  1.1752,  0.3233,  0.1996, -0.5985]],
       dtype=torch.float64)
	q_value: tensor([[-1.1936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8572262469682234, distance: 0.43239521686141213 entropy 0.03264415264129639
epoch: 72, step: 14
	action: tensor([[ 1.4160,  0.4917, -0.6409,  0.4584, -0.4530, -0.0607, -0.0895]],
       dtype=torch.float64)
	q_value: tensor([[-1.9419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9026686633259768, distance: 0.3570121834088443 entropy 0.03264415264129639
epoch: 72, step: 15
	action: tensor([[ 0.8243, -0.0840, -0.3452,  0.8935, -0.1689,  0.2698,  0.3933]],
       dtype=torch.float64)
	q_value: tensor([[-3.1767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9715179485866313, distance: 0.1931267029479062 entropy 0.03264415264129639
epoch: 72, step: 16
	action: tensor([[ 1.1032,  0.1131, -0.4804,  0.6834, -0.1545,  0.0398, -0.2701]],
       dtype=torch.float64)
	q_value: tensor([[-0.9731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9887533434404654, distance: 0.12135799980404328 entropy 0.03264415264129639
epoch: 72, step: 17
	action: tensor([[ 0.8914,  0.0110, -0.2585,  0.1752, -0.2223,  0.5956,  0.0434]],
       dtype=torch.float64)
	q_value: tensor([[-2.1161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9285574175355717, distance: 0.3058688608638511 entropy 0.03264415264129639
epoch: 72, step: 18
	action: tensor([[ 0.8715,  0.0705, -0.3746,  0.4316, -0.0809,  0.2106, -0.3992]],
       dtype=torch.float64)
	q_value: tensor([[-1.4344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9618197479357038, distance: 0.22360220858988455 entropy 0.03264415264129639
epoch: 72, step: 19
	action: tensor([[ 0.8322, -0.0103,  0.1007,  1.0554, -0.2661, -0.0941, -0.1255]],
       dtype=torch.float64)
	q_value: tensor([[-1.6621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05653644360852985 entropy 0.03264415264129639
epoch: 72, step: 20
	action: tensor([[ 0.2715,  0.0225, -0.2810,  0.5558, -0.1523,  0.2490, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[-3.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6918926441839279, distance: 0.6351959592640054 entropy 0.03264415264129639
epoch: 72, step: 21
	action: tensor([[ 0.1361,  0.2483, -0.4743,  0.9026, -0.4730,  0.0453, -0.1876]],
       dtype=torch.float64)
	q_value: tensor([[-0.5610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5462517554445244, distance: 0.7708398748090737 entropy 0.03264415264129639
epoch: 72, step: 22
	action: tensor([[ 1.1082, -0.3443, -0.1218,  0.7333, -0.0789,  0.3668, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[-1.0724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8932539236935795, distance: 0.373880361151787 entropy 0.03264415264129639
epoch: 72, step: 23
	action: tensor([[ 0.6062, -0.0979, -0.1665,  0.3411,  0.2796,  0.0747, -0.1725]],
       dtype=torch.float64)
	q_value: tensor([[-1.4278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.808324030143662, distance: 0.501003232507935 entropy 0.03264415264129639
epoch: 72, step: 24
	action: tensor([[ 0.8842,  0.5554, -0.1569,  0.4171, -0.4098,  0.6368,  0.3201]],
       dtype=torch.float64)
	q_value: tensor([[-0.4840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9191602773147205, distance: 0.32536370195675385 entropy 0.03264415264129639
epoch: 72, step: 25
	action: tensor([[ 0.5785,  0.0234, -0.2699,  0.7011, -0.2227,  0.2121,  0.1627]],
       dtype=torch.float64)
	q_value: tensor([[-1.8382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9121159197963131, distance: 0.3392437061721992 entropy 0.03264415264129639
epoch: 72, step: 26
	action: tensor([[ 0.8372,  0.1293, -0.4718,  0.8261, -0.1748, -0.0238, -0.2905]],
       dtype=torch.float64)
	q_value: tensor([[-0.7701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9715625732471239, distance: 0.1929753516328222 entropy 0.03264415264129639
epoch: 72, step: 27
	action: tensor([[ 0.6908,  0.5059, -0.4662,  0.6801, -0.1623,  0.2948, -0.0544]],
       dtype=torch.float64)
	q_value: tensor([[-1.7071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8473857315674967, distance: 0.44704810972343145 entropy 0.03264415264129639
epoch: 72, step: 28
	action: tensor([[ 0.6950,  0.2615, -0.4976,  0.7380,  0.0419, -0.2865, -0.2280]],
       dtype=torch.float64)
	q_value: tensor([[-1.7085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9417276841177089, distance: 0.2762408059604967 entropy 0.03264415264129639
epoch: 72, step: 29
	action: tensor([[ 1.4599,  0.0260,  0.0819,  0.8212, -0.7354, -0.1069, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[-1.3171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7497439494563319, distance: 0.5724650619814143 entropy 0.03264415264129639
epoch: 72, step: 30
	action: tensor([[ 1.1930,  0.3264, -0.2866,  0.3398, -0.4140,  0.4739,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-2.4116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9732762684521111, distance: 0.18707047262862186 entropy 0.03264415264129639
epoch: 72, step: 31
	action: tensor([[ 0.5047, -0.4718, -0.6684,  0.6521, -0.2350,  0.4510,  0.2851]],
       dtype=torch.float64)
	q_value: tensor([[-2.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5290772438195475, distance: 0.7852926485300082 entropy 0.03264415264129639
epoch: 72, step: 32
	action: tensor([[ 0.9753,  0.0621, -0.2150,  0.6728, -0.3812, -0.4094,  0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-0.7552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9481555754420539, distance: 0.2605599861582931 entropy 0.03264415264129639
epoch: 72, step: 33
	action: tensor([[ 0.8735,  0.1995,  0.0105,  0.3423, -0.1868, -0.0756,  0.1542]],
       dtype=torch.float64)
	q_value: tensor([[-1.1332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9565289874779439, distance: 0.23859238100971367 entropy 0.03264415264129639
epoch: 72, step: 34
	action: tensor([[ 1.1288,  0.0455, -0.4795,  0.8533, -0.3250, -0.6916, -0.3695]],
       dtype=torch.float64)
	q_value: tensor([[-0.8929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9336542379949553, distance: 0.2947564290981449 entropy 0.03264415264129639
epoch: 72, step: 35
	action: tensor([[ 0.6858,  0.2748, -0.0598,  0.6709, -0.2089, -0.0345,  0.4064]],
       dtype=torch.float64)
	q_value: tensor([[-2.2239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9805364291006794, distance: 0.1596496438270531 entropy 0.03264415264129639
epoch: 72, step: 36
	action: tensor([[ 0.6006,  0.1505, -0.5462,  0.3878, -0.2754,  0.2799,  0.3870]],
       dtype=torch.float64)
	q_value: tensor([[-0.7099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8753438041514677, distance: 0.40403001342562245 entropy 0.03264415264129639
epoch: 72, step: 37
	action: tensor([[ 0.7289,  0.4716, -0.3954,  0.6946, -0.5335,  0.4325,  0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-0.9774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8529463154065333, distance: 0.43882832142621636 entropy 0.03264415264129639
epoch: 72, step: 38
	action: tensor([[ 0.3411,  0.0525, -0.1815,  1.0468, -0.1357,  0.0181,  0.3279]],
       dtype=torch.float64)
	q_value: tensor([[-1.8773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8244439053958991, distance: 0.479473543737667 entropy 0.03264415264129639
epoch: 72, step: 39
	action: tensor([[ 0.6377,  0.4192, -1.1217,  0.6658,  0.4147,  0.3401,  0.2167]],
       dtype=torch.float64)
	q_value: tensor([[-0.5274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7804220923070662, distance: 0.5362298852692569 entropy 0.03264415264129639
epoch: 72, step: 40
	action: tensor([[ 0.9850,  0.3109, -0.7929, -0.0247,  0.4237,  0.3770, -0.2180]],
       dtype=torch.float64)
	q_value: tensor([[-1.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.952582552135215, distance: 0.2491872291481211 entropy 0.03264415264129639
epoch: 72, step: 41
	action: tensor([[ 0.6603,  0.0727, -0.5421,  0.3997, -0.4622, -0.1545, -0.1954]],
       dtype=torch.float64)
	q_value: tensor([[-1.9723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.880158825068519, distance: 0.3961500566663805 entropy 0.03264415264129639
epoch: 72, step: 42
	action: tensor([[ 0.5259,  1.1455, -0.4867,  0.5893, -0.1368,  0.3336,  0.1363]],
       dtype=torch.float64)
	q_value: tensor([[-1.2170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 72, step: 43
	action: tensor([[ 0.7918, -0.2851, -0.1743,  0.1140, -0.2995,  0.1237,  0.0687]],
       dtype=torch.float64)
	q_value: tensor([[-3.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5900494089927226, distance: 0.7326936905223428 entropy 0.03264415264129639
epoch: 72, step: 44
	action: tensor([[ 0.1996,  0.0222, -0.3986,  0.1008,  0.1653, -0.4697, -0.3379]],
       dtype=torch.float64)
	q_value: tensor([[-0.7167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47269535542186936, distance: 0.8309741072881119 entropy 0.03264415264129639
epoch: 72, step: 45
	action: tensor([[ 0.4994, -0.0269, -0.5011,  0.2569,  0.3873,  0.2060,  0.3383]],
       dtype=torch.float64)
	q_value: tensor([[-0.5086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7585544159290559, distance: 0.5622977253234396 entropy 0.03264415264129639
epoch: 72, step: 46
	action: tensor([[ 0.8778,  0.2054, -0.2976,  0.4204, -0.1278, -0.1738, -0.0646]],
       dtype=torch.float64)
	q_value: tensor([[-0.5752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9590228313648578, distance: 0.2316475262616777 entropy 0.03264415264129639
epoch: 72, step: 47
	action: tensor([[ 0.5132,  0.0369, -0.3009,  1.0108, -0.1440,  0.0568,  0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-1.2241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.865424369759463, distance: 0.41979755464567503 entropy 0.03264415264129639
epoch: 72, step: 48
	action: tensor([[ 1.2273,  0.3208, -0.2790,  0.4593, -0.3741,  0.3622,  0.2400]],
       dtype=torch.float64)
	q_value: tensor([[-0.8368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.969379046717935, distance: 0.2002470130459896 entropy 0.03264415264129639
epoch: 72, step: 49
	action: tensor([[ 0.7867,  0.1575, -0.3183,  0.2064, -0.1540,  0.1101,  0.1636]],
       dtype=torch.float64)
	q_value: tensor([[-2.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9269992638525751, distance: 0.3091863504974596 entropy 0.03264415264129639
epoch: 72, step: 50
	action: tensor([[ 0.7689,  0.2561, -0.2393,  0.5792, -0.1463, -0.3252,  0.1065]],
       dtype=torch.float64)
	q_value: tensor([[-0.9506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9816957865219845, distance: 0.1548218417118598 entropy 0.03264415264129639
epoch: 72, step: 51
	action: tensor([[ 0.8316,  0.4486, -0.2765,  0.9675, -0.4109,  0.3503,  0.5083]],
       dtype=torch.float64)
	q_value: tensor([[-0.9700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 72, step: 52
	action: tensor([[ 0.4373,  0.0996, -0.3554,  0.1385,  0.2613,  0.0880, -0.1212]],
       dtype=torch.float64)
	q_value: tensor([[-3.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7499314127641403, distance: 0.5722506090267576 entropy 0.03264415264129639
epoch: 72, step: 53
	action: tensor([[ 0.6413,  0.3572, -0.2358,  0.9538, -0.0148,  0.4429, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[-0.4907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8167060257616781, distance: 0.4899263377974637 entropy 0.03264415264129639
epoch: 72, step: 54
	action: tensor([[ 0.6080,  0.4762, -0.1865,  0.6106, -0.2625,  0.0589, -0.1378]],
       dtype=torch.float64)
	q_value: tensor([[-1.4498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9075340840231554, distance: 0.3479745896473687 entropy 0.03264415264129639
epoch: 72, step: 55
	action: tensor([[ 0.6685,  0.2934, -0.1232,  0.4442, -0.0430,  0.3605,  0.4821]],
       dtype=torch.float64)
	q_value: tensor([[-1.3171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9672389883304413, distance: 0.20712634686347847 entropy 0.03264415264129639
epoch: 72, step: 56
	action: tensor([[ 0.5973, -0.1311, -0.3232,  0.6103, -0.2155,  0.1479, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[-0.7629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8535389750429467, distance: 0.4379431399204266 entropy 0.03264415264129639
epoch: 72, step: 57
	action: tensor([[ 0.6311, -0.3386,  0.0128,  0.4753, -0.0824,  0.4845,  0.2490]],
       dtype=torch.float64)
	q_value: tensor([[-0.7421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8195831236462852, distance: 0.4860660329567791 entropy 0.03264415264129639
epoch: 72, step: 58
	action: tensor([[ 0.7735,  0.0419, -0.1110,  0.6521, -0.3887,  0.1734, -0.3378]],
       dtype=torch.float64)
	q_value: tensor([[-0.5587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9803929486632497, distance: 0.16023701138528434 entropy 0.03264415264129639
epoch: 72, step: 59
	action: tensor([[ 1.0706,  0.1277,  0.0130,  0.6618, -0.3909,  0.2440,  0.4779]],
       dtype=torch.float64)
	q_value: tensor([[-1.4979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9760585330495768, distance: 0.1770647342077435 entropy 0.03264415264129639
epoch: 72, step: 60
	action: tensor([[ 0.5571,  0.3189,  0.0082,  0.4012, -0.4889, -0.1294,  0.1070]],
       dtype=torch.float64)
	q_value: tensor([[-1.2263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9554549017927634, distance: 0.2415219762535721 entropy 0.03264415264129639
epoch: 72, step: 61
	action: tensor([[ 0.4970,  0.4125, -0.0470,  0.2791, -0.2748,  0.3416, -0.4794]],
       dtype=torch.float64)
	q_value: tensor([[-0.8420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9203203010326427, distance: 0.32302083511236784 entropy 0.03264415264129639
epoch: 72, step: 62
	action: tensor([[ 8.2075e-01,  6.1092e-01,  7.4558e-04,  4.1103e-01, -2.3462e-01,
          1.9162e-01,  2.1452e-01]], dtype=torch.float64)
	q_value: tensor([[-1.4409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 72, step: 63
	action: tensor([[ 0.6378,  0.1465, -0.4549,  0.4154, -0.1316,  0.0947,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-3.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8924595814197863, distance: 0.37526888299639155 entropy 0.03264415264129639
epoch: 72, step: 64
	action: tensor([[ 0.6872, -0.3748, -0.4350,  0.5220,  0.1895,  0.2109,  0.3060]],
       dtype=torch.float64)
	q_value: tensor([[-0.9153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7101774237875488, distance: 0.6160596993106782 entropy 0.03264415264129639
epoch: 72, step: 65
	action: tensor([[ 0.5491,  0.1895, -0.4759,  0.7344,  0.0769,  0.2607, -0.1610]],
       dtype=torch.float64)
	q_value: tensor([[-0.5294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8587941876106984, distance: 0.43001438117899066 entropy 0.03264415264129639
epoch: 72, step: 66
	action: tensor([[ 0.5923,  0.1528, -0.5129,  0.6040, -0.4470,  0.3863,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-1.1142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8529585786872961, distance: 0.43881002339061637 entropy 0.03264415264129639
epoch: 72, step: 67
	action: tensor([[ 0.6552,  0.1399, -0.4918,  0.2431, -0.1031, -0.1214,  0.2219]],
       dtype=torch.float64)
	q_value: tensor([[-1.3705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8742513416784068, distance: 0.40579657135913744 entropy 0.03264415264129639
epoch: 72, step: 68
	action: tensor([[ 0.5158, -0.0652, -0.4499,  0.4984, -0.2136,  0.2448,  0.2082]],
       dtype=torch.float64)
	q_value: tensor([[-0.7798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7943518758971034, distance: 0.5189422965134063 entropy 0.03264415264129639
epoch: 72, step: 69
	action: tensor([[ 0.6619,  0.0154,  0.0711,  0.3471, -0.5226,  0.1746,  0.1936]],
       dtype=torch.float64)
	q_value: tensor([[-0.6777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9005262371816317, distance: 0.36092001477806973 entropy 0.03264415264129639
epoch: 72, step: 70
	action: tensor([[ 0.9304,  0.4657, -0.6185,  0.7972, -0.4175, -0.4594, -0.4492]],
       dtype=torch.float64)
	q_value: tensor([[-0.7577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9758244374491021, distance: 0.17792828289432266 entropy 0.03264415264129639
epoch: 72, step: 71
	action: tensor([[ 0.7089,  0.3800, -0.7348,  0.5299,  0.2826, -0.0574, -0.2263]],
       dtype=torch.float64)
	q_value: tensor([[-2.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9298359329608603, distance: 0.30311963595962726 entropy 0.03264415264129639
epoch: 72, step: 72
	action: tensor([[ 0.6033,  0.2311, -0.0629,  0.7016, -0.0742,  0.5003, -0.3594]],
       dtype=torch.float64)
	q_value: tensor([[-1.5662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9363565949202034, distance: 0.28869110111252083 entropy 0.03264415264129639
epoch: 72, step: 73
	action: tensor([[ 1.5653, -0.3160,  0.0840,  0.0694, -0.1470, -0.0637,  0.2360]],
       dtype=torch.float64)
	q_value: tensor([[-1.4045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1783343413686458, distance: 1.0372997215543553 entropy 0.03264415264129639
epoch: 72, step: 74
	action: tensor([[ 0.5711, -0.0075, -0.9298,  0.3466, -0.5216,  0.2749, -0.1180]],
       dtype=torch.float64)
	q_value: tensor([[-1.5961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7017977979406451, distance: 0.6249022902493826 entropy 0.03264415264129639
epoch: 72, step: 75
	action: tensor([[ 0.8791,  0.0738, -0.3847,  0.4260, -0.2297, -0.1676, -0.1320]],
       dtype=torch.float64)
	q_value: tensor([[-1.6567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9234353797608177, distance: 0.3166436329243733 entropy 0.03264415264129639
epoch: 72, step: 76
	action: tensor([[ 8.3278e-01, -4.1150e-01, -6.1889e-01,  3.9016e-01, -1.2588e-01,
          4.5113e-04, -9.2025e-02]], dtype=torch.float64)
	q_value: tensor([[-1.2887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5914906330221478, distance: 0.7314046260480082 entropy 0.03264415264129639
epoch: 72, step: 77
	action: tensor([[ 0.5005, -0.0022, -0.2681,  0.8309, -0.2447,  0.1372,  0.5054]],
       dtype=torch.float64)
	q_value: tensor([[-0.9503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8788196512977919, distance: 0.39835731107126776 entropy 0.03264415264129639
epoch: 72, step: 78
	action: tensor([[ 0.9992,  0.1269,  0.0552,  0.8066, -0.1596, -0.1197,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-0.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9823490168803023, distance: 0.15203414744215624 entropy 0.03264415264129639
epoch: 72, step: 79
	action: tensor([[ 0.7568,  0.3605, -0.6066,  0.7337, -0.4201,  0.1145,  0.0721]],
       dtype=torch.float64)
	q_value: tensor([[-1.2649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9068155600721056, distance: 0.3493239746061861 entropy 0.03264415264129639
epoch: 72, step: 80
	action: tensor([[ 0.8483,  0.2922, -0.5412,  0.4679, -0.1940,  0.2667,  0.4040]],
       dtype=torch.float64)
	q_value: tensor([[-1.6907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9766810213453152, distance: 0.17474769491815215 entropy 0.03264415264129639
epoch: 72, step: 81
	action: tensor([[ 0.8252, -0.1372, -0.4720,  0.2045,  0.2659,  0.5233, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-1.3550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8515664402587427, distance: 0.4408823822673222 entropy 0.03264415264129639
epoch: 72, step: 82
	action: tensor([[ 0.6994,  0.3402, -0.3340,  0.4615, -0.0240,  0.1300,  0.0928]],
       dtype=torch.float64)
	q_value: tensor([[-1.0826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9642338319533856, distance: 0.21641776100479482 entropy 0.03264415264129639
epoch: 72, step: 83
	action: tensor([[ 0.9396,  0.1549, -0.3929,  0.3408, -0.1616,  0.2286,  0.3769]],
       dtype=torch.float64)
	q_value: tensor([[-1.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9667727134018953, distance: 0.20859511419990515 entropy 0.03264415264129639
epoch: 72, step: 84
	action: tensor([[ 0.8564,  0.0244, -0.6799,  0.6646, -0.4767, -0.0993, -0.4701]],
       dtype=torch.float64)
	q_value: tensor([[-1.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9308004368303622, distance: 0.30102902345010435 entropy 0.03264415264129639
epoch: 72, step: 85
	action: tensor([[ 0.6056,  0.2911, -0.4956,  0.4856,  0.1707,  0.0467,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[-2.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9182339407910987, distance: 0.3272225515597079 entropy 0.03264415264129639
epoch: 72, step: 86
	action: tensor([[ 0.9070,  0.0234, -0.5902,  0.9631, -0.1363, -0.1757, -0.1173]],
       dtype=torch.float64)
	q_value: tensor([[-0.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9684241824628611, distance: 0.20334523273136013 entropy 0.03264415264129639
epoch: 72, step: 87
	action: tensor([[ 0.5930,  0.1910, -0.7951,  0.7646,  0.3513, -0.1404, -0.1329]],
       dtype=torch.float64)
	q_value: tensor([[-1.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8177004296068144, distance: 0.48859555973606167 entropy 0.03264415264129639
epoch: 72, step: 88
	action: tensor([[ 0.9453,  0.2095, -0.2856,  0.3326, -0.0368, -0.1387, -0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-1.1838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9472610594911695, distance: 0.262798204500382 entropy 0.03264415264129639
epoch: 72, step: 89
	action: tensor([[ 0.8547,  0.0603, -0.7396,  1.0092, -0.5460,  0.0632, -0.0998]],
       dtype=torch.float64)
	q_value: tensor([[-1.4705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9019262349848668, distance: 0.3583712134976664 entropy 0.03264415264129639
epoch: 72, step: 90
	action: tensor([[ 0.6892,  0.4523, -0.6486,  0.4796,  0.1559, -0.0269, -0.0930]],
       dtype=torch.float64)
	q_value: tensor([[-1.9752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9335170587358281, distance: 0.29506099707602135 entropy 0.03264415264129639
epoch: 72, step: 91
	action: tensor([[ 0.7063, -0.1341, -0.4192,  0.6800,  0.2179,  0.2350,  0.5885]],
       dtype=torch.float64)
	q_value: tensor([[-1.4673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.898328659592763, distance: 0.36488496428340134 entropy 0.03264415264129639
epoch: 72, step: 92
	action: tensor([[ 0.1959, -0.0472, -0.3041,  0.9429, -0.0355,  0.1385, -0.6286]],
       dtype=torch.float64)
	q_value: tensor([[-0.6339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6770351605927754, distance: 0.6503307886107075 entropy 0.03264415264129639
epoch: 72, step: 93
	action: tensor([[ 1.1472,  0.1357, -0.0185,  0.5220, -0.0347,  0.1991,  0.0770]],
       dtype=torch.float64)
	q_value: tensor([[-1.0676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.947008095384406, distance: 0.2634277107667468 entropy 0.03264415264129639
epoch: 72, step: 94
	action: tensor([[ 0.6095,  0.5876, -0.6111,  0.6335, -0.0312,  0.7350, -0.4067]],
       dtype=torch.float64)
	q_value: tensor([[-1.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.716146897294587, distance: 0.6096822005381042 entropy 0.03264415264129639
epoch: 72, step: 95
	action: tensor([[ 0.6732,  0.0564, -0.6654,  0.5300, -0.2768,  0.4065,  0.4773]],
       dtype=torch.float64)
	q_value: tensor([[-2.4272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8581378012669723, distance: 0.4310126698152323 entropy 0.03264415264129639
epoch: 72, step: 96
	action: tensor([[ 1.0449,  0.3429,  0.1686,  0.3530, -0.7214, -0.1041,  0.2253]],
       dtype=torch.float64)
	q_value: tensor([[-1.1349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9515282655743644, distance: 0.2519422324345339 entropy 0.03264415264129639
epoch: 72, step: 97
	action: tensor([[ 0.6694,  0.0790,  0.0124,  0.4726, -0.2377,  0.0999,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-1.5200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9517150710768506, distance: 0.25145628295573286 entropy 0.03264415264129639
epoch: 72, step: 98
	action: tensor([[ 1.1651,  0.3448, -0.1384,  0.4536,  0.1723,  0.1437,  0.1700]],
       dtype=torch.float64)
	q_value: tensor([[-0.6924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9522998924304799, distance: 0.24992883941866337 entropy 0.03264415264129639
epoch: 72, step: 99
	action: tensor([[ 0.8148,  0.0913, -0.7797,  0.4925, -0.1431,  0.3993,  0.0526]],
       dtype=torch.float64)
	q_value: tensor([[-1.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9126273528733706, distance: 0.3382551675960444 entropy 0.03264415264129639
epoch: 72, step: 100
	action: tensor([[ 0.8867,  0.2668, -0.4419,  0.4650, -0.1708,  0.1001, -0.0177]],
       dtype=torch.float64)
	q_value: tensor([[-1.5928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9871167645311865, distance: 0.1298880406440133 entropy 0.03264415264129639
epoch: 72, step: 101
	action: tensor([[ 0.7798,  0.1307, -0.0160,  0.7546, -0.1649,  0.6000,  0.1624]],
       dtype=torch.float64)
	q_value: tensor([[-1.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9783696118507186, distance: 0.1683018344278448 entropy 0.03264415264129639
epoch: 72, step: 102
	action: tensor([[ 1.0158,  0.1068, -0.3134,  0.4520, -0.0391,  0.0474,  0.3651]],
       dtype=torch.float64)
	q_value: tensor([[-1.2433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9561662072678684, distance: 0.2395858793519917 entropy 0.03264415264129639
epoch: 72, step: 103
	action: tensor([[ 0.7225,  0.1698, -0.0966,  0.2438, -0.0973,  0.1501,  0.1125]],
       dtype=torch.float64)
	q_value: tensor([[-1.0944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.939869059174939, distance: 0.2806116469836339 entropy 0.03264415264129639
epoch: 72, step: 104
	action: tensor([[ 1.1140,  0.3274, -0.5942,  0.2048,  0.0988, -0.0815, -0.3446]],
       dtype=torch.float64)
	q_value: tensor([[-0.7316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9316699580294154, distance: 0.2991317674189728 entropy 0.03264415264129639
epoch: 72, step: 105
	action: tensor([[ 0.6841,  0.0467,  0.0391,  0.4808, -0.4694,  0.0590, -0.1499]],
       dtype=torch.float64)
	q_value: tensor([[-2.2141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.948031476235085, distance: 0.2608716489931019 entropy 0.03264415264129639
epoch: 72, step: 106
	action: tensor([[ 1.0917,  0.2978, -0.4702,  0.6447, -0.4624, -0.0297, -0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-1.0377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9916156158956404, distance: 0.10478334932330939 entropy 0.03264415264129639
epoch: 72, step: 107
	action: tensor([[ 0.9885, -0.2964,  0.1205,  0.8019, -0.6315,  0.1530,  0.1662]],
       dtype=torch.float64)
	q_value: tensor([[-2.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9015725487951521, distance: 0.3590168340578617 entropy 0.03264415264129639
epoch: 72, step: 108
	action: tensor([[ 0.9857,  0.2762,  0.0142,  0.4662, -0.2126,  0.2261,  0.1178]],
       dtype=torch.float64)
	q_value: tensor([[-1.1871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9800710070831283, distance: 0.16154717567925253 entropy 0.03264415264129639
epoch: 72, step: 109
	action: tensor([[ 0.7575,  0.0166, -0.7560,  0.7012, -0.1520,  0.0541,  0.0428]],
       dtype=torch.float64)
	q_value: tensor([[-1.3574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8806705844979704, distance: 0.3953033091924948 entropy 0.03264415264129639
epoch: 72, step: 110
	action: tensor([[ 0.8426,  0.4438, -0.4441,  0.7114, -0.3817,  0.1577,  0.2397]],
       dtype=torch.float64)
	q_value: tensor([[-1.2628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9251370054180392, distance: 0.3131052075947353 entropy 0.03264415264129639
epoch: 72, step: 111
	action: tensor([[ 0.6811, -0.2874, -0.7510,  0.5983, -0.2988,  0.1305,  0.1843]],
       dtype=torch.float64)
	q_value: tensor([[-1.6292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6931055861803248, distance: 0.6339444220808241 entropy 0.03264415264129639
epoch: 72, step: 112
	action: tensor([[ 0.6723,  0.2623,  0.2108,  0.8204, -0.1685,  0.1164,  0.2074]],
       dtype=torch.float64)
	q_value: tensor([[-0.9298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9737954018723556, distance: 0.18524455234910783 entropy 0.03264415264129639
epoch: 72, step: 113
	action: tensor([[ 0.9404,  0.3461, -0.4014,  0.7004, -0.3896, -0.1255,  0.3170]],
       dtype=torch.float64)
	q_value: tensor([[-0.8191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9872991199309816, distance: 0.1289655162104117 entropy 0.03264415264129639
epoch: 72, step: 114
	action: tensor([[ 0.9508,  0.3228, -0.1628,  0.6847,  0.3982, -0.2335,  0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-1.4503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09656548999404117 entropy 0.03264415264129639
epoch: 72, step: 115
	action: tensor([[ 0.7417,  0.3641,  0.0256,  1.0127, -0.2846,  0.1343, -0.1621]],
       dtype=torch.float64)
	q_value: tensor([[-3.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8923206414891026, distance: 0.37551122444931384 entropy 0.03264415264129639
epoch: 72, step: 116
	action: tensor([[ 1.0097,  0.1067, -0.4880,  0.5953,  0.0274,  0.4770,  0.1199]],
       dtype=torch.float64)
	q_value: tensor([[-1.5983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06555209854769471 entropy 0.03264415264129639
epoch: 72, step: 117
	action: tensor([[ 0.4962, -0.0690, -0.4167,  0.2445,  0.1051, -0.0266,  0.5790]],
       dtype=torch.float64)
	q_value: tensor([[-3.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6858567065378671, distance: 0.6413876444842941 entropy 0.03264415264129639
epoch: 72, step: 118
	action: tensor([[ 0.4929,  0.2033, -0.7350,  0.5338, -0.7543, -0.3340, -0.2255]],
       dtype=torch.float64)
	q_value: tensor([[-0.4714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8399644445941711, distance: 0.45778855830524506 entropy 0.03264415264129639
epoch: 72, step: 119
	action: tensor([[ 0.5198,  0.2404, -0.2998,  0.6338, -0.4190,  0.2967,  0.5786]],
       dtype=torch.float64)
	q_value: tensor([[-1.5466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8589750384423662, distance: 0.42973892022663746 entropy 0.03264415264129639
epoch: 72, step: 120
	action: tensor([[ 0.6872, -0.0513, -0.2697,  0.2740, -0.2059, -0.2900,  0.3579]],
       dtype=torch.float64)
	q_value: tensor([[-0.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7526345996205643, distance: 0.5691492529062664 entropy 0.03264415264129639
epoch: 72, step: 121
	action: tensor([[ 0.8182,  0.7697, -0.1796,  0.4635, -0.3998,  0.1343, -0.2465]],
       dtype=torch.float64)
	q_value: tensor([[-0.5568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 72, step: 122
	action: tensor([[ 0.6544,  0.4381, -0.8081,  0.6672, -0.0741,  0.5636, -0.3307]],
       dtype=torch.float64)
	q_value: tensor([[-3.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7586391575740195, distance: 0.5621990401142507 entropy 0.03264415264129639
epoch: 72, step: 123
	action: tensor([[ 0.8215,  0.1484, -0.3459,  0.4632, -0.6290,  0.1916, -0.1158]],
       dtype=torch.float64)
	q_value: tensor([[-2.3141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9619848548311041, distance: 0.22311821145276786 entropy 0.03264415264129639
epoch: 72, step: 124
	action: tensor([[ 0.6996,  0.1780, -0.4103,  0.5070, -0.4485,  0.1568,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[-1.6710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9309718936834781, distance: 0.3006558599700888 entropy 0.03264415264129639
epoch: 72, step: 125
	action: tensor([[ 0.3895, -0.0373, -0.4902,  0.5178,  0.0086, -0.2569,  0.1398]],
       dtype=torch.float64)
	q_value: tensor([[-1.2617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6743179761037229, distance: 0.6530607569709209 entropy 0.03264415264129639
epoch: 72, step: 126
	action: tensor([[ 0.8871,  0.4306, -0.1231,  0.4611, -0.1017,  0.1223, -0.2285]],
       dtype=torch.float64)
	q_value: tensor([[-0.4662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9820210438069402, distance: 0.15344012032221627 entropy 0.03264415264129639
epoch: 72, step: 127
	action: tensor([[ 0.8285, -0.0252, -0.2305,  0.1284, -0.5093,  0.0967,  0.1656]],
       dtype=torch.float64)
	q_value: tensor([[-1.6730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8080946897960333, distance: 0.5013028681549919 entropy 0.03264415264129639
LOSS epoch 72 actor 170.62183602105208 critic 2175.8598397701144 
epoch: 73, step: 0
	action: tensor([[ 1.0739,  0.3442,  0.0543,  0.6476, -0.3785, -0.6031, -0.2418]],
       dtype=torch.float64)
	q_value: tensor([[-0.9232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.950583097079087, distance: 0.25438673136956114 entropy 0.03264415264129639
epoch: 73, step: 1
	action: tensor([[ 0.9604,  0.3467, -0.4282,  0.5161, -0.0950,  0.5374,  0.3430]],
       dtype=torch.float64)
	q_value: tensor([[-1.6964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9759024279095503, distance: 0.17764105238317607 entropy 0.03264415264129639
epoch: 73, step: 2
	action: tensor([[ 1.1236, -0.0462, -0.4284,  0.7983, -0.2326,  0.5469,  0.2763]],
       dtype=torch.float64)
	q_value: tensor([[-1.4640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0632735358757411 entropy 0.03264415264129639
epoch: 73, step: 3
	action: tensor([[ 0.4358, -0.1845, -0.1988,  0.6685, -0.0784,  0.1815, -0.3358]],
       dtype=torch.float64)
	q_value: tensor([[-3.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7689931625129598, distance: 0.5500081318543647 entropy 0.03264415264129639
epoch: 73, step: 4
	action: tensor([[ 0.9357,  0.2354, -0.1222,  0.5494,  0.1443,  0.3317, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[-0.6219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08296687916122628 entropy 0.03264415264129639
epoch: 73, step: 5
	action: tensor([[ 0.3508,  0.3682, -0.1770,  0.3223, -0.2018,  0.2026, -0.2931]],
       dtype=torch.float64)
	q_value: tensor([[-3.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8179843513706784, distance: 0.48821493083960793 entropy 0.03264415264129639
epoch: 73, step: 6
	action: tensor([[ 0.9603,  0.3292, -0.4096,  0.3350,  0.2319,  0.4586, -0.1058]],
       dtype=torch.float64)
	q_value: tensor([[-0.8158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07858787134535782 entropy 0.03264415264129639
epoch: 73, step: 7
	action: tensor([[ 1.1062, -0.2769, -1.0538,  0.6861, -0.0621, -0.1143,  0.1517]],
       dtype=torch.float64)
	q_value: tensor([[-3.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7143003543761434, distance: 0.6116620615177001 entropy 0.03264415264129639
epoch: 73, step: 8
	action: tensor([[ 0.9439, -0.4472, -0.4682,  0.5883, -0.4186,  0.1453, -0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-1.4763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7161756664862478, distance: 0.6096513033797385 entropy 0.03264415264129639
epoch: 73, step: 9
	action: tensor([[ 1.1294,  0.4479, -1.2242,  0.8581, -0.2393,  0.0903,  0.2308]],
       dtype=torch.float64)
	q_value: tensor([[-1.0622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9702468836455449, distance: 0.19738898942749739 entropy 0.03264415264129639
epoch: 73, step: 10
	action: tensor([[ 1.0366,  0.2851, -0.2394,  0.5770, -0.2546,  0.2375,  0.2823]],
       dtype=torch.float64)
	q_value: tensor([[-2.5359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9882458844332492, distance: 0.12406568320664682 entropy 0.03264415264129639
epoch: 73, step: 11
	action: tensor([[ 0.9067,  0.2668,  0.1008,  0.8596, -0.0200,  0.5596,  0.0742]],
       dtype=torch.float64)
	q_value: tensor([[-1.3177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9234619282460965, distance: 0.3165887306885244 entropy 0.03264415264129639
epoch: 73, step: 12
	action: tensor([[ 1.2265,  0.5200, -0.2178,  0.9429, -0.4504,  0.3180,  0.1962]],
       dtype=torch.float64)
	q_value: tensor([[-1.3110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 73, step: 13
	action: tensor([[ 1.4776,  0.2380, -0.1894,  0.5207, -0.2413,  0.0465,  0.0723]],
       dtype=torch.float64)
	q_value: tensor([[-3.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8235272720330213, distance: 0.48072365484022533 entropy 0.03264415264129639
epoch: 73, step: 14
	action: tensor([[ 1.0760,  0.2089, -0.2806,  0.7116,  0.2450,  0.0794,  0.6442]],
       dtype=torch.float64)
	q_value: tensor([[-2.1594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9886398243087917, distance: 0.12196893095335135 entropy 0.03264415264129639
epoch: 73, step: 15
	action: tensor([[ 0.4653, -0.0201, -0.5484,  1.0863, -0.0286, -0.0024,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-0.9143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7642628130792376, distance: 0.5556108801469416 entropy 0.03264415264129639
epoch: 73, step: 16
	action: tensor([[ 0.8138,  0.3777, -0.3085,  0.6048, -0.0922,  0.0927, -0.1641]],
       dtype=torch.float64)
	q_value: tensor([[-0.6567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9698207769741808, distance: 0.19879740934585655 entropy 0.03264415264129639
epoch: 73, step: 17
	action: tensor([[ 0.6923, -0.0034, -0.4931,  0.4840,  0.2448,  0.4168, -0.3177]],
       dtype=torch.float64)
	q_value: tensor([[-1.3913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9244679193695486, distance: 0.3145012799794757 entropy 0.03264415264129639
epoch: 73, step: 18
	action: tensor([[ 0.9291,  0.3066, -0.1147,  0.7229, -0.4632, -0.0988,  0.3612]],
       dtype=torch.float64)
	q_value: tensor([[-1.0546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9862123616111439, distance: 0.13436978910937514 entropy 0.03264415264129639
epoch: 73, step: 19
	action: tensor([[ 0.6778,  0.1501, -0.0354,  0.6470, -0.3586, -0.1208, -0.3318]],
       dtype=torch.float64)
	q_value: tensor([[-1.0569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9816107820336261, distance: 0.1551809204420942 entropy 0.03264415264129639
epoch: 73, step: 20
	action: tensor([[ 1.2212,  0.0440, -0.2961,  0.5921, -0.3211,  0.0915,  0.0350]],
       dtype=torch.float64)
	q_value: tensor([[-1.1260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9255322113565816, distance: 0.31227766402949597 entropy 0.03264415264129639
epoch: 73, step: 21
	action: tensor([[ 1.4311,  0.5597,  0.2227,  0.0513, -0.2483,  0.4074, -0.0763]],
       dtype=torch.float64)
	q_value: tensor([[-1.6566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8037271430574053, distance: 0.5069753168532991 entropy 0.03264415264129639
epoch: 73, step: 22
	action: tensor([[ 0.9340,  0.1617, -0.4875,  0.1123, -0.1441,  0.4606,  0.2520]],
       dtype=torch.float64)
	q_value: tensor([[-2.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.944983211908234, distance: 0.26841347180554 entropy 0.03264415264129639
epoch: 73, step: 23
	action: tensor([[ 0.9033, -0.0257, -0.5003,  0.2941, -0.1513,  0.3911, -0.5339]],
       dtype=torch.float64)
	q_value: tensor([[-1.3428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9108096006622612, distance: 0.3417556848640099 entropy 0.03264415264129639
epoch: 73, step: 24
	action: tensor([[ 1.0127, -0.0339, -0.4430,  0.7582, -0.3748,  0.1128, -0.1983]],
       dtype=torch.float64)
	q_value: tensor([[-1.8382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9807687656122666, distance: 0.15869391478300704 entropy 0.03264415264129639
epoch: 73, step: 25
	action: tensor([[ 0.6929,  0.3483, -0.7505,  0.8125, -0.1557,  0.5410, -0.1027]],
       dtype=torch.float64)
	q_value: tensor([[-1.6701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7868355223742943, distance: 0.5283407524741472 entropy 0.03264415264129639
epoch: 73, step: 26
	action: tensor([[ 1.0753,  0.2385, -0.0241,  0.5105, -0.4927, -0.0433, -0.1651]],
       dtype=torch.float64)
	q_value: tensor([[-1.8280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9519712392389561, distance: 0.2507883648419386 entropy 0.03264415264129639
epoch: 73, step: 27
	action: tensor([[ 0.3860,  0.3452, -0.6706,  0.5871, -0.5809,  0.4787,  0.1748]],
       dtype=torch.float64)
	q_value: tensor([[-1.6785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.665583739153718, distance: 0.6617598077919205 entropy 0.03264415264129639
epoch: 73, step: 28
	action: tensor([[ 0.7358, -0.1160,  0.0623,  0.8794, -0.3720,  0.3256, -0.4925]],
       dtype=torch.float64)
	q_value: tensor([[-1.3964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9915147647405985, distance: 0.10541165628305914 entropy 0.03264415264129639
epoch: 73, step: 29
	action: tensor([[ 1.2596,  0.3064, -0.7124,  0.6517, -0.5646, -0.0641,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-1.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9802387475861246, distance: 0.1608658751687225 entropy 0.03264415264129639
epoch: 73, step: 30
	action: tensor([[ 0.6989, -0.2586, -0.6659,  0.1174,  0.1161, -0.0758, -0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-2.4258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5585346199474787, distance: 0.7603350639980616 entropy 0.03264415264129639
epoch: 73, step: 31
	action: tensor([[ 0.3399,  0.2380, -0.3811,  0.7460, -0.0480,  0.1221,  0.1820]],
       dtype=torch.float64)
	q_value: tensor([[-0.6415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7782332695178183, distance: 0.538895913261882 entropy 0.03264415264129639
epoch: 73, step: 32
	action: tensor([[ 0.7011,  0.6341, -0.4690,  0.7010,  0.0921,  0.1509,  0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-0.5700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 73, step: 33
	action: tensor([[ 0.5682,  0.1572, -0.1100,  0.0779, -0.3922,  0.3125,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-3.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8466300636361892, distance: 0.4481535200428795 entropy 0.03264415264129639
epoch: 73, step: 34
	action: tensor([[ 0.4673,  0.3152, -0.2194,  0.4711, -0.0786,  0.2692, -0.0269]],
       dtype=torch.float64)
	q_value: tensor([[-0.7951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8736782007694529, distance: 0.40672029574588375 entropy 0.03264415264129639
epoch: 73, step: 35
	action: tensor([[ 0.7978, -0.1544,  0.0703,  0.3572, -0.6629, -0.0133,  0.3035]],
       dtype=torch.float64)
	q_value: tensor([[-0.7165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7959721002723339, distance: 0.5168939782738974 entropy 0.03264415264129639
epoch: 73, step: 36
	action: tensor([[ 0.7585,  0.1189, -0.2207,  0.7204, -0.3566, -0.1224,  0.3011]],
       dtype=torch.float64)
	q_value: tensor([[-0.6459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9743530229539621, distance: 0.1832629971191055 entropy 0.03264415264129639
epoch: 73, step: 37
	action: tensor([[ 0.6573,  0.3242, -0.6674,  0.5032, -0.0950, -0.1504, -0.2908]],
       dtype=torch.float64)
	q_value: tensor([[-0.7529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9240047680939869, distance: 0.3154640434565446 entropy 0.03264415264129639
epoch: 73, step: 38
	action: tensor([[ 0.9168, -0.1283, -0.2381,  0.3181, -0.4474,  0.4148,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-1.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8783625065022168, distance: 0.39910799197814656 entropy 0.03264415264129639
epoch: 73, step: 39
	action: tensor([[ 0.6078, -0.0539, -0.4145,  0.5712, -0.0443,  0.5249,  0.2344]],
       dtype=torch.float64)
	q_value: tensor([[-1.2469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8845177339661257, distance: 0.38887886619980755 entropy 0.03264415264129639
epoch: 73, step: 40
	action: tensor([[ 0.4694,  0.2813, -0.1709,  0.4849, -0.4636,  0.0808, -0.0908]],
       dtype=torch.float64)
	q_value: tensor([[-0.7566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.898674906672557, distance: 0.364263116961133 entropy 0.03264415264129639
epoch: 73, step: 41
	action: tensor([[ 0.7405, -0.0865, -0.5085,  0.9777, -0.2636, -0.4259, -0.5295]],
       dtype=torch.float64)
	q_value: tensor([[-0.8613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9218233219198625, distance: 0.3199597136134039 entropy 0.03264415264129639
epoch: 73, step: 42
	action: tensor([[ 1.0215,  0.6618, -0.6297,  0.5368, -0.6768,  0.1215, -0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-1.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9338075249695847, distance: 0.2944157257306854 entropy 0.03264415264129639
epoch: 73, step: 43
	action: tensor([[ 0.8182,  0.0309, -0.3602,  0.3217, -0.5474,  0.4807,  0.1153]],
       dtype=torch.float64)
	q_value: tensor([[-2.5811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 73, step: 44
	action: tensor([[ 1.0870,  0.4150, -0.2424,  0.9288, -0.0272,  0.1255, -0.4419]],
       dtype=torch.float64)
	q_value: tensor([[-3.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9165989755879269, distance: 0.33047787225265257 entropy 0.03264415264129639
epoch: 73, step: 45
	action: tensor([[ 1.3218,  0.4314, -0.8615,  0.8048, -0.6494,  0.3521, -0.0651]],
       dtype=torch.float64)
	q_value: tensor([[-2.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9900510291178181, distance: 0.11414207757789596 entropy 0.03264415264129639
epoch: 73, step: 46
	action: tensor([[ 0.4155,  0.0616, -0.2625,  0.9747, -0.1527,  0.3526, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-3.1850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7911394072145421, distance: 0.5229798382501417 entropy 0.03264415264129639
epoch: 73, step: 47
	action: tensor([[ 0.8322,  0.0166, -0.1651,  0.7974, -0.1069,  0.2168,  0.2397]],
       dtype=torch.float64)
	q_value: tensor([[-0.8084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9885797921763705, distance: 0.12229077488663041 entropy 0.03264415264129639
epoch: 73, step: 48
	action: tensor([[ 0.9252,  0.0931, -0.0131,  0.5924, -0.5374,  0.3604, -0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-0.7925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08119662779404414 entropy 0.03264415264129639
epoch: 73, step: 49
	action: tensor([[ 1.2436,  0.4347, -0.3064,  0.5454, -0.1235,  0.2847, -0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-3.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9540851558673503, distance: 0.24520721821328545 entropy 0.03264415264129639
epoch: 73, step: 50
	action: tensor([[ 0.5854,  0.0038, -0.5611,  0.9440, -0.3644, -0.0876, -0.4485]],
       dtype=torch.float64)
	q_value: tensor([[-2.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8411050463206441, distance: 0.45615427109737633 entropy 0.03264415264129639
epoch: 73, step: 51
	action: tensor([[ 1.1461,  0.1890, -0.0354,  0.8543, -0.6618,  0.1212, -0.2666]],
       dtype=torch.float64)
	q_value: tensor([[-1.4560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.957308863733051, distance: 0.23644250417567958 entropy 0.03264415264129639
epoch: 73, step: 52
	action: tensor([[ 0.8382,  0.3638, -0.2829,  0.8677, -0.0895, -0.1274,  0.2470]],
       dtype=torch.float64)
	q_value: tensor([[-2.1787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9555397105836954, distance: 0.24129195155591007 entropy 0.03264415264129639
epoch: 73, step: 53
	action: tensor([[ 0.5561,  0.1636, -0.1350,  0.3959, -0.0862,  0.1712,  0.4451]],
       dtype=torch.float64)
	q_value: tensor([[-1.0081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9211992421984903, distance: 0.3212342845434173 entropy 0.03264415264129639
epoch: 73, step: 54
	action: tensor([[ 0.9080,  0.4038, -0.0382,  0.6637,  0.0352,  0.2158, -0.1788]],
       dtype=torch.float64)
	q_value: tensor([[-0.4101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9520481811119488, distance: 0.25058740337130486 entropy 0.03264415264129639
epoch: 73, step: 55
	action: tensor([[ 0.5327,  0.2894, -0.1826,  0.8696, -0.6222, -0.2398,  0.0860]],
       dtype=torch.float64)
	q_value: tensor([[-1.4116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9293468051324517, distance: 0.3041743549725712 entropy 0.03264415264129639
epoch: 73, step: 56
	action: tensor([[ 1.1152, -0.0158, -0.2670,  0.8392, -0.2195, -0.2157,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-0.9926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9585833765282645, distance: 0.2328863520652204 entropy 0.03264415264129639
epoch: 73, step: 57
	action: tensor([[ 0.9413,  0.3829, -0.2848,  0.6461,  0.1598,  0.0536,  0.1630]],
       dtype=torch.float64)
	q_value: tensor([[-1.3385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9780805125380303, distance: 0.16942281397177403 entropy 0.03264415264129639
epoch: 73, step: 58
	action: tensor([[ 0.7638,  0.0893, -0.2264,  0.4228, -0.4645,  0.3999,  0.4116]],
       dtype=torch.float64)
	q_value: tensor([[-1.1045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9578754813277036, distance: 0.23486817275064392 entropy 0.03264415264129639
epoch: 73, step: 59
	action: tensor([[ 0.9479,  0.1580, -0.6229,  0.4533, -0.3785,  0.4457, -0.2149]],
       dtype=torch.float64)
	q_value: tensor([[-0.9357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9710435856024628, distance: 0.19472830558420182 entropy 0.03264415264129639
epoch: 73, step: 60
	action: tensor([[ 1.0806,  0.2580, -0.2335,  0.3141, -0.2832,  0.1685, -0.5537]],
       dtype=torch.float64)
	q_value: tensor([[-2.0322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9665530637050147, distance: 0.20928343978340752 entropy 0.03264415264129639
epoch: 73, step: 61
	action: tensor([[ 1.3151,  0.1748, -0.2565,  0.6258, -0.1075, -0.0432, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[-2.2420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9021035544136584, distance: 0.3580470955846588 entropy 0.03264415264129639
epoch: 73, step: 62
	action: tensor([[ 0.8265,  0.0494, -0.7031,  0.8694, -0.1859,  0.3952, -0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-1.8198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8972139854756013, distance: 0.3668797206363138 entropy 0.03264415264129639
epoch: 73, step: 63
	action: tensor([[ 0.8490,  0.3231, -0.1281,  0.2798, -0.4080,  0.1837,  0.0595]],
       dtype=torch.float64)
	q_value: tensor([[-1.5065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9878990150294337, distance: 0.125882991223958 entropy 0.03264415264129639
epoch: 73, step: 64
	action: tensor([[ 0.6817,  0.4453, -0.3444,  0.6681,  0.0440,  0.3036,  0.2252]],
       dtype=torch.float64)
	q_value: tensor([[-1.2008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8929341701284959, distance: 0.3744399142045911 entropy 0.03264415264129639
epoch: 73, step: 65
	action: tensor([[ 0.7949,  0.0634, -0.1939,  0.4735, -0.3799,  0.3733, -0.1319]],
       dtype=torch.float64)
	q_value: tensor([[-1.0265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9715901258193815, distance: 0.19288184359723864 entropy 0.03264415264129639
epoch: 73, step: 66
	action: tensor([[ 0.6303, -0.0405, -0.3930,  0.6777, -0.4398,  0.4189,  0.6980]],
       dtype=torch.float64)
	q_value: tensor([[-1.2556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8764517116691345, distance: 0.40223055641947464 entropy 0.03264415264129639
epoch: 73, step: 67
	action: tensor([[ 0.8387, -0.2351, -0.5296,  0.8211, -0.0324, -0.2356,  0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-0.7342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8503736531509047, distance: 0.44265026608594293 entropy 0.03264415264129639
epoch: 73, step: 68
	action: tensor([[ 0.9593,  0.4504, -0.5373,  0.3394, -0.0179,  0.1464,  0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-0.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9904762022889979, distance: 0.1116764948369402 entropy 0.03264415264129639
epoch: 73, step: 69
	action: tensor([[ 0.3547,  0.4432,  0.0504,  0.5087, -0.2744,  0.4904,  0.1253]],
       dtype=torch.float64)
	q_value: tensor([[-1.5993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8441531620684213, distance: 0.4517578322618043 entropy 0.03264415264129639
epoch: 73, step: 70
	action: tensor([[ 0.5533,  0.0393, -0.4516,  0.5656,  0.1182,  0.2198, -0.2175]],
       dtype=torch.float64)
	q_value: tensor([[-0.8212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.867722238863332, distance: 0.4161981170918515 entropy 0.03264415264129639
epoch: 73, step: 71
	action: tensor([[ 0.9079,  0.0748, -0.4170,  0.7454, -0.1158,  0.4242,  0.3096]],
       dtype=torch.float64)
	q_value: tensor([[-0.7493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9854638916861675, distance: 0.13796876235013894 entropy 0.03264415264129639
epoch: 73, step: 72
	action: tensor([[ 0.6944,  0.2621, -0.5527,  0.7532, -0.4804, -0.3529,  0.0924]],
       dtype=torch.float64)
	q_value: tensor([[-1.1453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9516652670682773, distance: 0.2515859331740638 entropy 0.03264415264129639
epoch: 73, step: 73
	action: tensor([[ 0.9373,  0.2616, -0.6034,  0.8879, -0.0543,  0.3578,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-1.1568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9333369044549175, distance: 0.295460502113083 entropy 0.03264415264129639
epoch: 73, step: 74
	action: tensor([[ 0.9608,  0.3466, -0.3676,  0.7426, -0.6058,  0.0807, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[-1.6730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9712830643321124, distance: 0.19392140136253827 entropy 0.03264415264129639
epoch: 73, step: 75
	action: tensor([[ 0.3348, -0.1779, -0.4498,  0.4529,  0.0113,  0.1374, -0.0878]],
       dtype=torch.float64)
	q_value: tensor([[-1.8107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6123966231398432, distance: 0.7124435668704971 entropy 0.03264415264129639
epoch: 73, step: 76
	action: tensor([[ 0.9409,  0.3073, -0.1307,  1.0969,  0.1191,  0.0331, -0.0588]],
       dtype=torch.float64)
	q_value: tensor([[-0.3579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9125694337522967, distance: 0.33836726329730565 entropy 0.03264415264129639
epoch: 73, step: 77
	action: tensor([[ 0.8157,  0.0676, -0.4079,  0.9422,  0.2422,  0.1855, -0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-1.3750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9609810473477041, distance: 0.22604478769387157 entropy 0.03264415264129639
epoch: 73, step: 78
	action: tensor([[ 0.9008, -0.2147, -0.3584,  0.6292, -0.6861, -0.2085, -0.4574]],
       dtype=torch.float64)
	q_value: tensor([[-1.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8633576318579232, distance: 0.4230087815853843 entropy 0.03264415264129639
epoch: 73, step: 79
	action: tensor([[ 0.9417, -0.7418, -0.2500,  0.2117, -0.4707,  0.0526, -0.2335]],
       dtype=torch.float64)
	q_value: tensor([[-1.6649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17484627971533173, distance: 1.0394991159153448 entropy 0.03264415264129639
epoch: 73, step: 80
	action: tensor([[ 1.0091,  0.1520, -0.5469,  0.5829, -0.1720,  0.1121,  0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-0.9254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9823846854893499, distance: 0.1518804565999633 entropy 0.03264415264129639
epoch: 73, step: 81
	action: tensor([[ 1.0424, -0.2579, -0.5522,  0.8588, -0.0458, -0.0537, -0.1625]],
       dtype=torch.float64)
	q_value: tensor([[-1.3658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9039901591771989, distance: 0.35458027140694254 entropy 0.03264415264129639
epoch: 73, step: 82
	action: tensor([[ 1.1857, -0.0468, -0.3566,  0.5018, -0.4140,  0.2878, -0.1780]],
       dtype=torch.float64)
	q_value: tensor([[-1.3335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9233134435624932, distance: 0.31689567466577584 entropy 0.03264415264129639
epoch: 73, step: 83
	action: tensor([[ 0.7655, -0.0160, -0.0749,  0.5578, -0.0025,  0.4197,  0.1884]],
       dtype=torch.float64)
	q_value: tensor([[-1.9388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.979749431918942, distance: 0.16284532628178697 entropy 0.03264415264129639
epoch: 73, step: 84
	action: tensor([[ 0.5200,  0.0310, -0.0779,  0.3781, -0.2530, -0.0018,  0.2167]],
       dtype=torch.float64)
	q_value: tensor([[-0.6988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8520595751475409, distance: 0.44014941001231805 entropy 0.03264415264129639
epoch: 73, step: 85
	action: tensor([[ 1.0420,  0.2746, -0.4073,  0.9302, -0.2770,  0.0223, -0.3737]],
       dtype=torch.float64)
	q_value: tensor([[-0.3724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9743222547742865, distance: 0.1833728926962033 entropy 0.03264415264129639
epoch: 73, step: 86
	action: tensor([[ 0.8832, -0.0331, -0.4991,  1.0558, -0.6319,  0.1962,  0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-2.1749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9478022783390937, distance: 0.2614462800301353 entropy 0.03264415264129639
epoch: 73, step: 87
	action: tensor([[ 1.3242,  0.2079, -0.1043,  0.5935, -0.3228,  0.3560, -0.0176]],
       dtype=torch.float64)
	q_value: tensor([[-1.5224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9189074672873919, distance: 0.3258720596975212 entropy 0.03264415264129639
epoch: 73, step: 88
	action: tensor([[ 1.1784, -0.0895, -0.2828,  0.7973, -0.5599,  0.1088,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[-2.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9579422789576574, distance: 0.23468188146278154 entropy 0.03264415264129639
epoch: 73, step: 89
	action: tensor([[ 1.1354, -0.4656, -0.8191,  0.6785, -0.2260,  0.1456, -0.2295]],
       dtype=torch.float64)
	q_value: tensor([[-1.6790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7018782317363143, distance: 0.6248180074161194 entropy 0.03264415264129639
epoch: 73, step: 90
	action: tensor([[ 0.9710, -0.0536, -0.9662,  0.2412, -0.2798,  0.3295, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[-1.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8186165093543104, distance: 0.48736638454833 entropy 0.03264415264129639
epoch: 73, step: 91
	action: tensor([[ 0.7028, -0.0531, -0.1302,  0.4787, -0.2447,  0.0837,  0.1541]],
       dtype=torch.float64)
	q_value: tensor([[-1.7702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9016294616324542, distance: 0.35891302348002146 entropy 0.03264415264129639
epoch: 73, step: 92
	action: tensor([[ 0.9668, -0.1593,  0.2050,  0.6016, -0.4816, -0.0343,  0.0779]],
       dtype=torch.float64)
	q_value: tensor([[-0.5488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8663157890571931, distance: 0.4184048901788698 entropy 0.03264415264129639
epoch: 73, step: 93
	action: tensor([[ 0.8683,  0.7732, -0.1202,  0.4122,  0.2137,  0.1574, -0.3874]],
       dtype=torch.float64)
	q_value: tensor([[-0.9139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 73, step: 94
	action: tensor([[ 0.5239,  0.0533, -0.2706,  0.3277,  0.1362, -0.1404,  0.3515]],
       dtype=torch.float64)
	q_value: tensor([[-3.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7973437558662607, distance: 0.5151535394123142 entropy 0.03264415264129639
epoch: 73, step: 95
	action: tensor([[0.7226, 0.2689, 0.1245, 0.5971, 0.1727, 0.1111, 0.1672]],
       dtype=torch.float64)
	q_value: tensor([[-0.3476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9905889868763182, distance: 0.11101326677875935 entropy 0.03264415264129639
epoch: 73, step: 96
	action: tensor([[ 1.0567,  0.2388, -0.6413,  0.4492, -0.4077,  0.1090, -0.0631]],
       dtype=torch.float64)
	q_value: tensor([[-0.5400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9790550845699096, distance: 0.1656136004414212 entropy 0.03264415264129639
epoch: 73, step: 97
	action: tensor([[ 0.9791, -0.3049,  0.0146,  0.4110, -0.2757,  0.0066,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-1.9546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7031912686110582, distance: 0.6234405259452604 entropy 0.03264415264129639
epoch: 73, step: 98
	action: tensor([[ 0.5987,  0.5820, -0.6924,  0.4088,  0.2352, -0.1361, -0.0379]],
       dtype=torch.float64)
	q_value: tensor([[-0.7938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9130319310655639, distance: 0.3374711152754477 entropy 0.03264415264129639
epoch: 73, step: 99
	action: tensor([[ 1.0699, -0.2082, -0.5446,  0.7543, -0.3480, -0.1558,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[-1.3080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8821868548892423, distance: 0.39278380075672426 entropy 0.03264415264129639
epoch: 73, step: 100
	action: tensor([[ 1.1561,  0.0455, -0.1818,  0.6137, -0.4311, -0.1277, -0.0224]],
       dtype=torch.float64)
	q_value: tensor([[-1.3478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9088294944693672, distance: 0.34552849993434687 entropy 0.03264415264129639
epoch: 73, step: 101
	action: tensor([[ 0.8536, -0.1101, -0.5051,  0.5455, -0.3956,  0.0057,  0.1265]],
       dtype=torch.float64)
	q_value: tensor([[-1.5293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8820210841342151, distance: 0.3930600397919545 entropy 0.03264415264129639
epoch: 73, step: 102
	action: tensor([[ 0.5518, -0.2012, -0.4844,  0.7508, -0.4303,  0.6641, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[-0.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.772072357467841, distance: 0.5463301804906325 entropy 0.03264415264129639
epoch: 73, step: 103
	action: tensor([[ 1.0806, -0.0195,  0.1696,  0.8983,  0.1915, -0.0727,  0.2195]],
       dtype=torch.float64)
	q_value: tensor([[-1.1614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9392817116470638, distance: 0.2819787970270948 entropy 0.03264415264129639
epoch: 73, step: 104
	action: tensor([[ 1.3010,  0.1517, -0.5934,  0.8442,  0.1487,  0.3580, -0.2565]],
       dtype=torch.float64)
	q_value: tensor([[-0.8078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9891044082843166, distance: 0.11944888596391304 entropy 0.03264415264129639
epoch: 73, step: 105
	action: tensor([[ 1.2617,  0.5236, -0.5469,  1.0273, -0.1098,  0.4317, -0.1062]],
       dtype=torch.float64)
	q_value: tensor([[-2.4114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8544846199871751, distance: 0.4365270315448317 entropy 0.03264415264129639
epoch: 73, step: 106
	action: tensor([[ 1.1311,  0.5360, -0.3186,  0.3843, -0.3440,  0.2422, -0.1825]],
       dtype=torch.float64)
	q_value: tensor([[-2.7622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 73, step: 107
	action: tensor([[ 0.7237,  0.0403, -0.4229,  0.6294, -0.0834, -0.1054,  0.2789]],
       dtype=torch.float64)
	q_value: tensor([[-3.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9093506376040638, distance: 0.3445395404645972 entropy 0.03264415264129639
epoch: 73, step: 108
	action: tensor([[ 0.9645,  0.3883,  0.0127,  0.9506,  0.0851,  0.3102, -0.4573]],
       dtype=torch.float64)
	q_value: tensor([[-0.6359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8755703981825347, distance: 0.40366263324527524 entropy 0.03264415264129639
epoch: 73, step: 109
	action: tensor([[ 0.9033,  0.2809, -0.4292,  1.0750, -0.2671, -0.1876, -0.1068]],
       dtype=torch.float64)
	q_value: tensor([[-1.9189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9452901408472597, distance: 0.26766370897982766 entropy 0.03264415264129639
epoch: 73, step: 110
	action: tensor([[ 1.0834, -0.1140, -0.7723,  0.4705, -0.3925,  0.3970, -0.0712]],
       dtype=torch.float64)
	q_value: tensor([[-1.6791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8787289417497798, distance: 0.39850637835970637 entropy 0.03264415264129639
epoch: 73, step: 111
	action: tensor([[ 1.0853, -0.2313, -0.7893,  0.6004,  0.1750,  0.1335, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-1.8949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8315442800401976, distance: 0.46967730385865525 entropy 0.03264415264129639
epoch: 73, step: 112
	action: tensor([[ 0.7551, -0.2334, -0.0616,  0.5902, -0.3112, -0.0289, -0.0645]],
       dtype=torch.float64)
	q_value: tensor([[-1.3406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8536270609470332, distance: 0.437811424266216 entropy 0.03264415264129639
epoch: 73, step: 113
	action: tensor([[ 1.3172,  0.1458, -0.1796,  0.6505, -0.1614,  0.1496,  0.3592]],
       dtype=torch.float64)
	q_value: tensor([[-0.6596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9180566226439102, distance: 0.3275771673638689 entropy 0.03264415264129639
epoch: 73, step: 114
	action: tensor([[ 1.1169,  0.0943, -0.8049, -0.0542, -0.4624, -0.1081,  0.1966]],
       dtype=torch.float64)
	q_value: tensor([[-1.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7271731753384724, distance: 0.5977233577848147 entropy 0.03264415264129639
epoch: 73, step: 115
	action: tensor([[ 0.0783, -0.0045, -0.3186,  0.6278, -0.2889, -0.1607,  0.2090]],
       dtype=torch.float64)
	q_value: tensor([[-1.7803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5491661754026945, distance: 0.7683603388904428 entropy 0.03264415264129639
epoch: 73, step: 116
	action: tensor([[ 0.8472,  0.2371, -0.4920,  0.5944,  0.0533, -0.1566,  0.2440]],
       dtype=torch.float64)
	q_value: tensor([[-0.2665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9720466213669023, distance: 0.1913259360837752 entropy 0.03264415264129639
epoch: 73, step: 117
	action: tensor([[ 0.8245, -0.0296, -0.4226,  0.1461, -0.3649,  0.1820,  0.0341]],
       dtype=torch.float64)
	q_value: tensor([[-0.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8194391238295806, distance: 0.4862599711856101 entropy 0.03264415264129639
epoch: 73, step: 118
	action: tensor([[ 0.4724, -0.2183, -0.6721,  0.3046,  0.0608,  0.3243, -0.0594]],
       dtype=torch.float64)
	q_value: tensor([[-1.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.622227853775331, distance: 0.7033502724484335 entropy 0.03264415264129639
epoch: 73, step: 119
	action: tensor([[ 0.8211,  0.1905, -0.5641,  0.7809, -0.2472,  0.0566,  0.5281]],
       dtype=torch.float64)
	q_value: tensor([[-0.5995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9513559148231137, distance: 0.25238974997000885 entropy 0.03264415264129639
epoch: 73, step: 120
	action: tensor([[ 0.7582,  0.1400, -0.0186,  0.6798,  0.0118, -0.0961,  0.3506]],
       dtype=torch.float64)
	q_value: tensor([[-1.0112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9861203274540534, distance: 0.13481751051630028 entropy 0.03264415264129639
epoch: 73, step: 121
	action: tensor([[ 0.5978,  0.0036, -0.5498,  0.7502, -0.0440, -0.0174,  0.1691]],
       dtype=torch.float64)
	q_value: tensor([[-0.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8531299062184059, distance: 0.4385543058200942 entropy 0.03264415264129639
epoch: 73, step: 122
	action: tensor([[ 0.8955,  0.2544, -0.0381,  0.3670, -0.8428,  0.1510,  0.2483]],
       dtype=torch.float64)
	q_value: tensor([[-0.6385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9850865432740574, distance: 0.13974808084652388 entropy 0.03264415264129639
epoch: 73, step: 123
	action: tensor([[ 0.8082, -0.6086,  0.0311,  0.0915,  0.1868,  0.0379, -0.2255]],
       dtype=torch.float64)
	q_value: tensor([[-1.3446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21715156329356, distance: 1.0125011789998848 entropy 0.03264415264129639
epoch: 73, step: 124
	action: tensor([[ 1.2333, -0.3462, -0.3342,  0.6756, -0.4072,  0.0702, -0.3704]],
       dtype=torch.float64)
	q_value: tensor([[-0.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7780588505208796, distance: 0.5391077917755212 entropy 0.03264415264129639
epoch: 73, step: 125
	action: tensor([[ 0.8871,  0.5098, -0.4268,  0.7911, -0.0548,  0.1051, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-1.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8968402553974969, distance: 0.3675461030070143 entropy 0.03264415264129639
epoch: 73, step: 126
	action: tensor([[ 0.7766, -0.3636, -0.0482,  0.1827, -0.3081,  0.6694,  0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-1.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.723937685928596, distance: 0.6012571519214969 entropy 0.03264415264129639
epoch: 73, step: 127
	action: tensor([[ 1.0940, -0.1623, -0.0033,  0.7507, -0.1030,  0.1268, -0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-0.9410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.92614739611917, distance: 0.31098511279664903 entropy 0.03264415264129639
LOSS epoch 73 actor 206.03302002746887 critic 1374.4539418832983 
epoch: 74, step: 0
	action: tensor([[ 1.0483, -0.0862, -0.2574,  1.0744, -0.9129,  0.0148,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[-1.0398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.04315081738104843 entropy 0.03264415264129639
epoch: 74, step: 1
	action: tensor([[ 0.5865,  0.2998, -0.0520,  0.4231,  0.1681,  0.3175,  0.0739]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9520058364323561, distance: 0.2506980217171709 entropy 0.03264415264129639
epoch: 74, step: 2
	action: tensor([[ 0.9688, -0.1557, -0.5984,  0.4004, -0.4131,  0.1622,  0.2997]],
       dtype=torch.float64)
	q_value: tensor([[-0.5259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8171034066811974, distance: 0.4893949700468106 entropy 0.03264415264129639
epoch: 74, step: 3
	action: tensor([[ 1.3757,  0.1245, -0.4532,  0.4102,  0.0637,  0.4484,  0.1250]],
       dtype=torch.float64)
	q_value: tensor([[-1.0632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9148182996281385, distance: 0.3339872176410527 entropy 0.03264415264129639
epoch: 74, step: 4
	action: tensor([[ 1.0487, -0.0150, -0.2167,  1.0196, -0.3766,  0.3182,  0.1313]],
       dtype=torch.float64)
	q_value: tensor([[-1.8375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07844095346442273 entropy 0.03264415264129639
epoch: 74, step: 5
	action: tensor([[ 0.7222,  0.6346, -0.4809,  0.3606, -0.2661,  0.0540,  0.1008]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9259385173013857, distance: 0.3114245850257007 entropy 0.03264415264129639
epoch: 74, step: 6
	action: tensor([[ 0.7578,  0.3805, -0.5980,  1.2962,  0.3344, -0.1073, -0.0173]],
       dtype=torch.float64)
	q_value: tensor([[-1.3142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7612961665427291, distance: 0.559096006334635 entropy 0.03264415264129639
epoch: 74, step: 7
	action: tensor([[ 1.3212,  0.2435, -0.4821,  1.1052, -0.3733,  0.1634, -0.2284]],
       dtype=torch.float64)
	q_value: tensor([[-1.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9550628727092488, distance: 0.24258243221114859 entropy 0.03264415264129639
epoch: 74, step: 8
	action: tensor([[ 1.2084,  0.3352, -0.2518,  1.3675,  0.1401, -0.1108,  0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-2.5145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8531228073784757, distance: 0.43856490426501815 entropy 0.03264415264129639
epoch: 74, step: 9
	action: tensor([[ 1.3565,  0.1762, -0.4444,  1.4889, -0.2125,  0.5199,  0.2707]],
       dtype=torch.float64)
	q_value: tensor([[-1.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8385698884809396, distance: 0.45977882562857225 entropy 0.03264415264129639
epoch: 74, step: 10
	action: tensor([[ 1.1876,  0.3308, -0.4609,  0.3614,  0.0964,  0.1957, -0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-2.1342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9605187599283804, distance: 0.2273799076622918 entropy 0.03264415264129639
epoch: 74, step: 11
	action: tensor([[ 0.8805, -0.0396, -0.5257,  0.4751, -0.2742,  0.1514, -0.1127]],
       dtype=torch.float64)
	q_value: tensor([[-1.7781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9129298479343446, distance: 0.33766911900893753 entropy 0.03264415264129639
epoch: 74, step: 12
	action: tensor([[ 1.2066,  0.0036, -0.4537,  0.7156, -0.3610,  0.1908,  0.1205]],
       dtype=torch.float64)
	q_value: tensor([[-1.1398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9717554591243032, distance: 0.19231977978467799 entropy 0.03264415264129639
epoch: 74, step: 13
	action: tensor([[ 1.2039, -0.2296, -0.1091,  0.7522, -0.5931,  0.0265, -0.2495]],
       dtype=torch.float64)
	q_value: tensor([[-1.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8631743554223857, distance: 0.4232923743082001 entropy 0.03264415264129639
epoch: 74, step: 14
	action: tensor([[ 1.7011,  0.5030, -0.3536,  0.7358, -0.4081,  0.3002,  0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-1.6766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8511677777732753, distance: 0.44147404566440707 entropy 0.03264415264129639
epoch: 74, step: 15
	action: tensor([[ 1.1832,  0.2520, -0.2310,  0.1801, -0.2672,  0.1332,  0.5230]],
       dtype=torch.float64)
	q_value: tensor([[-2.6791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8931022675588693, distance: 0.3741458562760144 entropy 0.03264415264129639
epoch: 74, step: 16
	action: tensor([[ 1.2649,  0.2034, -0.1700,  0.5719, -0.5479,  0.3072,  0.1936]],
       dtype=torch.float64)
	q_value: tensor([[-1.2153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9451351933323664, distance: 0.26804247528880704 entropy 0.03264415264129639
epoch: 74, step: 17
	action: tensor([[ 1.1040, -0.0387, -0.8712,  0.4335,  0.2189,  0.3047, -0.3571]],
       dtype=torch.float64)
	q_value: tensor([[-1.7884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9053364306283463, distance: 0.35208549303064535 entropy 0.03264415264129639
epoch: 74, step: 18
	action: tensor([[ 1.2432,  0.0309, -0.3868,  0.9785, -0.1273, -0.0568, -0.2519]],
       dtype=torch.float64)
	q_value: tensor([[-1.8333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9826048828856397, distance: 0.15092819276766495 entropy 0.03264415264129639
epoch: 74, step: 19
	action: tensor([[ 1.0194, -0.0197, -0.4882,  1.0523, -0.3796,  0.1949, -0.2135]],
       dtype=torch.float64)
	q_value: tensor([[-1.8789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9843963096241437, distance: 0.14294545628613067 entropy 0.03264415264129639
epoch: 74, step: 20
	action: tensor([[ 1.1118,  0.2654, -0.3401,  0.5495, -0.6005,  0.1273,  0.1203]],
       dtype=torch.float64)
	q_value: tensor([[-1.7336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.985736720715544, distance: 0.13666785754105104 entropy 0.03264415264129639
epoch: 74, step: 21
	action: tensor([[ 1.2586,  0.0100, -0.2223,  0.4696, -0.3932,  0.2480,  0.0851]],
       dtype=torch.float64)
	q_value: tensor([[-1.6648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8901124506161957, distance: 0.3793420068063428 entropy 0.03264415264129639
epoch: 74, step: 22
	action: tensor([[ 0.9111,  0.0494, -1.0341,  0.6359, -0.1439, -0.0907, -0.5860]],
       dtype=torch.float64)
	q_value: tensor([[-1.5854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8866575021207959, distance: 0.38525925727029614 entropy 0.03264415264129639
epoch: 74, step: 23
	action: tensor([[ 1.0732,  0.2888, -0.8832,  1.1701, -0.2005,  0.2935, -0.2593]],
       dtype=torch.float64)
	q_value: tensor([[-2.1081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8760978411440171, distance: 0.40280618463754014 entropy 0.03264415264129639
epoch: 74, step: 24
	action: tensor([[ 0.8668,  0.5712,  0.0781,  1.0289, -0.2824,  0.1239,  0.4655]],
       dtype=torch.float64)
	q_value: tensor([[-2.4905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 25
	action: tensor([[ 0.9781,  0.1320, -0.2772,  0.3006, -0.3412,  0.5858, -0.3232]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.977840284255422, distance: 0.1703486851748953 entropy 0.03264415264129639
epoch: 74, step: 26
	action: tensor([[ 1.0815,  0.2846,  0.5997,  1.0061, -0.3572,  0.0658,  0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-1.8619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8006711820302637, distance: 0.5109068657875132 entropy 0.03264415264129639
epoch: 74, step: 27
	action: tensor([[ 1.2633,  0.7216, -0.4611,  1.3981, -0.6257,  0.1208,  0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-1.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 28
	action: tensor([[ 0.9398,  0.2101, -0.2213,  0.5121,  0.0861,  0.2437,  0.5537]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06950062143837284 entropy 0.03264415264129639
epoch: 74, step: 29
	action: tensor([[ 1.0797, -0.0012, -0.5309,  0.4703, -0.0476, -0.1316,  0.3052]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8826002926941002, distance: 0.3920940034013942 entropy 0.03264415264129639
epoch: 74, step: 30
	action: tensor([[ 0.7591,  0.1134, -0.5001,  0.8528, -0.3433, -0.2518, -0.6782]],
       dtype=torch.float64)
	q_value: tensor([[-0.9859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9477641548735402, distance: 0.26154173840134276 entropy 0.03264415264129639
epoch: 74, step: 31
	action: tensor([[ 1.5924,  0.1246, -1.0249,  0.7969, -0.5335,  0.0673,  0.2599]],
       dtype=torch.float64)
	q_value: tensor([[-1.8373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9095929970332495, distance: 0.34407865308639907 entropy 0.03264415264129639
epoch: 74, step: 32
	action: tensor([[ 0.8978,  0.0292,  0.1171,  0.4820, -0.7026,  0.1518, -0.2473]],
       dtype=torch.float64)
	q_value: tensor([[-2.7020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9579988982069804, distance: 0.23452386069223105 entropy 0.03264415264129639
epoch: 74, step: 33
	action: tensor([[ 0.6796,  0.3303, -0.2390,  1.0336, -0.3224,  0.6441, -0.0873]],
       dtype=torch.float64)
	q_value: tensor([[-1.3526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.779183581152178, distance: 0.5377400391031675 entropy 0.03264415264129639
epoch: 74, step: 34
	action: tensor([[ 1.2935,  0.1028, -0.5315,  0.8764, -0.0874, -0.0861, -0.1095]],
       dtype=torch.float64)
	q_value: tensor([[-1.5466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9752530054551032, distance: 0.18001882577595144 entropy 0.03264415264129639
epoch: 74, step: 35
	action: tensor([[ 1.4293,  0.1038, -0.4253,  0.7448, -0.3188, -0.0899, -0.5328]],
       dtype=torch.float64)
	q_value: tensor([[-1.8847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8948286625276503, distance: 0.3711123367898383 entropy 0.03264415264129639
epoch: 74, step: 36
	action: tensor([[ 1.2609, -0.0420, -0.4024,  1.3022, -0.5438,  0.4184,  0.0402]],
       dtype=torch.float64)
	q_value: tensor([[-2.6577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9686392621341277, distance: 0.20265150315812136 entropy 0.03264415264129639
epoch: 74, step: 37
	action: tensor([[ 1.6555,  0.2157, -0.5372,  1.0023,  0.1795, -0.0412,  0.2112]],
       dtype=torch.float64)
	q_value: tensor([[-2.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9079489998693103, distance: 0.3471929908227917 entropy 0.03264415264129639
epoch: 74, step: 38
	action: tensor([[ 0.9462,  0.6130, -0.3820,  0.6534, -0.1831,  0.3419,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-2.1319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 39
	action: tensor([[ 0.8807, -0.1481, -0.5104,  0.5952, -0.1534, -0.0239, -0.1302]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8648657563062052, distance: 0.42066792661516317 entropy 0.03264415264129639
epoch: 74, step: 40
	action: tensor([[ 1.2808,  0.1075, -0.5119,  0.8015, -0.2863,  0.3955,  0.1458]],
       dtype=torch.float64)
	q_value: tensor([[-0.9530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06771513088249348 entropy 0.03264415264129639
epoch: 74, step: 41
	action: tensor([[ 0.5716, -0.2891, -0.1833,  0.4831, -0.0323, -0.1079, -0.0898]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6682760941945578, distance: 0.6590905402758133 entropy 0.03264415264129639
epoch: 74, step: 42
	action: tensor([[ 1.0795,  0.6394, -0.0820,  0.7826, -0.3367,  0.3774, -0.2664]],
       dtype=torch.float64)
	q_value: tensor([[-0.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 43
	action: tensor([[ 0.9651, -0.0559, -0.6527,  0.3233,  0.0832, -0.0805, -0.5388]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8224678403787008, distance: 0.48216447739202944 entropy 0.03264415264129639
epoch: 74, step: 44
	action: tensor([[ 1.1800,  0.2013, -0.5641,  0.9293, -0.2869,  0.1151,  0.1933]],
       dtype=torch.float64)
	q_value: tensor([[-1.5055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08305997335900353 entropy 0.03264415264129639
epoch: 74, step: 45
	action: tensor([[ 1.1364,  0.0046, -0.4440,  0.8420,  0.0975,  0.0357,  0.6232]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9903491419593208, distance: 0.11241898425627543 entropy 0.03264415264129639
epoch: 74, step: 46
	action: tensor([[ 1.1366,  0.3893, -0.3663,  0.5363, -0.2848,  0.3884,  0.0749]],
       dtype=torch.float64)
	q_value: tensor([[-0.9088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9817752788998476, distance: 0.1544852922208922 entropy 0.03264415264129639
epoch: 74, step: 47
	action: tensor([[ 1.2810,  0.4121, -0.6943,  0.9064, -0.3971,  0.4472, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-1.8622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9543870062147773, distance: 0.24439987655164105 entropy 0.03264415264129639
epoch: 74, step: 48
	action: tensor([[ 1.4171,  0.4756, -0.2417,  0.3976, -0.0553,  0.3489, -0.2389]],
       dtype=torch.float64)
	q_value: tensor([[-2.6595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8668448836969599, distance: 0.41757608901535304 entropy 0.03264415264129639
epoch: 74, step: 49
	action: tensor([[ 1.2217,  0.6155, -0.5605,  0.7592, -0.2109, -0.2031,  0.0341]],
       dtype=torch.float64)
	q_value: tensor([[-2.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 50
	action: tensor([[0.8752, 0.1776, 0.0491, 0.5826, 0.0633, 0.5339, 0.0532]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09187264680067936 entropy 0.03264415264129639
epoch: 74, step: 51
	action: tensor([[ 0.9767, -0.0811, -0.2285,  0.2333,  0.0768, -0.0492,  0.1777]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7796047758285272, distance: 0.537226940141221 entropy 0.03264415264129639
epoch: 74, step: 52
	action: tensor([[ 1.0051, -0.3233, -0.6145,  0.5169, -0.3929,  0.0985,  0.1342]],
       dtype=torch.float64)
	q_value: tensor([[-0.6415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7431696572038846, distance: 0.5799357198654201 entropy 0.03264415264129639
epoch: 74, step: 53
	action: tensor([[ 1.2388, -0.2958, -0.4913,  1.0022, -0.4933, -0.0373, -0.1332]],
       dtype=torch.float64)
	q_value: tensor([[-1.0458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9195084341513725, distance: 0.3246623151786185 entropy 0.03264415264129639
epoch: 74, step: 54
	action: tensor([[ 0.9138,  0.3406, -0.2353,  0.8517, -0.4970, -0.3962,  0.2351]],
       dtype=torch.float64)
	q_value: tensor([[-1.7578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9841776782088301, distance: 0.1439434142586441 entropy 0.03264415264129639
epoch: 74, step: 55
	action: tensor([[ 0.6850,  0.0760, -0.3733,  0.6414, -0.3118,  0.1311,  0.0515]],
       dtype=torch.float64)
	q_value: tensor([[-1.1538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9361725832976266, distance: 0.2891081449139512 entropy 0.03264415264129639
epoch: 74, step: 56
	action: tensor([[ 0.5173,  0.2332, -0.4693,  0.7498,  0.0737,  0.1526,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[-0.8058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8481883428435287, distance: 0.4458710282827587 entropy 0.03264415264129639
epoch: 74, step: 57
	action: tensor([[ 1.4054,  0.5720, -0.2692,  0.5533, -0.1490,  0.3431, -0.2646]],
       dtype=torch.float64)
	q_value: tensor([[-0.7000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 58
	action: tensor([[ 1.2628,  0.7106, -0.4343,  0.4257, -0.0095,  0.0144,  0.2243]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 59
	action: tensor([[ 0.6815,  0.5328, -0.0180,  0.6755, -0.1203,  0.4013, -0.1522]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 60
	action: tensor([[ 0.6767, -0.1848,  0.2403,  0.6770,  0.0963, -0.0418,  0.1319]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9009412119132432, distance: 0.3601664029191471 entropy 0.03264415264129639
epoch: 74, step: 61
	action: tensor([[ 1.6254,  0.3780, -0.1785,  0.3151, -0.1412, -0.0130, -0.0926]],
       dtype=torch.float64)
	q_value: tensor([[-0.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7065507912046378, distance: 0.6199021815521962 entropy 0.03264415264129639
epoch: 74, step: 62
	action: tensor([[ 1.2243, -0.0911, -0.1420,  0.8779,  0.0093,  0.1161,  0.2218]],
       dtype=torch.float64)
	q_value: tensor([[-2.4549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9361267212801125, distance: 0.2892119929355234 entropy 0.03264415264129639
epoch: 74, step: 63
	action: tensor([[ 1.0839,  0.1660, -0.5937,  0.7286, -0.4654,  0.3741, -0.2434]],
       dtype=torch.float64)
	q_value: tensor([[-1.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9922612621686638, distance: 0.10066807414866451 entropy 0.03264415264129639
epoch: 74, step: 64
	action: tensor([[ 0.6870,  0.0276, -0.3884,  1.1876, -0.6434,  0.0054, -0.0299]],
       dtype=torch.float64)
	q_value: tensor([[-2.2001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9017592401981467, distance: 0.3586761914411713 entropy 0.03264415264129639
epoch: 74, step: 65
	action: tensor([[ 1.5311,  0.1965, -0.2811,  0.7772, -0.8042,  0.3899, -0.2087]],
       dtype=torch.float64)
	q_value: tensor([[-1.2691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9108326473752377, distance: 0.34171152734194815 entropy 0.03264415264129639
epoch: 74, step: 66
	action: tensor([[ 0.7709,  0.0762,  0.0105, -0.0889, -0.2755,  0.4278, -0.1074]],
       dtype=torch.float64)
	q_value: tensor([[-2.9846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8519686435210415, distance: 0.4402846582163505 entropy 0.03264415264129639
epoch: 74, step: 67
	action: tensor([[ 1.1609, -0.0984, -0.1633,  0.5540, -0.1165,  0.2264, -0.0742]],
       dtype=torch.float64)
	q_value: tensor([[-0.9433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.900587210923832, distance: 0.3608093825034815 entropy 0.03264415264129639
epoch: 74, step: 68
	action: tensor([[ 0.7318,  0.4304, -0.2779,  0.7578, -0.3802,  0.2379, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-1.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8921146047833701, distance: 0.37587030961678086 entropy 0.03264415264129639
epoch: 74, step: 69
	action: tensor([[ 0.8377,  0.3608, -0.4480,  0.8876, -0.0545, -0.1646, -0.2610]],
       dtype=torch.float64)
	q_value: tensor([[-1.3123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9453329521155418, distance: 0.26755896309286203 entropy 0.03264415264129639
epoch: 74, step: 70
	action: tensor([[ 1.1404, -0.0546, -0.6456,  1.0380, -0.6259, -0.2220, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[-1.5006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9895031459769419, distance: 0.11724282438967865 entropy 0.03264415264129639
epoch: 74, step: 71
	action: tensor([[ 1.4764,  0.1152, -0.0091,  0.5278, -0.2065, -0.0564, -0.1127]],
       dtype=torch.float64)
	q_value: tensor([[-1.8077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7240202348919652, distance: 0.6011672503612383 entropy 0.03264415264129639
epoch: 74, step: 72
	action: tensor([[ 1.8648,  0.4666, -0.3175,  0.8311, -0.4166,  0.1681,  0.2223]],
       dtype=torch.float64)
	q_value: tensor([[-1.9059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 73
	action: tensor([[ 0.5570,  0.5228, -0.2040,  0.4697, -0.0609, -0.0155, -0.2142]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9049212016718777, distance: 0.3528568358297895 entropy 0.03264415264129639
epoch: 74, step: 74
	action: tensor([[ 1.0140,  0.3429, -0.4164,  0.8753, -0.4357,  0.5164,  0.1728]],
       dtype=torch.float64)
	q_value: tensor([[-0.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.921933091112562, distance: 0.3197350042900138 entropy 0.03264415264129639
epoch: 74, step: 75
	action: tensor([[ 1.1073, -0.1738, -0.6413,  0.9653, -0.1731, -0.2569, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[-1.8115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9359882947772051, distance: 0.2895252141937257 entropy 0.03264415264129639
epoch: 74, step: 76
	action: tensor([[ 1.4385,  0.0629, -0.2124,  0.7166, -0.2810,  0.5980,  0.2291]],
       dtype=torch.float64)
	q_value: tensor([[-1.3536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9199674209486564, distance: 0.32373533136687155 entropy 0.03264415264129639
epoch: 74, step: 77
	action: tensor([[ 1.0876, -0.1179, -0.2917,  0.4263, -0.5492,  0.4270,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-1.9825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9059581076703614, distance: 0.350927476179647 entropy 0.03264415264129639
epoch: 74, step: 78
	action: tensor([[ 1.0793,  0.2048, -0.2733,  0.5540, -0.4257,  0.1359, -0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-1.5158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9855227429083752, distance: 0.1376891873385735 entropy 0.03264415264129639
epoch: 74, step: 79
	action: tensor([[ 1.1347,  0.1939, -0.2233,  0.5155, -0.2504, -0.0987, -0.1661]],
       dtype=torch.float64)
	q_value: tensor([[-1.6309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9436524733768272, distance: 0.2716402503081016 entropy 0.03264415264129639
epoch: 74, step: 80
	action: tensor([[ 1.1376,  0.2218, -0.2763,  0.3899, -0.2354,  0.0452,  0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-1.5587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9423775718687577, distance: 0.27469608555418845 entropy 0.03264415264129639
epoch: 74, step: 81
	action: tensor([[ 1.2005,  0.6918, -0.1857,  0.7686, -0.2685,  0.0113,  0.1521]],
       dtype=torch.float64)
	q_value: tensor([[-1.4387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 82
	action: tensor([[ 0.9799,  0.6125,  0.1848,  0.5224, -0.2538,  0.2011, -0.3541]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 83
	action: tensor([[ 0.6484,  0.2237, -0.3326,  1.1932, -0.4003,  0.5171, -0.0891]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7566474192088822, distance: 0.5645139408743285 entropy 0.03264415264129639
epoch: 74, step: 84
	action: tensor([[ 1.4087,  0.0936, -0.8044,  0.7834,  0.1420, -0.2405,  0.0712]],
       dtype=torch.float64)
	q_value: tensor([[-1.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9128617060376959, distance: 0.3378012246362936 entropy 0.03264415264129639
epoch: 74, step: 85
	action: tensor([[ 1.0255,  0.1009, -0.7147,  1.0420,  0.0309,  0.0760, -0.0740]],
       dtype=torch.float64)
	q_value: tensor([[-1.9333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9670564979751651, distance: 0.20770242902610306 entropy 0.03264415264129639
epoch: 74, step: 86
	action: tensor([[ 1.2252,  0.2076, -0.5075,  0.6736, -0.2526,  0.0150,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-1.5965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9773594468340552, distance: 0.17218694077775065 entropy 0.03264415264129639
epoch: 74, step: 87
	action: tensor([[ 0.8785,  0.1579, -0.3627,  0.4465, -0.4680, -0.0123,  0.0460]],
       dtype=torch.float64)
	q_value: tensor([[-1.8114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9705097897861459, distance: 0.19651496458635284 entropy 0.03264415264129639
epoch: 74, step: 88
	action: tensor([[ 0.7829,  0.2253, -0.3859,  0.9274, -0.2804,  0.2028, -0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-1.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9230312395839875, distance: 0.3174782219935541 entropy 0.03264415264129639
epoch: 74, step: 89
	action: tensor([[ 1.2521,  0.3559, -0.4790,  0.9482, -0.2246, -0.0175,  0.1514]],
       dtype=torch.float64)
	q_value: tensor([[-1.3706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9548452227453538, distance: 0.24316918846576477 entropy 0.03264415264129639
epoch: 74, step: 90
	action: tensor([[ 0.4712,  0.1785, -0.3607,  0.7137, -0.9069, -0.3129,  0.2501]],
       dtype=torch.float64)
	q_value: tensor([[-1.8899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9037613483890048, distance: 0.35500253807173576 entropy 0.03264415264129639
epoch: 74, step: 91
	action: tensor([[ 0.9691,  0.0394, -0.3374,  0.5347, -0.2657,  0.0260, -0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-0.8225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9579367956990719, distance: 0.23469717924247638 entropy 0.03264415264129639
epoch: 74, step: 92
	action: tensor([[ 0.9556,  0.2284, -0.4439,  0.6457, -0.3307,  0.0124,  0.2657]],
       dtype=torch.float64)
	q_value: tensor([[-1.1168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08380801376612297 entropy 0.03264415264129639
epoch: 74, step: 93
	action: tensor([[ 0.6224, -0.2200, -0.0443,  0.4168, -0.0616,  0.1359,  0.1130]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7765034824157255, distance: 0.5409935329192614 entropy 0.03264415264129639
epoch: 74, step: 94
	action: tensor([[ 0.8806, -0.1153, -0.2965,  0.9694, -0.3744,  0.2192, -0.1003]],
       dtype=torch.float64)
	q_value: tensor([[-0.3198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9742827462568981, distance: 0.1835139098773616 entropy 0.03264415264129639
epoch: 74, step: 95
	action: tensor([[ 0.8718, -0.0231, -0.4756,  0.6860,  0.0836,  0.0651, -0.2726]],
       dtype=torch.float64)
	q_value: tensor([[-1.2013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9514775432363005, distance: 0.25207401808108987 entropy 0.03264415264129639
epoch: 74, step: 96
	action: tensor([[ 0.8323,  0.1115, -0.5332,  0.8380, -0.2368,  0.3524, -0.4920]],
       dtype=torch.float64)
	q_value: tensor([[-1.1057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9346206746288204, distance: 0.292601744268276 entropy 0.03264415264129639
epoch: 74, step: 97
	action: tensor([[ 0.8636, -0.1660, -0.2794,  0.2560, -0.5004,  0.3407,  0.1467]],
       dtype=torch.float64)
	q_value: tensor([[-1.8478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.811605150457156, distance: 0.4966966216265155 entropy 0.03264415264129639
epoch: 74, step: 98
	action: tensor([[ 1.0347,  0.3890, -0.3543,  0.6794, -0.2129,  0.6370,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-0.9476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9376008348612797, distance: 0.28585519075410903 entropy 0.03264415264129639
epoch: 74, step: 99
	action: tensor([[ 1.2281,  0.4804, -0.1548,  0.8520, -0.2744,  0.4710,  0.1446]],
       dtype=torch.float64)
	q_value: tensor([[-1.8002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 100
	action: tensor([[ 1.0045,  0.0559, -0.3247,  0.2525,  0.0358, -0.2157,  0.3705]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8378347848753188, distance: 0.46082448301124856 entropy 0.03264415264129639
epoch: 74, step: 101
	action: tensor([[ 0.5647,  0.4253, -0.3802,  0.7999,  0.1250, -0.1115, -0.0487]],
       dtype=torch.float64)
	q_value: tensor([[-0.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8646003291354076, distance: 0.42108085645455906 entropy 0.03264415264129639
epoch: 74, step: 102
	action: tensor([[ 0.7732, -0.0415, -0.6951,  0.8862, -0.4760,  0.5013, -0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-0.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8366449706643118, distance: 0.4625119394521188 entropy 0.03264415264129639
epoch: 74, step: 103
	action: tensor([[ 1.0860,  0.2322, -0.2613,  0.4397, -0.2351, -0.0714, -0.2950]],
       dtype=torch.float64)
	q_value: tensor([[-1.5065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9582902773853826, distance: 0.2337089499212718 entropy 0.03264415264129639
epoch: 74, step: 104
	action: tensor([[ 1.4645,  0.0396, -0.8085,  0.8496, -0.3028, -0.1038,  0.2045]],
       dtype=torch.float64)
	q_value: tensor([[-1.6637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9228765754898366, distance: 0.3177970386081369 entropy 0.03264415264129639
epoch: 74, step: 105
	action: tensor([[ 1.0703,  0.3207, -0.3562,  0.6642, -0.6680, -0.3750, -0.1983]],
       dtype=torch.float64)
	q_value: tensor([[-2.0901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9746113477473157, distance: 0.1823377186853307 entropy 0.03264415264129639
epoch: 74, step: 106
	action: tensor([[ 0.9223,  0.5311, -0.1707,  0.6216, -0.4342, -0.0706, -0.4051]],
       dtype=torch.float64)
	q_value: tensor([[-1.8757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 107
	action: tensor([[ 0.4880,  0.3530, -0.6060,  0.6544, -0.0450, -0.0979,  0.3557]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8063601518053974, distance: 0.5035632873353767 entropy 0.03264415264129639
epoch: 74, step: 108
	action: tensor([[ 0.9996,  0.2767, -0.2726,  0.4411, -0.1350,  0.2552, -0.1506]],
       dtype=torch.float64)
	q_value: tensor([[-0.6887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08733593241895664 entropy 0.03264415264129639
epoch: 74, step: 109
	action: tensor([[ 0.7040,  0.2632, -0.2911,  0.8265, -0.3859,  0.1876, -0.1253]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9180138628823968, distance: 0.3276626245123304 entropy 0.03264415264129639
epoch: 74, step: 110
	action: tensor([[ 1.0662,  0.3567, -0.3495,  0.5871,  0.1193,  0.4385, -0.3093]],
       dtype=torch.float64)
	q_value: tensor([[-1.2374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9762788170551999, distance: 0.17624827079485972 entropy 0.03264415264129639
epoch: 74, step: 111
	action: tensor([[ 1.1764,  0.0190, -0.1179,  0.9291, -0.0892, -0.0877,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[-1.9240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9650887650125923, distance: 0.21381555629588092 entropy 0.03264415264129639
epoch: 74, step: 112
	action: tensor([[ 1.1253, -0.2269, -0.5026,  0.9487, -0.2931, -0.0666,  0.5264]],
       dtype=torch.float64)
	q_value: tensor([[-1.2621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9396830437345264, distance: 0.2810453487677463 entropy 0.03264415264129639
epoch: 74, step: 113
	action: tensor([[ 0.8931,  0.7057, -0.4584,  0.7205, -0.3560,  0.0893,  0.2987]],
       dtype=torch.float64)
	q_value: tensor([[-0.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 114
	action: tensor([[ 0.9293, -0.1460, -0.0056,  0.6057, -0.3668,  0.2983, -0.4133]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9381353807193179, distance: 0.2846281601896113 entropy 0.03264415264129639
epoch: 74, step: 115
	action: tensor([[ 1.2165,  0.3630, -0.6827,  0.5328, -0.8215,  0.0744,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-1.3587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9863064507031696, distance: 0.13391052356941974 entropy 0.03264415264129639
epoch: 74, step: 116
	action: tensor([[ 0.7131, -0.1457, -0.6898,  0.4991, -0.2257,  0.4826, -0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-2.3983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8060351136258943, distance: 0.5039857434241256 entropy 0.03264415264129639
epoch: 74, step: 117
	action: tensor([[ 1.1003,  0.1679, -0.2742,  0.8300, -0.2500, -0.1396,  0.1376]],
       dtype=torch.float64)
	q_value: tensor([[-1.0761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9909240217828748, distance: 0.10901930666821985 entropy 0.03264415264129639
epoch: 74, step: 118
	action: tensor([[ 0.9555,  0.3965, -0.8143,  0.9634, -0.3708,  0.0743,  0.2607]],
       dtype=torch.float64)
	q_value: tensor([[-1.2585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9021479298912225, distance: 0.3579659368066473 entropy 0.03264415264129639
epoch: 74, step: 119
	action: tensor([[ 1.0845,  0.0069, -0.0924,  0.8151, -0.5298,  0.5883,  0.2237]],
       dtype=torch.float64)
	q_value: tensor([[-1.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08293138007134364 entropy 0.03264415264129639
epoch: 74, step: 120
	action: tensor([[ 0.7940, -0.0961, -0.3104,  0.3549, -0.6172, -0.0733, -0.1920]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8267653871717184, distance: 0.4762928128509174 entropy 0.03264415264129639
epoch: 74, step: 121
	action: tensor([[ 1.2560,  0.2384, -0.4423,  0.8501, -0.1515,  0.4873, -0.2851]],
       dtype=torch.float64)
	q_value: tensor([[-1.0107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9805505873835504, distance: 0.15959156671365657 entropy 0.03264415264129639
epoch: 74, step: 122
	action: tensor([[ 1.3967,  0.2925, -0.5462,  0.9789, -0.3937, -0.0244,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[-2.4309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9656385676846853, distance: 0.2121252282150512 entropy 0.03264415264129639
epoch: 74, step: 123
	action: tensor([[ 1.5456, -0.2178, -0.5814,  1.1559, -0.1910,  0.3613, -0.3323]],
       dtype=torch.float64)
	q_value: tensor([[-2.3480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9541682370890805, distance: 0.24498527104514517 entropy 0.03264415264129639
epoch: 74, step: 124
	action: tensor([[ 1.8614,  0.1173, -0.1445,  1.0222, -0.6711,  0.0162,  0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-2.6486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 74, step: 125
	action: tensor([[ 0.1772,  0.2771,  0.0848,  0.5785, -0.6821,  0.2328, -0.1916]],
       dtype=torch.float64)
	q_value: tensor([[-2.9702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7554947279044173, distance: 0.5658493316299408 entropy 0.03264415264129639
epoch: 74, step: 126
	action: tensor([[ 0.9687,  0.0967, -0.4756,  0.8033, -0.0490,  0.0021,  0.2976]],
       dtype=torch.float64)
	q_value: tensor([[-0.8386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9868309878594104, distance: 0.13132073121719315 entropy 0.03264415264129639
epoch: 74, step: 127
	action: tensor([[ 1.0689,  0.0958, -0.4680,  0.2178,  0.0990, -0.5831,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[-0.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7573446145637479, distance: 0.5637047059860112 entropy 0.03264415264129639
LOSS epoch 74 actor 498.6845212950053 critic 2870.9499760896906 
epoch: 75, step: 0
	action: tensor([[ 1.3021,  0.2031, -0.0909,  1.0482, -0.1467, -0.4392, -0.0803]],
       dtype=torch.float64)
	q_value: tensor([[-1.1013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9139534669911245, distance: 0.33567838840814845 entropy 0.03264415264129639
epoch: 75, step: 1
	action: tensor([[ 1.4800,  0.2464, -0.7350,  1.5238, -0.2788, -0.0136, -0.0806]],
       dtype=torch.float64)
	q_value: tensor([[-1.6486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9355654015748138, distance: 0.2904800136934187 entropy 0.03264415264129639
epoch: 75, step: 2
	action: tensor([[ 1.6448,  0.2156, -0.3177,  0.4269, -0.0024, -0.0807,  0.1733]],
       dtype=torch.float64)
	q_value: tensor([[-2.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6825751841299675, distance: 0.6447288903853308 entropy 0.03264415264129639
epoch: 75, step: 3
	action: tensor([[ 1.4180,  0.4647, -0.7696,  1.3362, -0.3630,  0.0886,  0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-1.9593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9007744938582157, distance: 0.36046935937460944 entropy 0.03264415264129639
epoch: 75, step: 4
	action: tensor([[ 1.0228,  0.4132, -0.1666,  1.0883, -0.3864,  0.0968,  0.0865]],
       dtype=torch.float64)
	q_value: tensor([[-2.6941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 5
	action: tensor([[ 0.5314,  0.2244, -0.1482,  0.9568, -0.2187,  0.5834,  0.5265]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8331877768543576, distance: 0.4673805426129129 entropy 0.03264415264129639
epoch: 75, step: 6
	action: tensor([[ 1.1340, -0.2295, -0.8288,  0.7859, -0.0089,  0.0517, -0.0597]],
       dtype=torch.float64)
	q_value: tensor([[-0.7154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8710990608724083, distance: 0.4108513648233909 entropy 0.03264415264129639
epoch: 75, step: 7
	action: tensor([[ 1.6059, -0.0155, -0.7011,  0.9309, -0.1725,  0.4023,  0.1456]],
       dtype=torch.float64)
	q_value: tensor([[-1.4047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9434290410941764, distance: 0.27217827884116025 entropy 0.03264415264129639
epoch: 75, step: 8
	action: tensor([[ 0.9874,  0.3928, -0.5436,  1.1901, -0.0838,  0.5757,  0.1465]],
       dtype=torch.float64)
	q_value: tensor([[-2.3595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7702320325110987, distance: 0.5485313258086227 entropy 0.03264415264129639
epoch: 75, step: 9
	action: tensor([[ 1.2530,  0.3020, -0.4514,  0.8395, -0.4566,  0.7232, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[-1.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9570452331059801, distance: 0.23717143241536062 entropy 0.03264415264129639
epoch: 75, step: 10
	action: tensor([[ 1.2929, -0.0951, -0.2909,  0.8375, -0.6380,  0.2416,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-2.4213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9506922290596332, distance: 0.2541056830811057 entropy 0.03264415264129639
epoch: 75, step: 11
	action: tensor([[ 1.0004,  0.1352, -0.5445,  1.2310, -0.4271,  0.6807,  0.1609]],
       dtype=torch.float64)
	q_value: tensor([[-1.7962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8501221110472023, distance: 0.44302218728899856 entropy 0.03264415264129639
epoch: 75, step: 12
	action: tensor([[ 1.4026,  0.4928, -0.4699,  1.2317, -0.7017,  0.2498,  0.0332]],
       dtype=torch.float64)
	q_value: tensor([[-1.8309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8429588419448586, distance: 0.4534855340610826 entropy 0.03264415264129639
epoch: 75, step: 13
	action: tensor([[ 0.5124,  0.7426, -0.1298,  0.9338, -0.3516, -0.5352,  0.1352]],
       dtype=torch.float64)
	q_value: tensor([[-2.7761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 14
	action: tensor([[ 0.9695, -0.0171, -0.1921,  0.6526, -0.0920,  0.0561, -0.0248]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9662254493116316, distance: 0.21030591290700598 entropy 0.03264415264129639
epoch: 75, step: 15
	action: tensor([[ 1.4202, -0.0703, -0.4662,  0.4300, -0.5907,  0.2602,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-0.9143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7887086487361379, distance: 0.5260143032530443 entropy 0.03264415264129639
epoch: 75, step: 16
	action: tensor([[ 1.4672,  0.1945,  0.0828,  1.0416, -0.6033,  0.5332, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[-2.0747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8386529469222126, distance: 0.45966052853503186 entropy 0.03264415264129639
epoch: 75, step: 17
	action: tensor([[ 1.5571,  0.1443, -0.6205,  0.8648, -0.2069,  0.0293, -0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-2.3772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9176864496697539, distance: 0.3283162361008376 entropy 0.03264415264129639
epoch: 75, step: 18
	action: tensor([[ 0.9402, -0.0449, -0.4635,  1.3158, -0.0549, -0.3221, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-2.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9804212917867361, distance: 0.1601211535797787 entropy 0.03264415264129639
epoch: 75, step: 19
	action: tensor([[ 1.4106, -0.3985, -0.6381,  1.9182, -0.8779,  0.5041, -0.0504]],
       dtype=torch.float64)
	q_value: tensor([[-1.1470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8558074073864452, distance: 0.43453840765246776 entropy 0.03264415264129639
epoch: 75, step: 20
	action: tensor([[ 1.5840, -0.2119, -0.8216,  1.6084, -0.3973,  0.0133,  0.3722]],
       dtype=torch.float64)
	q_value: tensor([[-2.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9734324887404973, distance: 0.1865228874648855 entropy 0.03264415264129639
epoch: 75, step: 21
	action: tensor([[ 1.7221,  0.4943, -0.7625,  0.5442, -0.6126,  0.5086, -0.1056]],
       dtype=torch.float64)
	q_value: tensor([[-2.1217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 22
	action: tensor([[ 0.8198,  0.5067, -0.5714,  0.6178, -0.3605,  0.4733, -0.1388]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8757158312625645, distance: 0.4034266642191917 entropy 0.03264415264129639
epoch: 75, step: 23
	action: tensor([[ 1.1802,  0.3545, -0.2805,  0.5571, -0.7109, -0.0121,  0.2640]],
       dtype=torch.float64)
	q_value: tensor([[-1.9133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9604270766381419, distance: 0.22764376524856553 entropy 0.03264415264129639
epoch: 75, step: 24
	action: tensor([[ 0.6801, -0.0144, -0.2198,  0.7917, -0.6446,  0.0981, -0.2688]],
       dtype=torch.float64)
	q_value: tensor([[-1.6236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9494139182577607, distance: 0.25737846948946747 entropy 0.03264415264129639
epoch: 75, step: 25
	action: tensor([[ 0.9152,  0.3369, -0.3573,  1.0326, -0.0096,  0.1810, -0.1446]],
       dtype=torch.float64)
	q_value: tensor([[-1.1565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8952796105215156, distance: 0.37031586436428016 entropy 0.03264415264129639
epoch: 75, step: 26
	action: tensor([[ 1.2957,  0.3040, -0.3592,  1.0673, -0.2300,  0.2074, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-1.5051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9180729383317378, distance: 0.3275445539114868 entropy 0.03264415264129639
epoch: 75, step: 27
	action: tensor([[ 1.0692,  0.2736, -0.6059,  0.7347, -0.6195,  0.4193, -0.3677]],
       dtype=torch.float64)
	q_value: tensor([[-2.0663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9763606736544955, distance: 0.17594391093622194 entropy 0.03264415264129639
epoch: 75, step: 28
	action: tensor([[ 0.8261,  0.4050, -0.6068,  0.7732, -0.0254,  0.2215,  0.0666]],
       dtype=torch.float64)
	q_value: tensor([[-2.5340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9009044706953115, distance: 0.3602331901560416 entropy 0.03264415264129639
epoch: 75, step: 29
	action: tensor([[ 1.4876,  0.0937, -0.5484,  0.6758, -0.6435,  0.1699, -0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-1.3348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9083387471059223, distance: 0.3464571973638185 entropy 0.03264415264129639
epoch: 75, step: 30
	action: tensor([[ 1.3788,  0.2450, -0.4966,  0.9072, -0.1972,  0.2076, -0.0880]],
       dtype=torch.float64)
	q_value: tensor([[-2.5279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9686357361671398, distance: 0.20266289514816166 entropy 0.03264415264129639
epoch: 75, step: 31
	action: tensor([[ 1.0983,  0.5945, -0.5977,  1.1586, -0.3723, -0.2127,  0.4816]],
       dtype=torch.float64)
	q_value: tensor([[-2.2687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 32
	action: tensor([[ 0.4517, -0.1865, -0.2548,  0.0414, -0.4281,  0.4347, -0.1887]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5816456324567421, distance: 0.7401655147849427 entropy 0.03264415264129639
epoch: 75, step: 33
	action: tensor([[ 1.0920,  0.6085, -0.5622,  0.6973, -0.1748, -0.0341, -0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-0.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9355732055617098, distance: 0.2904624224387293 entropy 0.03264415264129639
epoch: 75, step: 34
	action: tensor([[ 1.3736, -0.0604, -0.3223,  0.8220, -0.6902,  0.1482,  0.1233]],
       dtype=torch.float64)
	q_value: tensor([[-1.9881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9206466381962221, distance: 0.32235867235644106 entropy 0.03264415264129639
epoch: 75, step: 35
	action: tensor([[ 1.2895, -0.2759, -0.5392,  0.7812, -0.2948,  0.1995,  0.3457]],
       dtype=torch.float64)
	q_value: tensor([[-1.8725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8788188367714549, distance: 0.39835864986909947 entropy 0.03264415264129639
epoch: 75, step: 36
	action: tensor([[ 1.3860,  0.7179, -0.7213,  0.8265, -0.5286,  0.0957,  0.4251]],
       dtype=torch.float64)
	q_value: tensor([[-1.3020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 37
	action: tensor([[ 1.2758,  0.5321, -0.7355,  0.1060, -0.5224,  0.3992,  0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.95400458309121, distance: 0.24542227248072335 entropy 0.03264415264129639
epoch: 75, step: 38
	action: tensor([[ 0.9610, -0.1916, -0.2326,  0.5849, -0.1913,  0.4058, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[-2.5598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9346050488492198, distance: 0.29263670835812 entropy 0.03264415264129639
epoch: 75, step: 39
	action: tensor([[ 1.4459,  0.0198, -0.2859,  1.3232, -0.7097,  0.3213, -0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-0.9959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9421177088445488, distance: 0.27531479478293885 entropy 0.03264415264129639
epoch: 75, step: 40
	action: tensor([[ 1.6620,  0.5520, -0.4748,  0.8145, -0.6484,  0.3801,  0.2454]],
       dtype=torch.float64)
	q_value: tensor([[-2.4515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 41
	action: tensor([[ 1.1872, -0.2011, -0.3618,  0.7128, -0.2529,  0.2211, -0.4223]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9148755586292117, distance: 0.33387494593719513 entropy 0.03264415264129639
epoch: 75, step: 42
	action: tensor([[ 1.3601, -0.1090, -0.0409,  0.9919, -0.4272,  0.1722,  0.4985]],
       dtype=torch.float64)
	q_value: tensor([[-1.8240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9082685294145996, distance: 0.34658987483937004 entropy 0.03264415264129639
epoch: 75, step: 43
	action: tensor([[ 1.3538,  0.5809, -0.4338,  0.7815, -0.1552,  0.0864,  0.3205]],
       dtype=torch.float64)
	q_value: tensor([[-1.2363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 44
	action: tensor([[ 1.0068,  0.4851, -0.4875,  0.7558, -0.1700,  0.3247, -0.1077]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9180603756621287, distance: 0.327569665737784 entropy 0.03264415264129639
epoch: 75, step: 45
	action: tensor([[ 1.6919, -0.0425, -0.6950,  1.1741, -0.4864,  0.2560,  0.3260]],
       dtype=torch.float64)
	q_value: tensor([[-1.9135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9693768888162234, distance: 0.20025406876622734 entropy 0.03264415264129639
epoch: 75, step: 46
	action: tensor([[ 1.6566,  0.3462, -0.2439,  0.8066, -0.5629,  0.5736,  0.0958]],
       dtype=torch.float64)
	q_value: tensor([[-2.2820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 47
	action: tensor([[ 1.0591, -0.0403, -0.1069,  0.5459,  0.0723,  0.2518,  0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9483749224839086, distance: 0.2600082041422102 entropy 0.03264415264129639
epoch: 75, step: 48
	action: tensor([[ 1.2942,  0.0981, -0.5423,  0.7745, -0.6547,  0.1454,  0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-0.8567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9805934311570773, distance: 0.15941569316167786 entropy 0.03264415264129639
epoch: 75, step: 49
	action: tensor([[ 1.1778,  0.5327, -0.2180,  0.5760, -0.5004,  0.1543, -0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-2.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9296622382566851, distance: 0.30349459805476964 entropy 0.03264415264129639
epoch: 75, step: 50
	action: tensor([[ 1.5031,  0.3531, -0.3926,  0.7581, -0.3436,  0.2169, -0.2399]],
       dtype=torch.float64)
	q_value: tensor([[-2.0574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9043491289932797, distance: 0.3539167831052292 entropy 0.03264415264129639
epoch: 75, step: 51
	action: tensor([[ 1.6104,  0.2780, -0.6168,  0.7233, -0.3942,  0.4995, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-2.7726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9428432398360803, distance: 0.27358387406037493 entropy 0.03264415264129639
epoch: 75, step: 52
	action: tensor([[ 1.5432, -0.2794, -0.4855,  0.8333, -0.3595,  0.3054,  0.5336]],
       dtype=torch.float64)
	q_value: tensor([[-2.9230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8175988796153606, distance: 0.48873162688904886 entropy 0.03264415264129639
epoch: 75, step: 53
	action: tensor([[ 1.3569,  0.3072, -0.5083,  0.9719, -0.5349,  0.1378, -0.0742]],
       dtype=torch.float64)
	q_value: tensor([[-1.6232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9627363133810772, distance: 0.22090197701078404 entropy 0.03264415264129639
epoch: 75, step: 54
	action: tensor([[ 1.4388,  0.5200, -0.8186,  1.0666, -0.4331,  0.0098, -0.2364]],
       dtype=torch.float64)
	q_value: tensor([[-2.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9490828979463447, distance: 0.25821920053211217 entropy 0.03264415264129639
epoch: 75, step: 55
	action: tensor([[ 1.4405,  0.3061, -1.0939,  0.8877, -0.7660,  0.1694, -0.2996]],
       dtype=torch.float64)
	q_value: tensor([[-3.1209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09738647108700785 entropy 0.03264415264129639
epoch: 75, step: 56
	action: tensor([[ 0.8460, -0.1078, -1.0550,  0.6697, -0.3646, -0.0092, -0.0689]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7879739259105771, distance: 0.5269280636220098 entropy 0.03264415264129639
epoch: 75, step: 57
	action: tensor([[ 1.2848, -0.0632, -0.1319,  0.7213, -0.1094,  0.3942, -0.2283]],
       dtype=torch.float64)
	q_value: tensor([[-1.3824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9325734291275386, distance: 0.2971476020418066 entropy 0.03264415264129639
epoch: 75, step: 58
	action: tensor([[ 1.5552,  0.1538, -0.0451,  0.9744, -0.0463,  0.4586, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[-1.7437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8128813121925845, distance: 0.49501148451946414 entropy 0.03264415264129639
epoch: 75, step: 59
	action: tensor([[ 1.1018,  0.1047, -0.4083,  1.4246, -0.6226, -0.2528, -0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-2.2072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9484575417728176, distance: 0.2598000660092931 entropy 0.03264415264129639
epoch: 75, step: 60
	action: tensor([[ 1.3644,  0.5528, -0.5833,  1.0234, -0.1351,  0.0872,  0.3802]],
       dtype=torch.float64)
	q_value: tensor([[-1.8980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.888530073351659, distance: 0.38206350096746083 entropy 0.03264415264129639
epoch: 75, step: 61
	action: tensor([[ 1.0421,  0.0555, -0.8611,  0.5742, -0.1092,  0.2110,  0.3211]],
       dtype=torch.float64)
	q_value: tensor([[-2.0726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9444978090783664, distance: 0.26959495245877146 entropy 0.03264415264129639
epoch: 75, step: 62
	action: tensor([[ 0.8385,  0.3260, -0.4061,  0.7238, -0.4969,  0.3437,  0.3052]],
       dtype=torch.float64)
	q_value: tensor([[-1.3006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9367735687460742, distance: 0.2877438352500722 entropy 0.03264415264129639
epoch: 75, step: 63
	action: tensor([[ 0.7256,  0.2485, -0.8637,  0.8526, -0.2066,  0.3205, -0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-1.2555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8047400488299643, distance: 0.505665450382842 entropy 0.03264415264129639
epoch: 75, step: 64
	action: tensor([[ 1.3963,  0.1580, -0.4076,  1.3281, -0.2311,  0.2723,  0.6057]],
       dtype=torch.float64)
	q_value: tensor([[-1.5075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9217103307116531, distance: 0.3201908540366364 entropy 0.03264415264129639
epoch: 75, step: 65
	action: tensor([[ 1.2097,  0.1201, -0.2436,  0.9635, -0.4407,  0.3353,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-1.6064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9743158172048216, distance: 0.18339587761457238 entropy 0.03264415264129639
epoch: 75, step: 66
	action: tensor([[ 2.0107, -0.0592, -0.2816,  0.9815, -0.3426, -0.4381,  0.1246]],
       dtype=torch.float64)
	q_value: tensor([[-1.5745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 67
	action: tensor([[ 0.6501,  0.1444, -0.1984,  0.7623, -0.2136,  0.3910,  0.3722]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9261586260948442, distance: 0.3109614678075788 entropy 0.03264415264129639
epoch: 75, step: 68
	action: tensor([[ 1.0488,  0.4903, -0.3241,  0.6926, -0.1834,  0.1544,  0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-0.6638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9413470593790926, distance: 0.2771415162349479 entropy 0.03264415264129639
epoch: 75, step: 69
	action: tensor([[ 1.2839,  0.0373, -0.0293,  0.8620, -0.2457,  0.0624,  0.3517]],
       dtype=torch.float64)
	q_value: tensor([[-1.5153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9196490089129085, distance: 0.32437868750152726 entropy 0.03264415264129639
epoch: 75, step: 70
	action: tensor([[ 1.5446, -0.2209, -0.7698,  0.6767, -0.0433,  0.5487,  0.1691]],
       dtype=torch.float64)
	q_value: tensor([[-1.1756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8435071583872978, distance: 0.45269315897956475 entropy 0.03264415264129639
epoch: 75, step: 71
	action: tensor([[ 1.0097,  0.6369, -0.0942,  0.6206, -0.2364,  0.0579, -0.1942]],
       dtype=torch.float64)
	q_value: tensor([[-2.0689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 72
	action: tensor([[ 1.0880, -0.0699, -0.1598,  0.3163, -0.1582, -0.1636, -0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7698067527277186, distance: 0.5490387321303565 entropy 0.03264415264129639
epoch: 75, step: 73
	action: tensor([[ 1.4733,  0.4983, -0.3195,  0.6177, -0.3461,  0.2283,  0.0926]],
       dtype=torch.float64)
	q_value: tensor([[-0.9816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8907424691723325, distance: 0.3782530024362181 entropy 0.03264415264129639
epoch: 75, step: 74
	action: tensor([[ 0.9209,  0.3514, -0.3871,  1.3858, -0.0547,  0.0059, -0.3105]],
       dtype=torch.float64)
	q_value: tensor([[-2.4161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7992777817125237, distance: 0.5126894931128813 entropy 0.03264415264129639
epoch: 75, step: 75
	action: tensor([[ 1.6189,  0.1858, -0.9169,  1.2278, -0.3755,  0.0046,  0.1664]],
       dtype=torch.float64)
	q_value: tensor([[-1.8462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.990631031011675, distance: 0.11076501074880661 entropy 0.03264415264129639
epoch: 75, step: 76
	action: tensor([[ 1.5862,  0.4398, -0.4131,  1.1998, -0.1420,  0.1053,  0.3995]],
       dtype=torch.float64)
	q_value: tensor([[-2.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8554869229097894, distance: 0.43502104519870277 entropy 0.03264415264129639
epoch: 75, step: 77
	action: tensor([[ 1.6635,  0.1865, -0.6107,  0.6927, -0.4314, -0.0890, -0.4283]],
       dtype=torch.float64)
	q_value: tensor([[-2.2234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 78
	action: tensor([[ 0.5502,  0.1556, -0.4740,  0.2007,  0.0594,  0.0200,  0.2475]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8251067784104117, distance: 0.47856747818281425 entropy 0.03264415264129639
epoch: 75, step: 79
	action: tensor([[ 0.9179,  0.1551, -0.6586,  0.4637, -0.1723,  0.1289,  0.1899]],
       dtype=torch.float64)
	q_value: tensor([[-0.4807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9511860991543049, distance: 0.2528299103053857 entropy 0.03264415264129639
epoch: 75, step: 80
	action: tensor([[ 1.2934, -0.1570, -0.3536,  0.9741, -0.2396,  0.1671, -0.1642]],
       dtype=torch.float64)
	q_value: tensor([[-1.1215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.961981842709409, distance: 0.22312705063896882 entropy 0.03264415264129639
epoch: 75, step: 81
	action: tensor([[ 1.2966e+00,  3.0967e-04, -4.3830e-01,  7.5834e-01, -8.7226e-01,
          3.1181e-01, -7.1101e-02]], dtype=torch.float64)
	q_value: tensor([[-1.7741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9705724720326958, distance: 0.19630600452980845 entropy 0.03264415264129639
epoch: 75, step: 82
	action: tensor([[ 1.3922,  0.3048, -0.6586,  0.2263, -0.6794,  0.4330,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[-2.2430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9082463086136284, distance: 0.34663185082774406 entropy 0.03264415264129639
epoch: 75, step: 83
	action: tensor([[ 1.3847, -0.1760, -0.7878,  0.8295, -0.0463,  0.1067,  0.0435]],
       dtype=torch.float64)
	q_value: tensor([[-2.6234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8937318871410446, distance: 0.37304238359541547 entropy 0.03264415264129639
epoch: 75, step: 84
	action: tensor([[ 1.2460,  0.0165, -0.4407,  1.3428, -0.1129,  0.3661,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-1.7815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9451844131487374, distance: 0.2679222163975509 entropy 0.03264415264129639
epoch: 75, step: 85
	action: tensor([[ 1.6621, -0.0774, -0.1777,  1.0654, -0.4364,  0.0364,  0.1989]],
       dtype=torch.float64)
	q_value: tensor([[-1.7871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7956757899415349, distance: 0.517269185432292 entropy 0.03264415264129639
epoch: 75, step: 86
	action: tensor([[ 1.5919,  0.4953, -0.2155,  0.6370, -0.4633,  0.7627,  0.1683]],
       dtype=torch.float64)
	q_value: tensor([[-1.9447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8436141344326207, distance: 0.4525384055663659 entropy 0.03264415264129639
epoch: 75, step: 87
	action: tensor([[ 0.9557, -0.0471, -0.4153,  1.1990,  0.1724,  0.5720,  0.1694]],
       dtype=torch.float64)
	q_value: tensor([[-2.8219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9418577283700902, distance: 0.2759323954120709 entropy 0.03264415264129639
epoch: 75, step: 88
	action: tensor([[ 1.7871,  0.5600, -0.6556,  0.8908, -0.6610, -0.3609,  0.3574]],
       dtype=torch.float64)
	q_value: tensor([[-1.1678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 89
	action: tensor([[ 0.7674, -0.1791, -0.1431,  0.6494, -0.0332,  0.0563, -0.2697]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8935128057225313, distance: 0.37342671608231487 entropy 0.03264415264129639
epoch: 75, step: 90
	action: tensor([[ 1.2769, -0.0027, -0.3701,  0.8940, -0.4409,  0.0168,  0.1065]],
       dtype=torch.float64)
	q_value: tensor([[-0.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9681926415366027, distance: 0.20408942163086094 entropy 0.03264415264129639
epoch: 75, step: 91
	action: tensor([[ 1.1097,  0.2303, -0.5180,  0.3095, -0.4905,  0.3215, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-1.6080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9726229422560422, distance: 0.18934335983577114 entropy 0.03264415264129639
epoch: 75, step: 92
	action: tensor([[ 1.2622,  0.1965, -0.0401,  0.8179,  0.0348, -0.5218,  0.0970]],
       dtype=torch.float64)
	q_value: tensor([[-1.8314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9025278885203214, distance: 0.3572702717226934 entropy 0.03264415264129639
epoch: 75, step: 93
	action: tensor([[ 1.2946,  0.1154, -0.5577,  0.9286, -0.2845, -0.0795, -0.1345]],
       dtype=torch.float64)
	q_value: tensor([[-1.2268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9848035839603902, distance: 0.1410676009860063 entropy 0.03264415264129639
epoch: 75, step: 94
	action: tensor([[ 1.7607,  0.1596, -0.5808,  0.8268, -0.5413, -0.2181,  0.4049]],
       dtype=torch.float64)
	q_value: tensor([[-2.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 95
	action: tensor([[ 1.0164,  0.0678, -0.8299,  0.7788, -0.1185, -0.0216,  0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9494935804780084, distance: 0.25717573171981656 entropy 0.03264415264129639
epoch: 75, step: 96
	action: tensor([[ 1.1770,  0.2080, -0.4823,  1.0489, -0.4112, -0.4311, -0.2492]],
       dtype=torch.float64)
	q_value: tensor([[-1.3586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9908543422171633, distance: 0.1094369968251947 entropy 0.03264415264129639
epoch: 75, step: 97
	action: tensor([[ 1.6154,  0.2171, -0.4039,  0.8658, -0.4553,  0.3229,  0.2937]],
       dtype=torch.float64)
	q_value: tensor([[-2.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9121542818474345, distance: 0.3391696568969228 entropy 0.03264415264129639
epoch: 75, step: 98
	action: tensor([[ 1.2476, -0.4724, -1.0246,  1.5118, -0.5497,  0.0252, -0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-2.3412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8495099629218434, distance: 0.44392598589434323 entropy 0.03264415264129639
epoch: 75, step: 99
	action: tensor([[ 1.9697,  0.2372, -0.1325,  1.4637, -0.4207,  0.1651,  0.4867]],
       dtype=torch.float64)
	q_value: tensor([[-2.1028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 100
	action: tensor([[ 0.8263,  0.5570, -0.0382,  0.5792, -0.1140,  0.4581,  0.1730]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 101
	action: tensor([[ 0.5108, -0.0359, -0.2994,  0.8765, -0.2281, -0.0562, -0.1846]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8511940099722792, distance: 0.44143513827868347 entropy 0.03264415264129639
epoch: 75, step: 102
	action: tensor([[ 1.0340,  0.2754, -0.8389,  0.4842, -0.3704, -0.1245,  0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-0.6511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9765817929483793, distance: 0.1751190989692986 entropy 0.03264415264129639
epoch: 75, step: 103
	action: tensor([[ 1.1104,  0.6566, -0.5035,  0.7834, -0.3598,  0.1581,  0.6236]],
       dtype=torch.float64)
	q_value: tensor([[-1.6614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 104
	action: tensor([[ 0.8634, -0.1417, -0.3435,  0.6163, -0.5118,  0.3012, -0.0300]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9135736653955966, distance: 0.3364183997080881 entropy 0.03264415264129639
epoch: 75, step: 105
	action: tensor([[ 1.2114, -0.0129, -0.4362,  0.6875, -0.2264,  0.1279,  0.0378]],
       dtype=torch.float64)
	q_value: tensor([[-1.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9544627364531943, distance: 0.24419690643211808 entropy 0.03264415264129639
epoch: 75, step: 106
	action: tensor([[ 1.2698, -0.1800, -0.0981,  0.9294, -0.2725,  0.0578, -0.0583]],
       dtype=torch.float64)
	q_value: tensor([[-1.4798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9078597670319294, distance: 0.3473612318584766 entropy 0.03264415264129639
epoch: 75, step: 107
	action: tensor([[ 1.6905,  0.1424, -0.7649,  1.1232, -0.3454, -0.1551,  0.2897]],
       dtype=torch.float64)
	q_value: tensor([[-1.4287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 108
	action: tensor([[ 0.5287, -0.1696, -0.2936,  0.6172, -0.1863,  0.2699, -0.4347]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8013144100473889, distance: 0.5100818592439796 entropy 0.03264415264129639
epoch: 75, step: 109
	action: tensor([[ 0.9469,  0.4074, -0.3706,  0.8813, -0.4614, -0.1354,  0.2204]],
       dtype=torch.float64)
	q_value: tensor([[-0.8057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.951007217204003, distance: 0.25329274310818906 entropy 0.03264415264129639
epoch: 75, step: 110
	action: tensor([[ 1.0459, -0.1212, -0.4858,  0.6872, -0.3633,  0.2097,  0.0769]],
       dtype=torch.float64)
	q_value: tensor([[-1.3235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9482703242977495, distance: 0.2602714737202398 entropy 0.03264415264129639
epoch: 75, step: 111
	action: tensor([[ 1.6266,  0.4699, -0.5087,  1.2215,  0.0395,  0.1074,  0.1706]],
       dtype=torch.float64)
	q_value: tensor([[-1.2164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8647568105463614, distance: 0.4208374645216817 entropy 0.03264415264129639
epoch: 75, step: 112
	action: tensor([[ 0.9448, -0.3106, -0.4844,  0.9187,  0.3185, -0.0989,  0.5434]],
       dtype=torch.float64)
	q_value: tensor([[-2.5251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8794335466339307, distance: 0.39734699790981703 entropy 0.03264415264129639
epoch: 75, step: 113
	action: tensor([[ 1.6772,  0.2179, -0.4776,  0.6390, -0.4567,  0.1741,  0.3171]],
       dtype=torch.float64)
	q_value: tensor([[-0.5314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8605024265306329, distance: 0.427405414738064 entropy 0.03264415264129639
epoch: 75, step: 114
	action: tensor([[ 1.2939,  0.6477, -0.2461,  1.0000, -0.3931,  0.5915,  0.6561]],
       dtype=torch.float64)
	q_value: tensor([[-2.2622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 75, step: 115
	action: tensor([[ 0.5315,  0.1183, -0.1995,  0.7012, -0.0445,  0.0474,  0.2045]],
       dtype=torch.float64)
	q_value: tensor([[-2.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8958206931609021, distance: 0.36935792778977056 entropy 0.03264415264129639
epoch: 75, step: 116
	action: tensor([[ 0.9668,  0.1305, -0.2369,  0.6874, -0.3005, -0.1758,  0.1250]],
       dtype=torch.float64)
	q_value: tensor([[-0.4079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9851232130851948, distance: 0.1395761659843819 entropy 0.03264415264129639
epoch: 75, step: 117
	action: tensor([[ 1.4841,  0.0880, -0.3076,  0.5364, -0.2100,  0.5023,  0.0770]],
       dtype=torch.float64)
	q_value: tensor([[-0.9484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8887741699404106, distance: 0.3816449508194245 entropy 0.03264415264129639
epoch: 75, step: 118
	action: tensor([[ 1.5208,  0.2645, -0.7353,  0.6316, -0.4877, -0.2460,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[-2.0985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8581891052570234, distance: 0.4309347256217597 entropy 0.03264415264129639
epoch: 75, step: 119
	action: tensor([[ 1.3389, -0.0571, -0.3175,  0.7925, -0.3985, -0.0996, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-2.4736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8839972975764903, distance: 0.3897541486638901 entropy 0.03264415264129639
epoch: 75, step: 120
	action: tensor([[ 0.9889,  0.4151, -0.8112,  0.1219, -0.2512, -0.4934, -0.3851]],
       dtype=torch.float64)
	q_value: tensor([[-1.6695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9331863991053327, distance: 0.29579384483447047 entropy 0.03264415264129639
epoch: 75, step: 121
	action: tensor([[ 1.2414,  0.6745, -0.9596,  0.5921, -0.3083, -0.0230, -0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-1.9401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9656184409820021, distance: 0.21218734373401313 entropy 0.03264415264129639
epoch: 75, step: 122
	action: tensor([[ 1.3347,  0.3995, -0.2046,  0.6815, -0.1280,  0.5170, -0.2834]],
       dtype=torch.float64)
	q_value: tensor([[-2.6928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9118408457638466, distance: 0.33977420173099016 entropy 0.03264415264129639
epoch: 75, step: 123
	action: tensor([[ 1.3510,  0.5221, -0.5944,  0.6893, -0.4648,  0.0248,  0.1016]],
       dtype=torch.float64)
	q_value: tensor([[-2.4660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9396132648638915, distance: 0.2812078682288964 entropy 0.03264415264129639
epoch: 75, step: 124
	action: tensor([[ 1.2050, -0.0287, -0.1673,  0.5335,  0.1000,  0.7035,  0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-2.3905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9439444969716759, distance: 0.2709354420302139 entropy 0.03264415264129639
epoch: 75, step: 125
	action: tensor([[ 1.3475,  0.0765, -0.0893,  0.9872, -0.3489,  0.1401, -0.1231]],
       dtype=torch.float64)
	q_value: tensor([[-1.4457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9050309729881125, distance: 0.35265308511872523 entropy 0.03264415264129639
epoch: 75, step: 126
	action: tensor([[ 1.3504,  0.7066, -0.4261,  0.9418, -0.1931,  0.6741, -0.1513]],
       dtype=torch.float64)
	q_value: tensor([[-1.9311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7429535008587935, distance: 0.5801797144403754 entropy 0.03264415264129639
epoch: 75, step: 127
	action: tensor([[ 1.5242,  0.7465, -0.8352,  1.4296, -0.5206,  0.2061, -0.3710]],
       dtype=torch.float64)
	q_value: tensor([[-2.9726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7927247936097529, distance: 0.5209911803901723 entropy 0.03264415264129639
LOSS epoch 75 actor 205.4196884966906 critic 1738.47779493578 
epoch: 76, step: 0
	action: tensor([[ 1.8887,  0.6331, -0.8191,  1.6943, -0.6376, -0.1963,  0.2317]],
       dtype=torch.float64)
	q_value: tensor([[-3.9972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 1
	action: tensor([[ 1.0799,  0.4144, -0.4190,  0.8261, -0.2030, -0.0284, -0.1535]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9591257096918775, distance: 0.23135655341557426 entropy 0.03264415264129639
epoch: 76, step: 2
	action: tensor([[ 1.6190,  0.5638, -0.1322,  0.8956,  0.0589,  0.6576,  0.1721]],
       dtype=torch.float64)
	q_value: tensor([[-1.8927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 3
	action: tensor([[ 1.1285, -0.2842, -0.1203,  0.2215, -0.0167, -0.1045, -0.0422]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5545299570408371, distance: 0.7637758904408282 entropy 0.03264415264129639
epoch: 76, step: 4
	action: tensor([[ 1.5942, -0.0099, -0.8135,  1.0804, -0.2328,  0.0252, -0.0430]],
       dtype=torch.float64)
	q_value: tensor([[-0.8487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9466080106204664, distance: 0.2644202702306839 entropy 0.03264415264129639
epoch: 76, step: 5
	action: tensor([[ 1.9142,  0.1238, -0.4649,  1.3518, -0.4477,  0.5162,  0.4495]],
       dtype=torch.float64)
	q_value: tensor([[-2.5972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 6
	action: tensor([[ 1.1878,  0.0800, -0.4233,  0.9031, -0.1635, -0.1477, -0.3811]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9866961649300409, distance: 0.1319912431883001 entropy 0.03264415264129639
epoch: 76, step: 7
	action: tensor([[ 1.5687,  0.6651, -0.1875,  1.4831, -0.7954,  0.2058, -0.0676]],
       dtype=torch.float64)
	q_value: tensor([[-1.9694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6297271786298555, distance: 0.6963340158177425 entropy 0.03264415264129639
epoch: 76, step: 8
	action: tensor([[ 1.7551,  0.6472, -0.8581,  0.8934,  0.0037, -0.0120, -0.0949]],
       dtype=torch.float64)
	q_value: tensor([[-3.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 9
	action: tensor([[ 1.0474,  0.7158, -0.5206,  0.1670, -0.4312, -0.0809, -0.3085]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9624494943574617, distance: 0.22175048991346683 entropy 0.03264415264129639
epoch: 76, step: 10
	action: tensor([[ 1.8279, -0.1881, -0.3090,  0.7496, -0.7813,  0.3119,  0.0585]],
       dtype=torch.float64)
	q_value: tensor([[-2.3607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 11
	action: tensor([[ 0.9398, -0.0129, -0.4089,  0.7516, -0.7413,  0.3269, -0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.969809689645351, distance: 0.19883392337146855 entropy 0.03264415264129639
epoch: 76, step: 12
	action: tensor([[ 1.6540,  0.8009, -0.6774,  0.6682, -0.2416,  0.1385,  0.1096]],
       dtype=torch.float64)
	q_value: tensor([[-1.6015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 13
	action: tensor([[ 1.3045,  0.1974, -0.0616,  0.7457, -0.8344,  0.5612,  0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9646522544527356, distance: 0.21514811890609514 entropy 0.03264415264129639
epoch: 76, step: 14
	action: tensor([[ 1.3048,  0.1552, -0.4846,  0.8124, -0.4250,  0.0037,  0.0218]],
       dtype=torch.float64)
	q_value: tensor([[-2.1212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9725124642797727, distance: 0.18972501535134612 entropy 0.03264415264129639
epoch: 76, step: 15
	action: tensor([[ 1.6244,  0.3378, -0.4425,  0.9294, -0.0358,  0.3206, -0.1096]],
       dtype=torch.float64)
	q_value: tensor([[-1.9805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.892010655285053, distance: 0.3760513448859258 entropy 0.03264415264129639
epoch: 76, step: 16
	action: tensor([[ 1.7593,  0.4406, -0.3193,  0.8081, -0.4310,  0.4444,  0.3020]],
       dtype=torch.float64)
	q_value: tensor([[-2.8054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 17
	action: tensor([[ 1.4828,  0.1236, -0.3089,  0.6674, -0.1010,  0.3524,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8956722674423495, distance: 0.36962094884884983 entropy 0.03264415264129639
epoch: 76, step: 18
	action: tensor([[ 1.5197,  0.2887, -0.8650,  1.0636, -0.1324, -0.2067, -0.1974]],
       dtype=torch.float64)
	q_value: tensor([[-2.1086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9801218813415258, distance: 0.1613408470264309 entropy 0.03264415264129639
epoch: 76, step: 19
	action: tensor([[ 1.9288,  0.0189, -0.6511,  1.0107, -0.3986,  0.5403,  0.2320]],
       dtype=torch.float64)
	q_value: tensor([[-2.8407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 20
	action: tensor([[ 1.1518,  0.2560, -0.6952,  0.2907, -0.0689,  0.3606, -0.1389]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9702647293446799, distance: 0.19732978432048573 entropy 0.03264415264129639
epoch: 76, step: 21
	action: tensor([[ 1.7981,  0.6393, -0.4946,  1.0698, -0.3035,  0.1143, -0.1335]],
       dtype=torch.float64)
	q_value: tensor([[-2.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 22
	action: tensor([[ 1.0728,  0.2840, -0.1147,  0.7246,  0.0130, -0.2648, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9903387858625773, distance: 0.11247928509765685 entropy 0.03264415264129639
epoch: 76, step: 23
	action: tensor([[ 1.7934,  0.0329, -0.4664,  1.0213, -0.2062,  0.5297, -0.3918]],
       dtype=torch.float64)
	q_value: tensor([[-1.2049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 24
	action: tensor([[ 0.6834,  0.2085, -0.5154,  0.5760, -0.1634,  0.0939, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9160256256153437, distance: 0.3316118807691011 entropy 0.03264415264129639
epoch: 76, step: 25
	action: tensor([[ 1.3740,  0.5929, -0.4551,  0.9709, -0.0660, -0.1239,  0.5157]],
       dtype=torch.float64)
	q_value: tensor([[-0.9425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 26
	action: tensor([[ 0.7477,  0.0922, -0.2948,  0.7484, -0.6303,  0.0947, -0.1464]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9568182671001995, distance: 0.23779719454586307 entropy 0.03264415264129639
epoch: 76, step: 27
	action: tensor([[ 1.7635,  0.3726, -0.7362,  1.0061, -0.6911, -0.0540,  0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-1.2564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9490412483265784, distance: 0.2583247891523536 entropy 0.03264415264129639
epoch: 76, step: 28
	action: tensor([[ 1.9286,  0.1876, -0.8378,  0.5895, -0.1157,  0.1305,  0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-2.8854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 29
	action: tensor([[ 0.7859,  0.4608, -0.3921,  1.2410, -0.3221,  0.1757,  0.4554]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7195816515004431, distance: 0.6059822549964626 entropy 0.03264415264129639
epoch: 76, step: 30
	action: tensor([[ 1.5694,  0.1333, -0.9126,  1.1792, -0.3600,  0.4647, -0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-1.2458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9847152899048075, distance: 0.14147682217462032 entropy 0.03264415264129639
epoch: 76, step: 31
	action: tensor([[ 1.8087,  0.1334, -0.5240,  1.2597, -0.4479, -0.0443,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[-3.3036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 32
	action: tensor([[ 1.1005,  0.2004, -0.2236,  0.7603, -0.1814,  0.2238, -0.1621]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9896811629618834, distance: 0.11624440802605704 entropy 0.03264415264129639
epoch: 76, step: 33
	action: tensor([[ 1.6592,  0.3116, -1.0949,  1.5179, -0.4912,  0.3389, -0.1777]],
       dtype=torch.float64)
	q_value: tensor([[-1.6740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9179232882404981, distance: 0.3278435681067798 entropy 0.03264415264129639
epoch: 76, step: 34
	action: tensor([[ 1.8367,  0.6415, -1.0601,  1.2617, -0.1782,  0.2673, -0.1857]],
       dtype=torch.float64)
	q_value: tensor([[-3.7222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 35
	action: tensor([[ 0.8310,  0.3727, -0.3425,  0.5917, -0.2845, -0.4261,  0.1950]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9893330105827874, distance: 0.11818915438362149 entropy 0.03264415264129639
epoch: 76, step: 36
	action: tensor([[ 1.0829,  0.6232, -0.8149,  1.0310, -0.0587, -0.1047,  0.3990]],
       dtype=torch.float64)
	q_value: tensor([[-0.9688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8714162407785682, distance: 0.4103455730847269 entropy 0.03264415264129639
epoch: 76, step: 37
	action: tensor([[ 1.6402,  0.3921, -0.7460,  0.8100, -0.4649,  0.0466, -0.0234]],
       dtype=torch.float64)
	q_value: tensor([[-1.8694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9308589607935983, distance: 0.3009017022854038 entropy 0.03264415264129639
epoch: 76, step: 38
	action: tensor([[ 1.9089,  0.6352, -0.5304,  1.0890, -0.8440,  0.3360,  0.1899]],
       dtype=torch.float64)
	q_value: tensor([[-3.0561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 39
	action: tensor([[ 1.2526,  0.0559, -0.3523,  0.5679, -0.8299,  0.5100, -0.3662]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9727747230011494, distance: 0.1888177624592024 entropy 0.03264415264129639
epoch: 76, step: 40
	action: tensor([[ 1.8796, -0.1466, -0.6246,  1.1326, -0.5237,  0.2065, -0.2412]],
       dtype=torch.float64)
	q_value: tensor([[-2.6570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 41
	action: tensor([[ 1.3623,  0.1493, -0.2004,  0.3124, -0.1431,  0.1597,  0.1380]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8361928946775059, distance: 0.4631514853555807 entropy 0.03264415264129639
epoch: 76, step: 42
	action: tensor([[ 1.2335,  0.3864, -0.3926,  0.7285, -0.4891,  0.2512,  0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-1.6302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9672017278496773, distance: 0.20724410015803618 entropy 0.03264415264129639
epoch: 76, step: 43
	action: tensor([[ 1.7315, -0.0371, -0.6666,  0.8633, -0.7148,  0.6771,  0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-2.0759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 44
	action: tensor([[ 0.7286,  0.1039, -0.3536,  0.5851, -0.3636,  0.0130,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9354302939212871, distance: 0.2907843961414636 entropy 0.03264415264129639
epoch: 76, step: 45
	action: tensor([[ 1.4776, -0.1687, -0.0592,  0.9363, -0.4184,  0.6009, -0.1381]],
       dtype=torch.float64)
	q_value: tensor([[-0.7125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8860903841483223, distance: 0.3862218918947391 entropy 0.03264415264129639
epoch: 76, step: 46
	action: tensor([[ 1.6262,  0.6151, -0.7899,  1.7135, -0.4166,  0.1498,  0.3631]],
       dtype=torch.float64)
	q_value: tensor([[-2.2551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7680240679703023, distance: 0.5511605913402434 entropy 0.03264415264129639
epoch: 76, step: 47
	action: tensor([[ 2.0552,  0.0196, -0.4431,  1.3013, -0.8076, -0.2729,  0.1435]],
       dtype=torch.float64)
	q_value: tensor([[-3.1765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 48
	action: tensor([[ 1.2078, -0.0376, -0.3929,  0.7218, -0.4283,  0.2431, -0.3443]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9531826757451395, distance: 0.24760532917808126 entropy 0.03264415264129639
epoch: 76, step: 49
	action: tensor([[ 1.9633,  0.2112, -0.8611,  1.4332, -0.4133, -0.0787, -0.1449]],
       dtype=torch.float64)
	q_value: tensor([[-2.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 50
	action: tensor([[ 1.1253, -0.3822, -0.0846,  0.6703, -0.4498, -0.2074,  0.1259]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7037432758720483, distance: 0.6228605162959888 entropy 0.03264415264129639
epoch: 76, step: 51
	action: tensor([[ 1.7899,  0.2214, -0.6479,  1.0417, -0.8481,  0.3384,  0.3906]],
       dtype=torch.float64)
	q_value: tensor([[-0.9135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 52
	action: tensor([[ 0.3971,  0.5613, -0.4907,  0.5845, -0.0114,  0.6442,  0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6731633641381374, distance: 0.6542173523466134 entropy 0.03264415264129639
epoch: 76, step: 53
	action: tensor([[ 1.5890, -0.1831, -0.5166,  0.6122, -0.5742,  0.6096, -0.3613]],
       dtype=torch.float64)
	q_value: tensor([[-1.1905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.824508231104605, distance: 0.4793856934657906 entropy 0.03264415264129639
epoch: 76, step: 54
	action: tensor([[ 1.6910,  0.4409, -0.5368,  1.3235, -0.3318,  0.2590, -0.1142]],
       dtype=torch.float64)
	q_value: tensor([[-3.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 55
	action: tensor([[ 0.7618,  0.2741, -0.5330,  0.3983,  0.0351, -0.3863, -0.1827]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9367880535634493, distance: 0.2877108731204642 entropy 0.03264415264129639
epoch: 76, step: 56
	action: tensor([[ 1.3202,  0.2937, -0.7679,  1.2489, -0.5082, -0.1511, -0.4562]],
       dtype=torch.float64)
	q_value: tensor([[-1.0337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9802219367543269, distance: 0.16093428465575482 entropy 0.03264415264129639
epoch: 76, step: 57
	action: tensor([[ 2.1182,  0.1438, -0.7165,  1.7593, -0.6519, -0.0088,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[-3.0198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 58
	action: tensor([[ 0.6503,  0.0010, -0.3577,  0.1720,  0.0151,  0.5387,  0.1208]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8544252808970368, distance: 0.4366160272120887 entropy 0.03264415264129639
epoch: 76, step: 59
	action: tensor([[ 1.0774,  0.4606, -0.1154,  0.9323, -0.4569,  0.2141,  0.2601]],
       dtype=torch.float64)
	q_value: tensor([[-0.7225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 60
	action: tensor([[ 1.2805,  0.1875, -0.7248,  0.3606, -0.1163, -0.2091, -0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8508108183516878, distance: 0.44200314460802975 entropy 0.03264415264129639
epoch: 76, step: 61
	action: tensor([[ 1.3865,  0.2288, -0.4929,  0.8120, -0.9225,  0.2444,  0.1054]],
       dtype=torch.float64)
	q_value: tensor([[-2.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9797982576994236, distance: 0.16264889108632433 entropy 0.03264415264129639
epoch: 76, step: 62
	action: tensor([[ 1.2024, -0.0513, -0.2828,  1.2803, -0.4084,  0.5916,  0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-2.5086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9549582227622114, distance: 0.24286473192051833 entropy 0.03264415264129639
epoch: 76, step: 63
	action: tensor([[ 1.7504,  0.3776, -0.7974,  1.7274, -0.1678,  0.3899, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-1.8453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 64
	action: tensor([[ 0.8144, -0.2860, -0.5922,  0.3565, -0.1367,  0.0743, -0.1918]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6899899020069198, distance: 0.6371542927560464 entropy 0.03264415264129639
epoch: 76, step: 65
	action: tensor([[ 1.5586,  0.5122, -0.3022,  0.7471, -0.2677,  0.2438,  0.4351]],
       dtype=torch.float64)
	q_value: tensor([[-0.8508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8572147579002869, distance: 0.4324126140299385 entropy 0.03264415264129639
epoch: 76, step: 66
	action: tensor([[ 1.4409,  0.2425, -0.5721,  1.5529, -0.0971,  0.3154, -0.2347]],
       dtype=torch.float64)
	q_value: tensor([[-2.2302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 67
	action: tensor([[ 1.2561,  0.2241, -0.2036,  0.5916, -0.6048,  0.2707, -0.2511]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9622192749316387, distance: 0.22242921927587747 entropy 0.03264415264129639
epoch: 76, step: 68
	action: tensor([[ 1.7619,  0.4858, -1.2730,  1.0147, -0.1638,  0.1704, -0.1453]],
       dtype=torch.float64)
	q_value: tensor([[-2.2913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 69
	action: tensor([[ 1.3092,  0.5239, -0.3491,  0.8773,  0.2033,  0.4304, -0.1049]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 70
	action: tensor([[ 0.9548,  0.1891, -0.3972,  0.7029, -0.0418,  0.2675, -0.3139]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9884559200500201, distance: 0.12295221476848099 entropy 0.03264415264129639
epoch: 76, step: 71
	action: tensor([[ 1.4845,  0.2921, -0.8765,  1.3654, -0.3377, -0.1831, -0.2419]],
       dtype=torch.float64)
	q_value: tensor([[-1.6248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9804712790554582, distance: 0.15991661673211766 entropy 0.03264415264129639
epoch: 76, step: 72
	action: tensor([[ 1.9555,  0.5007,  0.0504,  1.4526, -0.6465,  0.1202,  0.2345]],
       dtype=torch.float64)
	q_value: tensor([[-3.0575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 73
	action: tensor([[ 0.8513,  0.4654, -0.0698,  0.5229, -0.0873,  0.1259,  0.1037]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9657967049228404, distance: 0.21163654687397043 entropy 0.03264415264129639
epoch: 76, step: 74
	action: tensor([[ 1.8668,  0.5531, -0.6631,  0.9851, -0.5199,  0.3287, -0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-0.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 75
	action: tensor([[ 0.9513, -0.2022, -0.5192,  0.3182, -0.3629, -0.2561,  0.2937]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6715012333177877, distance: 0.6558787570369041 entropy 0.03264415264129639
epoch: 76, step: 76
	action: tensor([[ 1.8573, -0.2808, -0.2068,  0.9440, -0.3915,  0.2866, -0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-0.7834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 77
	action: tensor([[ 1.2432,  0.2443, -0.4446,  0.4993,  0.2452, -0.0546, -0.2900]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9481951596572061, distance: 0.2604604958838969 entropy 0.03264415264129639
epoch: 76, step: 78
	action: tensor([[ 1.7661,  0.5004, -1.0688,  1.0408, -0.2273,  0.1978, -0.1550]],
       dtype=torch.float64)
	q_value: tensor([[-1.8310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 79
	action: tensor([[ 1.1163,  0.3364, -0.4136,  0.6555, -0.7247,  0.0745,  0.0620]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9915337436895688, distance: 0.10529370301922117 entropy 0.03264415264129639
epoch: 76, step: 80
	action: tensor([[ 1.2179,  0.1308, -0.3451,  0.5994, -0.4710,  0.1432, -0.1929]],
       dtype=torch.float64)
	q_value: tensor([[-1.9043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9638391242711771, distance: 0.21760865417312464 entropy 0.03264415264129639
epoch: 76, step: 81
	action: tensor([[ 2.1088,  0.3245, -0.6275,  1.4994, -0.5595,  0.6456,  0.0497]],
       dtype=torch.float64)
	q_value: tensor([[-2.0093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 82
	action: tensor([[ 0.7209,  0.1263, -0.3684,  0.7576, -0.0263,  0.4315,  0.0913]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9268440041437773, distance: 0.30951496831880365 entropy 0.03264415264129639
epoch: 76, step: 83
	action: tensor([[ 1.4884,  0.6138, -1.0994,  0.6744, -0.1942,  0.4338, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-0.9447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9870192515688098, distance: 0.13037867404072806 entropy 0.03264415264129639
epoch: 76, step: 84
	action: tensor([[ 2.0635,  0.3412, -0.9426,  1.2696, -0.2916,  0.2383,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[-3.3271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 85
	action: tensor([[ 1.3599, -0.1108,  0.3034,  0.6035,  0.0355,  0.1795,  0.1892]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7079978314663422, distance: 0.6183718792700045 entropy 0.03264415264129639
epoch: 76, step: 86
	action: tensor([[ 1.3028,  0.2984, -0.5667,  0.8478, -0.7284,  0.6893,  0.5231]],
       dtype=torch.float64)
	q_value: tensor([[-1.1350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9851532207481921, distance: 0.1394353268109366 entropy 0.03264415264129639
epoch: 76, step: 87
	action: tensor([[ 1.5101,  0.5546, -0.5232,  0.8029, -0.3108,  0.2828,  0.4368]],
       dtype=torch.float64)
	q_value: tensor([[-2.2913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9000540580683855, distance: 0.3617756028820638 entropy 0.03264415264129639
epoch: 76, step: 88
	action: tensor([[ 1.4105,  0.1508, -0.9487,  1.0947, -0.7319,  0.3716,  0.2747]],
       dtype=torch.float64)
	q_value: tensor([[-2.3940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9825034445280335, distance: 0.15136761645991922 entropy 0.03264415264129639
epoch: 76, step: 89
	action: tensor([[ 1.8378,  0.1143, -0.2996,  1.0331, -0.7214,  0.0985,  0.2086]],
       dtype=torch.float64)
	q_value: tensor([[-2.6014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 90
	action: tensor([[ 1.0429,  0.1400, -0.9327,  0.4559, -0.6043,  0.2589, -0.1711]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388015422390914, distance: 0.2830915668595808 entropy 0.03264415264129639
epoch: 76, step: 91
	action: tensor([[ 1.6496,  0.6585, -0.5187,  1.2784, -0.4163,  0.7719, -0.0422]],
       dtype=torch.float64)
	q_value: tensor([[-2.2454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7993467560364649, distance: 0.5126013976097878 entropy 0.03264415264129639
epoch: 76, step: 92
	action: tensor([[ 2.1266,  0.2945, -0.4081,  0.9092, -0.5786,  0.0570,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-3.6332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 93
	action: tensor([[ 0.9132,  0.5596, -0.4001,  0.7483, -0.4099, -0.0812,  0.4059]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 94
	action: tensor([[ 1.1884, -0.0477, -0.3750,  0.6364, -0.4135,  0.3491,  0.1282]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9577750196963023, distance: 0.23514807136288907 entropy 0.03264415264129639
epoch: 76, step: 95
	action: tensor([[ 2.0385,  0.3647, -0.6551,  0.9924, -0.5592, -0.6082,  0.2268]],
       dtype=torch.float64)
	q_value: tensor([[-1.5695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 96
	action: tensor([[ 1.2020, -0.2735, -0.4271,  0.1467, -0.0697,  0.0130,  0.1769]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5246422507544033, distance: 0.7889817948984889 entropy 0.03264415264129639
epoch: 76, step: 97
	action: tensor([[ 1.2247,  0.7253, -0.5550,  0.6594, -0.5461,  0.3169, -0.0720]],
       dtype=torch.float64)
	q_value: tensor([[-1.0372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 98
	action: tensor([[ 1.1628,  0.1738, -0.4156,  0.7253, -0.0716, -0.3684, -0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9632530630273365, distance: 0.21936496416132206 entropy 0.03264415264129639
epoch: 76, step: 99
	action: tensor([[ 1.9213,  0.2519, -0.4577,  1.0033, -0.2935, -0.1122, -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-1.7711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 100
	action: tensor([[ 1.0029,  0.8064, -0.1988,  0.7331, -0.3687,  0.0889,  0.6527]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 101
	action: tensor([[ 0.9841,  0.1286, -0.7885,  0.8126, -0.6942,  0.1666, -0.3052]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9498353300445432, distance: 0.25630417047775517 entropy 0.03264415264129639
epoch: 76, step: 102
	action: tensor([[ 1.3883,  0.2865, -0.3638,  0.7483, -0.1137,  0.4201,  0.2217]],
       dtype=torch.float64)
	q_value: tensor([[-2.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.944092038659957, distance: 0.27057864734674114 entropy 0.03264415264129639
epoch: 76, step: 103
	action: tensor([[ 1.6044,  0.5340, -0.6231,  1.1830, -0.6857, -0.1619, -0.2353]],
       dtype=torch.float64)
	q_value: tensor([[-1.9893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8834667083404861, distance: 0.39064448750526315 entropy 0.03264415264129639
epoch: 76, step: 104
	action: tensor([[ 2.6163,  0.2960, -0.6811,  1.1186, -0.8371,  0.2153, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-3.4325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 105
	action: tensor([[ 0.7470,  0.2312, -0.0968,  1.2747, -0.4153,  0.0722,  0.1607]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.870889054541087, distance: 0.4111859096081819 entropy 0.03264415264129639
epoch: 76, step: 106
	action: tensor([[ 2.0051,  0.4293, -0.3859,  0.8508, -0.6302,  0.2897,  0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-1.1327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 107
	action: tensor([[ 1.0014,  0.8810, -0.0287,  0.5460, -0.0461,  0.3304,  0.0751]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 108
	action: tensor([[ 0.8849,  0.1251, -0.0482,  0.4414, -0.1119,  0.4448, -0.0058]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09922241576675099 entropy 0.03264415264129639
epoch: 76, step: 109
	action: tensor([[ 0.9344,  0.1461, -0.3313,  0.7754, -0.3702,  0.5164, -0.4613]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9741693827334091, distance: 0.18391793644809304 entropy 0.03264415264129639
epoch: 76, step: 110
	action: tensor([[ 2.0396,  0.4168, -0.2729,  1.4267, -0.3360,  0.1757, -0.2492]],
       dtype=torch.float64)
	q_value: tensor([[-2.0326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 111
	action: tensor([[ 1.4374,  0.2045, -0.8337,  0.8169, -0.6245, -0.0391,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9666385066779402, distance: 0.20901595289213615 entropy 0.03264415264129639
epoch: 76, step: 112
	action: tensor([[ 1.4555,  0.3107, -0.4850,  0.7521, -0.1010,  0.2128,  0.2218]],
       dtype=torch.float64)
	q_value: tensor([[-2.6195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9373504755742864, distance: 0.2864280739013587 entropy 0.03264415264129639
epoch: 76, step: 113
	action: tensor([[ 1.7714,  0.2292, -0.6019,  0.9020, -0.2032,  0.2685, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-2.0990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 114
	action: tensor([[ 0.8800,  0.5547, -0.3469,  0.7634, -0.7113,  0.2977, -0.3289]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8748224600536731, distance: 0.40487401032665316 entropy 0.03264415264129639
epoch: 76, step: 115
	action: tensor([[ 1.5495,  0.3821, -0.6222,  1.1529, -0.2848,  0.0727, -0.0121]],
       dtype=torch.float64)
	q_value: tensor([[-2.2966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9391512656038732, distance: 0.28228153352163327 entropy 0.03264415264129639
epoch: 76, step: 116
	action: tensor([[ 1.7250, -0.0656, -0.3133,  1.1024, -0.6321,  0.6144,  0.2565]],
       dtype=torch.float64)
	q_value: tensor([[-2.8162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 117
	action: tensor([[ 1.0774,  0.6833,  0.3729,  1.1132, -0.3411,  0.5425,  0.1952]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 118
	action: tensor([[ 0.8898, -0.0672,  0.0116,  0.3356, -0.4605,  0.3344,  0.0862]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8992383186753783, distance: 0.36324897380076027 entropy 0.03264415264129639
epoch: 76, step: 119
	action: tensor([[ 0.9346,  0.3256, -0.4047,  0.9083, -0.6507,  0.3144,  0.2529]],
       dtype=torch.float64)
	q_value: tensor([[-0.8989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9255949836743632, distance: 0.31214601969171624 entropy 0.03264415264129639
epoch: 76, step: 120
	action: tensor([[ 1.7498, -0.2250, -0.7061,  0.9752, -0.3476,  0.1658,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-1.5975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8673276563038442, distance: 0.416818411196453 entropy 0.03264415264129639
epoch: 76, step: 121
	action: tensor([[ 2.1974, -0.0786, -0.5261,  1.0101, -0.3511,  0.2973, -0.1184]],
       dtype=torch.float64)
	q_value: tensor([[-2.3708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 122
	action: tensor([[ 0.8644,  0.0955, -0.4791,  0.6356, -0.0278,  0.3877, -0.0615]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9655810561959532, distance: 0.21230267333909336 entropy 0.03264415264129639
epoch: 76, step: 123
	action: tensor([[ 1.3919,  0.6288, -0.6404,  1.5017, -0.5359,  0.2050,  0.1216]],
       dtype=torch.float64)
	q_value: tensor([[-1.1981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.73439593386356, distance: 0.5897582881621684 entropy 0.03264415264129639
epoch: 76, step: 124
	action: tensor([[ 1.9301,  0.1740, -0.0875,  1.0307, -0.1874,  0.1522,  0.1388]],
       dtype=torch.float64)
	q_value: tensor([[-2.9925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 76, step: 125
	action: tensor([[ 1.0833,  0.4415, -0.4688,  0.4992,  0.1715,  0.2044, -0.4165]],
       dtype=torch.float64)
	q_value: tensor([[-2.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9831414041891209, distance: 0.14858240801387407 entropy 0.03264415264129639
epoch: 76, step: 126
	action: tensor([[ 1.5705,  0.4023, -0.5472,  1.0836, -0.5413,  0.0299, -0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-2.0415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9266580835800489, distance: 0.3099080234702443 entropy 0.03264415264129639
epoch: 76, step: 127
	action: tensor([[ 2.1683,  0.0533, -0.5001,  0.9715, -0.8381,  0.2780, -0.2007]],
       dtype=torch.float64)
	q_value: tensor([[-3.0721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 76 actor 485.93612429921956 critic 2031.427140021073 
epoch: 77, step: 0
	action: tensor([[ 1.0700,  0.3950, -0.3045,  1.0320, -0.2758,  0.2483,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8904219537043885, distance: 0.3788074134391669 entropy 0.03264415264129639
epoch: 77, step: 1
	action: tensor([[ 1.8518,  0.4001, -0.6055,  0.9077, -0.2780,  0.3210,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-2.0846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 2
	action: tensor([[ 0.9005,  0.1741, -0.3785,  0.8897,  0.0219, -0.0052, -0.1163]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9786861074619971, distance: 0.16706600176331937 entropy 0.03264415264129639
epoch: 77, step: 3
	action: tensor([[ 2.1504,  0.4867, -0.5820,  0.8677, -0.4306, -0.0162, -0.0558]],
       dtype=torch.float64)
	q_value: tensor([[-1.4082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 4
	action: tensor([[ 1.1168, -0.1906, -0.3438,  0.6648, -0.1728,  0.1498, -0.2136]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9010079307894985, distance: 0.3600450913944564 entropy 0.03264415264129639
epoch: 77, step: 5
	action: tensor([[ 1.8634,  0.4335, -0.7625,  1.0141, -0.0783,  0.6197,  0.1655]],
       dtype=torch.float64)
	q_value: tensor([[-1.6401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 6
	action: tensor([[ 0.5264, -0.3236, -0.2245,  0.9884,  0.2220,  0.1363,  0.0134]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8399322380761634, distance: 0.457834620049782 entropy 0.03264415264129639
epoch: 77, step: 7
	action: tensor([[ 1.6829, -0.0114, -0.3707,  1.2156, -0.3112,  0.2794,  0.1859]],
       dtype=torch.float64)
	q_value: tensor([[-0.4599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9238889704456112, distance: 0.3157042959593727 entropy 0.03264415264129639
epoch: 77, step: 8
	action: tensor([[ 2.1786e+00,  4.6754e-01, -6.2610e-01,  1.0807e+00, -3.6978e-01,
          9.6166e-04, -7.2547e-02]], dtype=torch.float64)
	q_value: tensor([[-2.5914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 9
	action: tensor([[ 0.5862,  0.2096, -0.1934,  0.6216, -0.0770, -0.1695,  0.3643]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9246242268477418, distance: 0.3141756940795165 entropy 0.03264415264129639
epoch: 77, step: 10
	action: tensor([[ 1.5155,  0.1915, -0.0195,  1.0154, -0.3780,  0.3006,  0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-0.5198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8313367302961822, distance: 0.4699665531540797 entropy 0.03264415264129639
epoch: 77, step: 11
	action: tensor([[ 1.6317,  0.1008, -0.8383,  1.7136, -0.5727, -0.0500, -0.0831]],
       dtype=torch.float64)
	q_value: tensor([[-2.5831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9391390348856732, distance: 0.28230990167465325 entropy 0.03264415264129639
epoch: 77, step: 12
	action: tensor([[ 2.3245,  0.3478, -0.3883,  1.6733, -0.8069,  0.6900,  0.1016]],
       dtype=torch.float64)
	q_value: tensor([[-3.5562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 13
	action: tensor([[ 0.9514, -0.2973, -0.1949,  0.4770, -0.6462,  0.2222,  0.6396]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7942538279536353, distance: 0.5190659912027971 entropy 0.03264415264129639
epoch: 77, step: 14
	action: tensor([[ 1.2858,  0.0846, -0.5138,  1.0719,  0.0025, -0.5118,  0.4587]],
       dtype=torch.float64)
	q_value: tensor([[-0.8701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.980019098025901, distance: 0.16175742985825134 entropy 0.03264415264129639
epoch: 77, step: 15
	action: tensor([[ 1.9938, -0.1957, -0.8269,  0.9423, -0.3367,  0.3849,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[-1.5289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 16
	action: tensor([[ 1.0461,  0.5572, -0.3040,  0.7625, -0.3478, -0.3106, -0.2961]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.940811422470947, distance: 0.27840411147323685 entropy 0.03264415264129639
epoch: 77, step: 17
	action: tensor([[ 1.9502,  0.0834, -0.2804,  0.9502, -0.0926,  0.3126, -0.2094]],
       dtype=torch.float64)
	q_value: tensor([[-2.2880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 18
	action: tensor([[ 1.6241,  0.2299, -0.6813,  0.3128, -0.0078,  0.1643, -0.4890]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7308114429116772, distance: 0.5937245275098307 entropy 0.03264415264129639
epoch: 77, step: 19
	action: tensor([[ 1.8394,  0.3574, -0.6573,  1.3911, -0.4335,  0.1893, -0.2350]],
       dtype=torch.float64)
	q_value: tensor([[-3.3807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 20
	action: tensor([[ 0.7923, -0.2077, -0.7869,  0.3435, -0.4965,  0.4395,  0.1171]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7228987136642935, distance: 0.6023875178113773 entropy 0.03264415264129639
epoch: 77, step: 21
	action: tensor([[ 1.8001,  0.3256, -0.3706,  0.6689, -0.2125, -0.1055, -0.3141]],
       dtype=torch.float64)
	q_value: tensor([[-1.4633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8027330564891454, distance: 0.5082575645189455 entropy 0.03264415264129639
epoch: 77, step: 22
	action: tensor([[ 2.1900, -0.0163, -0.9344,  1.3214, -0.4797, -0.0473, -0.4360]],
       dtype=torch.float64)
	q_value: tensor([[-3.1252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 23
	action: tensor([[ 0.9063,  0.6112, -0.4031,  0.5142, -0.2070,  0.4416,  0.1625]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 24
	action: tensor([[ 1.1070,  0.1398, -0.0984,  0.0736,  0.1445,  0.1985, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8541396826710731, distance: 0.4370441086257903 entropy 0.03264415264129639
epoch: 77, step: 25
	action: tensor([[ 1.9037,  0.2552, -0.6472,  1.3097, -0.5986,  0.0615, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-1.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 26
	action: tensor([[ 1.4105,  0.5748, -0.2973,  0.6801, -0.1683,  0.3884,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 27
	action: tensor([[ 0.5093,  0.4997, -0.4922,  0.4366,  0.0654,  0.3151,  0.3493]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8315379111347061, distance: 0.469686182459178 entropy 0.03264415264129639
epoch: 77, step: 28
	action: tensor([[ 1.6093,  0.3508, -0.6619,  0.9470, -0.4449, -0.0527,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-0.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9333555366108791, distance: 0.29541920901959984 entropy 0.03264415264129639
epoch: 77, step: 29
	action: tensor([[ 1.9022,  0.4562, -0.4190,  0.9981, -0.1660, -0.3254,  0.1044]],
       dtype=torch.float64)
	q_value: tensor([[-3.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 30
	action: tensor([[ 0.8212,  0.2582, -0.2678,  0.6544, -0.1902,  0.1194,  0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9790189542457856, distance: 0.16575638198720746 entropy 0.03264415264129639
epoch: 77, step: 31
	action: tensor([[ 1.2468,  0.2269, -1.0604,  0.7163, -0.2911,  0.0817,  0.5882]],
       dtype=torch.float64)
	q_value: tensor([[-1.0500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9800483946851659, distance: 0.1616387993107144 entropy 0.03264415264129639
epoch: 77, step: 32
	action: tensor([[ 1.0439,  0.0510, -0.4610,  0.9550, -0.3380,  0.0979,  0.1274]],
       dtype=torch.float64)
	q_value: tensor([[-2.1220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08368538006931488 entropy 0.03264415264129639
epoch: 77, step: 33
	action: tensor([[ 0.8141,  0.6011, -0.5465,  0.5751, -0.3661,  0.4268,  0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8586415842436981, distance: 0.43024668012370887 entropy 0.03264415264129639
epoch: 77, step: 34
	action: tensor([[ 1.3405,  0.2513, -0.7219,  0.8648, -0.7199,  0.5629, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[-1.9870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08544911006870759 entropy 0.03264415264129639
epoch: 77, step: 35
	action: tensor([[ 0.5914, -0.1094, -0.4177,  0.7202,  0.1196,  0.0611, -0.3955]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8329245129261136, distance: 0.4677492084284034 entropy 0.03264415264129639
epoch: 77, step: 36
	action: tensor([[ 1.7738,  0.4181, -1.0340,  1.1757, -0.3256,  0.3188, -0.1940]],
       dtype=torch.float64)
	q_value: tensor([[-0.8827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9839022436839575, distance: 0.14519089009401684 entropy 0.03264415264129639
epoch: 77, step: 37
	action: tensor([[ 2.2679,  0.8426, -0.4729,  1.2883, -0.9130,  0.0527,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-4.0116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 38
	action: tensor([[ 1.0040,  0.5686,  0.1916,  0.7490, -0.3652,  0.7413, -0.0945]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 39
	action: tensor([[ 0.6352,  0.4670, -0.3917,  0.9977,  0.3535,  0.0572,  0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7781779721953419, distance: 0.5389630956648609 entropy 0.03264415264129639
epoch: 77, step: 40
	action: tensor([[ 1.6946,  0.7444, -0.7084,  1.4970, -0.7058,  0.4735,  0.3052]],
       dtype=torch.float64)
	q_value: tensor([[-1.0687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 41
	action: tensor([[ 1.0543,  0.3371, -0.5643,  0.7039, -0.5560,  0.3219,  0.1523]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9797484079023661, distance: 0.1628494435540709 entropy 0.03264415264129639
epoch: 77, step: 42
	action: tensor([[ 1.6152,  0.2094, -0.3026,  0.6164, -0.2936, -0.0125, -0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-2.1161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.771690707098256, distance: 0.5467873867842623 entropy 0.03264415264129639
epoch: 77, step: 43
	action: tensor([[ 1.8198,  0.3415, -0.8789,  1.0735, -0.5265,  0.0634, -0.1094]],
       dtype=torch.float64)
	q_value: tensor([[-2.7870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 44
	action: tensor([[ 0.6310,  0.4251, -0.4704,  0.1498,  0.1879,  0.0191,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9343654661756395, distance: 0.2931722740727833 entropy 0.03264415264129639
epoch: 77, step: 45
	action: tensor([[ 1.4947,  0.3288, -0.9253,  1.3952, -0.0875,  0.3538,  0.2383]],
       dtype=torch.float64)
	q_value: tensor([[-0.9546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9259734110308439, distance: 0.31135121324674875 entropy 0.03264415264129639
epoch: 77, step: 46
	action: tensor([[ 2.1806,  0.3346, -0.2845,  1.5650, -0.6146,  0.6220, -0.6841]],
       dtype=torch.float64)
	q_value: tensor([[-3.0582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 47
	action: tensor([[ 1.0521,  0.1871, -0.4999,  0.4270, -0.0572,  0.2414,  0.5963]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9832322257914602, distance: 0.1481816416563049 entropy 0.03264415264129639
epoch: 77, step: 48
	action: tensor([[ 1.3538,  0.2236, -0.5257,  1.2168, -0.1094,  0.0030,  0.2183]],
       dtype=torch.float64)
	q_value: tensor([[-1.2319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9664500159115136, distance: 0.20960558604861218 entropy 0.03264415264129639
epoch: 77, step: 49
	action: tensor([[ 2.1334,  0.2929, -0.9612,  1.5197, -0.5945, -0.0229,  0.0911]],
       dtype=torch.float64)
	q_value: tensor([[-2.1947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 50
	action: tensor([[ 1.2901,  0.6854, -0.7106,  1.2107,  0.0582,  0.2100,  0.1083]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7740252328462878, distance: 0.5439846775266339 entropy 0.03264415264129639
epoch: 77, step: 51
	action: tensor([[ 2.0281,  0.2477, -0.4475,  1.2809, -0.2982,  0.4412, -0.1126]],
       dtype=torch.float64)
	q_value: tensor([[-2.8926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 52
	action: tensor([[ 1.1370,  0.2633, -0.3797,  0.1571, -0.4507, -0.0388,  0.2971]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.895239468418497, distance: 0.3703868335106033 entropy 0.03264415264129639
epoch: 77, step: 53
	action: tensor([[ 1.4711, -0.2068, -0.6295,  0.7094, -0.1389,  0.6654,  0.5447]],
       dtype=torch.float64)
	q_value: tensor([[-1.6064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9034349353768172, distance: 0.35560406020771745 entropy 0.03264415264129639
epoch: 77, step: 54
	action: tensor([[ 1.9316, -0.0834, -0.3835,  1.2667,  0.1183,  0.0754, -0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-2.0104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 55
	action: tensor([[ 0.9381,  0.1713, -0.2520,  0.7156, -0.6611,  0.1110, -0.1568]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08161944429891918 entropy 0.03264415264129639
epoch: 77, step: 56
	action: tensor([[ 1.6304, -0.0829, -0.6973,  0.6589, -0.2021,  0.0043,  0.1257]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7255176498235933, distance: 0.5995341211790052 entropy 0.03264415264129639
epoch: 77, step: 57
	action: tensor([[ 1.7465,  0.0991, -0.5556,  1.6029, -0.4515,  0.9149,  0.1141]],
       dtype=torch.float64)
	q_value: tensor([[-2.4791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 58
	action: tensor([[ 1.3360,  0.1587, -0.1000,  0.4470, -0.2860,  0.3128, -0.1685]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8901063188051629, distance: 0.3793525904487463 entropy 0.03264415264129639
epoch: 77, step: 59
	action: tensor([[ 1.8039,  0.3901, -0.7241,  1.5533, -0.6640,  0.5453, -0.1904]],
       dtype=torch.float64)
	q_value: tensor([[-2.2871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 60
	action: tensor([[ 0.5232,  0.1515, -0.1101,  0.6125, -0.3725,  0.5591,  0.2195]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8884689826311236, distance: 0.3821681809390756 entropy 0.03264415264129639
epoch: 77, step: 61
	action: tensor([[ 1.4638, -0.0037, -0.6909,  0.6198, -0.3425,  0.1713,  0.1745]],
       dtype=torch.float64)
	q_value: tensor([[-0.9063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8708924571373056, distance: 0.41118049136562207 entropy 0.03264415264129639
epoch: 77, step: 62
	action: tensor([[ 1.1152,  0.5532, -0.7469,  0.9969, -0.2485,  0.5585, -0.1132]],
       dtype=torch.float64)
	q_value: tensor([[-2.3658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8315877335828328, distance: 0.4696167227781523 entropy 0.03264415264129639
epoch: 77, step: 63
	action: tensor([[ 2.1607,  0.4867, -0.8936,  1.4205, -0.6344,  0.3438,  0.1105]],
       dtype=torch.float64)
	q_value: tensor([[-2.9685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 64
	action: tensor([[ 0.6620,  0.2451, -0.2234,  0.5595,  0.1093,  0.3559, -0.0991]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9479635638458701, distance: 0.261042046682996 entropy 0.03264415264129639
epoch: 77, step: 65
	action: tensor([[ 1.5129,  0.4167, -0.4179,  0.9748, -0.4054, -0.1175, -0.4625]],
       dtype=torch.float64)
	q_value: tensor([[-0.9863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.895256758435066, distance: 0.3703562673295833 entropy 0.03264415264129639
epoch: 77, step: 66
	action: tensor([[ 2.1109,  0.5238, -0.5132,  1.4786, -0.5018,  0.3867,  0.0428]],
       dtype=torch.float64)
	q_value: tensor([[-3.4455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 67
	action: tensor([[ 1.0888,  0.3958, -0.3401,  0.7256, -0.1281,  0.1304, -0.1496]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.968982477437504, distance: 0.2015395324339032 entropy 0.03264415264129639
epoch: 77, step: 68
	action: tensor([[ 1.9078,  0.1780, -0.5010,  1.3038, -0.1916,  0.1515, -0.1979]],
       dtype=torch.float64)
	q_value: tensor([[-2.0761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 69
	action: tensor([[ 1.0307,  0.3536, -0.7017,  0.5175, -0.5879,  0.4501, -0.2373]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9815781770400613, distance: 0.15531843124176695 entropy 0.03264415264129639
epoch: 77, step: 70
	action: tensor([[ 1.7292, -0.1380, -0.7003,  1.4076,  0.0422,  0.0426,  0.3051]],
       dtype=torch.float64)
	q_value: tensor([[-2.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9683751456108185, distance: 0.20350306778525065 entropy 0.03264415264129639
epoch: 77, step: 71
	action: tensor([[ 2.0862,  0.2585, -0.2923,  1.3898, -0.0407,  0.0666,  0.1629]],
       dtype=torch.float64)
	q_value: tensor([[-2.2863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 72
	action: tensor([[ 0.9428, -0.0187,  0.0681,  0.1221,  0.0654,  0.0480, -0.1481]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7945966805900486, distance: 0.5186333286106686 entropy 0.03264415264129639
epoch: 77, step: 73
	action: tensor([[ 1.5215,  0.3395, -0.7099,  1.2883, -0.4458,  0.1203,  0.4250]],
       dtype=torch.float64)
	q_value: tensor([[-0.8679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9542181029185273, distance: 0.2448519604577611 entropy 0.03264415264129639
epoch: 77, step: 74
	action: tensor([[ 1.8316,  0.1945, -0.7335,  1.1307, -0.5012,  0.5408, -0.2442]],
       dtype=torch.float64)
	q_value: tensor([[-2.8035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 75
	action: tensor([[ 0.8164,  0.0830, -0.5506,  0.6406, -0.0491,  0.4305, -0.1311]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9396103346915756, distance: 0.28121469073315314 entropy 0.03264415264129639
epoch: 77, step: 76
	action: tensor([[ 1.9043,  0.4920, -0.3598,  1.6765, -0.5113, -0.0309,  0.1398]],
       dtype=torch.float64)
	q_value: tensor([[-1.4611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 77
	action: tensor([[ 0.3585,  0.5537, -0.0395,  0.1919, -0.1828,  0.0077, -0.3159]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8483581839525857, distance: 0.4456215467099959 entropy 0.03264415264129639
epoch: 77, step: 78
	action: tensor([[ 1.3193,  0.6534, -0.5528,  0.8787, -0.0577, -0.0666, -0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-0.9244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 79
	action: tensor([[ 1.1200,  0.1010, -0.5678,  0.7597, -0.0018,  0.0095, -0.0207]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9875592043905155, distance: 0.12763822933987454 entropy 0.03264415264129639
epoch: 77, step: 80
	action: tensor([[ 1.8827,  0.2102, -1.0188,  1.1447, -0.5765,  0.3358,  0.1350]],
       dtype=torch.float64)
	q_value: tensor([[-1.7041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 81
	action: tensor([[ 1.1639,  0.0312, -0.5570,  0.7169, -0.3567,  0.0796, -0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9704171565925634, distance: 0.19682336415023682 entropy 0.03264415264129639
epoch: 77, step: 82
	action: tensor([[ 1.7546,  0.3362, -0.8332,  1.0357, -0.4295, -0.0074, -0.1748]],
       dtype=torch.float64)
	q_value: tensor([[-2.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 83
	action: tensor([[ 0.8798,  0.0798, -0.1335,  0.4435, -0.4700,  0.4860,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9762831664860145, distance: 0.17623211192950447 entropy 0.03264415264129639
epoch: 77, step: 84
	action: tensor([[ 1.4756,  0.5948, -0.8045,  1.0475, -0.0018,  0.0782,  0.3558]],
       dtype=torch.float64)
	q_value: tensor([[-1.4668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9336141571275365, distance: 0.29484544992820144 entropy 0.03264415264129639
epoch: 77, step: 85
	action: tensor([[ 1.8994,  0.2622, -0.2167,  1.3005,  0.0109,  0.3034, -0.1040]],
       dtype=torch.float64)
	q_value: tensor([[-2.8301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 86
	action: tensor([[ 0.6939,  0.0145, -0.2181,  0.6654, -0.3445,  0.1090, -0.1523]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9345749385789042, distance: 0.29270407102297846 entropy 0.03264415264129639
epoch: 77, step: 87
	action: tensor([[ 1.7799,  0.4486, -0.1588,  0.6304, -0.1054,  0.2699, -0.3010]],
       dtype=torch.float64)
	q_value: tensor([[-1.0594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7887060137823403, distance: 0.5260175831295195 entropy 0.03264415264129639
epoch: 77, step: 88
	action: tensor([[ 1.8606, -0.0578, -0.9791,  1.0960, -0.0692,  0.6886,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[-3.2319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 89
	action: tensor([[ 0.6574,  0.5934, -0.1235,  1.1970,  0.0898,  0.0410,  0.1342]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 90
	action: tensor([[ 0.7003, -0.0602, -0.5305,  0.5324, -0.5181, -0.0776,  0.1354]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8505188823467715, distance: 0.44243539300289825 entropy 0.03264415264129639
epoch: 77, step: 91
	action: tensor([[ 1.5840,  0.4341, -0.1734,  0.8749, -0.2042,  0.5791, -0.2475]],
       dtype=torch.float64)
	q_value: tensor([[-0.9302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8191154410630515, distance: 0.4866956234184543 entropy 0.03264415264129639
epoch: 77, step: 92
	action: tensor([[ 1.6712,  0.3502, -0.4573,  1.4874, -0.4486,  0.1890,  0.1452]],
       dtype=torch.float64)
	q_value: tensor([[-3.4257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 93
	action: tensor([[ 0.8313,  0.5798, -0.0542,  0.0539, -0.7504, -0.2918, -0.1037]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9831367757135305, distance: 0.1486028030355198 entropy 0.03264415264129639
epoch: 77, step: 94
	action: tensor([[ 1.5214, -0.2704, -0.2364,  0.7133, -0.6357,  0.1309, -0.0330]],
       dtype=torch.float64)
	q_value: tensor([[-1.7045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7006336129992838, distance: 0.626120915011029 entropy 0.03264415264129639
epoch: 77, step: 95
	action: tensor([[ 2.0010,  0.2788, -0.7611,  1.3063, -0.3535,  0.3855, -0.3217]],
       dtype=torch.float64)
	q_value: tensor([[-2.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 96
	action: tensor([[ 1.0168,  0.1174, -0.5458,  0.0855,  0.2371,  0.0580, -0.1422]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8482180156683481, distance: 0.44582745159136555 entropy 0.03264415264129639
epoch: 77, step: 97
	action: tensor([[ 1.7165, -0.1163, -0.2399,  0.9377, -0.6250, -0.1916, -0.2758]],
       dtype=torch.float64)
	q_value: tensor([[-1.4048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7195071421249432, distance: 0.6060627567909339 entropy 0.03264415264129639
epoch: 77, step: 98
	action: tensor([[ 2.0535,  0.0950, -0.6830,  1.0207, -0.7059,  0.3609, -0.4314]],
       dtype=torch.float64)
	q_value: tensor([[-2.8779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 99
	action: tensor([[ 0.9249, -0.0751, -0.1118,  0.5916, -0.4920,  0.1123, -0.1002]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9368896214004778, distance: 0.2874796358261129 entropy 0.03264415264129639
epoch: 77, step: 100
	action: tensor([[ 2.0079,  0.0523, -0.6285,  0.9408, -0.4181,  0.5581, -0.2155]],
       dtype=torch.float64)
	q_value: tensor([[-1.3241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 101
	action: tensor([[ 0.7571,  0.3332, -0.1218,  0.7178,  0.0263,  0.1538,  0.1720]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9542657443901363, distance: 0.24472452860393865 entropy 0.03264415264129639
epoch: 77, step: 102
	action: tensor([[ 1.4416,  0.1487, -0.5556,  0.9835,  0.0479,  0.6815, -0.2175]],
       dtype=torch.float64)
	q_value: tensor([[-0.8934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9600188062755092, distance: 0.22881504234135674 entropy 0.03264415264129639
epoch: 77, step: 103
	action: tensor([[ 2.0057,  0.5191, -0.8599,  1.6589, -0.1525,  0.1626,  0.1354]],
       dtype=torch.float64)
	q_value: tensor([[-2.9946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 104
	action: tensor([[ 0.9980,  0.1535, -0.6080,  0.4371, -0.1093,  0.2683, -0.0673]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9698737222890891, distance: 0.19862295137811614 entropy 0.03264415264129639
epoch: 77, step: 105
	action: tensor([[ 0.9728,  0.3531, -0.2625,  1.0436, -0.6987,  0.4994, -0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-1.7172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8776849245901258, distance: 0.4002180641624765 entropy 0.03264415264129639
epoch: 77, step: 106
	action: tensor([[ 1.2891,  0.7034, -0.6092,  0.6822, -0.3830,  0.5026, -0.3174]],
       dtype=torch.float64)
	q_value: tensor([[-2.3509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8830339767714753, distance: 0.3913691197509305 entropy 0.03264415264129639
epoch: 77, step: 107
	action: tensor([[ 1.3262,  0.0173, -0.2955,  1.2493, -0.0258,  0.5636,  0.1971]],
       dtype=torch.float64)
	q_value: tensor([[-3.5939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9278631086615236, distance: 0.3073515479407781 entropy 0.03264415264129639
epoch: 77, step: 108
	action: tensor([[ 2.1757,  0.1949, -0.3449,  1.2877, -0.5440,  0.0759,  0.1030]],
       dtype=torch.float64)
	q_value: tensor([[-2.0645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 109
	action: tensor([[ 0.9727, -0.0976, -0.5512,  0.3567, -0.2698,  0.2945, -0.0885]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8631656096927506, distance: 0.42330590226021086 entropy 0.03264415264129639
epoch: 77, step: 110
	action: tensor([[ 1.7179,  0.5023, -0.8244,  0.9835, -0.4371,  0.3922, -0.0657]],
       dtype=torch.float64)
	q_value: tensor([[-1.4930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.976351322962261, distance: 0.17597870538589558 entropy 0.03264415264129639
epoch: 77, step: 111
	action: tensor([[ 1.7835,  0.5014, -0.4865,  0.6693, -0.6203,  0.3215,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-3.8496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 112
	action: tensor([[ 0.7000,  0.0108,  0.3147,  0.4881, -0.2253,  0.2193, -0.0531]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.954851735069641, distance: 0.24315165262781116 entropy 0.03264415264129639
epoch: 77, step: 113
	action: tensor([[ 1.6209,  0.4926, -0.2968,  0.9140, -0.3876,  0.5447,  0.1340]],
       dtype=torch.float64)
	q_value: tensor([[-0.7559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8410363794545805, distance: 0.45625282457625876 entropy 0.03264415264129639
epoch: 77, step: 114
	action: tensor([[ 2.1427,  0.6028, -1.0443,  1.3513, -0.3389,  0.1136,  0.1999]],
       dtype=torch.float64)
	q_value: tensor([[-3.2478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 115
	action: tensor([[ 0.6924, -0.4078, -0.0013,  1.0557, -0.3423, -0.0769, -0.2935]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.903704437980484, distance: 0.3551074873474047 entropy 0.03264415264129639
epoch: 77, step: 116
	action: tensor([[ 2.2758,  0.2707, -0.2518,  0.8758, -0.3193,  0.1925, -0.1436]],
       dtype=torch.float64)
	q_value: tensor([[-0.9537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 117
	action: tensor([[ 0.7881, -0.0638, -0.2818,  1.0098,  0.1118,  0.2246, -0.2734]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9645220495783097, distance: 0.2155440081446904 entropy 0.03264415264129639
epoch: 77, step: 118
	action: tensor([[ 1.9779,  0.3858, -0.8040,  1.2223, -0.3794,  0.7210, -0.4739]],
       dtype=torch.float64)
	q_value: tensor([[-1.2170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 119
	action: tensor([[ 0.9442,  0.1833, -0.3672,  0.6282,  0.1796,  0.2881, -0.3206]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08866329662139309 entropy 0.03264415264129639
epoch: 77, step: 120
	action: tensor([[ 0.4809, -0.1559, -0.2506,  0.4550, -0.0072,  0.2441,  0.1609]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7505852440867671, distance: 0.5715020138825913 entropy 0.03264415264129639
epoch: 77, step: 121
	action: tensor([[ 1.4020,  0.2873, -0.4841,  1.2596, -0.6029, -0.0045,  0.0698]],
       dtype=torch.float64)
	q_value: tensor([[-0.3895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.94293293392455, distance: 0.2733691270148376 entropy 0.03264415264129639
epoch: 77, step: 122
	action: tensor([[ 1.8626,  0.2453, -0.6729,  0.7388, -0.6066,  0.5656, -0.1219]],
       dtype=torch.float64)
	q_value: tensor([[-2.7937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 123
	action: tensor([[ 0.7170, -0.0520, -0.2030,  0.4308,  0.0152,  0.0777,  0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8761658159181076, distance: 0.40269567641049636 entropy 0.03264415264129639
epoch: 77, step: 124
	action: tensor([[ 1.0988,  0.2468, -0.3889,  0.7831, -0.6997,  0.0197, -0.0723]],
       dtype=torch.float64)
	q_value: tensor([[-0.5361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08243635954332212 entropy 0.03264415264129639
epoch: 77, step: 125
	action: tensor([[ 1.0859,  0.1867,  0.0347,  0.6867, -0.5178,  0.2780, -0.3335]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9736848421015403, distance: 0.1856349235006535 entropy 0.03264415264129639
epoch: 77, step: 126
	action: tensor([[ 1.8692,  0.3958, -1.0388,  1.5928, -0.2726,  0.4668,  0.0988]],
       dtype=torch.float64)
	q_value: tensor([[-2.1299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 77, step: 127
	action: tensor([[ 1.2057,  0.3135,  0.0925,  0.6273, -0.1290,  0.7558,  0.1408]],
       dtype=torch.float64)
	q_value: tensor([[-3.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8985745755967216, distance: 0.36444341715427236 entropy 0.03264415264129639
LOSS epoch 77 actor 619.2342909931784 critic 2516.999039740237 
epoch: 78, step: 0
	action: tensor([[ 1.3377,  0.0582, -0.5255,  0.9613,  0.1334,  0.1369,  0.2030]],
       dtype=torch.float64)
	q_value: tensor([[-2.3806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.983696420820397, distance: 0.14611613360227232 entropy 0.03264415264129639
epoch: 78, step: 1
	action: tensor([[ 0.8735,  0.0426, -0.2941,  0.7887, -0.4683,  0.4057,  0.1462]],
       dtype=torch.float64)
	q_value: tensor([[-2.2468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9827010532721391, distance: 0.15051040483920872 entropy 0.03264415264129639
epoch: 78, step: 2
	action: tensor([[ 1.0081,  0.4051, -0.3099,  0.8215, -0.5501,  0.0501,  0.1942]],
       dtype=torch.float64)
	q_value: tensor([[-1.7607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9457090176256503, distance: 0.2666370792034332 entropy 0.03264415264129639
epoch: 78, step: 3
	action: tensor([[ 1.0054,  0.1488, -0.5767,  0.6770, -0.2503,  0.1276,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[-2.1778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9908533188028246, distance: 0.1094431197450186 entropy 0.03264415264129639
epoch: 78, step: 4
	action: tensor([[ 1.1528,  0.1261, -0.1758,  0.4224, -0.2565,  0.0619, -0.1029]],
       dtype=torch.float64)
	q_value: tensor([[-2.0471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.919597142955358, distance: 0.3244833626082635 entropy 0.03264415264129639
epoch: 78, step: 5
	action: tensor([[ 0.8278,  0.2366, -0.2559,  0.5089, -0.4482,  0.0492, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[-2.0705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9886370011225487, distance: 0.12198408563169387 entropy 0.03264415264129639
epoch: 78, step: 6
	action: tensor([[ 1.2237,  0.2512, -0.7255,  0.6724, -0.2752,  0.3094, -0.3664]],
       dtype=torch.float64)
	q_value: tensor([[-1.6139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.02874414414958863 entropy 0.03264415264129639
epoch: 78, step: 7
	action: tensor([[ 0.4064,  0.0495, -0.2670, -0.1361, -0.1908, -0.1233, -0.1393]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6033147541076321, distance: 0.7207418118605062 entropy 0.03264415264129639
epoch: 78, step: 8
	action: tensor([[ 1.0154,  0.1900, -0.5032,  0.2148, -0.2828,  0.2063, -0.1610]],
       dtype=torch.float64)
	q_value: tensor([[-0.6457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9368368094900353, distance: 0.28759989476156966 entropy 0.03264415264129639
epoch: 78, step: 9
	action: tensor([[ 1.3832,  0.2299, -0.5168,  1.0853, -0.1962,  0.2157, -0.0352]],
       dtype=torch.float64)
	q_value: tensor([[-2.2424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9735571503867111, distance: 0.1860847660586613 entropy 0.03264415264129639
epoch: 78, step: 10
	action: tensor([[ 1.4352,  0.0710, -0.3395,  0.6569, -0.5250,  0.2185,  0.1174]],
       dtype=torch.float64)
	q_value: tensor([[-3.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.897123987879465, distance: 0.3670403021545627 entropy 0.03264415264129639
epoch: 78, step: 11
	action: tensor([[ 1.3991,  0.2230, -0.5460,  0.7864, -0.3704,  0.7653, -0.2367]],
       dtype=torch.float64)
	q_value: tensor([[-2.8323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9837974874976263, distance: 0.14566253905010135 entropy 0.03264415264129639
epoch: 78, step: 12
	action: tensor([[ 1.6618,  0.1427, -0.6375,  0.7042, -0.3132,  0.2362, -0.0287]],
       dtype=torch.float64)
	q_value: tensor([[-3.8802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 13
	action: tensor([[ 0.5673,  0.1167, -0.6077,  0.7247, -0.4006,  0.0144,  0.5547]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8270082842158247, distance: 0.47595878405605163 entropy 0.03264415264129639
epoch: 78, step: 14
	action: tensor([[ 0.6094,  0.0999, -0.3808,  0.4849, -0.3774,  0.5535, -0.0561]],
       dtype=torch.float64)
	q_value: tensor([[-1.0450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8828923749550033, distance: 0.3916059484218785 entropy 0.03264415264129639
epoch: 78, step: 15
	action: tensor([[ 1.3025,  0.2439, -0.1973,  0.4300, -0.3708,  0.3723,  0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-1.6000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9342644578247904, distance: 0.29339777627238517 entropy 0.03264415264129639
epoch: 78, step: 16
	action: tensor([[ 1.4086,  0.7116, -0.6166,  1.2432, -0.2506,  0.2193,  0.1139]],
       dtype=torch.float64)
	q_value: tensor([[-2.4603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 17
	action: tensor([[ 0.3305,  0.3096, -0.2289,  0.2145,  0.4407,  0.3298, -0.2621]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8208016117636611, distance: 0.48442187114038143 entropy 0.03264415264129639
epoch: 78, step: 18
	action: tensor([[ 1.3108, -0.5458, -0.4392,  0.9265, -0.5835, -0.1456, -0.4030]],
       dtype=torch.float64)
	q_value: tensor([[-0.7477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7067886070596154, distance: 0.6196509413999147 entropy 0.03264415264129639
epoch: 78, step: 19
	action: tensor([[ 1.3270,  0.4047, -0.9531,  0.8282, -0.0173,  0.5862, -0.1434]],
       dtype=torch.float64)
	q_value: tensor([[-2.7204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9800897644031874, distance: 0.16147113306542843 entropy 0.03264415264129639
epoch: 78, step: 20
	action: tensor([[ 1.7104,  0.5044, -0.1437,  0.9642, -0.6149,  0.3017,  0.3064]],
       dtype=torch.float64)
	q_value: tensor([[-3.7705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 21
	action: tensor([[ 0.3382, -0.2553, -0.6108,  0.5797,  0.1513,  0.1663,  0.2621]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5243146998446366, distance: 0.789253576731845 entropy 0.03264415264129639
epoch: 78, step: 22
	action: tensor([[ 0.5748,  0.4276, -0.5673,  0.4956,  0.0643,  0.5729, -0.3692]],
       dtype=torch.float64)
	q_value: tensor([[-0.5379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8284990046764489, distance: 0.4739036085956972 entropy 0.03264415264129639
epoch: 78, step: 23
	action: tensor([[ 1.3566,  0.0544, -0.0544,  0.9232,  0.0195, -0.3964, -0.1807]],
       dtype=torch.float64)
	q_value: tensor([[-2.0734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8722878074924432, distance: 0.4089525058241158 entropy 0.03264415264129639
epoch: 78, step: 24
	action: tensor([[ 1.4736,  0.2114, -0.5818,  0.9404, -0.5137,  0.6001,  0.2912]],
       dtype=torch.float64)
	q_value: tensor([[-2.2857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9830170522768076, distance: 0.14912938595872852 entropy 0.03264415264129639
epoch: 78, step: 25
	action: tensor([[ 1.4886,  0.1675, -0.6251,  0.5585, -0.1671,  0.1394,  0.2539]],
       dtype=torch.float64)
	q_value: tensor([[-3.3734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.877487268272784, distance: 0.4005413019004889 entropy 0.03264415264129639
epoch: 78, step: 26
	action: tensor([[ 0.9311,  0.0468, -0.4805,  0.3691, -0.1152,  0.1907, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-2.8151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9354988809587035, distance: 0.2906299170481781 entropy 0.03264415264129639
epoch: 78, step: 27
	action: tensor([[ 1.1639, -0.0322, -0.5014,  0.9713,  0.1173,  0.5323, -0.2985]],
       dtype=torch.float64)
	q_value: tensor([[-1.5967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0848245235442112 entropy 0.03264415264129639
epoch: 78, step: 28
	action: tensor([[ 0.5827,  0.3873, -0.0294,  0.4233, -0.2462, -0.0762, -0.1222]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9464487748208015, distance: 0.264814279021787 entropy 0.03264415264129639
epoch: 78, step: 29
	action: tensor([[ 0.9271,  0.3828, -0.7057,  0.3368, -0.1843, -0.0628,  0.3250]],
       dtype=torch.float64)
	q_value: tensor([[-1.1235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9826233436672693, distance: 0.15084808430910235 entropy 0.03264415264129639
epoch: 78, step: 30
	action: tensor([[ 1.4785, -0.0647, -0.2980,  0.7527, -0.2739,  0.0895,  0.3040]],
       dtype=torch.float64)
	q_value: tensor([[-1.8351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8462602933339846, distance: 0.4486934371345491 entropy 0.03264415264129639
epoch: 78, step: 31
	action: tensor([[ 1.4051,  0.0479, -0.7704,  0.7859, -0.4996,  0.3494,  0.2320]],
       dtype=torch.float64)
	q_value: tensor([[-2.3230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9717914183831042, distance: 0.1921973157778635 entropy 0.03264415264129639
epoch: 78, step: 32
	action: tensor([[ 0.8397, -0.0939, -0.1127,  0.8113, -0.3609, -0.1584, -0.3546]],
       dtype=torch.float64)
	q_value: tensor([[-3.0170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9677247001549574, distance: 0.20558519495486194 entropy 0.03264415264129639
epoch: 78, step: 33
	action: tensor([[ 1.5029,  0.5219, -0.8607,  0.9879, -0.1559, -0.1859, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-1.7097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9604277486770441, distance: 0.22764183228402046 entropy 0.03264415264129639
epoch: 78, step: 34
	action: tensor([[ 1.4568,  0.0902, -0.5016,  1.0304, -0.0030,  0.3689,  0.1012]],
       dtype=torch.float64)
	q_value: tensor([[-3.9087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9623263425304692, distance: 0.22211382222147485 entropy 0.03264415264129639
epoch: 78, step: 35
	action: tensor([[ 1.0766,  0.2217, -0.3375,  1.1858, -0.4411,  0.0942,  0.3442]],
       dtype=torch.float64)
	q_value: tensor([[-2.8856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9313591417321712, distance: 0.29981133335659604 entropy 0.03264415264129639
epoch: 78, step: 36
	action: tensor([[ 1.3626, -0.2541, -0.2948,  0.6568, -0.3170,  0.1234, -0.2836]],
       dtype=torch.float64)
	q_value: tensor([[-2.1000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7879617405946309, distance: 0.5269432049028981 entropy 0.03264415264129639
epoch: 78, step: 37
	action: tensor([[ 1.4561,  0.3904, -0.6160,  0.7412, -0.2521,  0.5471, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[-2.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9630872097908715, distance: 0.21985944659544726 entropy 0.03264415264129639
epoch: 78, step: 38
	action: tensor([[ 1.2418,  0.5157, -0.2327,  1.1150, -0.5256,  0.3887,  0.1390]],
       dtype=torch.float64)
	q_value: tensor([[-3.7217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7947640057782548, distance: 0.5184220416168932 entropy 0.03264415264129639
epoch: 78, step: 39
	action: tensor([[ 1.3096,  0.8616, -0.4919,  0.7743, -0.3191,  0.4596, -0.3100]],
       dtype=torch.float64)
	q_value: tensor([[-3.0940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 40
	action: tensor([[ 0.6587, -0.0767,  0.0111,  0.7009,  0.0423, -0.0884,  0.1662]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.926316844751374, distance: 0.3106281432343142 entropy 0.03264415264129639
epoch: 78, step: 41
	action: tensor([[ 0.8653,  0.3777, -0.7114,  0.4574, -0.6361,  0.3041,  0.3108]],
       dtype=torch.float64)
	q_value: tensor([[-0.5825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9575966696243735, distance: 0.23564415784091672 entropy 0.03264415264129639
epoch: 78, step: 42
	action: tensor([[ 1.7590,  0.6679, -0.5989,  1.2308, -0.4721,  0.1377, -0.1803]],
       dtype=torch.float64)
	q_value: tensor([[-2.2851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8420394311427215, distance: 0.4548110815536352 entropy 0.03264415264129639
epoch: 78, step: 43
	action: tensor([[ 1.6400,  0.3916,  0.1164,  0.9885, -0.5375,  0.3005, -0.1779]],
       dtype=torch.float64)
	q_value: tensor([[-4.6499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 44
	action: tensor([[ 0.8416,  0.4368, -0.5224,  0.5978, -0.0924, -0.1743, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9698900678235102, distance: 0.1985690609039299 entropy 0.03264415264129639
epoch: 78, step: 45
	action: tensor([[ 1.0988, -0.0023, -0.1379,  0.6316,  0.0438,  0.2255, -0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-1.8624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.964491246349882, distance: 0.21563755936048876 entropy 0.03264415264129639
epoch: 78, step: 46
	action: tensor([[ 1.3132,  0.1122, -0.3037,  0.8828, -0.3181,  0.8035, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-1.7454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9685024616772926, distance: 0.20309302100871396 entropy 0.03264415264129639
epoch: 78, step: 47
	action: tensor([[ 1.3405,  0.2669, -0.2809,  0.6468, -0.6067,  0.0168, -0.1809]],
       dtype=torch.float64)
	q_value: tensor([[-3.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9177758162426123, distance: 0.32813796372238135 entropy 0.03264415264129639
epoch: 78, step: 48
	action: tensor([[ 1.0184, -0.2526, -0.3779,  0.7382, -0.3020, -0.1032,  0.3019]],
       dtype=torch.float64)
	q_value: tensor([[-3.1511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8736719591009183, distance: 0.40673034382123485 entropy 0.03264415264129639
epoch: 78, step: 49
	action: tensor([[ 1.1903,  0.0539, -0.3947,  0.2522, -0.1264, -0.0623, -0.4106]],
       dtype=torch.float64)
	q_value: tensor([[-1.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.813671958031064, distance: 0.493964573917714 entropy 0.03264415264129639
epoch: 78, step: 50
	action: tensor([[ 1.2055,  0.6069, -0.7787,  0.4841, -0.0819,  0.0389,  0.2048]],
       dtype=torch.float64)
	q_value: tensor([[-2.4375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9631931812493706, distance: 0.2195436269018847 entropy 0.03264415264129639
epoch: 78, step: 51
	action: tensor([[ 1.0197,  0.5562, -0.0929,  0.0527, -0.2600,  0.0518, -0.2430]],
       dtype=torch.float64)
	q_value: tensor([[-2.8685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.967395087240192, distance: 0.206632302247158 entropy 0.03264415264129639
epoch: 78, step: 52
	action: tensor([[ 1.5069, -0.2190, -0.5555,  0.4376, -0.5617, -0.1258, -0.2281]],
       dtype=torch.float64)
	q_value: tensor([[-2.3379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5621935641405069, distance: 0.7571776113600945 entropy 0.03264415264129639
epoch: 78, step: 53
	action: tensor([[ 1.3951,  0.1241, -0.2303,  0.6585, -0.6773,  0.3667,  0.1996]],
       dtype=torch.float64)
	q_value: tensor([[-3.0874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9297809917974594, distance: 0.3032382899038528 entropy 0.03264415264129639
epoch: 78, step: 54
	action: tensor([[ 1.0238,  0.2504, -0.4303,  0.2108, -0.4163,  0.1751, -0.1731]],
       dtype=torch.float64)
	q_value: tensor([[-2.8565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9602091581344828, distance: 0.2282696942595989 entropy 0.03264415264129639
epoch: 78, step: 55
	action: tensor([[ 1.4989, -0.0838, -0.5441,  0.8902, -0.1147,  0.3473, -0.2228]],
       dtype=torch.float64)
	q_value: tensor([[-2.3658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9351617789069899, distance: 0.29138838657988086 entropy 0.03264415264129639
epoch: 78, step: 56
	action: tensor([[ 1.2818, -0.3348, -0.5489,  0.9845, -0.1380,  0.0875, -0.2475]],
       dtype=torch.float64)
	q_value: tensor([[-3.2475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9146814962732748, distance: 0.33425530480883814 entropy 0.03264415264129639
epoch: 78, step: 57
	action: tensor([[ 1.3332,  0.2957, -0.6741,  1.0464, -0.6514,  0.0474,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-2.4771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9818622088623565, distance: 0.1541164126892373 entropy 0.03264415264129639
epoch: 78, step: 58
	action: tensor([[ 1.0897,  0.1618, -0.0828,  0.6360, -0.3292,  0.2791,  0.3520]],
       dtype=torch.float64)
	q_value: tensor([[-3.3103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9792610188708782, distance: 0.1647974174210613 entropy 0.03264415264129639
epoch: 78, step: 59
	action: tensor([[ 1.3527,  0.3131, -0.4887,  0.9040, -0.5434, -0.2140,  0.1014]],
       dtype=torch.float64)
	q_value: tensor([[-1.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9454581020714559, distance: 0.2672525245699943 entropy 0.03264415264129639
epoch: 78, step: 60
	action: tensor([[ 1.3507,  0.1048, -0.1843,  0.7921, -0.1912, -0.0854, -0.0851]],
       dtype=torch.float64)
	q_value: tensor([[-2.9465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8875223841355345, distance: 0.38378654451983624 entropy 0.03264415264129639
epoch: 78, step: 61
	action: tensor([[ 1.4487,  0.5410, -0.4042,  1.1383, -0.1207,  0.1607, -0.3337]],
       dtype=torch.float64)
	q_value: tensor([[-2.4819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 62
	action: tensor([[ 0.9497,  0.0512, -0.0518,  0.6733, -0.4034,  0.1208, -0.1759]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.986305398068756, distance: 0.13391567037663532 entropy 0.03264415264129639
epoch: 78, step: 63
	action: tensor([[ 1.4931, -0.0737, -0.4198,  0.8343, -0.1430,  0.2991, -0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-1.8468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9079871819745382, distance: 0.3471209767531012 entropy 0.03264415264129639
epoch: 78, step: 64
	action: tensor([[ 0.7284,  0.3283, -0.6403,  0.8649, -0.4144,  0.1425, -0.6378]],
       dtype=torch.float64)
	q_value: tensor([[-2.9422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8559239533928353, distance: 0.4343627607878948 entropy 0.03264415264129639
epoch: 78, step: 65
	action: tensor([[ 1.2801, -0.1024, -0.4868,  0.8683,  0.0936,  0.1793,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[-2.8925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9612145382765046, distance: 0.22536744246193932 entropy 0.03264415264129639
epoch: 78, step: 66
	action: tensor([[ 1.2593,  0.1854, -0.8536,  1.2320, -0.9984,  0.3627, -0.1488]],
       dtype=torch.float64)
	q_value: tensor([[-2.1538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9357410895622119, distance: 0.29008373060107207 entropy 0.03264415264129639
epoch: 78, step: 67
	action: tensor([[ 1.5648,  0.2875, -0.7628,  1.1825, -0.4806,  0.2092, -0.0466]],
       dtype=torch.float64)
	q_value: tensor([[-4.0369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9774951345462168, distance: 0.17167019627705438 entropy 0.03264415264129639
epoch: 78, step: 68
	action: tensor([[ 1.9097,  0.2575, -0.8843,  0.8890, -0.6711,  0.2898, -0.1918]],
       dtype=torch.float64)
	q_value: tensor([[-4.0653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 69
	action: tensor([[ 1.0558,  0.1344, -0.2060,  0.1425, -0.3842,  0.1832, -0.3801]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8879391700927932, distance: 0.3830748238082879 entropy 0.03264415264129639
epoch: 78, step: 70
	action: tensor([[ 0.7973,  0.2971, -0.6952,  0.2313, -0.2545,  0.3249, -0.1390]],
       dtype=torch.float64)
	q_value: tensor([[-2.3853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9536129549433164, distance: 0.2464648822833007 entropy 0.03264415264129639
epoch: 78, step: 71
	action: tensor([[ 0.8564,  0.0848, -0.6283,  0.8183, -0.6356,  0.1899, -0.2703]],
       dtype=torch.float64)
	q_value: tensor([[-2.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.932644169378626, distance: 0.29699168567282064 entropy 0.03264415264129639
epoch: 78, step: 72
	action: tensor([[ 1.3638, -0.0140, -0.3493,  0.7463, -0.4684,  0.4528,  0.4391]],
       dtype=torch.float64)
	q_value: tensor([[-2.4696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9537021760674346, distance: 0.2462277421736919 entropy 0.03264415264129639
epoch: 78, step: 73
	action: tensor([[ 0.8001,  0.4282, -0.6058,  1.1145, -0.0959,  0.1620,  0.3723]],
       dtype=torch.float64)
	q_value: tensor([[-2.3815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7841709037269358, distance: 0.5316327042104165 entropy 0.03264415264129639
epoch: 78, step: 74
	action: tensor([[ 0.7952,  0.3346, -0.5560,  0.7952, -0.3799, -0.0202,  0.1480]],
       dtype=torch.float64)
	q_value: tensor([[-1.8295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9353979744465478, distance: 0.29085716109426346 entropy 0.03264415264129639
epoch: 78, step: 75
	action: tensor([[ 1.0108,  0.0827, -0.3999,  0.9148,  0.0149,  0.6580,  0.0193]],
       dtype=torch.float64)
	q_value: tensor([[-1.7872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9696139270896724, distance: 0.19947752957050954 entropy 0.03264415264129639
epoch: 78, step: 76
	action: tensor([[ 1.0914,  0.0309, -0.6432,  0.9147, -0.0835,  0.3985,  0.0791]],
       dtype=torch.float64)
	q_value: tensor([[-2.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9867431675766274, distance: 0.1317578733714111 entropy 0.03264415264129639
epoch: 78, step: 77
	action: tensor([[ 1.1326, -0.0334, -0.4954,  1.0072, -0.3596,  0.3588, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-2.1979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08064946083280787 entropy 0.03264415264129639
epoch: 78, step: 78
	action: tensor([[ 0.4559,  0.2038, -0.3025,  0.7620, -0.5215,  0.5351,  0.0676]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7800926232868142, distance: 0.5366320316286821 entropy 0.03264415264129639
epoch: 78, step: 79
	action: tensor([[ 1.0321,  0.2365, -0.6378,  0.6102, -0.2502,  0.1632, -0.4196]],
       dtype=torch.float64)
	q_value: tensor([[-1.4986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09393169240536116 entropy 0.03264415264129639
epoch: 78, step: 80
	action: tensor([[ 0.5969,  0.0510, -0.2056,  0.2950, -0.2945,  0.5528, -0.1939]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8774467489476153, distance: 0.4006075330640436 entropy 0.03264415264129639
epoch: 78, step: 81
	action: tensor([[ 0.8969,  0.3798, -0.8611,  0.9276, -0.2409,  0.4470,  0.2226]],
       dtype=torch.float64)
	q_value: tensor([[-1.4303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.836917789406965, distance: 0.46212555816239037 entropy 0.03264415264129639
epoch: 78, step: 82
	action: tensor([[ 0.7729,  0.2906, -0.5156,  0.5731, -0.3983, -0.1464, -0.1933]],
       dtype=torch.float64)
	q_value: tensor([[-2.4723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9713580749153308, distance: 0.19366796783298967 entropy 0.03264415264129639
epoch: 78, step: 83
	action: tensor([[ 1.2938,  0.5633, -0.3265,  0.8435, -0.1326,  0.2616,  0.1029]],
       dtype=torch.float64)
	q_value: tensor([[-1.8946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 84
	action: tensor([[ 0.8688, -0.1768, -0.2275,  0.5372, -0.1268,  0.4856,  0.0543]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9268207313946653, distance: 0.3095641966241967 entropy 0.03264415264129639
epoch: 78, step: 85
	action: tensor([[ 1.2284,  0.3513, -0.2698,  0.8076, -0.5472,  0.2108,  0.1344]],
       dtype=torch.float64)
	q_value: tensor([[-1.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9586492993854194, distance: 0.23270093560126787 entropy 0.03264415264129639
epoch: 78, step: 86
	action: tensor([[ 0.6920,  0.4916, -0.6712,  0.1347, -0.1516,  0.0819,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-2.6968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9647484144867612, distance: 0.2148552750536761 entropy 0.03264415264129639
epoch: 78, step: 87
	action: tensor([[0.9948, 0.0875, 0.4216, 0.0333, 0.0268, 0.3993, 0.0622]],
       dtype=torch.float64)
	q_value: tensor([[-1.7885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8689857219122993, distance: 0.4142056392458786 entropy 0.03264415264129639
epoch: 78, step: 88
	action: tensor([[ 1.2200,  0.3914, -0.5650,  0.5678, -0.1919,  0.3650, -0.2000]],
       dtype=torch.float64)
	q_value: tensor([[-1.1756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9821209144963603, distance: 0.15301335720324655 entropy 0.03264415264129639
epoch: 78, step: 89
	action: tensor([[ 1.2001, -0.3810, -0.7381,  0.8701, -0.1041,  0.3105, -0.1329]],
       dtype=torch.float64)
	q_value: tensor([[-3.1880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8757118676408474, distance: 0.4034330971299763 entropy 0.03264415264129639
epoch: 78, step: 90
	action: tensor([[ 0.9982,  0.0398, -0.0054,  1.2416, -0.5475,  0.3314,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[-2.2477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9480784481769811, distance: 0.2607537274400235 entropy 0.03264415264129639
epoch: 78, step: 91
	action: tensor([[ 1.5035,  0.3747, -0.3538,  0.4213, -0.3944,  0.1756,  0.3757]],
       dtype=torch.float64)
	q_value: tensor([[-2.1839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8501118694618871, distance: 0.443037323517735 entropy 0.03264415264129639
epoch: 78, step: 92
	action: tensor([[ 1.3245,  0.0407, -0.3240,  0.3756, -0.1042,  0.0658,  0.2542]],
       dtype=torch.float64)
	q_value: tensor([[-2.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8068665442945022, distance: 0.5029044157190677 entropy 0.03264415264129639
epoch: 78, step: 93
	action: tensor([[ 1.5692, -0.1422, -0.2846,  0.5772,  0.0195, -0.1393,  0.2042]],
       dtype=torch.float64)
	q_value: tensor([[-1.9760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6000599487643964, distance: 0.7236926172989936 entropy 0.03264415264129639
epoch: 78, step: 94
	action: tensor([[1.1000, 0.3147, 0.0236, 0.7929, 0.1210, 0.2332, 0.0737]],
       dtype=torch.float64)
	q_value: tensor([[-2.2199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9138782644366032, distance: 0.3358250437015189 entropy 0.03264415264129639
epoch: 78, step: 95
	action: tensor([[ 1.3242,  0.3514, -0.3912,  0.8237,  0.0027,  0.1763,  0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-1.7750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9346268955679222, distance: 0.29258782319365534 entropy 0.03264415264129639
epoch: 78, step: 96
	action: tensor([[ 1.4674,  0.4356, -0.5423,  1.2217, -0.5435,  0.1850, -0.1333]],
       dtype=torch.float64)
	q_value: tensor([[-2.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8894729921191779, distance: 0.38044414074959815 entropy 0.03264415264129639
epoch: 78, step: 97
	action: tensor([[ 1.4256,  0.6351, -0.7774,  0.8026,  0.1225,  0.3351, -0.4504]],
       dtype=torch.float64)
	q_value: tensor([[-4.0204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9110733435023883, distance: 0.3412500118946755 entropy 0.03264415264129639
epoch: 78, step: 98
	action: tensor([[ 1.5851,  0.4946, -0.8619,  0.7069, -0.5479,  0.8130,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-4.2282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9885717631523349, distance: 0.12233375584935831 entropy 0.03264415264129639
epoch: 78, step: 99
	action: tensor([[ 1.2657,  0.5583, -0.2536,  1.0484, -0.8418, -0.0143,  0.0708]],
       dtype=torch.float64)
	q_value: tensor([[-4.7090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 100
	action: tensor([[ 0.5991,  0.4555, -0.0791,  0.1491,  0.0498, -0.0444,  0.0486]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9398648227162114, distance: 0.28062153190067035 entropy 0.03264415264129639
epoch: 78, step: 101
	action: tensor([[ 0.5050, -0.3735,  0.0371,  0.3297, -0.2401,  0.1914, -0.3158]],
       dtype=torch.float64)
	q_value: tensor([[-0.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5979880658519174, distance: 0.7255647347129413 entropy 0.03264415264129639
epoch: 78, step: 102
	action: tensor([[ 1.4460, -0.0082, -0.8248,  0.8090, -0.3359,  0.2543, -0.2836]],
       dtype=torch.float64)
	q_value: tensor([[-0.7548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9393332075351517, distance: 0.28185919691089273 entropy 0.03264415264129639
epoch: 78, step: 103
	action: tensor([[ 1.8681, -0.1284, -0.4483,  0.9914,  0.2532, -0.1453, -0.1379]],
       dtype=torch.float64)
	q_value: tensor([[-3.6151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 104
	action: tensor([[ 0.9660,  0.2258, -0.5149,  0.6704, -0.3461,  0.4585, -0.0427]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9746294160783571, distance: 0.18227282503532394 entropy 0.03264415264129639
epoch: 78, step: 105
	action: tensor([[ 1.2012,  0.3371, -0.1580,  0.7034, -0.7308,  0.1674, -0.0213]],
       dtype=torch.float64)
	q_value: tensor([[-2.3843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9561716518102582, distance: 0.23957099955374778 entropy 0.03264415264129639
epoch: 78, step: 106
	action: tensor([[ 0.9478, -0.1678, -0.4110,  0.4270,  0.2132,  0.3161,  0.3602]],
       dtype=torch.float64)
	q_value: tensor([[-2.8119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8735883685886936, distance: 0.40686488709125995 entropy 0.03264415264129639
epoch: 78, step: 107
	action: tensor([[ 0.8690,  0.0791, -0.3673,  0.4527, -0.2079,  0.6802,  0.4401]],
       dtype=torch.float64)
	q_value: tensor([[-1.0624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9805854930735579, distance: 0.15944829361046284 entropy 0.03264415264129639
epoch: 78, step: 108
	action: tensor([[ 0.5015,  0.3966, -0.3726,  0.7133,  0.0719, -0.1694, -0.2002]],
       dtype=torch.float64)
	q_value: tensor([[-1.5998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8697562468809898, distance: 0.4129858239939076 entropy 0.03264415264129639
epoch: 78, step: 109
	action: tensor([[ 0.9672, -0.1850, -0.5176,  0.6964, -0.2888,  0.1645,  0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-1.2652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9158870448236291, distance: 0.3318853933337026 entropy 0.03264415264129639
epoch: 78, step: 110
	action: tensor([[ 1.2831, -0.0279, -0.1235,  0.6840, -0.5816,  0.4951, -0.2494]],
       dtype=torch.float64)
	q_value: tensor([[-1.5020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9485687450365637, distance: 0.2595196543251994 entropy 0.03264415264129639
epoch: 78, step: 111
	action: tensor([[ 1.3216,  0.2150, -0.3213,  1.1416, -0.4101, -0.0495,  0.1772]],
       dtype=torch.float64)
	q_value: tensor([[-2.9570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9393681967836742, distance: 0.2817779047948234 entropy 0.03264415264129639
epoch: 78, step: 112
	action: tensor([[ 1.7726,  0.1103, -0.7210,  0.6553, -0.5249,  0.2889,  0.1094]],
       dtype=torch.float64)
	q_value: tensor([[-2.6367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 113
	action: tensor([[ 0.4740,  0.4726, -0.2315,  0.2373, -0.2321,  0.4225,  0.4586]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8674066595480621, distance: 0.4166942899608701 entropy 0.03264415264129639
epoch: 78, step: 114
	action: tensor([[ 0.9850,  0.5100, -0.2734,  0.4319, -0.3883,  0.2571,  0.3954]],
       dtype=torch.float64)
	q_value: tensor([[-1.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9702419161191198, distance: 0.19740546659388306 entropy 0.03264415264129639
epoch: 78, step: 115
	action: tensor([[ 1.3189, -0.4595, -0.3906,  0.7345, -0.5586,  0.2150,  0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-1.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.751325086856133, distance: 0.5706537568246441 entropy 0.03264415264129639
epoch: 78, step: 116
	action: tensor([[ 0.9986,  0.2732,  0.1161,  0.4860, -0.4459,  0.1460, -0.4359]],
       dtype=torch.float64)
	q_value: tensor([[-2.1910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9770366633556594, distance: 0.17341002050763915 entropy 0.03264415264129639
epoch: 78, step: 117
	action: tensor([[ 1.0170,  0.6888, -0.4516,  0.7099, -0.1472,  0.2391, -0.3935]],
       dtype=torch.float64)
	q_value: tensor([[-2.3689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 118
	action: tensor([[ 0.7446,  0.1934, -0.2849,  0.4314, -0.2915,  0.1822,  0.3090]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9535586373143237, distance: 0.24660914100111472 entropy 0.03264415264129639
epoch: 78, step: 119
	action: tensor([[ 1.1509,  0.0064, -0.3923,  0.7305, -0.3316,  0.3524, -0.1077]],
       dtype=torch.float64)
	q_value: tensor([[-1.1297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9895550710469135, distance: 0.1169524807663176 entropy 0.03264415264129639
epoch: 78, step: 120
	action: tensor([[ 0.9173, -0.0873, -0.0558,  0.3431,  0.0676,  0.5175, -0.2541]],
       dtype=torch.float64)
	q_value: tensor([[-2.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9266214705985928, distance: 0.3099853683334128 entropy 0.03264415264129639
epoch: 78, step: 121
	action: tensor([[ 1.1593,  0.0091, -0.6864,  1.1751, -0.1088,  0.1446, -0.0887]],
       dtype=torch.float64)
	q_value: tensor([[-1.5246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9819426503385338, distance: 0.15377427819670486 entropy 0.03264415264129639
epoch: 78, step: 122
	action: tensor([[ 1.3391,  0.2077, -0.4632,  1.1057,  0.0527,  0.4153,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-2.5730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9391497577541846, distance: 0.2822850310100346 entropy 0.03264415264129639
epoch: 78, step: 123
	action: tensor([[ 1.6910,  0.4311, -0.1602,  0.9424, -0.4994,  0.4047,  0.0910]],
       dtype=torch.float64)
	q_value: tensor([[-2.8030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 78, step: 124
	action: tensor([[ 0.3430, -0.1080, -0.2973,  0.8051,  0.0969,  0.7600,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[-3.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7696694891983897, distance: 0.5492024027933124 entropy 0.03264415264129639
epoch: 78, step: 125
	action: tensor([[ 0.9996, -0.0549, -0.3983,  0.8412, -0.3719,  0.4296,  0.4861]],
       dtype=torch.float64)
	q_value: tensor([[-0.9584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9846032142487963, distance: 0.14199456661577078 entropy 0.03264415264129639
epoch: 78, step: 126
	action: tensor([[ 1.4721,  0.3702, -0.2761,  0.5597, -0.1632,  0.4815,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[-1.6161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9026931945262566, distance: 0.3569671902464376 entropy 0.03264415264129639
epoch: 78, step: 127
	action: tensor([[ 1.3345,  0.4918, -0.4636,  0.8271, -0.1733,  0.5916, -0.1199]],
       dtype=torch.float64)
	q_value: tensor([[-3.2424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9048470904572349, distance: 0.35299432996739843 entropy 0.03264415264129639
LOSS epoch 78 actor 276.3196074468246 critic 2473.3826172961017 
epoch: 79, step: 0
	action: tensor([[ 0.7530,  0.2492, -0.2541,  0.3590, -0.3492,  0.3001,  0.0300]],
       dtype=torch.float64)
	q_value: tensor([[-4.1641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9741245457364256, distance: 0.18407749036947618 entropy 0.03264415264129639
epoch: 79, step: 1
	action: tensor([[ 0.5759,  0.1748, -0.6098,  0.3774,  0.1828,  0.3712,  0.4002]],
       dtype=torch.float64)
	q_value: tensor([[-1.8165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8811204347140995, distance: 0.394557494805039 entropy 0.03264415264129639
epoch: 79, step: 2
	action: tensor([[ 0.7955,  0.4356, -0.3875,  0.2837, -0.4647,  0.4330, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-1.2688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9679435796257225, distance: 0.2048869066449406 entropy 0.03264415264129639
epoch: 79, step: 3
	action: tensor([[ 0.8685,  0.1066, -0.4043,  0.6422, -0.2929,  0.0436,  0.3593]],
       dtype=torch.float64)
	q_value: tensor([[-2.5643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.979839053419963, distance: 0.16248458020926976 entropy 0.03264415264129639
epoch: 79, step: 4
	action: tensor([[ 0.2092,  0.1917, -0.4537,  0.1710,  0.0860, -0.1250, -0.0916]],
       dtype=torch.float64)
	q_value: tensor([[-1.5829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6479772357269724, distance: 0.6789566834356855 entropy 0.03264415264129639
epoch: 79, step: 5
	action: tensor([[ 0.8933,  0.6002, -0.5209,  0.6327,  0.0403,  0.0859, -0.3961]],
       dtype=torch.float64)
	q_value: tensor([[-0.7349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9096520309377331, distance: 0.34396629661711803 entropy 0.03264415264129639
epoch: 79, step: 6
	action: tensor([[ 0.3285,  0.1928,  0.1931,  0.7723, -0.5303,  1.0178,  0.0885]],
       dtype=torch.float64)
	q_value: tensor([[-3.1032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 79, step: 7
	action: tensor([[ 0.4162, -0.1299, -0.6302,  0.3137, -0.1846,  0.1530, -0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-4.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6136441410750242, distance: 0.7112961279126024 entropy 0.03264415264129639
epoch: 79, step: 8
	action: tensor([[ 0.3346, -0.2698, -0.3187, -0.1347,  0.0515,  0.1872, -0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-0.9561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35182143424708334, distance: 0.9213062683688602 entropy 0.03264415264129639
epoch: 79, step: 9
	action: tensor([[ 0.6543, -0.0644, -0.0711,  0.1844, -0.0255, -0.3353,  0.3225]],
       dtype=torch.float64)
	q_value: tensor([[-0.5454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6883775137693446, distance: 0.6388090900290258 entropy 0.03264415264129639
epoch: 79, step: 10
	action: tensor([[ 0.7743, -0.0987, -0.3628,  0.6887, -0.1708, -0.0608,  0.1034]],
       dtype=torch.float64)
	q_value: tensor([[-0.6694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.909525406592293, distance: 0.3442072497768172 entropy 0.03264415264129639
epoch: 79, step: 11
	action: tensor([[ 0.3103,  0.0913, -0.2447,  0.0215, -0.3356,  0.1193,  0.4742]],
       dtype=torch.float64)
	q_value: tensor([[-1.2639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6573794440341163, distance: 0.6698281590808608 entropy 0.03264415264129639
epoch: 79, step: 12
	action: tensor([[ 0.5734, -0.1149, -0.1047,  0.4853, -0.0720,  0.3636,  0.2765]],
       dtype=torch.float64)
	q_value: tensor([[-0.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8794899668209111, distance: 0.3972540159292183 entropy 0.03264415264129639
epoch: 79, step: 13
	action: tensor([[ 1.2146,  0.4373, -0.0114,  0.6199, -0.4018, -0.1697,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-0.7903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9143643190928904, distance: 0.3348760365424669 entropy 0.03264415264129639
epoch: 79, step: 14
	action: tensor([[ 0.7879,  0.0891,  0.0400,  0.6862, -0.0639,  0.2858, -0.0592]],
       dtype=torch.float64)
	q_value: tensor([[-2.7765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05062448928662159 entropy 0.03264415264129639
epoch: 79, step: 15
	action: tensor([[ 1.0008, -0.3180, -0.4336,  0.3230, -0.0761, -0.0211, -0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-4.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6344582063357356, distance: 0.6918711356456625 entropy 0.03264415264129639
epoch: 79, step: 16
	action: tensor([[ 0.3690,  0.4899, -0.6909,  0.7064,  0.0072, -0.0053, -0.1534]],
       dtype=torch.float64)
	q_value: tensor([[-1.5576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6851181543252394, distance: 0.6421411544015547 entropy 0.03264415264129639
epoch: 79, step: 17
	action: tensor([[ 0.8906,  0.0674, -0.1045,  0.7838, -0.1159,  0.0664, -0.0840]],
       dtype=torch.float64)
	q_value: tensor([[-1.8186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06160002864266115 entropy 0.03264415264129639
epoch: 79, step: 18
	action: tensor([[ 0.5627,  0.6743, -0.6122,  0.3483,  0.2167, -0.0822,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-4.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.87122591777449, distance: 0.41064914689838034 entropy 0.03264415264129639
epoch: 79, step: 19
	action: tensor([[ 0.8536,  0.4572, -0.4699, -0.0867, -0.7903, -0.1290,  0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-1.9175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9492606207622654, distance: 0.25776815798548186 entropy 0.03264415264129639
epoch: 79, step: 20
	action: tensor([[ 0.6046, -0.1461, -0.5191,  0.7434,  0.1202,  0.1629, -0.0701]],
       dtype=torch.float64)
	q_value: tensor([[-2.4891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.833420993133975, distance: 0.46705371141983576 entropy 0.03264415264129639
epoch: 79, step: 21
	action: tensor([[ 0.5495,  0.0050, -0.6531,  0.3220, -0.4307,  0.0037, -0.1738]],
       dtype=torch.float64)
	q_value: tensor([[-1.1358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7731394826078741, distance: 0.5450497595419289 entropy 0.03264415264129639
epoch: 79, step: 22
	action: tensor([[ 0.6802,  0.2055, -0.2776,  0.1550, -0.1295, -0.1320, -0.0434]],
       dtype=torch.float64)
	q_value: tensor([[-1.5305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8965579041392528, distance: 0.3680487515537475 entropy 0.03264415264129639
epoch: 79, step: 23
	action: tensor([[ 0.8977,  0.7459,  0.1614,  0.7811, -0.3379,  0.2864,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-1.1987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 79, step: 24
	action: tensor([[ 0.7174,  0.1927, -0.1030,  0.4022,  0.0206, -0.1061, -0.1405]],
       dtype=torch.float64)
	q_value: tensor([[-4.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9440348617269638, distance: 0.2707169720444903 entropy 0.03264415264129639
epoch: 79, step: 25
	action: tensor([[ 0.4336, -0.4872, -0.4540,  0.1901, -0.0945, -0.0639,  0.0385]],
       dtype=torch.float64)
	q_value: tensor([[-1.2203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2808904936819857, distance: 0.9704076426698998 entropy 0.03264415264129639
epoch: 79, step: 26
	action: tensor([[ 0.4823,  0.4061, -0.3567,  0.8767, -0.0801,  0.0550,  0.2328]],
       dtype=torch.float64)
	q_value: tensor([[-0.5433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7950785813200706, distance: 0.5180245834747964 entropy 0.03264415264129639
epoch: 79, step: 27
	action: tensor([[ 0.9117,  0.1758, -0.3533,  0.6390, -0.0777, -0.3436,  0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-1.3365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9766698018678807, distance: 0.17478972810669896 entropy 0.03264415264129639
epoch: 79, step: 28
	action: tensor([[ 0.4699, -0.2194, -0.1998,  0.7438, -0.2022, -0.0731, -0.3720]],
       dtype=torch.float64)
	q_value: tensor([[-1.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.799946471969768, distance: 0.5118347883454215 entropy 0.03264415264129639
epoch: 79, step: 29
	action: tensor([[ 0.3718,  0.3738, -0.4644,  0.4885, -0.0958,  0.4287, -0.2670]],
       dtype=torch.float64)
	q_value: tensor([[-1.1266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7655281345278964, distance: 0.5541177506828722 entropy 0.03264415264129639
epoch: 79, step: 30
	action: tensor([[ 0.7613, -0.0722, -0.1087,  0.8443, -0.2834,  0.2441,  0.0779]],
       dtype=torch.float64)
	q_value: tensor([[-1.7691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9840638813714431, distance: 0.14446011965983527 entropy 0.03264415264129639
epoch: 79, step: 31
	action: tensor([[ 0.5781, -0.1247, -0.6769,  0.3805, -0.4320,  0.0791,  0.2029]],
       dtype=torch.float64)
	q_value: tensor([[-1.5052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7155846250112662, distance: 0.6102857482967639 entropy 0.03264415264129639
epoch: 79, step: 32
	action: tensor([[ 0.9139, -0.2090, -0.7030,  0.0707,  0.0200,  0.2603,  0.2422]],
       dtype=torch.float64)
	q_value: tensor([[-1.2785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6678866189522576, distance: 0.659477344091008 entropy 0.03264415264129639
epoch: 79, step: 33
	action: tensor([[ 1.0863, -0.2793,  0.2241,  0.6547, -0.4154, -0.1097,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[-1.5960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7628381769888499, distance: 0.5572872194515056 entropy 0.03264415264129639
epoch: 79, step: 34
	action: tensor([[ 0.6897, -0.2040,  0.1085,  0.6907, -0.0037,  0.2268, -0.2181]],
       dtype=torch.float64)
	q_value: tensor([[-1.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9359423052808595, distance: 0.2896292008631624 entropy 0.03264415264129639
epoch: 79, step: 35
	action: tensor([[ 1.0972, -0.2756, -0.6436,  0.6036, -0.2992,  0.3021,  0.1410]],
       dtype=torch.float64)
	q_value: tensor([[-1.2212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8481108034915245, distance: 0.4459848803340911 entropy 0.03264415264129639
epoch: 79, step: 36
	action: tensor([[ 0.6837,  0.3168, -0.2794,  0.0394, -0.2738, -0.0592,  0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-2.1809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9241115402835047, distance: 0.31524235437774956 entropy 0.03264415264129639
epoch: 79, step: 37
	action: tensor([[ 0.1596,  0.3921,  0.2019,  0.6365,  0.1340, -0.1175, -0.3178]],
       dtype=torch.float64)
	q_value: tensor([[-1.3699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8274140003522783, distance: 0.4754003251943378 entropy 0.03264415264129639
epoch: 79, step: 38
	action: tensor([[ 0.9115,  0.2201,  0.0273,  0.8830,  0.0385,  0.4962, -0.2696]],
       dtype=torch.float64)
	q_value: tensor([[-0.8693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9403296521023127, distance: 0.2795348619321244 entropy 0.03264415264129639
epoch: 79, step: 39
	action: tensor([[ 1.3024,  0.2961, -0.2864,  0.9471, -0.0169,  0.0641,  0.1432]],
       dtype=torch.float64)
	q_value: tensor([[-2.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9258521972927186, distance: 0.3116060176918114 entropy 0.03264415264129639
epoch: 79, step: 40
	action: tensor([[ 1.3599,  0.2444, -0.6399, -0.0193, -0.1840, -0.0116,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[-2.8344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7091514163720858, distance: 0.6171491993150557 entropy 0.03264415264129639
epoch: 79, step: 41
	action: tensor([[ 1.0680,  0.2852, -0.9518,  0.8369, -0.5002,  0.3352, -0.5687]],
       dtype=torch.float64)
	q_value: tensor([[-3.2575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9402238328057894, distance: 0.2797826154708505 entropy 0.03264415264129639
epoch: 79, step: 42
	action: tensor([[ 0.5799,  0.2522, -0.3332,  0.2468,  0.2713,  0.5257,  0.3588]],
       dtype=torch.float64)
	q_value: tensor([[-4.5621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9280055071277353, distance: 0.3070480415912789 entropy 0.03264415264129639
epoch: 79, step: 43
	action: tensor([[ 0.3749,  0.3818, -0.3363,  0.2740, -0.0756,  0.8465,  0.2434]],
       dtype=torch.float64)
	q_value: tensor([[-1.1331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8100799846877645, distance: 0.49870309279764685 entropy 0.03264415264129639
epoch: 79, step: 44
	action: tensor([[ 1.1921,  0.0696, -0.3115,  0.6635, -0.4371,  0.0345,  0.2411]],
       dtype=torch.float64)
	q_value: tensor([[-1.6883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9570079621172294, distance: 0.237274304550064 entropy 0.03264415264129639
epoch: 79, step: 45
	action: tensor([[ 0.6476,  0.3094, -0.0338,  0.4002, -0.3639, -0.0321,  0.3745]],
       dtype=torch.float64)
	q_value: tensor([[-2.3706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9712606167030631, distance: 0.19399717938740296 entropy 0.03264415264129639
epoch: 79, step: 46
	action: tensor([[ 0.8483,  0.6159, -0.6983,  0.3213, -0.2833,  0.4483,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-1.0846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9285150143485497, distance: 0.3059596182955352 entropy 0.03264415264129639
epoch: 79, step: 47
	action: tensor([[ 0.6349, -0.2328, -0.4196,  0.5097, -0.3550, -0.0071, -0.1882]],
       dtype=torch.float64)
	q_value: tensor([[-3.0571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7569288498017687, distance: 0.5641874239913602 entropy 0.03264415264129639
epoch: 79, step: 48
	action: tensor([[ 1.0138,  0.2154, -0.3872,  0.7030, -0.2278,  0.4197,  0.0887]],
       dtype=torch.float64)
	q_value: tensor([[-1.2762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9888697957501285, distance: 0.1207280706740123 entropy 0.03264415264129639
epoch: 79, step: 49
	action: tensor([[ 0.3713,  0.3780, -0.1058,  0.2847,  0.0807, -0.3336, -0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-2.4999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.845661475130599, distance: 0.44956642134378033 entropy 0.03264415264129639
epoch: 79, step: 50
	action: tensor([[ 1.0878,  0.4196, -0.1164,  0.5048, -0.1952,  0.6101,  0.1883]],
       dtype=torch.float64)
	q_value: tensor([[-1.0565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9474380889918596, distance: 0.2623567646339338 entropy 0.03264415264129639
epoch: 79, step: 51
	action: tensor([[ 0.7959,  0.2503, -0.6852,  0.2571, -0.1134, -0.0730, -0.0638]],
       dtype=torch.float64)
	q_value: tensor([[-2.6475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9439112807376359, distance: 0.27101570285905574 entropy 0.03264415264129639
epoch: 79, step: 52
	action: tensor([[ 0.8060, -0.2623, -0.1987,  0.4079, -0.3210,  0.3460, -0.2208]],
       dtype=torch.float64)
	q_value: tensor([[-2.0034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8273124171196548, distance: 0.4755402137073214 entropy 0.03264415264129639
epoch: 79, step: 53
	action: tensor([[ 0.8920, -0.5037, -0.3920,  0.4903, -0.0768,  0.2199, -0.0463]],
       dtype=torch.float64)
	q_value: tensor([[-1.6427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.656746882058781, distance: 0.6704462078656324 entropy 0.03264415264129639
epoch: 79, step: 54
	action: tensor([[ 0.9311,  0.2246, -0.8890,  0.0147, -0.4985,  0.2019, -0.0402]],
       dtype=torch.float64)
	q_value: tensor([[-1.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081116557775136, distance: 0.3468861068104076 entropy 0.03264415264129639
epoch: 79, step: 55
	action: tensor([[ 0.4422,  0.1030, -0.2435,  0.4595, -0.4954,  0.2566,  0.2621]],
       dtype=torch.float64)
	q_value: tensor([[-2.9745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8347571730931046, distance: 0.46517675127992325 entropy 0.03264415264129639
epoch: 79, step: 56
	action: tensor([[ 0.7097,  0.4931, -0.0342,  0.4314,  0.3515,  0.1965, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[-1.0778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.959287889171728, distance: 0.23089711328683465 entropy 0.03264415264129639
epoch: 79, step: 57
	action: tensor([[ 0.2112,  0.3321,  0.0661,  0.6697,  0.2127,  0.4148, -0.3524]],
       dtype=torch.float64)
	q_value: tensor([[-1.3253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8322127039300001, distance: 0.46874454960745077 entropy 0.03264415264129639
epoch: 79, step: 58
	action: tensor([[ 1.2005,  0.1568, -0.3594,  0.6979, -0.1586,  0.6423,  0.3931]],
       dtype=torch.float64)
	q_value: tensor([[-1.1637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.986518099283201, distance: 0.1328716278061134 entropy 0.03264415264129639
epoch: 79, step: 59
	action: tensor([[ 0.8294,  0.0840, -0.6113,  0.5257, -0.2297,  0.2215, -0.1684]],
       dtype=torch.float64)
	q_value: tensor([[-2.5880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9406823730681648, distance: 0.27870744974949024 entropy 0.03264415264129639
epoch: 79, step: 60
	action: tensor([[ 0.5890, -0.0437, -0.3256,  0.4820, -0.2068,  0.2353, -0.0473]],
       dtype=torch.float64)
	q_value: tensor([[-2.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8640329529375063, distance: 0.42196217874488756 entropy 0.03264415264129639
epoch: 79, step: 61
	action: tensor([[ 0.6320,  0.0580, -0.1749,  0.0071, -0.4061,  0.3237, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[-1.1660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8186810690113013, distance: 0.4872796428973723 entropy 0.03264415264129639
epoch: 79, step: 62
	action: tensor([[ 0.9157,  0.1342, -0.0282,  0.2582,  0.0553, -0.0689, -0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-1.3971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9105648538657412, distance: 0.3422242684044335 entropy 0.03264415264129639
epoch: 79, step: 63
	action: tensor([[ 0.9617,  0.2117, -0.4690,  0.3049, -0.8088,  0.0075, -0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-1.4482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9572188902979117, distance: 0.2366915294914443 entropy 0.03264415264129639
epoch: 79, step: 64
	action: tensor([[ 0.4075,  0.1539, -0.1320,  0.5413, -0.1379, -0.1665, -0.2444]],
       dtype=torch.float64)
	q_value: tensor([[-2.7266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8610167695370401, distance: 0.4266167415300736 entropy 0.03264415264129639
epoch: 79, step: 65
	action: tensor([[ 0.5355,  0.7235, -0.0806,  0.6076, -0.4256,  0.6400,  0.2331]],
       dtype=torch.float64)
	q_value: tensor([[-0.9957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 79, step: 66
	action: tensor([[ 0.8901,  0.4840,  0.0771,  0.1461, -0.1295, -0.0485, -0.0734]],
       dtype=torch.float64)
	q_value: tensor([[-4.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9853311894524744, distance: 0.13859709994802563 entropy 0.03264415264129639
epoch: 79, step: 67
	action: tensor([[ 0.5005,  0.4409,  0.3443,  0.5033, -0.1632,  0.1881,  0.0923]],
       dtype=torch.float64)
	q_value: tensor([[-1.7964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9528646655678201, distance: 0.24844484483324739 entropy 0.03264415264129639
epoch: 79, step: 68
	action: tensor([[ 1.0471, -0.1323,  0.0877,  0.5289, -0.3380, -0.1447, -0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-1.1064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8238811282775028, distance: 0.48024144881407754 entropy 0.03264415264129639
epoch: 79, step: 69
	action: tensor([[ 1.0348,  0.1009, -0.1714,  0.3000, -0.3912,  0.1063,  0.3969]],
       dtype=torch.float64)
	q_value: tensor([[-1.7999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9185653216207703, distance: 0.32655879589335407 entropy 0.03264415264129639
epoch: 79, step: 70
	action: tensor([[0.9012, 0.3290, 0.1369, 0.3134, 0.2155, 0.3770, 0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-1.7327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.987006778407691, distance: 0.13044129923306963 entropy 0.03264415264129639
epoch: 79, step: 71
	action: tensor([[ 0.5782, -0.1514, -0.4135,  0.6481,  0.2339,  0.2224, -0.2671]],
       dtype=torch.float64)
	q_value: tensor([[-1.5105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8450939687556454, distance: 0.45039219607851255 entropy 0.03264415264129639
epoch: 79, step: 72
	action: tensor([[ 0.9042,  0.1064, -0.2321,  0.5883, -0.2261,  0.2804,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[-1.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09436269551604355 entropy 0.03264415264129639
epoch: 79, step: 73
	action: tensor([[ 0.7313,  0.3426, -0.3634,  0.5471,  0.0632,  0.0741,  0.1248]],
       dtype=torch.float64)
	q_value: tensor([[-4.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9517400716596297, distance: 0.25139117601948346 entropy 0.03264415264129639
epoch: 79, step: 74
	action: tensor([[ 0.8403,  0.0611, -0.1907, -0.0170, -0.1800,  0.2382, -0.1093]],
       dtype=torch.float64)
	q_value: tensor([[-1.4647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8250175678446197, distance: 0.4786895178827782 entropy 0.03264415264129639
epoch: 79, step: 75
	action: tensor([[ 0.5490,  0.0296, -0.2165,  0.9235, -0.1438, -0.0170, -0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-1.5849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9097630808424055, distance: 0.34375484101098225 entropy 0.03264415264129639
epoch: 79, step: 76
	action: tensor([[ 0.7936,  0.0669, -0.5507,  0.6475, -0.2022, -0.0286,  0.0795]],
       dtype=torch.float64)
	q_value: tensor([[-1.3338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.942126620239812, distance: 0.27529360062020325 entropy 0.03264415264129639
epoch: 79, step: 77
	action: tensor([[ 0.7661,  0.3723, -0.2220,  0.6311, -0.3501,  0.0286, -0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-1.6488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.966127094339501, distance: 0.21061190654309053 entropy 0.03264415264129639
epoch: 79, step: 78
	action: tensor([[ 0.6544,  0.8340, -0.2720,  0.6950, -0.2088,  0.1554, -0.0995]],
       dtype=torch.float64)
	q_value: tensor([[-2.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 79, step: 79
	action: tensor([[ 0.9472,  0.0810, -0.3580,  0.1104,  0.3373,  0.7209, -0.0319]],
       dtype=torch.float64)
	q_value: tensor([[-4.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9545969749850581, distance: 0.24383670880830857 entropy 0.03264415264129639
epoch: 79, step: 80
	action: tensor([[ 0.7561,  0.2723, -0.4880,  0.0340,  0.1658, -0.4261,  0.3923]],
       dtype=torch.float64)
	q_value: tensor([[-1.9842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8573754292115663, distance: 0.43216925604519807 entropy 0.03264415264129639
epoch: 79, step: 81
	action: tensor([[ 0.7371,  0.1839,  0.1352,  0.2917, -0.5301,  0.5740,  0.2998]],
       dtype=torch.float64)
	q_value: tensor([[-1.4089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.979689412477174, distance: 0.16308647147319974 entropy 0.03264415264129639
epoch: 79, step: 82
	action: tensor([[ 0.8992,  0.0824, -0.3964,  0.6462,  0.2937,  0.0122,  0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-1.6695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9759750016159039, distance: 0.17737335340499868 entropy 0.03264415264129639
epoch: 79, step: 83
	action: tensor([[ 1.0539e+00,  3.4569e-02, -4.3370e-01,  2.8138e-01, -3.7455e-01,
          9.9168e-04,  1.6700e-01]], dtype=torch.float64)
	q_value: tensor([[-1.4000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8623509748616671, distance: 0.4245640947664552 entropy 0.03264415264129639
epoch: 79, step: 84
	action: tensor([[ 0.4750, -0.1430, -0.8558,  0.8911, -0.4827, -0.3632, -0.2545]],
       dtype=torch.float64)
	q_value: tensor([[-2.0347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6654524837366201, distance: 0.661889662479819 entropy 0.03264415264129639
epoch: 79, step: 85
	action: tensor([[ 1.0308,  0.0127,  0.1090,  0.1752, -0.2340,  0.4308,  0.3024]],
       dtype=torch.float64)
	q_value: tensor([[-1.8594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8927957458654138, distance: 0.3746818906921209 entropy 0.03264415264129639
epoch: 79, step: 86
	action: tensor([[ 0.1693,  0.1901, -0.6425,  0.1450,  0.0301,  0.1433,  0.1305]],
       dtype=torch.float64)
	q_value: tensor([[-1.5545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6046034600093417, distance: 0.7195701274726422 entropy 0.03264415264129639
epoch: 79, step: 87
	action: tensor([[ 0.5919,  0.5721, -0.0642,  0.4631,  0.2160,  0.0327,  0.5925]],
       dtype=torch.float64)
	q_value: tensor([[-0.9260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9197738448680033, distance: 0.32412660682896505 entropy 0.03264415264129639
epoch: 79, step: 88
	action: tensor([[ 0.4528,  0.3950, -0.4650,  0.6964, -0.0695, -0.2605,  0.2875]],
       dtype=torch.float64)
	q_value: tensor([[-1.0148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.835977197177928, distance: 0.46345631875104076 entropy 0.03264415264129639
epoch: 79, step: 89
	action: tensor([[ 0.2343,  0.2973, -0.1372,  0.5576,  0.1393,  0.0926, -0.0788]],
       dtype=torch.float64)
	q_value: tensor([[-1.1823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7850959474774093, distance: 0.530492191653192 entropy 0.03264415264129639
epoch: 79, step: 90
	action: tensor([[ 0.5687, -0.1699, -0.2602,  0.4606, -0.3234,  0.1991,  0.3728]],
       dtype=torch.float64)
	q_value: tensor([[-0.7800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7886461378579364, distance: 0.5260921085777454 entropy 0.03264415264129639
epoch: 79, step: 91
	action: tensor([[ 0.7399, -0.0041, -0.1851,  0.2322, -0.2707,  0.3228,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[-0.8196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.882938691796942, distance: 0.3915284993890636 entropy 0.03264415264129639
epoch: 79, step: 92
	action: tensor([[ 0.7250,  0.0398, -0.3879,  0.0262,  0.3119,  0.4038, -0.2722]],
       dtype=torch.float64)
	q_value: tensor([[-1.3746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8625293668777341, distance: 0.42428888976633006 entropy 0.03264415264129639
epoch: 79, step: 93
	action: tensor([[ 9.4923e-01,  3.7359e-02, -2.0828e-04,  5.1321e-01, -4.0618e-01,
          4.5023e-01,  1.6784e-01]], dtype=torch.float64)
	q_value: tensor([[-1.4225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9860595094258208, distance: 0.1351125597253235 entropy 0.03264415264129639
epoch: 79, step: 94
	action: tensor([[ 0.7518,  0.3329, -0.4413,  1.1627, -0.2617,  0.0627, -0.0508]],
       dtype=torch.float64)
	q_value: tensor([[-1.9012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8220274691912709, distance: 0.48276211512195494 entropy 0.03264415264129639
epoch: 79, step: 95
	action: tensor([[ 0.6661,  0.3480, -0.4196,  0.3692, -0.5587,  0.1377,  0.0743]],
       dtype=torch.float64)
	q_value: tensor([[-2.4367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9441184313516555, distance: 0.27051477324319023 entropy 0.03264415264129639
epoch: 79, step: 96
	action: tensor([[ 0.6186,  0.2101, -0.0730,  0.2207,  0.0662,  0.2727, -0.1246]],
       dtype=torch.float64)
	q_value: tensor([[-1.9154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9428050281828408, distance: 0.27367530983659216 entropy 0.03264415264129639
epoch: 79, step: 97
	action: tensor([[ 0.8197,  0.3942, -0.4718,  0.2378, -0.0483,  0.1734,  0.0771]],
       dtype=torch.float64)
	q_value: tensor([[-1.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9848724369027404, distance: 0.14074765889062094 entropy 0.03264415264129639
epoch: 79, step: 98
	action: tensor([[ 0.9866,  0.2839,  0.1465,  0.2729, -0.5363,  0.4019,  0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-1.9254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9815526376823787, distance: 0.15542605791630315 entropy 0.03264415264129639
epoch: 79, step: 99
	action: tensor([[ 0.4086, -0.3681, -0.6089,  0.6482, -0.2564,  0.3895,  0.1995]],
       dtype=torch.float64)
	q_value: tensor([[-2.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5415097281828328, distance: 0.7748573481749571 entropy 0.03264415264129639
epoch: 79, step: 100
	action: tensor([[ 0.6287, -0.1205, -0.1038,  0.4275, -0.0322, -0.1372,  0.0603]],
       dtype=torch.float64)
	q_value: tensor([[-0.9944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.785450894950953, distance: 0.5300539154187146 entropy 0.03264415264129639
epoch: 79, step: 101
	action: tensor([[ 0.2078,  0.1201, -0.2635,  0.6087, -0.1128,  0.0598,  0.3868]],
       dtype=torch.float64)
	q_value: tensor([[-0.6843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7085716489116854, distance: 0.6177639949743309 entropy 0.03264415264129639
epoch: 79, step: 102
	action: tensor([[ 0.7127, -0.0617, -0.2719,  0.4890, -0.3007,  0.1683,  0.3296]],
       dtype=torch.float64)
	q_value: tensor([[-0.6129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8912211555255648, distance: 0.377423479255445 entropy 0.03264415264129639
epoch: 79, step: 103
	action: tensor([[ 0.6343, -0.1878, -0.1523, -0.0568, -0.4723,  0.1773,  0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-1.0751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5816156201714973, distance: 0.7401920636414776 entropy 0.03264415264129639
epoch: 79, step: 104
	action: tensor([[ 0.8636, -0.3896, -0.2420,  0.7272, -0.2107,  0.1116,  0.1951]],
       dtype=torch.float64)
	q_value: tensor([[-1.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.835609907485654, distance: 0.46397492817326413 entropy 0.03264415264129639
epoch: 79, step: 105
	action: tensor([[ 1.2466,  0.0508,  0.0466,  0.8904, -0.1202,  0.1249,  0.4800]],
       dtype=torch.float64)
	q_value: tensor([[-1.1697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9187330673382755, distance: 0.3262222864407787 entropy 0.03264415264129639
epoch: 79, step: 106
	action: tensor([[ 0.8557,  0.6489, -0.2929,  0.8577, -0.6093,  0.0306, -0.2505]],
       dtype=torch.float64)
	q_value: tensor([[-1.8797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 79, step: 107
	action: tensor([[ 0.1603,  0.0355,  0.2220, -0.0392, -0.2746,  0.1645, -0.3197]],
       dtype=torch.float64)
	q_value: tensor([[-4.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4799686003544087, distance: 0.8252232909019159 entropy 0.03264415264129639
epoch: 79, step: 108
	action: tensor([[ 0.8012,  0.6341, -0.0212,  0.3241, -0.5424,  0.2412, -0.0397]],
       dtype=torch.float64)
	q_value: tensor([[-0.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388440925064252, distance: 0.28299313532389503 entropy 0.03264415264129639
epoch: 79, step: 109
	action: tensor([[ 0.2604, -0.0495, -0.7494,  0.2680, -0.2951, -0.0219, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-2.4156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49908956747284294, distance: 0.8099099449125288 entropy 0.03264415264129639
epoch: 79, step: 110
	action: tensor([[ 0.8922,  0.1818,  0.0453,  0.2100, -0.2625,  0.3625,  0.2037]],
       dtype=torch.float64)
	q_value: tensor([[-0.9645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9664218168546311, distance: 0.2096936552166236 entropy 0.03264415264129639
epoch: 79, step: 111
	action: tensor([[ 1.0043,  0.3350, -0.7533,  0.4699, -0.3548, -0.3958,  0.3979]],
       dtype=torch.float64)
	q_value: tensor([[-1.5385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.977441446528023, distance: 0.17187484407521833 entropy 0.03264415264129639
epoch: 79, step: 112
	action: tensor([[ 3.6916e-01,  5.0101e-01, -4.2474e-01,  5.3037e-04,  1.3168e-01,
          3.6403e-01,  6.0267e-01]], dtype=torch.float64)
	q_value: tensor([[-2.3418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8440542959037667, distance: 0.4519011026792399 entropy 0.03264415264129639
epoch: 79, step: 113
	action: tensor([[ 0.4313,  0.6132, -0.5321,  0.3390,  0.1366,  0.1036, -0.1093]],
       dtype=torch.float64)
	q_value: tensor([[-1.3478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8168122663218891, distance: 0.48978433205700184 entropy 0.03264415264129639
epoch: 79, step: 114
	action: tensor([[ 1.1206,  0.0806, -0.1965,  0.4814, -0.2946, -0.3266,  0.2720]],
       dtype=torch.float64)
	q_value: tensor([[-1.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8880578435661972, distance: 0.3828719301558453 entropy 0.03264415264129639
epoch: 79, step: 115
	action: tensor([[ 0.6930, -0.1856, -0.4445,  0.6949, -0.0095,  0.5640,  0.0780]],
       dtype=torch.float64)
	q_value: tensor([[-1.8119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8822655430629185, distance: 0.392652607406242 entropy 0.03264415264129639
epoch: 79, step: 116
	action: tensor([[ 0.6159, -0.4363, -0.1404,  0.6529, -0.2143,  0.1873,  0.2224]],
       dtype=torch.float64)
	q_value: tensor([[-1.4669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7353071652020102, distance: 0.5887457510307706 entropy 0.03264415264129639
epoch: 79, step: 117
	action: tensor([[ 0.8863,  0.0457, -0.1147,  0.4798, -0.0187,  0.1664,  0.0608]],
       dtype=torch.float64)
	q_value: tensor([[-0.7599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9621765823644598, distance: 0.22255485732571767 entropy 0.03264415264129639
epoch: 79, step: 118
	action: tensor([[ 0.9910,  0.1694, -0.3473,  0.4357,  0.3568, -0.0226, -0.2651]],
       dtype=torch.float64)
	q_value: tensor([[-1.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9650857330713679, distance: 0.21382484073003458 entropy 0.03264415264129639
epoch: 79, step: 119
	action: tensor([[ 1.2378, -0.3645, -0.3492,  0.5134, -0.0872,  0.6377, -0.0393]],
       dtype=torch.float64)
	q_value: tensor([[-1.9854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8357671476609032, distance: 0.4637529775926366 entropy 0.03264415264129639
epoch: 79, step: 120
	action: tensor([[ 1.1266,  0.2829, -0.5839,  0.4459,  0.0710,  0.2520,  0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-2.4728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9906459355761001, distance: 0.1106768707759773 entropy 0.03264415264129639
epoch: 79, step: 121
	action: tensor([[ 0.6043,  0.3633, -0.3261,  0.6027, -0.1073,  0.0737, -0.0734]],
       dtype=torch.float64)
	q_value: tensor([[-2.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9091185596775889, distance: 0.34498029872030794 entropy 0.03264415264129639
epoch: 79, step: 122
	action: tensor([[ 0.6590,  0.0535, -0.4288,  0.1632,  0.0697,  0.0834,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-1.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8259006204253979, distance: 0.4774801317172153 entropy 0.03264415264129639
epoch: 79, step: 123
	action: tensor([[ 0.6727,  0.1390, -0.1450,  0.6929, -0.1050,  0.1573,  0.5251]],
       dtype=torch.float64)
	q_value: tensor([[-1.0415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.963257703613035, distance: 0.2193511124754364 entropy 0.03264415264129639
epoch: 79, step: 124
	action: tensor([[ 0.8462,  0.6304, -0.9391,  0.5719, -0.4072, -0.2436,  0.1274]],
       dtype=torch.float64)
	q_value: tensor([[-0.9845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9271541315597321, distance: 0.3088582140561473 entropy 0.03264415264129639
epoch: 79, step: 125
	action: tensor([[ 0.7146, -0.0091, -0.5006,  0.6234, -0.3013,  0.1598, -0.1222]],
       dtype=torch.float64)
	q_value: tensor([[-3.0168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8902165355178719, distance: 0.37916230887951824 entropy 0.03264415264129639
epoch: 79, step: 126
	action: tensor([[ 1.2644,  0.1385, -0.5397,  0.7301, -0.0529,  0.6977, -0.4390]],
       dtype=torch.float64)
	q_value: tensor([[-1.7375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07868142284633704 entropy 0.03264415264129639
epoch: 79, step: 127
	action: tensor([[ 0.3141,  0.1325, -0.2605,  0.2498, -0.3895, -0.3110,  0.1043]],
       dtype=torch.float64)
	q_value: tensor([[-4.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7139161083835542, distance: 0.6120732446052061 entropy 0.03264415264129639
LOSS epoch 79 actor 209.54388221182975 critic 2305.2102548410135 
epoch: 80, step: 0
	action: tensor([[ 0.8283, -0.2637, -0.3813,  0.1172, -0.0986,  0.0455,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-0.9358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5966438404396144, distance: 0.7267767741135822 entropy 0.03264415264129639
epoch: 80, step: 1
	action: tensor([[ 0.6821,  0.2449, -0.3146,  0.4431, -0.6024,  0.4183, -0.0310]],
       dtype=torch.float64)
	q_value: tensor([[-1.2675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9287302568548133, distance: 0.3054986462309204 entropy 0.03264415264129639
epoch: 80, step: 2
	action: tensor([[ 0.4804,  0.0734, -0.3139,  0.8592, -0.5878,  0.3576,  0.1299]],
       dtype=torch.float64)
	q_value: tensor([[-2.4188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8189960892537992, distance: 0.486856163446746 entropy 0.03264415264129639
epoch: 80, step: 3
	action: tensor([[ 1.1058,  0.3151, -0.3766,  0.3915, -0.4566,  0.1090,  0.0383]],
       dtype=torch.float64)
	q_value: tensor([[-1.7328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9765620633998076, distance: 0.1751928512716142 entropy 0.03264415264129639
epoch: 80, step: 4
	action: tensor([[ 0.6865,  0.0715, -0.0898,  0.4022, -0.1319,  0.0373, -0.0982]],
       dtype=torch.float64)
	q_value: tensor([[-3.0054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.929444998232908, distance: 0.3039629122503581 entropy 0.03264415264129639
epoch: 80, step: 5
	action: tensor([[ 0.6049,  0.1418, -0.3010,  0.3968, -0.4290,  0.2928, -0.0960]],
       dtype=torch.float64)
	q_value: tensor([[-1.2805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9089340806962137, distance: 0.34533025659017474 entropy 0.03264415264129639
epoch: 80, step: 6
	action: tensor([[ 8.0307e-01,  6.7105e-01,  1.3813e-01,  9.9456e-02, -6.7865e-01,
          5.1713e-04,  5.8612e-02]], dtype=torch.float64)
	q_value: tensor([[-1.8077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9805857546240696, distance: 0.1594472195702256 entropy 0.03264415264129639
epoch: 80, step: 7
	action: tensor([[ 0.5169,  0.1769,  0.0831,  0.7531, -0.1290, -0.1137,  0.0547]],
       dtype=torch.float64)
	q_value: tensor([[-2.4597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9634028691867702, distance: 0.21891736517366117 entropy 0.03264415264129639
epoch: 80, step: 8
	action: tensor([[ 1.4910,  0.3658, -0.2908,  0.3082,  0.2380,  0.2061,  0.4654]],
       dtype=torch.float64)
	q_value: tensor([[-1.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8188716484356665, distance: 0.4870234923446656 entropy 0.03264415264129639
epoch: 80, step: 9
	action: tensor([[ 0.5788, -0.0533, -0.4414,  0.5145, -0.2391, -0.2445,  0.1541]],
       dtype=torch.float64)
	q_value: tensor([[-2.9667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8132151598300559, distance: 0.4945697002332245 entropy 0.03264415264129639
epoch: 80, step: 10
	action: tensor([[ 0.8495,  0.2590, -0.6296,  0.3411, -0.1279,  0.2509, -0.1924]],
       dtype=torch.float64)
	q_value: tensor([[-1.0637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9699896640483899, distance: 0.1982403801555591 entropy 0.03264415264129639
epoch: 80, step: 11
	action: tensor([[ 0.5583,  0.2888, -1.1245,  0.3517, -0.2305,  0.0847, -0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-2.6612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8127915062045126, distance: 0.4951302585001059 entropy 0.03264415264129639
epoch: 80, step: 12
	action: tensor([[ 0.3187,  0.4437, -0.2062,  0.5203, -0.3280,  0.2080,  0.2318]],
       dtype=torch.float64)
	q_value: tensor([[-2.7167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7821654947580392, distance: 0.534096866591404 entropy 0.03264415264129639
epoch: 80, step: 13
	action: tensor([[ 0.6103,  0.3902, -0.2247,  0.2164, -0.0058, -0.1130,  0.1765]],
       dtype=torch.float64)
	q_value: tensor([[-1.2990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9326061049610984, distance: 0.29707559243127046 entropy 0.03264415264129639
epoch: 80, step: 14
	action: tensor([[ 0.6597,  0.3610, -0.1929,  0.6922,  0.0363,  0.1206,  0.2548]],
       dtype=torch.float64)
	q_value: tensor([[-1.1942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9302646670923715, distance: 0.30219211797689177 entropy 0.03264415264129639
epoch: 80, step: 15
	action: tensor([[ 0.6209, -0.0197, -0.3470,  0.2320,  0.2999, -0.0642, -0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-1.3717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7669372883763562, distance: 0.5524501435961052 entropy 0.03264415264129639
epoch: 80, step: 16
	action: tensor([[ 0.6067,  0.4497, -0.5089,  0.5700, -0.0160,  0.0319, -0.0791]],
       dtype=torch.float64)
	q_value: tensor([[-0.9005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8834350580488743, distance: 0.39069753317303557 entropy 0.03264415264129639
epoch: 80, step: 17
	action: tensor([[ 0.4786, -0.0985, -0.6300,  0.4533, -0.1742, -0.0504,  0.4145]],
       dtype=torch.float64)
	q_value: tensor([[-1.9784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.695476307148944, distance: 0.6314911043102697 entropy 0.03264415264129639
epoch: 80, step: 18
	action: tensor([[ 0.1614,  0.0766,  0.0613,  0.2978,  0.2128, -0.1850, -0.1340]],
       dtype=torch.float64)
	q_value: tensor([[-0.9769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6042958418947428, distance: 0.7198499854724287 entropy 0.03264415264129639
epoch: 80, step: 19
	action: tensor([[ 0.8343,  0.1584, -0.5684,  0.2996, -0.7037,  0.1822, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-0.4618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9261960915862313, distance: 0.3108825702889363 entropy 0.03264415264129639
epoch: 80, step: 20
	action: tensor([[ 0.6581,  0.0893, -0.4267,  0.4148,  0.0925, -0.0417, -0.1652]],
       dtype=torch.float64)
	q_value: tensor([[-2.6616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8957210864821805, distance: 0.36953445865677487 entropy 0.03264415264129639
epoch: 80, step: 21
	action: tensor([[ 0.8209,  0.1987, -0.3418,  0.2419, -0.0553, -0.0327, -0.4166]],
       dtype=torch.float64)
	q_value: tensor([[-1.3608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9416717128738076, distance: 0.2763734403884049 entropy 0.03264415264129639
epoch: 80, step: 22
	action: tensor([[ 0.5292,  0.0615, -0.0482,  0.5338, -0.2167,  0.0629, -0.1970]],
       dtype=torch.float64)
	q_value: tensor([[-2.2444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9160003682111943, distance: 0.3316617473245364 entropy 0.03264415264129639
epoch: 80, step: 23
	action: tensor([[ 0.5962,  0.0495, -0.4345,  0.4525, -0.1277,  0.1099, -0.4010]],
       dtype=torch.float64)
	q_value: tensor([[-1.2659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8724321343009003, distance: 0.4087213630742352 entropy 0.03264415264129639
epoch: 80, step: 24
	action: tensor([[ 0.7177,  0.1429, -0.2702,  0.4422, -0.0343,  0.5573, -0.1352]],
       dtype=torch.float64)
	q_value: tensor([[-1.8136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9671727874474171, distance: 0.20733551358856822 entropy 0.03264415264129639
epoch: 80, step: 25
	action: tensor([[ 0.3820, -0.1444, -0.5021,  1.1601, -0.0803,  0.1052,  0.3673]],
       dtype=torch.float64)
	q_value: tensor([[-2.0123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7025503290705088, distance: 0.6241133029790751 entropy 0.03264415264129639
epoch: 80, step: 26
	action: tensor([[ 0.5679,  0.2023, -0.1513,  0.4589, -0.1645,  0.1027, -0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-1.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9249633553243457, distance: 0.3134681322695904 entropy 0.03264415264129639
epoch: 80, step: 27
	action: tensor([[ 0.5214, -0.0031,  0.0159,  0.2089, -0.1588, -0.5251,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-1.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6842448969430099, distance: 0.6430309584125564 entropy 0.03264415264129639
epoch: 80, step: 28
	action: tensor([[ 0.8428, -0.1630, -0.2951,  0.3355,  0.1502,  0.1572, -0.2997]],
       dtype=torch.float64)
	q_value: tensor([[-0.8292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.817809215964457, distance: 0.4884497545013508 entropy 0.03264415264129639
epoch: 80, step: 29
	action: tensor([[ 0.3768,  0.2383, -0.5130,  0.3533,  0.3467,  0.1829,  0.1829]],
       dtype=torch.float64)
	q_value: tensor([[-1.6069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8028787579906604, distance: 0.508069830152243 entropy 0.03264415264129639
epoch: 80, step: 30
	action: tensor([[ 0.7864,  0.1568, -0.0986,  0.4061, -0.2261,  0.2676, -0.2970]],
       dtype=torch.float64)
	q_value: tensor([[-1.0729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9779418966944006, distance: 0.1699576730732583 entropy 0.03264415264129639
epoch: 80, step: 31
	action: tensor([[ 0.7278, -0.1778, -0.3434,  0.5417, -0.2418,  0.3267,  0.1211]],
       dtype=torch.float64)
	q_value: tensor([[-2.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8761488138373088, distance: 0.4027233199464379 entropy 0.03264415264129639
epoch: 80, step: 32
	action: tensor([[0.2196, 0.1408, 0.3361, 0.3506, 0.2356, 0.2725, 0.1114]],
       dtype=torch.float64)
	q_value: tensor([[-1.4421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8183345296628756, distance: 0.48774506855197486 entropy 0.03264415264129639
epoch: 80, step: 33
	action: tensor([[ 0.1696, -0.0063,  0.0266,  0.3098, -0.1454, -0.1800,  0.1356]],
       dtype=torch.float64)
	q_value: tensor([[-0.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.560007892252363, distance: 0.7590652965585754 entropy 0.03264415264129639
epoch: 80, step: 34
	action: tensor([[ 1.1102,  0.7184, -0.2237,  0.1719, -0.0848,  0.1206,  0.0945]],
       dtype=torch.float64)
	q_value: tensor([[-0.4716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9394077564680983, distance: 0.281685965719643 entropy 0.03264415264129639
epoch: 80, step: 35
	action: tensor([[ 0.5090,  0.0147, -0.6729,  0.4606, -0.3064,  0.2043, -0.3825]],
       dtype=torch.float64)
	q_value: tensor([[-3.0448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7542050474810703, distance: 0.5673396983098148 entropy 0.03264415264129639
epoch: 80, step: 36
	action: tensor([[ 0.5762,  0.1156, -0.2309,  0.3746,  0.1802, -0.1112, -0.1825]],
       dtype=torch.float64)
	q_value: tensor([[-2.0616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8792508880446768, distance: 0.3976478750259714 entropy 0.03264415264129639
epoch: 80, step: 37
	action: tensor([[ 0.5337,  0.5106,  0.0765,  0.4731, -0.3230,  0.4268, -0.3759]],
       dtype=torch.float64)
	q_value: tensor([[-1.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8965835797402566, distance: 0.3680030716045807 entropy 0.03264415264129639
epoch: 80, step: 38
	action: tensor([[ 0.7284,  0.0615, -0.2619,  0.4542,  0.1505, -0.0776, -0.0949]],
       dtype=torch.float64)
	q_value: tensor([[-2.3011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.920287837418738, distance: 0.3230866320212179 entropy 0.03264415264129639
epoch: 80, step: 39
	action: tensor([[ 0.5338,  0.0937, -0.4837,  0.8659, -0.1805,  0.3096, -0.2391]],
       dtype=torch.float64)
	q_value: tensor([[-1.2162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.819268127825824, distance: 0.4864901673304497 entropy 0.03264415264129639
epoch: 80, step: 40
	action: tensor([[ 0.3288,  0.0096, -0.4846,  0.1156,  0.0764, -0.3311, -0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-2.0226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5901683535524078, distance: 0.7325873896012903 entropy 0.03264415264129639
epoch: 80, step: 41
	action: tensor([[ 0.5344,  0.2368,  0.0899,  0.2106, -0.1398,  0.2981,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-0.8230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9236687153753571, distance: 0.3161607688147445 entropy 0.03264415264129639
epoch: 80, step: 42
	action: tensor([[ 0.6287,  0.3890,  0.1975,  0.2176,  0.0191,  0.2236, -0.1725]],
       dtype=torch.float64)
	q_value: tensor([[-1.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9725382534474348, distance: 0.18963599321321764 entropy 0.03264415264129639
epoch: 80, step: 43
	action: tensor([[ 0.9096,  0.4636, -0.4290, -0.0096,  0.0331,  0.1003, -0.3379]],
       dtype=torch.float64)
	q_value: tensor([[-1.3138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9737781887159638, distance: 0.1853053836576306 entropy 0.03264415264129639
epoch: 80, step: 44
	action: tensor([[ 0.5389,  0.5256, -0.3977,  0.6119, -0.0365, -0.0950, -0.1074]],
       dtype=torch.float64)
	q_value: tensor([[-2.7551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8670175335135321, distance: 0.41730528522699484 entropy 0.03264415264129639
epoch: 80, step: 45
	action: tensor([[ 0.3461, -0.2877, -0.6034,  0.6110, -0.1386, -0.0578, -0.0019]],
       dtype=torch.float64)
	q_value: tensor([[-1.8743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5202452733098133, distance: 0.7926223685768633 entropy 0.03264415264129639
epoch: 80, step: 46
	action: tensor([[ 0.6453,  0.1241, -0.1672,  0.5718,  0.1422,  0.0336, -0.1080]],
       dtype=torch.float64)
	q_value: tensor([[-0.8380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9417149779534943, distance: 0.2762709212019798 entropy 0.03264415264129639
epoch: 80, step: 47
	action: tensor([[ 0.9578,  0.1339, -0.5110,  0.3760,  0.2498,  0.2448,  0.1918]],
       dtype=torch.float64)
	q_value: tensor([[-1.1864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9660422158135858, distance: 0.21087561647830447 entropy 0.03264415264129639
epoch: 80, step: 48
	action: tensor([[ 0.2344, -0.2607, -0.1788,  0.3851,  0.4103,  0.3520, -0.0338]],
       dtype=torch.float64)
	q_value: tensor([[-1.8576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5863920977707067, distance: 0.7359547405356185 entropy 0.03264415264129639
epoch: 80, step: 49
	action: tensor([[ 0.8044,  0.1493, -0.1107,  0.7582, -0.4374, -0.3163, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-0.4909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9887423466668326, distance: 0.12141731611644942 entropy 0.03264415264129639
epoch: 80, step: 50
	action: tensor([[ 0.6309, -0.1219, -0.0929,  1.0567, -0.6543,  0.2292,  0.4208]],
       dtype=torch.float64)
	q_value: tensor([[-2.0044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9486143175586694, distance: 0.2594046504595455 entropy 0.03264415264129639
epoch: 80, step: 51
	action: tensor([[ 0.7565,  0.0480, -0.1025,  0.7683,  0.0663, -0.0480, -0.0455]],
       dtype=torch.float64)
	q_value: tensor([[-1.4941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9810254521140094, distance: 0.1576312834219753 entropy 0.03264415264129639
epoch: 80, step: 52
	action: tensor([[ 0.3078,  0.0889, -0.4636,  0.1945, -0.2377,  0.3773, -0.3929]],
       dtype=torch.float64)
	q_value: tensor([[-1.3943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6831208499350511, distance: 0.6441744948027355 entropy 0.03264415264129639
epoch: 80, step: 53
	action: tensor([[ 0.7881, -0.1011, -0.0619,  0.8267, -0.2196,  0.1435, -0.3373]],
       dtype=torch.float64)
	q_value: tensor([[-1.6062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9800151634036192, distance: 0.16177335564226167 entropy 0.03264415264129639
epoch: 80, step: 54
	action: tensor([[ 0.7468,  0.2656, -0.4302,  0.1319,  0.3253, -0.0120,  0.4127]],
       dtype=torch.float64)
	q_value: tensor([[-2.0554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.926507121193898, distance: 0.31022680710171546 entropy 0.03264415264129639
epoch: 80, step: 55
	action: tensor([[ 0.6658,  0.7845, -0.2691,  0.3390, -0.1431,  0.5148,  0.3649]],
       dtype=torch.float64)
	q_value: tensor([[-1.3217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 80, step: 56
	action: tensor([[ 0.7746,  0.4855, -0.5286,  0.1147, -0.2171, -0.1905,  0.2843]],
       dtype=torch.float64)
	q_value: tensor([[-4.5367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9743947426649348, distance: 0.18311388030351927 entropy 0.03264415264129639
epoch: 80, step: 57
	action: tensor([[ 0.0555,  0.1765, -0.3005,  0.3243, -0.6293,  0.6290, -0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-2.0152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49198899074727187, distance: 0.8156301201420081 entropy 0.03264415264129639
epoch: 80, step: 58
	action: tensor([[ 0.5944,  0.0729, -0.1225,  0.3092, -0.0399,  0.1178,  0.1592]],
       dtype=torch.float64)
	q_value: tensor([[-1.8271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8782419189905876, distance: 0.39930577440587117 entropy 0.03264415264129639
epoch: 80, step: 59
	action: tensor([[ 0.6553, -0.1778,  0.1586,  0.3691, -0.0820,  0.7465,  0.1011]],
       dtype=torch.float64)
	q_value: tensor([[-0.8516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.918900762730623, distance: 0.3258855306207568 entropy 0.03264415264129639
epoch: 80, step: 60
	action: tensor([[ 0.0768,  0.1788,  0.1588,  0.1942, -0.7034,  0.2589,  0.1275]],
       dtype=torch.float64)
	q_value: tensor([[-1.3300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.613190458439387, distance: 0.7117136290528788 entropy 0.03264415264129639
epoch: 80, step: 61
	action: tensor([[ 0.4589,  0.4850, -0.1197,  0.2570, -0.4077, -0.0189, -0.1161]],
       dtype=torch.float64)
	q_value: tensor([[-1.1529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.906636639694113, distance: 0.34965917651747613 entropy 0.03264415264129639
epoch: 80, step: 62
	action: tensor([[ 0.5044,  0.2812, -0.1241,  0.4016,  0.0058, -0.0824,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-1.6460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9149272541297366, distance: 0.33377355053649693 entropy 0.03264415264129639
epoch: 80, step: 63
	action: tensor([[ 0.6375, -0.2058, -0.1121,  0.0445, -0.0448,  0.0162, -0.4852]],
       dtype=torch.float64)
	q_value: tensor([[-0.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.594868971386043, distance: 0.7283740197235109 entropy 0.03264415264129639
epoch: 80, step: 64
	action: tensor([[ 0.3870,  0.3115, -0.4918,  0.0745, -0.3546,  0.1622, -0.1000]],
       dtype=torch.float64)
	q_value: tensor([[-1.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8123123368295674, distance: 0.49576350871108416 entropy 0.03264415264129639
epoch: 80, step: 65
	action: tensor([[ 1.0308, -0.0473, -0.4324,  0.7988, -0.0609, -0.3164,  0.1727]],
       dtype=torch.float64)
	q_value: tensor([[-1.6483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9440005492776997, distance: 0.270799948155299 entropy 0.03264415264129639
epoch: 80, step: 66
	action: tensor([[ 0.4509, -0.2275, -0.2921,  0.4052, -0.2657,  0.5144, -0.1090]],
       dtype=torch.float64)
	q_value: tensor([[-1.9401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7223220937950121, distance: 0.6030139459755385 entropy 0.03264415264129639
epoch: 80, step: 67
	action: tensor([[ 0.5540,  0.3049, -0.5062,  0.7150, -0.1398,  0.0930,  0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-1.2936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8583350220143118, distance: 0.43071296275402277 entropy 0.03264415264129639
epoch: 80, step: 68
	action: tensor([[ 0.5966,  0.0850, -0.1261,  0.4457, -0.2768, -0.0416,  0.3428]],
       dtype=torch.float64)
	q_value: tensor([[-1.6059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9070314165127247, distance: 0.3489191454970711 entropy 0.03264415264129639
epoch: 80, step: 69
	action: tensor([[ 0.6655, -0.0801,  0.3010,  0.8191,  0.0547,  0.2178, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-0.9217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9901661029467493, distance: 0.11348005093106281 entropy 0.03264415264129639
epoch: 80, step: 70
	action: tensor([[ 0.6927, -0.5397, -0.0155,  0.5236,  0.3566,  0.1368, -0.4955]],
       dtype=torch.float64)
	q_value: tensor([[-1.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6194390003220185, distance: 0.7059416942255798 entropy 0.03264415264129639
epoch: 80, step: 71
	action: tensor([[ 1.0534,  0.0833, -0.3137,  0.8737,  0.2925,  0.1715,  0.0536]],
       dtype=torch.float64)
	q_value: tensor([[-1.1262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.052532262633721134 entropy 0.03264415264129639
epoch: 80, step: 72
	action: tensor([[ 0.2797,  0.3602, -0.4784,  0.3241, -0.5368, -0.0117, -0.1518]],
       dtype=torch.float64)
	q_value: tensor([[-4.5367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7491214352243991, distance: 0.5731766258094437 entropy 0.03264415264129639
epoch: 80, step: 73
	action: tensor([[ 0.4647, -0.0183, -0.2974,  0.5528, -0.3629, -0.4517, -0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-1.6820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.808360758276857, distance: 0.5009552301552429 entropy 0.03264415264129639
epoch: 80, step: 74
	action: tensor([[ 0.6039,  0.6840, -0.1413,  1.0944, -0.2134, -0.1815,  0.3338]],
       dtype=torch.float64)
	q_value: tensor([[-1.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 80, step: 75
	action: tensor([[ 0.6890, -0.1836, -0.5571,  0.1078, -0.1198, -0.4060,  0.1849]],
       dtype=torch.float64)
	q_value: tensor([[-4.5367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5380742939586929, distance: 0.7777549053546509 entropy 0.03264415264129639
epoch: 80, step: 76
	action: tensor([[ 0.8624, -0.0983,  0.0342, -0.1840, -0.0872,  0.3487, -0.1957]],
       dtype=torch.float64)
	q_value: tensor([[-1.1801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6535541600812563, distance: 0.673557022803033 entropy 0.03264415264129639
epoch: 80, step: 77
	action: tensor([[ 0.3126, -0.2396, -0.2505,  0.7767,  0.2632, -0.1664,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-1.5637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6667714894422514, distance: 0.6605835722837541 entropy 0.03264415264129639
epoch: 80, step: 78
	action: tensor([[ 0.8559, -0.2243, -0.3377,  0.3036, -0.0226, -0.0088, -0.3404]],
       dtype=torch.float64)
	q_value: tensor([[-0.5982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7058076693484097, distance: 0.6206865959799236 entropy 0.03264415264129639
epoch: 80, step: 79
	action: tensor([[ 1.0826, -0.0866, -0.6586,  0.6175, -0.4490, -0.3933, -0.3520]],
       dtype=torch.float64)
	q_value: tensor([[-1.7236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8451222934759862, distance: 0.45035101686774653 entropy 0.03264415264129639
epoch: 80, step: 80
	action: tensor([[ 0.4367,  0.0956, -0.8513,  0.3091, -0.2090,  0.0618,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[-3.1592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7292290223702234, distance: 0.5954670703595398 entropy 0.03264415264129639
epoch: 80, step: 81
	action: tensor([[ 0.5495,  0.4749, -0.2510,  0.0587, -0.3556,  0.2396, -0.3939]],
       dtype=torch.float64)
	q_value: tensor([[-1.5564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9282563937591011, distance: 0.3065125737017553 entropy 0.03264415264129639
epoch: 80, step: 82
	action: tensor([[ 0.5961, -0.2459, -0.0144,  0.3986, -0.4679,  0.6419, -0.4244]],
       dtype=torch.float64)
	q_value: tensor([[-2.3429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8393146797897978, distance: 0.45871695692715714 entropy 0.03264415264129639
epoch: 80, step: 83
	action: tensor([[ 0.3984,  0.2694, -0.5682,  0.3821,  0.0096,  0.4478, -0.3826]],
       dtype=torch.float64)
	q_value: tensor([[-2.1881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8006236511993614, distance: 0.5109677761481379 entropy 0.03264415264129639
epoch: 80, step: 84
	action: tensor([[ 0.7930,  0.0513, -0.4284,  0.2252, -0.1960,  0.2950, -0.3436]],
       dtype=torch.float64)
	q_value: tensor([[-1.9509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9036086623063365, distance: 0.35528403861168667 entropy 0.03264415264129639
epoch: 80, step: 85
	action: tensor([[ 0.7049, -0.0452, -0.0268,  0.1921, -0.1738,  0.0784,  0.4402]],
       dtype=torch.float64)
	q_value: tensor([[-2.2952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8015406258982005, distance: 0.5097913966473251 entropy 0.03264415264129639
epoch: 80, step: 86
	action: tensor([[ 0.5799,  0.2048, -0.0176,  0.3928, -0.2747,  0.2568,  0.2112]],
       dtype=torch.float64)
	q_value: tensor([[-0.8085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9527844795077192, distance: 0.24865608071405507 entropy 0.03264415264129639
epoch: 80, step: 87
	action: tensor([[ 0.8020,  0.1546, -0.2971,  0.6687, -0.5109, -0.0461, -0.4974]],
       dtype=torch.float64)
	q_value: tensor([[-1.1347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9840587179136682, distance: 0.14448352100787856 entropy 0.03264415264129639
epoch: 80, step: 88
	action: tensor([[ 0.3025, -0.1182, -0.2825,  0.1752, -0.6123,  0.2737,  0.1455]],
       dtype=torch.float64)
	q_value: tensor([[-2.9193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5685889568049036, distance: 0.7516269130505059 entropy 0.03264415264129639
epoch: 80, step: 89
	action: tensor([[ 1.0345,  0.4545, -0.2326,  0.3605, -0.0168,  0.1053,  0.3119]],
       dtype=torch.float64)
	q_value: tensor([[-1.0777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9800049982676936, distance: 0.16181449280856877 entropy 0.03264415264129639
epoch: 80, step: 90
	action: tensor([[ 0.9697, -0.0403, -0.4064,  0.9876, -0.1984,  0.0791, -0.2929]],
       dtype=torch.float64)
	q_value: tensor([[-2.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9917254332097198, distance: 0.10409486976832028 entropy 0.03264415264129639
epoch: 80, step: 91
	action: tensor([[ 1.3011,  0.3902, -0.0037,  0.5545, -0.5653,  0.2652, -0.3654]],
       dtype=torch.float64)
	q_value: tensor([[-2.7674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8992713389013431, distance: 0.3631894494564218 entropy 0.03264415264129639
epoch: 80, step: 92
	action: tensor([[ 1.0836,  0.2598, -0.3203,  0.3098, -0.1615,  0.1987, -0.2267]],
       dtype=torch.float64)
	q_value: tensor([[-4.2013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9609470506812509, distance: 0.22614324108061504 entropy 0.03264415264129639
epoch: 80, step: 93
	action: tensor([[ 0.4669,  0.1358, -0.3747,  0.2719,  0.1189,  0.4068, -0.2668]],
       dtype=torch.float64)
	q_value: tensor([[-2.9923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8643678974534407, distance: 0.42144212234342243 entropy 0.03264415264129639
epoch: 80, step: 94
	action: tensor([[ 0.2892, -0.1908, -0.2031,  0.4072,  0.1826,  0.1282, -0.1367]],
       dtype=torch.float64)
	q_value: tensor([[-1.3570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6297075006884567, distance: 0.6963525187122562 entropy 0.03264415264129639
epoch: 80, step: 95
	action: tensor([[ 0.7293, -0.0970, -0.5524,  0.6324, -0.2913,  0.0380,  0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-0.5580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8709138283383145, distance: 0.41114645855695275 entropy 0.03264415264129639
epoch: 80, step: 96
	action: tensor([[ 0.3941,  0.2934, -0.2086,  0.4498, -0.0609, -0.0099,  0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-1.6242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8715318474278558, distance: 0.4101610655520931 entropy 0.03264415264129639
epoch: 80, step: 97
	action: tensor([[ 0.7615,  0.0957, -0.4933,  0.6419,  0.1742, -0.0098,  0.1115]],
       dtype=torch.float64)
	q_value: tensor([[-0.9395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9470161285684005, distance: 0.2634077431557509 entropy 0.03264415264129639
epoch: 80, step: 98
	action: tensor([[ 0.4078,  0.0421, -0.4051,  0.6862, -0.1767,  0.0783, -0.2293]],
       dtype=torch.float64)
	q_value: tensor([[-1.4405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8020593255525967, distance: 0.5091247573707698 entropy 0.03264415264129639
epoch: 80, step: 99
	action: tensor([[ 0.5355, -0.0192, -0.2782,  0.7813,  0.1824,  0.2690, -0.1047]],
       dtype=torch.float64)
	q_value: tensor([[-1.3786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9085733412656286, distance: 0.34601355894872554 entropy 0.03264415264129639
epoch: 80, step: 100
	action: tensor([[ 1.1152, -0.0089, -0.5408,  0.5072, -0.6254,  0.1881,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-1.2066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9234433963212775, distance: 0.3166270556902177 entropy 0.03264415264129639
epoch: 80, step: 101
	action: tensor([[ 0.7916, -0.0901, -0.4175,  0.2764, -0.3598, -0.1505,  0.2446]],
       dtype=torch.float64)
	q_value: tensor([[-2.6498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7780406715997399, distance: 0.5391298701448977 entropy 0.03264415264129639
epoch: 80, step: 102
	action: tensor([[ 0.0796,  0.1406, -0.3654,  0.2761,  0.0387, -0.2172, -0.4580]],
       dtype=torch.float64)
	q_value: tensor([[-1.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5438104660094033, distance: 0.7729107575551454 entropy 0.03264415264129639
epoch: 80, step: 103
	action: tensor([[ 0.3969,  0.1134, -0.2369,  0.3595, -0.0250, -0.0398,  0.2607]],
       dtype=torch.float64)
	q_value: tensor([[-0.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.796435061645719, distance: 0.5163072010460504 entropy 0.03264415264129639
epoch: 80, step: 104
	action: tensor([[ 0.7417,  0.5268, -0.1045,  0.6691, -0.0996,  0.4131,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-0.7082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 80, step: 105
	action: tensor([[ 0.6837,  0.2084, -0.3277,  0.6240, -0.5358,  0.1285, -0.3116]],
       dtype=torch.float64)
	q_value: tensor([[-4.5367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9389485303948032, distance: 0.2827513938833107 entropy 0.03264415264129639
epoch: 80, step: 106
	action: tensor([[ 0.7248,  0.0566, -0.2612,  0.4198, -0.4306,  0.6140,  0.0439]],
       dtype=torch.float64)
	q_value: tensor([[-2.4786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.93611316257875, distance: 0.2892426875471263 entropy 0.03264415264129639
epoch: 80, step: 107
	action: tensor([[ 0.3149, -0.0304, -0.1764, -0.2557, -0.2375,  0.1069,  0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-2.1462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4536701856187113, distance: 0.8458320631782834 entropy 0.03264415264129639
epoch: 80, step: 108
	action: tensor([[ 0.3289,  0.4899, -0.9527,  0.5253,  0.0924,  0.2732,  0.1751]],
       dtype=torch.float64)
	q_value: tensor([[-0.7656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6220613002719632, distance: 0.703505303125143 entropy 0.03264415264129639
epoch: 80, step: 109
	action: tensor([[0.8638, 0.4289, 0.0952, 0.4519, 0.3538, 0.3340, 0.1475]],
       dtype=torch.float64)
	q_value: tensor([[-2.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9703879210030136, distance: 0.19692059662440292 entropy 0.03264415264129639
epoch: 80, step: 110
	action: tensor([[ 0.5699,  0.2320, -0.0318,  0.5865, -0.4791, -0.2603,  0.3627]],
       dtype=torch.float64)
	q_value: tensor([[-1.5469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9426192151713693, distance: 0.27411950271690094 entropy 0.03264415264129639
epoch: 80, step: 111
	action: tensor([[ 0.6113,  0.1603,  0.0619,  0.4915, -0.0971, -0.1521,  0.1159]],
       dtype=torch.float64)
	q_value: tensor([[-1.1642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9314049395888704, distance: 0.29971129813041963 entropy 0.03264415264129639
epoch: 80, step: 112
	action: tensor([[ 0.8452,  0.0887, -0.8036,  0.3064, -0.1125, -0.0018,  0.1494]],
       dtype=torch.float64)
	q_value: tensor([[-0.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8757943363873997, distance: 0.4032992301949701 entropy 0.03264415264129639
epoch: 80, step: 113
	action: tensor([[ 1.1064,  0.2032, -0.2631, -0.0726, -0.1476,  0.1211, -0.0510]],
       dtype=torch.float64)
	q_value: tensor([[-2.0220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8291616254384744, distance: 0.4729872224495646 entropy 0.03264415264129639
epoch: 80, step: 114
	action: tensor([[ 0.7036,  0.6001, -0.1198,  0.7916,  0.1199,  0.0949, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-2.4703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 80, step: 115
	action: tensor([[ 0.5983, -0.0155, -0.6684, -0.1979, -0.0556,  0.0201, -0.1443]],
       dtype=torch.float64)
	q_value: tensor([[-4.5367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.65929406204197, distance: 0.6679539841207004 entropy 0.03264415264129639
epoch: 80, step: 116
	action: tensor([[ 0.3578,  0.3821, -0.6396,  0.1529,  0.0413,  0.2103, -0.1677]],
       dtype=torch.float64)
	q_value: tensor([[-1.4585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8055129986354506, distance: 0.5046636023791454 entropy 0.03264415264129639
epoch: 80, step: 117
	action: tensor([[ 0.1584,  0.1876, -0.3825,  0.4530, -0.0836, -0.0471, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[-1.6085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.644818900860648, distance: 0.681995669024598 entropy 0.03264415264129639
epoch: 80, step: 118
	action: tensor([[ 0.6984, -0.1336, -0.5368,  0.3657, -0.6242, -0.0311, -0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-0.8215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7793968269034399, distance: 0.5374803245461107 entropy 0.03264415264129639
epoch: 80, step: 119
	action: tensor([[ 0.3382, -0.2567, -0.5348,  0.1634, -0.1920,  0.0731,  0.1011]],
       dtype=torch.float64)
	q_value: tensor([[-1.7966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4464341608137853, distance: 0.8514150724498739 entropy 0.03264415264129639
epoch: 80, step: 120
	action: tensor([[ 0.4111, -0.1975, -0.4502,  0.5332, -0.0337,  0.2423, -0.2158]],
       dtype=torch.float64)
	q_value: tensor([[-0.7237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.683051415383384, distance: 0.6442450666868488 entropy 0.03264415264129639
epoch: 80, step: 121
	action: tensor([[ 1.2899, -0.0304, -0.5775,  0.7531,  0.0324,  0.3338,  0.2527]],
       dtype=torch.float64)
	q_value: tensor([[-1.0336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9722718664811786, distance: 0.19055353574976563 entropy 0.03264415264129639
epoch: 80, step: 122
	action: tensor([[ 0.9473,  0.1395, -0.4609,  0.4042, -0.1225, -0.1543,  0.1646]],
       dtype=torch.float64)
	q_value: tensor([[-2.7568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9373740587652677, distance: 0.28637415869739463 entropy 0.03264415264129639
epoch: 80, step: 123
	action: tensor([[ 1.1197, -0.3367, -0.3486,  0.3868, -0.4407,  0.2450, -0.2801]],
       dtype=torch.float64)
	q_value: tensor([[-1.8854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7235682666460987, distance: 0.6016593106309339 entropy 0.03264415264129639
epoch: 80, step: 124
	action: tensor([[ 0.9297, -0.0844, -0.3858,  0.4837,  0.3227, -0.2290, -0.6550]],
       dtype=torch.float64)
	q_value: tensor([[-2.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.851818070271533, distance: 0.44050852376576843 entropy 0.03264415264129639
epoch: 80, step: 125
	action: tensor([[ 0.8998,  0.2396, -0.4840,  0.1783, -0.0611, -0.1669,  0.1652]],
       dtype=torch.float64)
	q_value: tensor([[-2.2828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9218520766062392, distance: 0.31990086494133535 entropy 0.03264415264129639
epoch: 80, step: 126
	action: tensor([[ 0.7179, -0.2038, -0.5638,  0.7191, -0.3331,  0.0859,  0.0925]],
       dtype=torch.float64)
	q_value: tensor([[-1.8487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8337459230566693, distance: 0.46659797030543393 entropy 0.03264415264129639
epoch: 80, step: 127
	action: tensor([[ 0.6899, -0.1004, -0.4961,  0.3958, -0.2335, -0.1662,  0.1193]],
       dtype=torch.float64)
	q_value: tensor([[-1.5777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.792045850294253, distance: 0.521843752914226 entropy 0.03264415264129639
LOSS epoch 80 actor 80.46261550818518 critic 1052.4627676936348 
epoch: 81, step: 0
	action: tensor([[ 0.5334, -0.2079, -0.2940,  0.5813, -0.2145,  0.2575,  0.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7995740144720669, distance: 0.5123110310229286 entropy 0.03264415264129639
epoch: 81, step: 1
	action: tensor([[ 0.4044,  0.6369, -0.2788,  0.6817, -0.0236,  0.4025,  0.1312]],
       dtype=torch.float64)
	q_value: tensor([[-1.0233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 81, step: 2
	action: tensor([[0.4425, 0.3454, 0.2369, 0.4467, 0.0098, 0.0251, 0.4633]],
       dtype=torch.float64)
	q_value: tensor([[-4.6458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9288668493819044, distance: 0.3052057530382867 entropy 0.03264415264129639
epoch: 81, step: 3
	action: tensor([[ 0.7398,  0.2370, -0.4107,  0.2845, -0.4463,  0.3205, -0.0584]],
       dtype=torch.float64)
	q_value: tensor([[-0.7377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9433243394784945, distance: 0.2724300363902689 entropy 0.03264415264129639
epoch: 81, step: 4
	action: tensor([[ 0.4890, -0.0266,  0.1577,  0.4533, -0.4187,  0.3805, -0.0217]],
       dtype=torch.float64)
	q_value: tensor([[-2.3947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8931436920911705, distance: 0.3740733555851495 entropy 0.03264415264129639
epoch: 81, step: 5
	action: tensor([[0.6469, 0.2091, 0.2434, 0.7631, 0.2064, 0.4311, 0.2117]],
       dtype=torch.float64)
	q_value: tensor([[-1.3213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9788063203213323, distance: 0.166594199623584 entropy 0.03264415264129639
epoch: 81, step: 6
	action: tensor([[ 0.6267, -0.2381, -0.2016,  0.5708, -0.0158,  0.2937, -0.1183]],
       dtype=torch.float64)
	q_value: tensor([[-1.1978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8449380680274853, distance: 0.4506187812308432 entropy 0.03264415264129639
epoch: 81, step: 7
	action: tensor([[ 0.9641,  0.1880, -0.5515,  0.2485, -0.0891,  0.0240, -0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-1.2239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.934242469514502, distance: 0.29344684243760744 entropy 0.03264415264129639
epoch: 81, step: 8
	action: tensor([[ 0.2884,  0.2107, -0.1224,  0.4375,  0.0095, -0.1353,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-2.4256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7896348249668308, distance: 0.5248601694127153 entropy 0.03264415264129639
epoch: 81, step: 9
	action: tensor([[ 0.6153,  0.0667, -0.7779,  0.7323, -0.5258, -0.3041,  0.0941]],
       dtype=torch.float64)
	q_value: tensor([[-0.7385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8541174766059275, distance: 0.43707737559234844 entropy 0.03264415264129639
epoch: 81, step: 10
	action: tensor([[ 0.4622,  0.1068, -0.3546,  0.1621, -0.1719,  0.2526, -0.1596]],
       dtype=torch.float64)
	q_value: tensor([[-1.9993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8041325583642065, distance: 0.5064514497134458 entropy 0.03264415264129639
epoch: 81, step: 11
	action: tensor([[ 0.7629, -0.6015, -0.3497,  0.4649, -0.5150,  0.3003, -0.2463]],
       dtype=torch.float64)
	q_value: tensor([[-1.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5466391897260944, distance: 0.7705107125757535 entropy 0.03264415264129639
epoch: 81, step: 12
	action: tensor([[ 0.3738,  0.2894,  0.2111,  0.7085,  0.2750,  0.2252, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[-1.8612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9270234372737925, distance: 0.3091351543649012 entropy 0.03264415264129639
epoch: 81, step: 13
	action: tensor([[ 0.1298,  0.1684, -0.5276,  0.1943,  0.1800, -0.0395, -0.4829]],
       dtype=torch.float64)
	q_value: tensor([[-0.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5676683366351842, distance: 0.752428461994064 entropy 0.03264415264129639
epoch: 81, step: 14
	action: tensor([[ 0.5566, -0.1098, -0.0408,  0.0248,  0.4116,  0.2866,  0.3113]],
       dtype=torch.float64)
	q_value: tensor([[-1.0940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7044200555678196, distance: 0.6221486668479765 entropy 0.03264415264129639
epoch: 81, step: 15
	action: tensor([[ 0.4410, -0.1169, -0.4488,  0.1722,  0.0543, -0.0093, -0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-0.6134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5995128943998864, distance: 0.7241873963469576 entropy 0.03264415264129639
epoch: 81, step: 16
	action: tensor([[ 1.0152, -0.2151, -0.3606, -0.1519, -0.0822,  0.6794, -0.4850]],
       dtype=torch.float64)
	q_value: tensor([[-0.7783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6460637738818897, distance: 0.6807994578099275 entropy 0.03264415264129639
epoch: 81, step: 17
	action: tensor([[ 0.4086,  0.0824, -0.1434, -0.1388, -0.1176, -0.0446, -0.1810]],
       dtype=torch.float64)
	q_value: tensor([[-2.8845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6367223491613047, distance: 0.6897251042894524 entropy 0.03264415264129639
epoch: 81, step: 18
	action: tensor([[ 0.2650,  0.1906, -0.0413,  0.3847, -0.0839,  0.2045, -0.3957]],
       dtype=torch.float64)
	q_value: tensor([[-0.8607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7962893648840886, distance: 0.5164919352950402 entropy 0.03264415264129639
epoch: 81, step: 19
	action: tensor([[ 0.7640,  0.0897,  0.0151,  0.3334,  0.1018,  0.1163, -0.3079]],
       dtype=torch.float64)
	q_value: tensor([[-1.1620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9373589935759178, distance: 0.28640860146514246 entropy 0.03264415264129639
epoch: 81, step: 20
	action: tensor([[ 1.0215,  0.1056, -0.3557,  0.4201, -0.0965,  0.1683,  0.0397]],
       dtype=torch.float64)
	q_value: tensor([[-1.5438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.960553085176218, distance: 0.2272810433813188 entropy 0.03264415264129639
epoch: 81, step: 21
	action: tensor([[ 0.0768, -0.0502, -0.5859,  0.7167, -0.1165,  0.5048, -0.4862]],
       dtype=torch.float64)
	q_value: tensor([[-2.2678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41463914668231805, distance: 0.8755249581938458 entropy 0.03264415264129639
epoch: 81, step: 22
	action: tensor([[ 0.2615, -0.1350, -0.1100,  0.8631, -0.5044,  0.1135,  0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-1.7326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7591897637923973, distance: 0.5615574136039819 entropy 0.03264415264129639
epoch: 81, step: 23
	action: tensor([[ 0.3640, -0.2015, -0.2760,  0.0735,  0.2252,  0.3799, -0.0269]],
       dtype=torch.float64)
	q_value: tensor([[-1.0009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.577557168922785, distance: 0.7437734396065462 entropy 0.03264415264129639
epoch: 81, step: 24
	action: tensor([[ 0.7448,  0.0460, -0.0027,  0.7857, -0.1653,  0.1825, -0.0328]],
       dtype=torch.float64)
	q_value: tensor([[-0.6874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9909914422701498, distance: 0.10861362941931182 entropy 0.03264415264129639
epoch: 81, step: 25
	action: tensor([[ 0.8994,  0.2005, -0.5266,  0.9591, -0.2061,  0.2377, -0.0362]],
       dtype=torch.float64)
	q_value: tensor([[-1.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9374582920331913, distance: 0.28618150412444493 entropy 0.03264415264129639
epoch: 81, step: 26
	action: tensor([[ 0.7539,  0.5636,  0.0238,  0.2847,  0.2428,  0.1664, -0.1443]],
       dtype=torch.float64)
	q_value: tensor([[-2.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.970598587124769, distance: 0.1962188805405539 entropy 0.03264415264129639
epoch: 81, step: 27
	action: tensor([[ 0.4268,  0.0449, -0.0617,  0.4276, -0.1348,  0.0810,  0.3753]],
       dtype=torch.float64)
	q_value: tensor([[-1.8042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8323936663246696, distance: 0.46849170561397896 entropy 0.03264415264129639
epoch: 81, step: 28
	action: tensor([[ 0.9805, -0.2370, -0.0970,  0.8701, -0.2368, -0.3647,  0.1367]],
       dtype=torch.float64)
	q_value: tensor([[-0.6629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8867182920492763, distance: 0.38515592875618226 entropy 0.03264415264129639
epoch: 81, step: 29
	action: tensor([[ 0.5753,  0.2051, -0.3430,  0.6985, -0.3411, -0.0780, -0.8033]],
       dtype=torch.float64)
	q_value: tensor([[-1.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9071795849832816, distance: 0.3486409900743184 entropy 0.03264415264129639
epoch: 81, step: 30
	action: tensor([[ 0.6259, -0.0188, -0.4642,  0.2136, -0.0852,  0.0397, -0.1203]],
       dtype=torch.float64)
	q_value: tensor([[-3.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7933716632084393, distance: 0.5201775840303955 entropy 0.03264415264129639
epoch: 81, step: 31
	action: tensor([[ 0.6786, -0.0491, -0.1208,  0.4927, -0.3744,  0.0888,  0.4815]],
       dtype=torch.float64)
	q_value: tensor([[-1.2879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9021661372306586, distance: 0.3579326318876924 entropy 0.03264415264129639
epoch: 81, step: 32
	action: tensor([[ 0.6674, -0.0596, -0.2223,  0.1693,  0.0344,  0.4393,  0.0343]],
       dtype=torch.float64)
	q_value: tensor([[-1.0073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8502070309684624, distance: 0.44289666263672683 entropy 0.03264415264129639
epoch: 81, step: 33
	action: tensor([[ 0.7158,  0.0482, -0.0721,  0.5626,  0.0428, -0.0643, -0.2364]],
       dtype=torch.float64)
	q_value: tensor([[-1.2383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9510491874465358, distance: 0.25318422676180286 entropy 0.03264415264129639
epoch: 81, step: 34
	action: tensor([[ 0.9141, -0.1260, -0.6374,  0.3914,  0.1077, -0.1207,  0.0292]],
       dtype=torch.float64)
	q_value: tensor([[-1.4630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7916423236543847, distance: 0.5223498159117227 entropy 0.03264415264129639
epoch: 81, step: 35
	action: tensor([[ 0.5058, -0.2116, -0.1295,  0.7497, -0.4302, -0.2677,  0.2660]],
       dtype=torch.float64)
	q_value: tensor([[-1.7438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8233775680406509, distance: 0.480927513519102 entropy 0.03264415264129639
epoch: 81, step: 36
	action: tensor([[ 0.7209,  0.0335, -0.2629, -0.0009, -0.1950,  0.3258, -0.6251]],
       dtype=torch.float64)
	q_value: tensor([[-0.8688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.834811309226577, distance: 0.46510054544975077 entropy 0.03264415264129639
epoch: 81, step: 37
	action: tensor([[ 0.4194,  0.4400, -0.4960,  0.9720, -0.6028,  0.4078, -0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-2.4545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5955356385576402, distance: 0.7277744816213272 entropy 0.03264415264129639
epoch: 81, step: 38
	action: tensor([[ 0.3978, -0.1024, -0.2314,  0.0577, -0.3941,  0.4193, -0.4133]],
       dtype=torch.float64)
	q_value: tensor([[-2.7704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6347087337313613, distance: 0.691634004918493 entropy 0.03264415264129639
epoch: 81, step: 39
	action: tensor([[ 0.6969,  0.3669, -0.1213,  0.6573, -0.5714,  0.2309, -0.6037]],
       dtype=torch.float64)
	q_value: tensor([[-1.6935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.941711537170256, distance: 0.2762790757354495 entropy 0.03264415264129639
epoch: 81, step: 40
	action: tensor([[ 0.5889,  0.1612, -0.8391,  0.5616, -0.1384,  0.3933,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-3.3137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7924272821689791, distance: 0.5213649473721648 entropy 0.03264415264129639
epoch: 81, step: 41
	action: tensor([[ 0.8462,  0.0266, -0.3945,  0.0065, -0.4175, -0.0892,  0.0307]],
       dtype=torch.float64)
	q_value: tensor([[-2.2186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7667704750381845, distance: 0.5526478147890604 entropy 0.03264415264129639
epoch: 81, step: 42
	action: tensor([[ 0.1320,  0.3191,  0.0530,  0.2307, -0.2971,  0.4336,  0.8766]],
       dtype=torch.float64)
	q_value: tensor([[-1.8643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.718821292966732, distance: 0.6068032637894091 entropy 0.03264415264129639
epoch: 81, step: 43
	action: tensor([[ 0.2324, -0.1542,  0.0708,  0.1521,  0.0717,  0.2350, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[-0.9568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5195611651870435, distance: 0.7931872887392684 entropy 0.03264415264129639
epoch: 81, step: 44
	action: tensor([[ 0.2398,  0.1582, -0.4715,  0.9978, -0.0864,  0.2524, -0.2550]],
       dtype=torch.float64)
	q_value: tensor([[-0.4508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6017919981546853, distance: 0.7221238428987028 entropy 0.03264415264129639
epoch: 81, step: 45
	action: tensor([[ 0.6854,  0.1297, -0.4136,  0.2351,  0.1453,  0.5644,  0.2503]],
       dtype=torch.float64)
	q_value: tensor([[-1.7396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9322626443423804, distance: 0.29783162602925783 entropy 0.03264415264129639
epoch: 81, step: 46
	action: tensor([[ 0.5760,  0.3479,  0.1412,  0.3483, -0.0850, -0.0410,  0.0793]],
       dtype=torch.float64)
	q_value: tensor([[-1.5758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9494858305015831, distance: 0.2571954621761079 entropy 0.03264415264129639
epoch: 81, step: 47
	action: tensor([[ 0.2515,  0.5656, -0.1313,  0.6757, -0.5917,  0.3375,  0.1503]],
       dtype=torch.float64)
	q_value: tensor([[-1.0287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 81, step: 48
	action: tensor([[ 0.1055,  0.1834,  0.0851,  0.5821, -0.3670, -0.0236, -0.3203]],
       dtype=torch.float64)
	q_value: tensor([[-4.6458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7019932894411943, distance: 0.624697424031664 entropy 0.03264415264129639
epoch: 81, step: 49
	action: tensor([[ 0.3753, -0.3242, -0.8877,  0.4051, -0.0011,  0.1933,  0.1670]],
       dtype=torch.float64)
	q_value: tensor([[-1.1624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40336296769635316, distance: 0.8839176301307613 entropy 0.03264415264129639
epoch: 81, step: 50
	action: tensor([[ 0.9199,  0.5525, -0.3605,  0.0321, -0.3652, -0.0978, -0.2725]],
       dtype=torch.float64)
	q_value: tensor([[-1.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9787529010244564, distance: 0.16680402029731464 entropy 0.03264415264129639
epoch: 81, step: 51
	action: tensor([[ 0.5157,  0.2143, -0.3784,  0.4045,  0.1673, -0.0813, -0.0293]],
       dtype=torch.float64)
	q_value: tensor([[-3.1546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8782316186907972, distance: 0.39932266397150956 entropy 0.03264415264129639
epoch: 81, step: 52
	action: tensor([[-0.0418,  0.2954, -0.1541,  0.5098, -0.1926,  0.0999, -0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-1.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5734885310566147, distance: 0.7473465778794937 entropy 0.03264415264129639
epoch: 81, step: 53
	action: tensor([[ 0.2073, -0.0314, -0.2701, -0.1849, -0.2761,  0.2272, -0.0828]],
       dtype=torch.float64)
	q_value: tensor([[-0.9270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4447606850197614, distance: 0.8527010507372443 entropy 0.03264415264129639
epoch: 81, step: 54
	action: tensor([[ 0.5821, -0.0599,  0.3447,  0.0175, -0.4768,  0.0979, -0.1199]],
       dtype=torch.float64)
	q_value: tensor([[-0.9126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6881003328566094, distance: 0.639093129753395 entropy 0.03264415264129639
epoch: 81, step: 55
	action: tensor([[-0.0192, -0.0814,  0.0798,  0.1869,  0.1763, -0.0783,  0.1096]],
       dtype=torch.float64)
	q_value: tensor([[-1.2288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30682820264585564, distance: 0.9527460031306836 entropy 0.03264415264129639
epoch: 81, step: 56
	action: tensor([[ 0.7937, -0.2405, -0.1010,  0.3298,  0.1279,  0.7320, -0.2854]],
       dtype=torch.float64)
	q_value: tensor([[-0.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8836250598233827, distance: 0.39037898328050213 entropy 0.03264415264129639
epoch: 81, step: 57
	action: tensor([[ 0.4576, -0.0352, -0.3646,  0.4435, -0.0884, -0.0272,  0.2062]],
       dtype=torch.float64)
	q_value: tensor([[-1.8952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.767697347380669, distance: 0.5515485888163829 entropy 0.03264415264129639
epoch: 81, step: 58
	action: tensor([[ 0.1647,  0.0131, -0.5094,  0.9476,  0.3724, -0.3059, -0.3952]],
       dtype=torch.float64)
	q_value: tensor([[-0.8018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5307763051252863, distance: 0.7838747238340433 entropy 0.03264415264129639
epoch: 81, step: 59
	action: tensor([[ 0.6718,  0.1060, -0.2276,  1.0328, -0.7074, -0.0562,  0.6037]],
       dtype=torch.float64)
	q_value: tensor([[-1.1808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9386067964141352, distance: 0.28354163629743123 entropy 0.03264415264129639
epoch: 81, step: 60
	action: tensor([[ 0.9515,  0.4748, -0.5066,  0.5279, -0.3403, -0.0303, -0.0796]],
       dtype=torch.float64)
	q_value: tensor([[-1.6601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9808292559714085, distance: 0.15844413850282896 entropy 0.03264415264129639
epoch: 81, step: 61
	action: tensor([[ 0.5014, -0.0501, -0.0548,  0.4775,  0.1334,  0.2024, -0.8298]],
       dtype=torch.float64)
	q_value: tensor([[-3.1520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8783640131755212, distance: 0.3991055201777096 entropy 0.03264415264129639
epoch: 81, step: 62
	action: tensor([[ 0.4848, -0.2712, -0.3491,  0.6036,  0.2447,  0.2885, -0.0438]],
       dtype=torch.float64)
	q_value: tensor([[-1.8663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7603882515087816, distance: 0.5601582639031065 entropy 0.03264415264129639
epoch: 81, step: 63
	action: tensor([[ 0.7449,  0.0199, -0.1088,  0.4564, -0.2771,  0.3401,  0.1819]],
       dtype=torch.float64)
	q_value: tensor([[-0.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9541812593485126, distance: 0.24495046452451588 entropy 0.03264415264129639
epoch: 81, step: 64
	action: tensor([[ 0.5485, -0.1347,  0.0995,  0.0092,  0.1259, -0.3381,  0.1043]],
       dtype=torch.float64)
	q_value: tensor([[-1.4869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4918786845097175, distance: 0.8157186656704054 entropy 0.03264415264129639
epoch: 81, step: 65
	action: tensor([[ 0.9283,  0.1197, -0.2640,  0.3961,  0.0781,  0.0770,  0.0532]],
       dtype=torch.float64)
	q_value: tensor([[-0.5862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9566881829306101, distance: 0.23815510501829976 entropy 0.03264415264129639
epoch: 81, step: 66
	action: tensor([[ 0.3911,  0.0328, -0.1840,  0.3309, -0.0793,  0.0433, -0.2290]],
       dtype=torch.float64)
	q_value: tensor([[-1.7472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7730031786228251, distance: 0.5452134753335695 entropy 0.03264415264129639
epoch: 81, step: 67
	action: tensor([[ 0.6686,  0.3901, -0.2195,  0.8228, -0.1630,  0.6283, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.8921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8287229944325445, distance: 0.47359403536130756 entropy 0.03264415264129639
epoch: 81, step: 68
	action: tensor([[ 0.8435, -0.0867, -0.2975,  0.5493, -0.2201, -0.0736,  0.0653]],
       dtype=torch.float64)
	q_value: tensor([[-2.9759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9017062176343691, distance: 0.3587729708526619 entropy 0.03264415264129639
epoch: 81, step: 69
	action: tensor([[ 0.8517, -0.0994, -0.1390,  0.4351, -0.0078,  0.2381,  0.3531]],
       dtype=torch.float64)
	q_value: tensor([[-1.5652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.906641296747526, distance: 0.3496504557422017 entropy 0.03264415264129639
epoch: 81, step: 70
	action: tensor([[ 0.3915,  0.3478, -0.4350,  0.5542, -0.5308,  0.2972, -0.2473]],
       dtype=torch.float64)
	q_value: tensor([[-1.1997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.762824205632802, distance: 0.557303634284631 entropy 0.03264415264129639
epoch: 81, step: 71
	action: tensor([[ 0.4338,  0.3390, -0.0165,  0.1985,  0.3689, -0.0311, -0.5038]],
       dtype=torch.float64)
	q_value: tensor([[-2.3109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8818491023076587, distance: 0.3933464238701331 entropy 0.03264415264129639
epoch: 81, step: 72
	action: tensor([[ 0.4344,  0.0738, -0.3832,  0.4076,  0.2941, -0.0579, -0.2531]],
       dtype=torch.float64)
	q_value: tensor([[-1.1903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7887143083245864, distance: 0.5260072583730899 entropy 0.03264415264129639
epoch: 81, step: 73
	action: tensor([[ 0.7288, -0.1557, -0.1694,  0.7601,  0.6035, -0.1429, -0.3613]],
       dtype=torch.float64)
	q_value: tensor([[-0.9691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.904811400294996, distance: 0.3530605247050412 entropy 0.03264415264129639
epoch: 81, step: 74
	action: tensor([[ 1.0495, -0.0160, -0.3813,  0.5392, -0.0163,  0.3927,  0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-1.3362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9711051583789603, distance: 0.19452116075425752 entropy 0.03264415264129639
epoch: 81, step: 75
	action: tensor([[ 1.1169, -0.0094, -0.1930,  0.3515, -0.1443,  0.3605,  0.0825]],
       dtype=torch.float64)
	q_value: tensor([[-2.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9113375270406479, distance: 0.34074274166104146 entropy 0.03264415264129639
epoch: 81, step: 76
	action: tensor([[ 0.7924,  0.0106, -0.3033,  0.1974, -0.1313,  0.4285,  0.2185]],
       dtype=torch.float64)
	q_value: tensor([[-2.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9028632824752815, distance: 0.356655072443832 entropy 0.03264415264129639
epoch: 81, step: 77
	action: tensor([[ 0.1510, -0.3426,  0.1629,  0.4103, -0.2673,  0.1317,  0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-1.6162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47185838012919534, distance: 0.8316333362878893 entropy 0.03264415264129639
epoch: 81, step: 78
	action: tensor([[ 0.4353, -0.2627,  0.0319,  0.1029, -0.3406,  0.0868, -0.5424]],
       dtype=torch.float64)
	q_value: tensor([[-0.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5210849569446535, distance: 0.7919284269982342 entropy 0.03264415264129639
epoch: 81, step: 79
	action: tensor([[ 0.4049, -0.1625, -0.1314,  0.2158, -0.2633,  0.2866, -0.0437]],
       dtype=torch.float64)
	q_value: tensor([[-1.2561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6646228427437979, distance: 0.66270986102494 entropy 0.03264415264129639
epoch: 81, step: 80
	action: tensor([[ 0.5079, -0.1736, -0.2794,  0.3700, -0.4680,  0.4880, -0.1575]],
       dtype=torch.float64)
	q_value: tensor([[-0.8928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7477150389193921, distance: 0.5747809615567347 entropy 0.03264415264129639
epoch: 81, step: 81
	action: tensor([[ 0.2650, -0.0405, -0.0573,  0.0647, -0.1459,  0.4928,  0.2505]],
       dtype=torch.float64)
	q_value: tensor([[-1.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6397545642892124, distance: 0.6868405649758211 entropy 0.03264415264129639
epoch: 81, step: 82
	action: tensor([[ 0.7406, -0.3261, -0.1571,  0.0132,  0.1567, -0.1660,  0.3179]],
       dtype=torch.float64)
	q_value: tensor([[-0.7630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40638172952099416, distance: 0.8816786469488902 entropy 0.03264415264129639
epoch: 81, step: 83
	action: tensor([[ 0.6528, -0.3426, -0.3264,  0.1376, -0.2566, -0.0019, -0.3259]],
       dtype=torch.float64)
	q_value: tensor([[-0.7593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4768778309580408, distance: 0.8276719856587269 entropy 0.03264415264129639
epoch: 81, step: 84
	action: tensor([[ 0.7912,  0.2632, -0.1218,  0.1959,  0.5923,  0.5437,  0.0720]],
       dtype=torch.float64)
	q_value: tensor([[-1.2772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9919061181863291, distance: 0.1029520793380656 entropy 0.03264415264129639
epoch: 81, step: 85
	action: tensor([[ 0.6919, -0.0259, -0.3806,  0.3555,  0.0360,  0.7968, -0.2033]],
       dtype=torch.float64)
	q_value: tensor([[-1.4879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9346650542892903, distance: 0.292502417987397 entropy 0.03264415264129639
epoch: 81, step: 86
	action: tensor([[ 0.6191,  0.5211, -0.2878,  0.2304, -0.1890, -0.1759,  0.0687]],
       dtype=torch.float64)
	q_value: tensor([[-2.2279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.963772021624308, distance: 0.2178104655021313 entropy 0.03264415264129639
epoch: 81, step: 87
	action: tensor([[ 0.3644, -0.1180,  0.1340,  0.2020, -0.2409,  0.4720, -0.2853]],
       dtype=torch.float64)
	q_value: tensor([[-1.6855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.737395781000447, distance: 0.5864183381502175 entropy 0.03264415264129639
epoch: 81, step: 88
	action: tensor([[ 0.5868, -0.1004, -0.3728,  0.2598, -0.0479,  0.2347, -0.2589]],
       dtype=torch.float64)
	q_value: tensor([[-1.1871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.781729719367217, distance: 0.5346308269601963 entropy 0.03264415264129639
epoch: 81, step: 89
	action: tensor([[ 0.9247,  0.0197,  0.0399,  0.1279, -0.2933,  0.6633, -0.2714]],
       dtype=torch.float64)
	q_value: tensor([[-1.3046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9302689501913387, distance: 0.3021828376123401 entropy 0.03264415264129639
epoch: 81, step: 90
	action: tensor([[ 0.7173,  0.2816, -0.3842,  0.5248,  0.2215, -0.1735,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-2.5896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9617926035771073, distance: 0.22368167977982203 entropy 0.03264415264129639
epoch: 81, step: 91
	action: tensor([[ 0.5575,  0.0159, -0.3920,  0.7251, -0.2683,  0.1437, -0.1856]],
       dtype=torch.float64)
	q_value: tensor([[-1.5568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8717815717785372, distance: 0.4097622233945536 entropy 0.03264415264129639
epoch: 81, step: 92
	action: tensor([[ 0.6907, -0.2192, -0.0394,  0.0386, -0.1371,  0.1126,  0.0876]],
       dtype=torch.float64)
	q_value: tensor([[-1.7506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.604795260137958, distance: 0.7193955806947686 entropy 0.03264415264129639
epoch: 81, step: 93
	action: tensor([[ 1.0110,  0.1812,  0.1260,  0.5947, -0.5304,  0.1798,  0.0680]],
       dtype=torch.float64)
	q_value: tensor([[-0.8897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9780168591207866, distance: 0.1696686345708675 entropy 0.03264415264129639
epoch: 81, step: 94
	action: tensor([[ 0.5061,  0.5106, -0.4061,  0.6490, -0.3273,  0.0758,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-2.4545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8223761369323901, distance: 0.4822889912714327 entropy 0.03264415264129639
epoch: 81, step: 95
	action: tensor([[ 0.4318,  0.5821, -0.7309,  0.3843, -0.3951,  0.2357,  0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-2.0608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7678047481334611, distance: 0.5514210750292988 entropy 0.03264415264129639
epoch: 81, step: 96
	action: tensor([[ 0.2753,  0.4746, -0.4123, -0.1399,  0.2978,  0.7681, -0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-2.3710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8359470413112072, distance: 0.4634989204028334 entropy 0.03264415264129639
epoch: 81, step: 97
	action: tensor([[ 0.0761, -0.0436, -0.2906,  0.1990, -0.5303, -0.0799, -0.3037]],
       dtype=torch.float64)
	q_value: tensor([[-1.9096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42823341333149223, distance: 0.865298755970976 entropy 0.03264415264129639
epoch: 81, step: 98
	action: tensor([[ 0.2371,  0.4268, -0.4648,  0.4529, -0.1693,  0.3183, -0.0842]],
       dtype=torch.float64)
	q_value: tensor([[-1.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6693427317712453, distance: 0.6580300547988993 entropy 0.03264415264129639
epoch: 81, step: 99
	action: tensor([[ 0.3356,  0.2699, -0.2226,  0.3615,  0.1893, -0.0504, -0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-1.5889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7941037411117056, distance: 0.519255279713806 entropy 0.03264415264129639
epoch: 81, step: 100
	action: tensor([[ 0.3778, -0.1231, -0.2167,  0.6429,  0.2498,  0.0190,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[-0.8245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7479316821472333, distance: 0.5745341193621635 entropy 0.03264415264129639
epoch: 81, step: 101
	action: tensor([[ 0.5681, -0.2978, -0.0279,  0.5501, -0.3115, -0.0636, -0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-0.6644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7243711801772519, distance: 0.6007848963757794 entropy 0.03264415264129639
epoch: 81, step: 102
	action: tensor([[ 0.3387,  0.1038, -0.3789,  0.3631, -0.2722,  0.6193, -0.2984]],
       dtype=torch.float64)
	q_value: tensor([[-0.9749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7232377455779992, distance: 0.6020188961026077 entropy 0.03264415264129639
epoch: 81, step: 103
	action: tensor([[-0.0298,  0.2513, -0.4576,  0.5368, -0.1668, -0.0633,  0.3842]],
       dtype=torch.float64)
	q_value: tensor([[-1.9617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47666575008382217, distance: 0.827839743422943 entropy 0.03264415264129639
epoch: 81, step: 104
	action: tensor([[ 0.5954,  0.3857, -0.3443,  0.2773,  0.0912, -0.1460, -0.4282]],
       dtype=torch.float64)
	q_value: tensor([[-0.8142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9277671961256898, distance: 0.3075558059397793 entropy 0.03264415264129639
epoch: 81, step: 105
	action: tensor([[ 0.7746, -0.2474, -0.4051,  0.5820, -0.1426,  0.5669, -0.4089]],
       dtype=torch.float64)
	q_value: tensor([[-1.9221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8822423139271549, distance: 0.3926913408879778 entropy 0.03264415264129639
epoch: 81, step: 106
	action: tensor([[ 0.8132,  0.0197, -0.2177,  0.5265, -0.0715,  0.2577,  0.6684]],
       dtype=torch.float64)
	q_value: tensor([[-2.4027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9645550546033954, distance: 0.21544372486137575 entropy 0.03264415264129639
epoch: 81, step: 107
	action: tensor([[ 0.6077,  0.0816, -0.6275,  0.5382, -0.3067,  0.2148, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[-1.2417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.846651624461973, distance: 0.44812201810770597 entropy 0.03264415264129639
epoch: 81, step: 108
	action: tensor([[ 0.2773,  0.3533, -0.4140,  0.4635, -0.4264,  0.1160,  0.2821]],
       dtype=torch.float64)
	q_value: tensor([[-1.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7437637345642673, distance: 0.5792646033983048 entropy 0.03264415264129639
epoch: 81, step: 109
	action: tensor([[ 0.4391,  0.3468, -0.1550,  0.3357, -0.1360,  0.1705, -0.4108]],
       dtype=torch.float64)
	q_value: tensor([[-1.3451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8902095398560367, distance: 0.37917438924631897 entropy 0.03264415264129639
epoch: 81, step: 110
	action: tensor([[ 0.2567,  0.0854, -0.1983,  0.3445, -0.1409,  0.1785,  0.2312]],
       dtype=torch.float64)
	q_value: tensor([[-1.6978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7171330287023461, distance: 0.6086222335180842 entropy 0.03264415264129639
epoch: 81, step: 111
	action: tensor([[ 0.4903,  0.0490, -0.5847,  0.6078, -0.2749,  0.0320,  0.0561]],
       dtype=torch.float64)
	q_value: tensor([[-0.7079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.792163431203168, distance: 0.5216962022674555 entropy 0.03264415264129639
epoch: 81, step: 112
	action: tensor([[ 0.2832, -0.0579, -0.2643,  0.3255, -0.1176,  0.5216,  0.2898]],
       dtype=torch.float64)
	q_value: tensor([[-1.4142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.689765446235745, distance: 0.6373849095683848 entropy 0.03264415264129639
epoch: 81, step: 113
	action: tensor([[ 0.1954,  0.2458, -0.1226,  0.0679, -0.5280,  0.5871,  0.4125]],
       dtype=torch.float64)
	q_value: tensor([[-0.8606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6671996812652639, distance: 0.6601590176617291 entropy 0.03264415264129639
epoch: 81, step: 114
	action: tensor([[ 0.4449,  0.0106, -0.1011,  0.4290, -0.0812,  0.0942, -0.0946]],
       dtype=torch.float64)
	q_value: tensor([[-1.4274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8192051771144727, distance: 0.48657488463973 entropy 0.03264415264129639
epoch: 81, step: 115
	action: tensor([[ 0.8332,  0.1525, -0.2243,  0.0326, -0.2789,  0.1047, -0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-0.8901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8723917781595102, distance: 0.4087860075390835 entropy 0.03264415264129639
epoch: 81, step: 116
	action: tensor([[ 0.4040,  0.1851,  0.1255,  0.2848, -0.0578, -0.4129, -0.2137]],
       dtype=torch.float64)
	q_value: tensor([[-1.9102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7959381027354899, distance: 0.5169370419678699 entropy 0.03264415264129639
epoch: 81, step: 117
	action: tensor([[ 0.5489, -0.1509,  0.4878, -0.0643, -0.0334,  0.2036,  0.1289]],
       dtype=torch.float64)
	q_value: tensor([[-0.9454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5884213045159239, distance: 0.7341471825501812 entropy 0.03264415264129639
epoch: 81, step: 118
	action: tensor([[ 0.6805,  0.0108, -0.2021,  0.6719, -0.4459,  0.3717, -0.2785]],
       dtype=torch.float64)
	q_value: tensor([[-0.6519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9432138117520454, distance: 0.27269555077561525 entropy 0.03264415264129639
epoch: 81, step: 119
	action: tensor([[ 0.3019,  0.2480, -0.4651,  0.1901, -0.3725,  0.2831,  0.1922]],
       dtype=torch.float64)
	q_value: tensor([[-2.2856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.730061171404236, distance: 0.5945513543344825 entropy 0.03264415264129639
epoch: 81, step: 120
	action: tensor([[ 0.5369, -0.1634, -0.2485,  0.1518,  0.1404, -0.0202, -0.3018]],
       dtype=torch.float64)
	q_value: tensor([[-1.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6350319164485991, distance: 0.6913279838895432 entropy 0.03264415264129639
epoch: 81, step: 121
	action: tensor([[ 0.6714,  0.0348, -0.2597,  0.3577,  0.0462,  0.4020, -0.0612]],
       dtype=torch.float64)
	q_value: tensor([[-0.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9265161920319352, distance: 0.3102076616824386 entropy 0.03264415264129639
epoch: 81, step: 122
	action: tensor([[ 0.6234, -0.4471, -0.3571,  0.4907, -0.2125, -0.1191, -0.1781]],
       dtype=torch.float64)
	q_value: tensor([[-1.4574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5721953334126828, distance: 0.7484787108023696 entropy 0.03264415264129639
epoch: 81, step: 123
	action: tensor([[ 0.9157,  0.1233, -0.4357,  0.5154,  0.0696,  0.3298, -0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-1.0604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9839210885911287, distance: 0.14510588103994082 entropy 0.03264415264129639
epoch: 81, step: 124
	action: tensor([[ 0.8271,  0.2599, -0.2090,  0.4339, -0.1460, -0.2489, -0.1309]],
       dtype=torch.float64)
	q_value: tensor([[-2.2539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9735385095062574, distance: 0.18615034470874753 entropy 0.03264415264129639
epoch: 81, step: 125
	action: tensor([[ 0.5279,  0.1281, -0.1965,  0.5658, -0.0072,  0.3510, -0.0814]],
       dtype=torch.float64)
	q_value: tensor([[-1.9669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9220384842586493, distance: 0.31951910449637216 entropy 0.03264415264129639
epoch: 81, step: 126
	action: tensor([[ 0.5940,  0.2430,  0.1306,  0.7996, -0.5431,  0.0897, -0.2298]],
       dtype=torch.float64)
	q_value: tensor([[-1.3729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9693205086103083, distance: 0.20043832786502203 entropy 0.03264415264129639
epoch: 81, step: 127
	action: tensor([[ 0.5215,  0.4390, -0.0052,  0.5429, -0.1174,  0.0147, -0.4400]],
       dtype=torch.float64)
	q_value: tensor([[-2.1576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9336207823930466, distance: 0.2948307368655522 entropy 0.03264415264129639
LOSS epoch 81 actor 21.79989414332379 critic 231.0203467253204 
epoch: 82, step: 0
	action: tensor([[ 0.6641,  0.0344, -0.4907,  0.4556,  0.1341,  0.2652,  0.4896]],
       dtype=torch.float64)
	q_value: tensor([[-2.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8982501247112394, distance: 0.3650258627198338 entropy 0.03264415264129639
epoch: 82, step: 1
	action: tensor([[ 0.3107,  0.3861, -0.2651,  0.2930, -0.0520,  0.2709, -0.5841]],
       dtype=torch.float64)
	q_value: tensor([[-1.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8127250572625634, distance: 0.49521812300385976 entropy 0.03264415264129639
epoch: 82, step: 2
	action: tensor([[ 0.5287,  0.2102, -0.2253,  0.0057,  0.3657,  0.1536, -0.1875]],
       dtype=torch.float64)
	q_value: tensor([[-1.8739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8534278477821144, distance: 0.4381092530329643 entropy 0.03264415264129639
epoch: 82, step: 3
	action: tensor([[ 0.5641,  0.0429, -0.1976,  0.3473,  0.1051, -0.4566,  0.2785]],
       dtype=torch.float64)
	q_value: tensor([[-1.0011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.778914090824504, distance: 0.5380680752888043 entropy 0.03264415264129639
epoch: 82, step: 4
	action: tensor([[-0.0478, -0.2335,  0.1589,  0.2424,  0.1020,  0.0305,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-0.8792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24205363770433752, distance: 0.9962674234719949 entropy 0.03264415264129639
epoch: 82, step: 5
	action: tensor([[ 0.7814, -0.3440, -0.2835,  0.4173, -0.5573, -0.0767, -0.3266]],
       dtype=torch.float64)
	q_value: tensor([[-0.2749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.646580799350406, distance: 0.6803020245543255 entropy 0.03264415264129639
epoch: 82, step: 6
	action: tensor([[ 0.7491, -0.1371,  0.2251,  0.1786, -0.2583, -0.0222,  0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-1.9718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7197351527685071, distance: 0.6058163746640459 entropy 0.03264415264129639
epoch: 82, step: 7
	action: tensor([[ 0.6568,  0.0908, -0.4697,  0.2013, -0.3436,  0.3691,  0.1852]],
       dtype=torch.float64)
	q_value: tensor([[-1.0253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8736981202441927, distance: 0.4066882269576147 entropy 0.03264415264129639
epoch: 82, step: 8
	action: tensor([[ 0.1665,  0.5368, -0.0719,  0.3814,  0.0338,  0.1600, -0.0584]],
       dtype=torch.float64)
	q_value: tensor([[-1.7972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7641913853094129, distance: 0.5556950481156022 entropy 0.03264415264129639
epoch: 82, step: 9
	action: tensor([[ 0.3829,  0.3056, -0.8538,  0.2383, -0.2576,  0.1010,  0.0540]],
       dtype=torch.float64)
	q_value: tensor([[-1.1034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7678440167371543, distance: 0.5513744451971376 entropy 0.03264415264129639
epoch: 82, step: 10
	action: tensor([[ 0.1601,  0.1233, -0.2281, -0.0490, -0.1518,  0.4299, -0.2589]],
       dtype=torch.float64)
	q_value: tensor([[-1.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5945862494321048, distance: 0.7286281244418433 entropy 0.03264415264129639
epoch: 82, step: 11
	action: tensor([[ 0.4922,  0.1007, -0.6089, -0.4713, -0.0683,  0.2054, -0.2338]],
       dtype=torch.float64)
	q_value: tensor([[-1.1664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.665097027742189, distance: 0.6622411972779525 entropy 0.03264415264129639
epoch: 82, step: 12
	action: tensor([[ 0.2526, -0.2350, -0.1913,  0.8466, -0.6472,  0.0177,  0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-1.8165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6895305691304731, distance: 0.6376261444594873 entropy 0.03264415264129639
epoch: 82, step: 13
	action: tensor([[ 0.5309,  0.0016, -0.6904,  0.3559, -0.0677, -0.0765, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-1.0410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7598965688920343, distance: 0.5607326909443452 entropy 0.03264415264129639
epoch: 82, step: 14
	action: tensor([[ 0.6614, -0.1519, -0.6955,  0.1924, -0.0569,  0.0860,  0.4416]],
       dtype=torch.float64)
	q_value: tensor([[-1.4590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6875273559667102, distance: 0.6396798850643393 entropy 0.03264415264129639
epoch: 82, step: 15
	action: tensor([[ 0.5579, -0.1635,  0.1417,  0.4759,  0.0922, -0.0044, -0.1975]],
       dtype=torch.float64)
	q_value: tensor([[-1.3013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8130514102209401, distance: 0.4947864411991492 entropy 0.03264415264129639
epoch: 82, step: 16
	action: tensor([[ 0.3348,  0.4251,  0.0515,  0.9426, -0.2018,  0.1088,  0.1389]],
       dtype=torch.float64)
	q_value: tensor([[-0.8494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.839295802259415, distance: 0.4587439014824043 entropy 0.03264415264129639
epoch: 82, step: 17
	action: tensor([[ 0.3868, -0.2016, -0.9066,  0.4164, -0.0163,  0.2592,  0.1469]],
       dtype=torch.float64)
	q_value: tensor([[-1.3977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 82, step: 18
	action: tensor([[ 0.8740,  0.2363, -0.1070, -0.0953,  0.0258,  0.0745, -0.1681]],
       dtype=torch.float64)
	q_value: tensor([[-4.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.870412212647633, distance: 0.41194452060333403 entropy 0.03264415264129639
epoch: 82, step: 19
	action: tensor([[ 0.6913,  0.2875, -0.0130,  0.4306, -0.2647,  0.0652, -0.0865]],
       dtype=torch.float64)
	q_value: tensor([[-1.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9740379719582938, distance: 0.18438517532923027 entropy 0.03264415264129639
epoch: 82, step: 20
	action: tensor([[ 1.0759,  0.5834, -0.1409,  0.4994,  0.2838,  0.2966,  0.2799]],
       dtype=torch.float64)
	q_value: tensor([[-1.7316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 82, step: 21
	action: tensor([[ 0.5266, -0.3842,  0.1061,  0.3441, -0.4614,  0.1660,  0.3161]],
       dtype=torch.float64)
	q_value: tensor([[-4.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5910581357391165, distance: 0.7317917001973551 entropy 0.03264415264129639
epoch: 82, step: 22
	action: tensor([[ 0.6587,  0.3026, -0.0134, -0.1418, -0.0147, -0.2715,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[-0.7816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8110762689305099, distance: 0.4973933220970405 entropy 0.03264415264129639
epoch: 82, step: 23
	action: tensor([[ 0.8794,  0.1892, -0.2180,  0.4914, -0.0675,  0.2263,  0.2095]],
       dtype=torch.float64)
	q_value: tensor([[-1.1975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9902094322805846, distance: 0.11322977155611136 entropy 0.03264415264129639
epoch: 82, step: 24
	action: tensor([[ 1.0015,  0.4854, -0.5624,  0.2546, -0.3677, -0.1174,  0.2620]],
       dtype=torch.float64)
	q_value: tensor([[-1.7606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9843717836419819, distance: 0.14305775347570077 entropy 0.03264415264129639
epoch: 82, step: 25
	action: tensor([[ 0.2986, -0.1139, -0.3291,  0.1149,  0.1353, -0.0729, -0.5416]],
       dtype=torch.float64)
	q_value: tensor([[-2.8209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5151794314400264, distance: 0.7967961216775391 entropy 0.03264415264129639
epoch: 82, step: 26
	action: tensor([[ 0.5545,  0.3606,  0.1900, -0.0998, -0.3373,  0.3048, -0.2272]],
       dtype=torch.float64)
	q_value: tensor([[-0.8131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8944420249442333, distance: 0.37179386442505624 entropy 0.03264415264129639
epoch: 82, step: 27
	action: tensor([[ 0.3592,  0.1094, -0.4628, -0.1953,  0.4387,  0.1405, -0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-1.8049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.665069675181172, distance: 0.662268240368662 entropy 0.03264415264129639
epoch: 82, step: 28
	action: tensor([[ 0.0870, -0.1486, -0.0149,  0.1958,  0.1760,  0.1067,  0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-0.9388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4118812270970459, distance: 0.8775850464072321 entropy 0.03264415264129639
epoch: 82, step: 29
	action: tensor([[ 0.7975,  0.2612,  0.0615,  0.5287,  0.0810,  0.5580, -0.2347]],
       dtype=torch.float64)
	q_value: tensor([[-0.3554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9873649961924212, distance: 0.12863062559544106 entropy 0.03264415264129639
epoch: 82, step: 30
	action: tensor([[ 0.6013,  0.2376, -0.4204,  0.4435, -0.0014,  0.2006,  0.3258]],
       dtype=torch.float64)
	q_value: tensor([[-2.2351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.917505819463051, distance: 0.32867626996328586 entropy 0.03264415264129639
epoch: 82, step: 31
	action: tensor([[ 0.7603, -0.1911, -0.3050,  0.3870, -0.0891,  0.3897,  0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-1.3336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8507768723407916, distance: 0.44205342771163825 entropy 0.03264415264129639
epoch: 82, step: 32
	action: tensor([[ 0.4208,  0.4733, -0.4177,  0.4182,  0.1387, -0.0604,  0.3933]],
       dtype=torch.float64)
	q_value: tensor([[-1.4789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8560267250674456, distance: 0.4342078143105283 entropy 0.03264415264129639
epoch: 82, step: 33
	action: tensor([[ 0.3982,  0.3918, -0.1626, -0.0936, -0.0681,  0.3263, -0.2179]],
       dtype=torch.float64)
	q_value: tensor([[-1.2450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8570169459071445, distance: 0.43271203851931894 entropy 0.03264415264129639
epoch: 82, step: 34
	action: tensor([[ 0.5858, -0.0817, -0.2548,  0.1908, -0.0449,  0.0798, -0.2129]],
       dtype=torch.float64)
	q_value: tensor([[-1.4387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7531462843474422, distance: 0.5685602947536152 entropy 0.03264415264129639
epoch: 82, step: 35
	action: tensor([[ 0.5240, -0.0584, -0.4272,  0.3243, -0.7301,  0.2387, -0.3073]],
       dtype=torch.float64)
	q_value: tensor([[-1.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.744987161205114, distance: 0.5778800692887072 entropy 0.03264415264129639
epoch: 82, step: 36
	action: tensor([[ 0.6440, -0.1716, -0.3253,  0.8425, -0.1550, -0.2781,  0.7547]],
       dtype=torch.float64)
	q_value: tensor([[-2.2798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8635101262293319, distance: 0.42277267448824907 entropy 0.03264415264129639
epoch: 82, step: 37
	action: tensor([[ 0.4102,  0.3255, -0.4454,  0.2719, -0.3464,  0.3690,  0.1080]],
       dtype=torch.float64)
	q_value: tensor([[-0.8922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8150065366989907, distance: 0.4921923788468331 entropy 0.03264415264129639
epoch: 82, step: 38
	action: tensor([[ 0.5493,  0.1831,  0.1665, -0.0178,  0.0215,  0.1479, -0.2377]],
       dtype=torch.float64)
	q_value: tensor([[-1.7122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8344002992542117, distance: 0.46567879993280603 entropy 0.03264415264129639
epoch: 82, step: 39
	action: tensor([[ 0.6231, -0.2330, -0.6378,  0.2550, -0.0854,  0.3177, -0.2280]],
       dtype=torch.float64)
	q_value: tensor([[-1.0302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6769920868871362, distance: 0.6503741543576995 entropy 0.03264415264129639
epoch: 82, step: 40
	action: tensor([[ 1.0268, -0.0757, -0.3984,  0.2333, -0.2395,  0.2203,  0.2686]],
       dtype=torch.float64)
	q_value: tensor([[-1.5818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8261585737144359, distance: 0.4771262728258792 entropy 0.03264415264129639
epoch: 82, step: 41
	action: tensor([[ 0.7955,  0.5812, -0.0115,  0.6373, -0.3575, -0.1359, -0.5764]],
       dtype=torch.float64)
	q_value: tensor([[-2.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 82, step: 42
	action: tensor([[ 0.3045, -0.4154, -0.0507,  0.3941, -0.4181,  0.5439,  0.0417]],
       dtype=torch.float64)
	q_value: tensor([[-4.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5529498600362635, distance: 0.7651292605878449 entropy 0.03264415264129639
epoch: 82, step: 43
	action: tensor([[ 0.2079,  0.1218, -0.2033,  0.2539,  0.3984,  0.2110, -0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-0.9957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.693197224973311, distance: 0.6338497669870742 entropy 0.03264415264129639
epoch: 82, step: 44
	action: tensor([[ 0.5877,  0.1718, -0.2896,  0.4342, -0.0980,  0.0155, -0.1289]],
       dtype=torch.float64)
	q_value: tensor([[-0.6079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9052949967045422, distance: 0.35216253791510443 entropy 0.03264415264129639
epoch: 82, step: 45
	action: tensor([[ 0.6144,  0.2288, -0.4642,  0.5483, -0.1233,  0.0853,  0.3287]],
       dtype=torch.float64)
	q_value: tensor([[-1.4084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9119564584013569, distance: 0.3395513373038122 entropy 0.03264415264129639
epoch: 82, step: 46
	action: tensor([[-0.0131, -0.0962, -0.5104,  0.1341,  0.4274, -0.0346,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[-1.4445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2457483845270646, distance: 0.9938362140095774 entropy 0.03264415264129639
epoch: 82, step: 47
	action: tensor([[ 0.5975,  0.3074, -0.1327,  0.1793,  0.0274,  0.2087, -0.2877]],
       dtype=torch.float64)
	q_value: tensor([[-0.5330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9256962417167475, distance: 0.3119335470830399 entropy 0.03264415264129639
epoch: 82, step: 48
	action: tensor([[0.3764, 0.3101, 0.1326, 0.2235, 0.0938, 0.7192, 0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-1.5675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9088339533981056, distance: 0.34552005034976024 entropy 0.03264415264129639
epoch: 82, step: 49
	action: tensor([[0.4043, 0.2344, 0.1330, 0.3234, 0.0357, 0.0637, 0.2718]],
       dtype=torch.float64)
	q_value: tensor([[-1.3329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8661829332990494, distance: 0.41861274454928327 entropy 0.03264415264129639
epoch: 82, step: 50
	action: tensor([[ 1.1197,  0.0942, -0.3185,  0.0312,  0.0341,  0.3654,  0.0806]],
       dtype=torch.float64)
	q_value: tensor([[-0.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8539747117494866, distance: 0.4372911915737182 entropy 0.03264415264129639
epoch: 82, step: 51
	action: tensor([[ 0.5854, -0.3366, -0.0155,  0.4087,  0.2688,  0.1037,  0.4796]],
       dtype=torch.float64)
	q_value: tensor([[-2.4157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6841701828105662, distance: 0.6431070310683014 entropy 0.03264415264129639
epoch: 82, step: 52
	action: tensor([[ 0.4482, -0.2670, -0.1272,  0.0074, -0.2456,  0.5611, -0.4377]],
       dtype=torch.float64)
	q_value: tensor([[-0.5009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5703593797181739, distance: 0.7500830657540596 entropy 0.03264415264129639
epoch: 82, step: 53
	action: tensor([[ 0.8157,  0.2677, -0.2108,  0.3599, -0.2384, -0.2890, -0.4386]],
       dtype=torch.float64)
	q_value: tensor([[-1.6153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9604538468452597, distance: 0.22756675423697326 entropy 0.03264415264129639
epoch: 82, step: 54
	action: tensor([[ 0.4753,  0.1406, -0.2989,  0.0494, -0.5790,  0.0870, -0.2500]],
       dtype=torch.float64)
	q_value: tensor([[-2.5382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7839356897266059, distance: 0.5319223162370048 entropy 0.03264415264129639
epoch: 82, step: 55
	action: tensor([[ 0.3393, -0.1265, -0.2936, -0.0789,  0.0370, -0.0057,  0.4376]],
       dtype=torch.float64)
	q_value: tensor([[-1.8061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45202309886557623, distance: 0.8471061198915121 entropy 0.03264415264129639
epoch: 82, step: 56
	action: tensor([[ 0.6999,  0.1402, -0.4606,  0.4520, -0.0764,  0.0527,  0.1470]],
       dtype=torch.float64)
	q_value: tensor([[-0.6424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9242988904955359, distance: 0.3148529854350212 entropy 0.03264415264129639
epoch: 82, step: 57
	action: tensor([[ 0.1867, -0.2322,  0.1114,  0.2636, -0.0489, -0.2056, -0.2013]],
       dtype=torch.float64)
	q_value: tensor([[-1.5003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3999746739699672, distance: 0.8864239550215571 entropy 0.03264415264129639
epoch: 82, step: 58
	action: tensor([[ 0.6413,  0.0799, -0.1592,  0.2830,  0.1473, -0.3265, -0.1879]],
       dtype=torch.float64)
	q_value: tensor([[-0.4658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8167647827003165, distance: 0.4898478058129252 entropy 0.03264415264129639
epoch: 82, step: 59
	action: tensor([[ 0.5105,  0.1745, -0.2339,  0.2306, -0.2257,  0.3464,  0.2950]],
       dtype=torch.float64)
	q_value: tensor([[-1.1416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8714333417790088, distance: 0.41031828521726477 entropy 0.03264415264129639
epoch: 82, step: 60
	action: tensor([[ 0.7351, -0.1016, -0.0471,  0.5720, -0.0470, -0.4146,  0.0939]],
       dtype=torch.float64)
	q_value: tensor([[-1.1834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8351301089774605, distance: 0.4646515269806754 entropy 0.03264415264129639
epoch: 82, step: 61
	action: tensor([[ 0.5662,  0.2963,  0.0476,  0.0757,  0.0353,  0.2098, -0.0968]],
       dtype=torch.float64)
	q_value: tensor([[-1.0409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9148880533932982, distance: 0.33385044156623866 entropy 0.03264415264129639
epoch: 82, step: 62
	action: tensor([[ 0.7467, -0.0274, -0.0430, -0.1679,  0.1713,  0.2134, -0.2270]],
       dtype=torch.float64)
	q_value: tensor([[-1.0820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6854775494584568, distance: 0.6417745910409614 entropy 0.03264415264129639
epoch: 82, step: 63
	action: tensor([[ 0.4562,  0.1307, -0.5755,  0.9893, -0.5185,  0.2074,  0.0596]],
       dtype=torch.float64)
	q_value: tensor([[-1.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7082576932400422, distance: 0.6180966638818804 entropy 0.03264415264129639
epoch: 82, step: 64
	action: tensor([[ 0.9399, -0.1000, -0.1149,  0.1884,  0.0246, -0.0788, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-2.0799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7370544738191369, distance: 0.5867992988971849 entropy 0.03264415264129639
epoch: 82, step: 65
	action: tensor([[ 0.4617,  0.2638, -0.1486, -0.1789,  0.0714,  0.1245,  0.7522]],
       dtype=torch.float64)
	q_value: tensor([[-1.4648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7793521451430225, distance: 0.5375347533896007 entropy 0.03264415264129639
epoch: 82, step: 66
	action: tensor([[ 0.5899,  0.1857, -0.0505,  0.0398, -0.0114,  0.4822,  0.1506]],
       dtype=torch.float64)
	q_value: tensor([[-1.0224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9116322618678518, distance: 0.3401759158613713 entropy 0.03264415264129639
epoch: 82, step: 67
	action: tensor([[ 0.7977, -0.0884, -0.2512,  0.3579, -0.0434,  0.3934,  0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-1.2252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9015977651410725, distance: 0.3589708424545273 entropy 0.03264415264129639
epoch: 82, step: 68
	action: tensor([[ 0.7845,  0.1729, -0.3433,  0.5857,  0.0639, -0.1181,  0.1296]],
       dtype=torch.float64)
	q_value: tensor([[-1.3671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9692725989655837, distance: 0.20059477085902497 entropy 0.03264415264129639
epoch: 82, step: 69
	action: tensor([[ 0.7582,  0.0067, -0.0439,  0.4573, -0.1123, -0.1506, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[-1.5239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9023759745197031, distance: 0.35754857299654447 entropy 0.03264415264129639
epoch: 82, step: 70
	action: tensor([[ 0.5615,  0.1270, -0.2390,  0.1690, -0.1757,  0.1961,  0.1798]],
       dtype=torch.float64)
	q_value: tensor([[-1.2578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8618507422226751, distance: 0.425334852906485 entropy 0.03264415264129639
epoch: 82, step: 71
	action: tensor([[ 0.6913, -0.0246, -0.6042,  0.3348, -0.3292,  0.4448,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-1.1076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8380901756855834, distance: 0.46046146831790263 entropy 0.03264415264129639
epoch: 82, step: 72
	action: tensor([[ 1.0798, -0.0393, -0.4329,  0.1469, -0.0119, -0.4219,  0.2091]],
       dtype=torch.float64)
	q_value: tensor([[-2.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6518655987795735, distance: 0.6751964707636949 entropy 0.03264415264129639
epoch: 82, step: 73
	action: tensor([[ 0.6877, -0.0104, -0.2929, -0.2833, -0.1462,  0.0681,  0.2673]],
       dtype=torch.float64)
	q_value: tensor([[-1.9133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6105539092504957, distance: 0.7141350807918838 entropy 0.03264415264129639
epoch: 82, step: 74
	action: tensor([[ 0.1374,  0.1397, -0.4556,  0.1100, -0.2531, -0.4196, -0.1047]],
       dtype=torch.float64)
	q_value: tensor([[-1.2600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5728902458606686, distance: 0.747870561096314 entropy 0.03264415264129639
epoch: 82, step: 75
	action: tensor([[ 0.8294, -0.2319,  0.4924,  0.3530,  0.1355,  0.1470, -0.3663]],
       dtype=torch.float64)
	q_value: tensor([[-1.0564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7578523893436605, distance: 0.5631145997093566 entropy 0.03264415264129639
epoch: 82, step: 76
	action: tensor([[ 0.4159,  0.1186,  0.4761,  0.6412, -0.0388,  0.1590,  0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-1.4693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9643397528542158, distance: 0.2160970646278555 entropy 0.03264415264129639
epoch: 82, step: 77
	action: tensor([[ 0.7137, -0.1668, -0.5676,  0.5727, -0.4579, -0.1265, -0.3347]],
       dtype=torch.float64)
	q_value: tensor([[-0.8532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8106814201230231, distance: 0.49791282446463153 entropy 0.03264415264129639
epoch: 82, step: 78
	action: tensor([[ 0.7190,  0.1030,  0.0792,  0.1039, -0.1339, -0.5143,  0.1547]],
       dtype=torch.float64)
	q_value: tensor([[-2.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7555581261506868, distance: 0.565775966788515 entropy 0.03264415264129639
epoch: 82, step: 79
	action: tensor([[ 0.3565,  0.2733, -0.5670,  0.4765, -0.0293, -0.4269, -0.4010]],
       dtype=torch.float64)
	q_value: tensor([[-1.0993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.787451705772898, distance: 0.5275765763533978 entropy 0.03264415264129639
epoch: 82, step: 80
	action: tensor([[ 0.8948, -0.1333, -0.2168,  0.1976,  0.9105,  0.1502,  0.4152]],
       dtype=torch.float64)
	q_value: tensor([[-1.7597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7821749418298105, distance: 0.534085285078796 entropy 0.03264415264129639
epoch: 82, step: 81
	action: tensor([[ 0.7134,  0.1081, -0.4508,  0.6760,  0.3637,  0.1950, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-1.1175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9488545636380693, distance: 0.25879753617068324 entropy 0.03264415264129639
epoch: 82, step: 82
	action: tensor([[ 0.7171,  0.5181, -0.1582,  0.2813,  0.2932,  0.1471, -0.1896]],
       dtype=torch.float64)
	q_value: tensor([[-1.5761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9726249109810002, distance: 0.18933655173200423 entropy 0.03264415264129639
epoch: 82, step: 83
	action: tensor([[ 0.7124,  0.3317, -0.4377,  0.3705, -0.1657, -0.2616, -0.0763]],
       dtype=torch.float64)
	q_value: tensor([[-1.8631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9635809317838796, distance: 0.2183841467937163 entropy 0.03264415264129639
epoch: 82, step: 84
	action: tensor([[ 0.0942, -0.2512, -0.3566,  0.0219, -0.2924,  0.1876, -0.3400]],
       dtype=torch.float64)
	q_value: tensor([[-1.9204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2324489931410042, distance: 1.0025598682600378 entropy 0.03264415264129639
epoch: 82, step: 85
	action: tensor([[ 0.5670,  0.1700, -0.0424,  0.3448,  0.1752,  0.0016,  0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-0.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9001859826816492, distance: 0.36153675943608343 entropy 0.03264415264129639
epoch: 82, step: 86
	action: tensor([[ 0.6714,  0.2035, -0.1636,  0.1580,  0.0893,  0.1103,  0.1836]],
       dtype=torch.float64)
	q_value: tensor([[-0.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9088448107037076, distance: 0.34549947509933915 entropy 0.03264415264129639
epoch: 82, step: 87
	action: tensor([[ 0.7976,  0.0506, -0.0462,  0.0709, -0.6107,  0.4418, -0.0918]],
       dtype=torch.float64)
	q_value: tensor([[-1.0369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8695956358175021, distance: 0.41324038382708445 entropy 0.03264415264129639
epoch: 82, step: 88
	action: tensor([[ 0.5976,  0.1459,  0.3751,  0.3561, -0.0971,  0.2105, -0.2746]],
       dtype=torch.float64)
	q_value: tensor([[-2.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9480728872970406, distance: 0.2607676906335206 entropy 0.03264415264129639
epoch: 82, step: 89
	action: tensor([[ 1.0822,  0.1206, -0.1287,  0.4543, -0.1722,  0.3652, -0.1159]],
       dtype=torch.float64)
	q_value: tensor([[-1.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.974262618229007, distance: 0.1835857109076756 entropy 0.03264415264129639
epoch: 82, step: 90
	action: tensor([[ 0.6926, -0.6857, -0.5362,  0.6485,  0.5119,  0.5458, -0.2271]],
       dtype=torch.float64)
	q_value: tensor([[-2.7722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6274655818304786, distance: 0.6984573547150638 entropy 0.03264415264129639
epoch: 82, step: 91
	action: tensor([[ 1.1183,  0.6732, -0.0625,  0.2203, -0.4419,  0.0948,  0.4022]],
       dtype=torch.float64)
	q_value: tensor([[-1.3125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9376113768703707, distance: 0.2858310428724674 entropy 0.03264415264129639
epoch: 82, step: 92
	action: tensor([[ 0.0874,  0.1950, -0.1828,  0.6298,  0.1678, -0.3084,  0.1092]],
       dtype=torch.float64)
	q_value: tensor([[-2.8533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6283043274268316, distance: 0.6976706379508619 entropy 0.03264415264129639
epoch: 82, step: 93
	action: tensor([[ 0.6862,  0.0924, -0.6998,  0.1174, -0.2654, -0.0716,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-0.6016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8123216636674591, distance: 0.4957511904711793 entropy 0.03264415264129639
epoch: 82, step: 94
	action: tensor([[ 0.4310, -0.1773, -0.3135,  0.1512, -0.2224,  0.0323, -0.4940]],
       dtype=torch.float64)
	q_value: tensor([[-1.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5826669599666408, distance: 0.7392614805493343 entropy 0.03264415264129639
epoch: 82, step: 95
	action: tensor([[ 0.8220,  0.2329, -0.0497,  0.3996,  0.2142,  0.2400,  0.0048]],
       dtype=torch.float64)
	q_value: tensor([[-1.2214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9902656875406496, distance: 0.11290400155137492 entropy 0.03264415264129639
epoch: 82, step: 96
	action: tensor([[ 0.8081, -0.5174, -0.6231,  0.6119, -0.4969,  0.1411, -0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-1.5324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6168459534424369, distance: 0.7083426657881977 entropy 0.03264415264129639
epoch: 82, step: 97
	action: tensor([[ 0.4537,  0.0835, -0.2137,  0.5213, -0.5704, -0.1410,  0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-2.0793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.852818149942703, distance: 0.43901951141987156 entropy 0.03264415264129639
epoch: 82, step: 98
	action: tensor([[ 0.2186,  0.0824, -0.2500,  0.2925, -0.0459, -0.3397, -0.2202]],
       dtype=torch.float64)
	q_value: tensor([[-1.2033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6203747942902734, distance: 0.7050732098153168 entropy 0.03264415264129639
epoch: 82, step: 99
	action: tensor([[ 0.7842, -0.1924, -1.0898,  0.4236, -0.3346,  0.2858,  0.3101]],
       dtype=torch.float64)
	q_value: tensor([[-0.7715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.672343090734888, distance: 0.6550377940487333 entropy 0.03264415264129639
epoch: 82, step: 100
	action: tensor([[ 0.2344,  0.1927, -0.3420,  0.2380, -0.1112, -0.2791,  0.3812]],
       dtype=torch.float64)
	q_value: tensor([[-2.3054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6789852703363815, distance: 0.6483644187865503 entropy 0.03264415264129639
epoch: 82, step: 101
	action: tensor([[ 1.0372,  0.1229, -0.5600,  0.4107,  0.1694,  0.0293,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[-0.8583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9411696254216042, distance: 0.2775603970422242 entropy 0.03264415264129639
epoch: 82, step: 102
	action: tensor([[ 0.1812,  0.0780, -0.5049,  0.3713, -0.2803,  0.2009, -0.0702]],
       dtype=torch.float64)
	q_value: tensor([[-2.3443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5756573505742386, distance: 0.7454440198656203 entropy 0.03264415264129639
epoch: 82, step: 103
	action: tensor([[ 0.6670,  0.1604, -0.7763,  0.3821, -0.0201, -0.0709,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-1.0831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8790914323201646, distance: 0.3979103461515507 entropy 0.03264415264129639
epoch: 82, step: 104
	action: tensor([[ 0.2214,  0.1131, -0.3936,  0.6020,  0.0835,  0.1309, -0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-1.8258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6828116871025205, distance: 0.6444886622997126 entropy 0.03264415264129639
epoch: 82, step: 105
	action: tensor([[-0.0134, -0.1812, -0.3820,  0.4256, -0.0739,  0.4535,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[-0.9763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3455901527644005, distance: 0.9257241767207173 entropy 0.03264415264129639
epoch: 82, step: 106
	action: tensor([[ 0.1971, -0.0600, -0.3238,  0.2230,  0.0492,  0.2210,  0.0918]],
       dtype=torch.float64)
	q_value: tensor([[-0.7424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5420035261778375, distance: 0.7744399717265815 entropy 0.03264415264129639
epoch: 82, step: 107
	action: tensor([[ 0.1510,  0.2456, -0.3622,  0.3258, -0.0630,  0.3366,  0.1790]],
       dtype=torch.float64)
	q_value: tensor([[-0.6210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6546313365183852, distance: 0.6725090890215464 entropy 0.03264415264129639
epoch: 82, step: 108
	action: tensor([[ 0.8115,  0.0756, -0.4121,  0.2072, -0.0658, -0.0238, -0.2732]],
       dtype=torch.float64)
	q_value: tensor([[-0.9617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8678565803151372, distance: 0.41598671775207424 entropy 0.03264415264129639
epoch: 82, step: 109
	action: tensor([[ 0.4061,  0.3785, -0.5503, -0.1019,  0.1802,  0.3551, -0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-1.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8618766443604281, distance: 0.42529497720036236 entropy 0.03264415264129639
epoch: 82, step: 110
	action: tensor([[ 0.7400,  0.2492, -0.3860,  0.2729, -0.0310,  0.6617,  0.0482]],
       dtype=torch.float64)
	q_value: tensor([[-1.5565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9707951695476893, distance: 0.19556180538306278 entropy 0.03264415264129639
epoch: 82, step: 111
	action: tensor([[ 0.4306,  0.4541, -0.0340,  0.2365,  0.1724,  0.1011,  0.3534]],
       dtype=torch.float64)
	q_value: tensor([[-2.2418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9013397639470565, distance: 0.35944112790440996 entropy 0.03264415264129639
epoch: 82, step: 112
	action: tensor([[-0.1830,  0.1846, -0.1563, -0.1166, -0.1205, -0.0654, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[-0.8953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21317484730434821, distance: 1.0150695747569136 entropy 0.03264415264129639
epoch: 82, step: 113
	action: tensor([[ 0.0084, -0.0830, -0.3599,  0.7057, -0.0853, -0.2547, -0.1507]],
       dtype=torch.float64)
	q_value: tensor([[-0.6433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42452586626700906, distance: 0.8680996821220581 entropy 0.03264415264129639
epoch: 82, step: 114
	action: tensor([[ 0.5765,  0.1101, -0.3344, -0.5673,  0.0130,  0.5964, -0.4042]],
       dtype=torch.float64)
	q_value: tensor([[-0.7074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6947810741799837, distance: 0.6322115460213464 entropy 0.03264415264129639
epoch: 82, step: 115
	action: tensor([[0.5097, 0.3146, 0.1781, 0.3799, 0.3390, 0.1301, 0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-2.1724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9520499643244071, distance: 0.2505827439574979 entropy 0.03264415264129639
epoch: 82, step: 116
	action: tensor([[ 0.3981,  0.1586, -0.2681,  0.2616,  0.0734, -0.2332, -0.4098]],
       dtype=torch.float64)
	q_value: tensor([[-0.7816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7722067635678211, distance: 0.54616907467555 entropy 0.03264415264129639
epoch: 82, step: 117
	action: tensor([[ 0.8830, -0.0347, -0.7083, -0.2813, -0.0100,  0.2058, -0.1644]],
       dtype=torch.float64)
	q_value: tensor([[-1.1812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6541321984509514, distance: 0.672994879609969 entropy 0.03264415264129639
epoch: 82, step: 118
	action: tensor([[ 0.2215, -0.0357, -0.2998,  0.1996, -0.1496, -0.2283,  0.1951]],
       dtype=torch.float64)
	q_value: tensor([[-2.2874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5308114118822684, distance: 0.7838453989997539 entropy 0.03264415264129639
epoch: 82, step: 119
	action: tensor([[ 0.8656,  0.6238, -0.2571,  0.7909,  0.1562,  0.0130,  0.2654]],
       dtype=torch.float64)
	q_value: tensor([[-0.6338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 82, step: 120
	action: tensor([[ 0.3735,  0.5098, -0.1639,  0.2015, -0.2533, -0.4080, -0.2421]],
       dtype=torch.float64)
	q_value: tensor([[-4.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8696776895776638, distance: 0.4131103526614531 entropy 0.03264415264129639
epoch: 82, step: 121
	action: tensor([[ 0.5434, -0.2485,  0.0412,  0.6721, -0.0158,  0.4243,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[-1.6693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8948044419636489, distance: 0.3711550672184568 entropy 0.03264415264129639
epoch: 82, step: 122
	action: tensor([[ 1.1336,  0.0110, -0.1926,  0.3917,  0.0563,  0.0246,  0.1861]],
       dtype=torch.float64)
	q_value: tensor([[-1.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8790192800876059, distance: 0.39802905501754665 entropy 0.03264415264129639
epoch: 82, step: 123
	action: tensor([[ 0.4410,  0.6531, -0.1892, -0.1063, -0.0405,  0.2301,  0.0571]],
       dtype=torch.float64)
	q_value: tensor([[-1.9734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9105596952583979, distance: 0.34223413798727703 entropy 0.03264415264129639
epoch: 82, step: 124
	action: tensor([[ 0.3987, -0.0501, -0.5694,  0.3136, -0.0303, -0.0290, -0.0858]],
       dtype=torch.float64)
	q_value: tensor([[-1.6432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6642551973876237, distance: 0.6630729976655261 entropy 0.03264415264129639
epoch: 82, step: 125
	action: tensor([[ 0.4659, -0.2940, -0.2729,  0.1643, -0.5589, -0.1864,  0.3790]],
       dtype=torch.float64)
	q_value: tensor([[-0.9579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48549712672470213, distance: 0.8208250380823091 entropy 0.03264415264129639
epoch: 82, step: 126
	action: tensor([[ 0.0510,  0.1294, -0.5866,  0.0704,  0.3614, -0.2012,  0.2367]],
       dtype=torch.float64)
	q_value: tensor([[-0.8600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43660210834534807, distance: 0.8589429154643972 entropy 0.03264415264129639
epoch: 82, step: 127
	action: tensor([[ 0.6065, -0.3593, -0.0547,  0.0639, -0.6322,  0.5907, -0.2443]],
       dtype=torch.float64)
	q_value: tensor([[-0.8219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5648401748916798, distance: 0.7548855114863307 entropy 0.03264415264129639
LOSS epoch 82 actor 39.67537253584421 critic 427.20652726287085 
epoch: 83, step: 0
	action: tensor([[ 0.3019,  0.1411, -0.2414,  0.3090, -0.0357,  0.1718, -0.2594]],
       dtype=torch.float64)
	q_value: tensor([[-2.0715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7585974073105477, distance: 0.5622476622164597 entropy 0.03264415264129639
epoch: 83, step: 1
	action: tensor([[ 0.8290, -0.2587, -0.3680,  0.0762, -0.3201,  0.0476, -0.1556]],
       dtype=torch.float64)
	q_value: tensor([[-0.9943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5937695432330954, distance: 0.7293616660544381 entropy 0.03264415264129639
epoch: 83, step: 2
	action: tensor([[ 0.5172,  0.3641, -0.0022,  0.3279,  0.0388,  0.3676,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[-1.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9443772568287977, distance: 0.26988757738692787 entropy 0.03264415264129639
epoch: 83, step: 3
	action: tensor([[ 0.8339, -0.0108, -0.2652, -0.0876, -0.1141,  0.2052, -0.3045]],
       dtype=torch.float64)
	q_value: tensor([[-1.2677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7558503170673194, distance: 0.565437718625389 entropy 0.03264415264129639
epoch: 83, step: 4
	action: tensor([[ 0.3871, -0.0231, -0.4865,  0.0855,  0.2886, -0.0250, -0.5659]],
       dtype=torch.float64)
	q_value: tensor([[-1.9736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6331136147330108, distance: 0.6931424406329229 entropy 0.03264415264129639
epoch: 83, step: 5
	action: tensor([[ 0.3762,  0.4351, -0.4212,  0.2069,  0.1015,  0.3193, -0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-1.1203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8349488301634378, distance: 0.4649069051344958 entropy 0.03264415264129639
epoch: 83, step: 6
	action: tensor([[ 0.5933,  0.0922, -0.0011, -0.1704, -0.2990,  0.0177,  0.3643]],
       dtype=torch.float64)
	q_value: tensor([[-1.5126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6974716014224096, distance: 0.629418882414913 entropy 0.03264415264129639
epoch: 83, step: 7
	action: tensor([[ 0.8238, -0.0324, -0.3801,  0.2235, -0.0674,  0.3443,  0.0895]],
       dtype=torch.float64)
	q_value: tensor([[-0.9898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8720767165951049, distance: 0.40929033774066736 entropy 0.03264415264129639
epoch: 83, step: 8
	action: tensor([[ 0.4605, -0.0022, -0.4330,  0.2581, -0.1025,  0.2791,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-1.7038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7638028403020184, distance: 0.556152672765083 entropy 0.03264415264129639
epoch: 83, step: 9
	action: tensor([[-0.0251,  0.5925,  0.0336,  0.5392,  0.2196, -0.1765,  0.1405]],
       dtype=torch.float64)
	q_value: tensor([[-1.1081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6563239503227476, distance: 0.6708591184470352 entropy 0.03264415264129639
epoch: 83, step: 10
	action: tensor([[ 0.5457,  0.2684, -0.2183,  0.2648, -0.1167, -0.1753,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-0.7610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8821136367387321, distance: 0.39290583483260777 entropy 0.03264415264129639
epoch: 83, step: 11
	action: tensor([[ 0.1600, -0.3824, -0.3853,  0.4928, -0.0806, -0.1653, -0.3672]],
       dtype=torch.float64)
	q_value: tensor([[-1.1948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31197145352302924, distance: 0.9492047920202511 entropy 0.03264415264129639
epoch: 83, step: 12
	action: tensor([[ 0.5299, -0.1279, -0.1669,  0.9143, -0.5581,  0.2678, -0.0607]],
       dtype=torch.float64)
	q_value: tensor([[-0.7158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8693322350602776, distance: 0.4136575206358706 entropy 0.03264415264129639
epoch: 83, step: 13
	action: tensor([[ 0.2509,  0.2306, -0.7403,  0.1861,  0.0122,  0.4700,  0.5404]],
       dtype=torch.float64)
	q_value: tensor([[-1.8552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6561882622742474, distance: 0.6709915375983326 entropy 0.03264415264129639
epoch: 83, step: 14
	action: tensor([[ 0.9719, -0.0410, -0.2809,  0.5985,  0.3167,  0.1649,  0.0807]],
       dtype=torch.float64)
	q_value: tensor([[-1.5969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9614588126640456, distance: 0.2246566291132806 entropy 0.03264415264129639
epoch: 83, step: 15
	action: tensor([[ 0.4587,  0.0348, -0.0506, -0.1340,  0.1942,  0.0463,  0.0402]],
       dtype=torch.float64)
	q_value: tensor([[-1.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.621163901125749, distance: 0.7043400294174943 entropy 0.03264415264129639
epoch: 83, step: 16
	action: tensor([[ 0.5092,  0.0738,  0.1600,  0.2653, -0.1394,  0.0903, -0.2268]],
       dtype=torch.float64)
	q_value: tensor([[-0.6259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8415442849631295, distance: 0.4555233535139676 entropy 0.03264415264129639
epoch: 83, step: 17
	action: tensor([[ 5.7087e-01,  4.0886e-02, -2.2308e-01,  4.5871e-01,  1.7871e-01,
          3.0213e-04,  5.1842e-01]], dtype=torch.float64)
	q_value: tensor([[-1.1014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8681056462501763, distance: 0.41559450379366053 entropy 0.03264415264129639
epoch: 83, step: 18
	action: tensor([[ 0.3066,  0.7325,  0.1983,  0.3653,  0.0920, -0.2504,  0.1750]],
       dtype=torch.float64)
	q_value: tensor([[-0.8188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 83, step: 19
	action: tensor([[ 0.1972, -0.0023, -0.1377, -0.1264, -0.5073, -0.0255, -0.3560]],
       dtype=torch.float64)
	q_value: tensor([[-4.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44565871428701653, distance: 0.8520112037146466 entropy 0.03264415264129639
epoch: 83, step: 20
	action: tensor([[ 0.9916,  0.0750, -0.1271,  0.3561,  0.1999,  0.2537, -0.4146]],
       dtype=torch.float64)
	q_value: tensor([[-1.2547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9461874199293191, distance: 0.26545970092853155 entropy 0.03264415264129639
epoch: 83, step: 21
	action: tensor([[ 0.4527,  0.0528, -0.1552,  0.2797, -0.3294, -0.4489, -0.0884]],
       dtype=torch.float64)
	q_value: tensor([[-2.4601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.760449256889289, distance: 0.5600869509486618 entropy 0.03264415264129639
epoch: 83, step: 22
	action: tensor([[ 0.8213,  0.0771, -0.8763,  0.4098, -0.0881,  0.2824, -0.2175]],
       dtype=torch.float64)
	q_value: tensor([[-1.0780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8951606430248544, distance: 0.37052615314549253 entropy 0.03264415264129639
epoch: 83, step: 23
	action: tensor([[ 0.3425,  0.2567, -0.0414, -0.0485, -0.0553, -0.3210,  0.2459]],
       dtype=torch.float64)
	q_value: tensor([[-2.8052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6936585104181182, distance: 0.6333730834619287 entropy 0.03264415264129639
epoch: 83, step: 24
	action: tensor([[ 0.2379,  0.0523, -0.2613,  0.0355,  0.2049,  0.3990,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[-0.7845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6581125553103473, distance: 0.669111153736176 entropy 0.03264415264129639
epoch: 83, step: 25
	action: tensor([[ 0.4382,  0.2356, -0.6278,  0.4016, -0.1386,  0.4128,  0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-0.7285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7829427320281537, distance: 0.5331431817263717 entropy 0.03264415264129639
epoch: 83, step: 26
	action: tensor([[ 0.5699,  0.3827, -0.4750, -0.0226, -0.3975, -0.1496, -0.1867]],
       dtype=torch.float64)
	q_value: tensor([[-1.7980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9147489778683158, distance: 0.33412309115054206 entropy 0.03264415264129639
epoch: 83, step: 27
	action: tensor([[ 0.5921,  0.2411, -0.5275,  0.2888, -0.4009,  0.3843, -0.4110]],
       dtype=torch.float64)
	q_value: tensor([[-2.0845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8792560961316038, distance: 0.3976392993644299 entropy 0.03264415264129639
epoch: 83, step: 28
	action: tensor([[ 0.2544,  0.2775,  0.1613,  0.0785, -0.2893, -0.1382, -0.0898]],
       dtype=torch.float64)
	q_value: tensor([[-2.8689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7252321452660251, distance: 0.5998458447492453 entropy 0.03264415264129639
epoch: 83, step: 29
	action: tensor([[ 0.4740,  0.2324,  0.1649,  0.0759, -0.1325,  0.3496, -0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-0.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8819168927702505, distance: 0.39323356411828114 entropy 0.03264415264129639
epoch: 83, step: 30
	action: tensor([[ 0.3880,  0.1603, -0.1583,  0.0750, -0.4235,  0.1775,  0.6793]],
       dtype=torch.float64)
	q_value: tensor([[-1.1613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7618746141550217, distance: 0.5584181707290509 entropy 0.03264415264129639
epoch: 83, step: 31
	action: tensor([[ 0.7737,  0.1368,  0.1322,  0.1114, -0.1893,  0.2454,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[-0.9899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9131227116521109, distance: 0.33729493667881505 entropy 0.03264415264129639
epoch: 83, step: 32
	action: tensor([[ 0.6170, -0.2585,  0.2941,  0.2072,  0.4974,  0.2864, -0.1681]],
       dtype=torch.float64)
	q_value: tensor([[-1.4146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6986836700245678, distance: 0.6281567454807961 entropy 0.03264415264129639
epoch: 83, step: 33
	action: tensor([[ 0.7244,  0.2441, -0.3554,  0.6803, -0.1067,  0.3009,  0.1490]],
       dtype=torch.float64)
	q_value: tensor([[-0.7183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9473046467573243, distance: 0.2626895843528121 entropy 0.03264415264129639
epoch: 83, step: 34
	action: tensor([[ 0.6607,  0.1346, -0.0329,  0.1762,  0.0316,  0.3818, -0.1491]],
       dtype=torch.float64)
	q_value: tensor([[-1.9438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9341718425347986, distance: 0.2936043886685694 entropy 0.03264415264129639
epoch: 83, step: 35
	action: tensor([[ 0.2863,  0.0858, -0.0097,  0.2631,  0.0774,  0.3240, -0.3998]],
       dtype=torch.float64)
	q_value: tensor([[-1.4257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.783991047432436, distance: 0.5318541701248669 entropy 0.03264415264129639
epoch: 83, step: 36
	action: tensor([[ 0.4123,  0.1019, -0.2246,  0.2957,  0.1128,  0.1582,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-1.0197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8151951018481236, distance: 0.4919414673003363 entropy 0.03264415264129639
epoch: 83, step: 37
	action: tensor([[ 0.2861,  0.1081, -0.2503,  0.2092,  0.1407,  0.0813, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-0.7979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.707288306427114, distance: 0.6191227026828982 entropy 0.03264415264129639
epoch: 83, step: 38
	action: tensor([[ 0.6148, -0.0179,  0.0532,  0.0456, -0.2052,  0.2801, -0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-0.6663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7821526418393692, distance: 0.5341126230536868 entropy 0.03264415264129639
epoch: 83, step: 39
	action: tensor([[ 0.2138,  0.2613, -0.1903,  0.3068, -0.0321, -0.0811,  0.4263]],
       dtype=torch.float64)
	q_value: tensor([[-1.1668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7247260448467723, distance: 0.6003980246409841 entropy 0.03264415264129639
epoch: 83, step: 40
	action: tensor([[ 0.3173,  0.4004,  0.1269,  0.0579, -0.8334, -0.2123, -0.3172]],
       dtype=torch.float64)
	q_value: tensor([[-0.7268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8226119965952671, distance: 0.4819686787191699 entropy 0.03264415264129639
epoch: 83, step: 41
	action: tensor([[ 0.0148, -0.0297, -0.2745,  0.0322, -0.1689,  0.2369, -0.3421]],
       dtype=torch.float64)
	q_value: tensor([[-2.0623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3546671211391754, distance: 0.9192816464521907 entropy 0.03264415264129639
epoch: 83, step: 42
	action: tensor([[ 0.5107,  0.2132, -0.5322, -0.0844,  0.1920, -0.0937,  0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-0.8669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7800465772212215, distance: 0.5366882109636955 entropy 0.03264415264129639
epoch: 83, step: 43
	action: tensor([[ 0.2677,  0.8223, -0.5897,  0.1632, -0.3776, -0.0996, -0.3974]],
       dtype=torch.float64)
	q_value: tensor([[-1.2759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 83, step: 44
	action: tensor([[ 0.5325,  0.0647, -0.0691,  0.1259, -0.2438, -0.0737, -0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-4.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7612352241770735, distance: 0.5591673718806075 entropy 0.03264415264129639
epoch: 83, step: 45
	action: tensor([[ 0.2606,  0.3879, -0.2422,  0.3168, -0.1369,  0.0457,  0.6039]],
       dtype=torch.float64)
	q_value: tensor([[-0.9752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7811201961931797, distance: 0.535376789020522 entropy 0.03264415264129639
epoch: 83, step: 46
	action: tensor([[ 0.5238,  0.1853,  0.2035, -0.1457, -0.2934, -0.2195,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[-0.9724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6903044170254182, distance: 0.6368310041794362 entropy 0.03264415264129639
epoch: 83, step: 47
	action: tensor([[ 0.5213,  0.1831, -0.2541,  0.2243, -0.0694, -0.0521, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[-1.0093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8378264896757444, distance: 0.46083626908438735 entropy 0.03264415264129639
epoch: 83, step: 48
	action: tensor([[ 0.5032,  0.1867, -0.1055,  0.3824,  0.0667,  0.1795, -0.1136]],
       dtype=torch.float64)
	q_value: tensor([[-1.0457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.895721117627551, distance: 0.3695344034716559 entropy 0.03264415264129639
epoch: 83, step: 49
	action: tensor([[ 0.3717, -0.0088, -0.3089,  0.4624, -0.1689,  0.1704,  0.1283]],
       dtype=torch.float64)
	q_value: tensor([[-1.0779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7501495909528834, distance: 0.5720009178374433 entropy 0.03264415264129639
epoch: 83, step: 50
	action: tensor([[ 0.3457, -0.2475, -0.3478,  0.2345, -0.0778,  0.0774, -0.4632]],
       dtype=torch.float64)
	q_value: tensor([[-0.8904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5166944783913613, distance: 0.795550167822501 entropy 0.03264415264129639
epoch: 83, step: 51
	action: tensor([[ 0.4032,  0.2272, -0.2263,  0.5650, -0.1103,  0.3050, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[-0.9524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8399506531932615, distance: 0.45780828332668955 entropy 0.03264415264129639
epoch: 83, step: 52
	action: tensor([[ 0.5933,  0.1102, -0.5136,  0.3760, -0.4919, -0.0141, -0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-1.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8685255095704563, distance: 0.4149324891891968 entropy 0.03264415264129639
epoch: 83, step: 53
	action: tensor([[ 0.7501, -0.1300, -0.4682,  0.5798, -0.0740,  0.1168,  0.1695]],
       dtype=torch.float64)
	q_value: tensor([[-1.8767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8748617204237545, distance: 0.40481051351228625 entropy 0.03264415264129639
epoch: 83, step: 54
	action: tensor([[ 0.7203, -0.1351, -0.5527, -0.0036,  0.2747, -0.2079,  0.0776]],
       dtype=torch.float64)
	q_value: tensor([[-1.4003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5809511960966189, distance: 0.7407795692365897 entropy 0.03264415264129639
epoch: 83, step: 55
	action: tensor([[ 0.2057,  0.1776, -0.1135, -0.1334, -0.3800,  0.0736, -0.0435]],
       dtype=torch.float64)
	q_value: tensor([[-1.2172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5881503844664533, distance: 0.7343887675416686 entropy 0.03264415264129639
epoch: 83, step: 56
	action: tensor([[ 0.5054,  0.0335,  0.0957,  0.2060,  0.0082, -0.0208, -0.4374]],
       dtype=torch.float64)
	q_value: tensor([[-1.0622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7886694034350519, distance: 0.5260631519898384 entropy 0.03264415264129639
epoch: 83, step: 57
	action: tensor([[ 0.3752, -0.1143, -0.2640,  0.5725, -0.4864,  0.2702,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-1.1031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7394506640439503, distance: 0.5841194647718978 entropy 0.03264415264129639
epoch: 83, step: 58
	action: tensor([[ 0.4638,  0.2432, -0.2068, -0.0360,  0.0161,  0.0717, -0.1201]],
       dtype=torch.float64)
	q_value: tensor([[-1.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8186846377772944, distance: 0.48727484749193367 entropy 0.03264415264129639
epoch: 83, step: 59
	action: tensor([[ 0.6393,  0.2272, -0.0142,  0.2033, -0.2227, -0.2169, -0.2693]],
       dtype=torch.float64)
	q_value: tensor([[-1.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9045887354743438, distance: 0.35347322243807383 entropy 0.03264415264129639
epoch: 83, step: 60
	action: tensor([[ 0.7228,  0.1148, -0.0743,  0.1576, -0.3148, -0.3480, -0.2366]],
       dtype=torch.float64)
	q_value: tensor([[-1.5958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8348354970392111, distance: 0.4650664929470049 entropy 0.03264415264129639
epoch: 83, step: 61
	action: tensor([[ 0.5137,  0.0443, -0.5557,  0.1418,  0.2046,  0.2017, -0.0977]],
       dtype=torch.float64)
	q_value: tensor([[-1.7045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7859481424805721, distance: 0.5294393221043115 entropy 0.03264415264129639
epoch: 83, step: 62
	action: tensor([[-0.0786,  0.0213, -0.2488, -0.0692, -0.2155, -0.2126,  0.1455]],
       dtype=torch.float64)
	q_value: tensor([[-1.1826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22809670146313976, distance: 1.005398288669201 entropy 0.03264415264129639
epoch: 83, step: 63
	action: tensor([[ 0.1896, -0.0971, -0.1263,  0.4184, -0.0105, -0.0739, -0.0858]],
       dtype=torch.float64)
	q_value: tensor([[-0.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5712140755494, distance: 0.7493366139609603 entropy 0.03264415264129639
epoch: 83, step: 64
	action: tensor([[ 0.7339,  0.3621, -0.0144,  0.5553,  0.0422, -0.0513,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[-0.5634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.979918247795564, distance: 0.16216513770953095 entropy 0.03264415264129639
epoch: 83, step: 65
	action: tensor([[ 0.6276, -0.1495, -0.2352,  0.3949, -0.0750,  0.3362, -0.1470]],
       dtype=torch.float64)
	q_value: tensor([[-1.5831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8342891359098644, distance: 0.46583507354944703 entropy 0.03264415264129639
epoch: 83, step: 66
	action: tensor([[ 0.6271,  0.1352,  0.1416,  0.4701, -0.5351,  0.1602,  0.1870]],
       dtype=torch.float64)
	q_value: tensor([[-1.3608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9594488987465448, distance: 0.23044008127603433 entropy 0.03264415264129639
epoch: 83, step: 67
	action: tensor([[0.5558, 0.6332, 0.2893, 0.2474, 0.2838, 0.5569, 0.0460]],
       dtype=torch.float64)
	q_value: tensor([[-1.4418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9325367994989223, distance: 0.29722830411758583 entropy 0.03264415264129639
epoch: 83, step: 68
	action: tensor([[ 0.7875,  0.3953, -0.1848,  0.1830, -0.0634, -0.1608,  0.1034]],
       dtype=torch.float64)
	q_value: tensor([[-1.5368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9668643089023858, distance: 0.2083074054919836 entropy 0.03264415264129639
epoch: 83, step: 69
	action: tensor([[ 0.7070, -0.0141, -0.0699, -0.4529, -0.0803, -0.0502,  0.1071]],
       dtype=torch.float64)
	q_value: tensor([[-1.6488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4666424698731253, distance: 0.8357298394197189 entropy 0.03264415264129639
epoch: 83, step: 70
	action: tensor([[ 0.2947, -0.0256, -0.2714,  0.3248, -0.5838, -0.0664,  0.2802]],
       dtype=torch.float64)
	q_value: tensor([[-1.1666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6554333898095719, distance: 0.6717277476520087 entropy 0.03264415264129639
epoch: 83, step: 71
	action: tensor([[ 0.6686, -0.1424,  0.3550,  0.3486,  0.3837, -0.5111,  0.1966]],
       dtype=torch.float64)
	q_value: tensor([[-0.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.692106844881427, distance: 0.634975122158371 entropy 0.03264415264129639
epoch: 83, step: 72
	action: tensor([[ 0.7663,  0.1512, -0.2205,  1.0379,  0.2573,  0.1488, -0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-0.6606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9531865736867424, distance: 0.2475950213356408 entropy 0.03264415264129639
epoch: 83, step: 73
	action: tensor([[ 1.2303, -0.0484, -0.2496,  0.3551,  0.1335,  0.1795, -0.3136]],
       dtype=torch.float64)
	q_value: tensor([[-1.9230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.820502994427112, distance: 0.4848253249779655 entropy 0.03264415264129639
epoch: 83, step: 74
	action: tensor([[ 0.5874,  0.0971, -0.2712,  0.5017,  0.0902,  0.5072, -0.1860]],
       dtype=torch.float64)
	q_value: tensor([[-2.9216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9374790701496564, distance: 0.28613396139830777 entropy 0.03264415264129639
epoch: 83, step: 75
	action: tensor([[ 0.3853, -0.1055,  0.1267,  0.3349,  0.3366,  0.0602, -0.1049]],
       dtype=torch.float64)
	q_value: tensor([[-1.7335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7243280374912369, distance: 0.6008319133386357 entropy 0.03264415264129639
epoch: 83, step: 76
	action: tensor([[ 0.5357,  0.2497, -0.7388,  0.0768, -0.1024, -0.2885,  0.5497]],
       dtype=torch.float64)
	q_value: tensor([[-0.5048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8364116778424275, distance: 0.46284208606367694 entropy 0.03264415264129639
epoch: 83, step: 77
	action: tensor([[ 0.1052, -0.0404, -0.2602,  0.6549, -0.3320,  0.2996,  0.2611]],
       dtype=torch.float64)
	q_value: tensor([[-1.6955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5824801248576592, distance: 0.7394269413813553 entropy 0.03264415264129639
epoch: 83, step: 78
	action: tensor([[-0.0555,  0.1461,  0.0029,  0.3328, -0.1129,  0.1175,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[-0.8809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5131829910417123, distance: 0.7984349977804506 entropy 0.03264415264129639
epoch: 83, step: 79
	action: tensor([[ 0.4103, -0.0773, -0.0063,  0.5141,  0.2847, -0.3521,  0.2424]],
       dtype=torch.float64)
	q_value: tensor([[-0.6355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7132048404650853, distance: 0.6128336470925864 entropy 0.03264415264129639
epoch: 83, step: 80
	action: tensor([[ 0.6158,  0.4864,  0.1866,  0.3273,  0.1006, -0.0356,  0.0302]],
       dtype=torch.float64)
	q_value: tensor([[-0.5341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9646582513078681, distance: 0.21512986785771968 entropy 0.03264415264129639
epoch: 83, step: 81
	action: tensor([[ 0.1378,  0.2710, -0.0104,  0.5232, -0.5263, -0.0514, -0.1318]],
       dtype=torch.float64)
	q_value: tensor([[-1.1726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7398965537313462, distance: 0.5836194359171258 entropy 0.03264415264129639
epoch: 83, step: 82
	action: tensor([[ 0.6689, -0.0114, -0.4439,  0.2142,  0.2114,  0.1300, -0.3046]],
       dtype=torch.float64)
	q_value: tensor([[-1.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8286429809311574, distance: 0.4737046441739071 entropy 0.03264415264129639
epoch: 83, step: 83
	action: tensor([[ 0.6709,  0.0300, -0.2902,  0.0310, -0.3252,  0.0770,  0.2889]],
       dtype=torch.float64)
	q_value: tensor([[-1.4550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.78318044388442, distance: 0.532851163923597 entropy 0.03264415264129639
epoch: 83, step: 84
	action: tensor([[ 0.0803, -0.0339,  0.1151,  0.4490, -0.6026,  0.3419, -0.0987]],
       dtype=torch.float64)
	q_value: tensor([[-1.3150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6233024033055101, distance: 0.7023492419015536 entropy 0.03264415264129639
epoch: 83, step: 85
	action: tensor([[ 0.4259, -0.2406,  0.0406,  0.3407,  0.1605,  0.1956, -0.1378]],
       dtype=torch.float64)
	q_value: tensor([[-1.1624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6836285446307928, distance: 0.6436582489811955 entropy 0.03264415264129639
epoch: 83, step: 86
	action: tensor([[ 0.3694,  0.3958, -0.3702,  0.5571, -0.4668,  0.4580,  0.1249]],
       dtype=torch.float64)
	q_value: tensor([[-0.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7469318528402318, distance: 0.575672436856569 entropy 0.03264415264129639
epoch: 83, step: 87
	action: tensor([[ 0.6537,  0.3963, -0.1055,  0.2637, -0.5013,  0.0274,  0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-2.0062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9668194800293701, distance: 0.2084482661421595 entropy 0.03264415264129639
epoch: 83, step: 88
	action: tensor([[ 0.7430, -0.2625, -0.2545,  0.4971,  0.0359,  0.0212, -0.3063]],
       dtype=torch.float64)
	q_value: tensor([[-1.6665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7909402754997035, distance: 0.5232290883832985 entropy 0.03264415264129639
epoch: 83, step: 89
	action: tensor([[ 0.3962,  0.0821, -0.0009,  0.3273,  0.0772,  0.2421, -0.4419]],
       dtype=torch.float64)
	q_value: tensor([[-1.4400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8514340225437612, distance: 0.44107899421675795 entropy 0.03264415264129639
epoch: 83, step: 90
	action: tensor([[ 0.5938, -0.0105, -0.2105,  0.3242, -0.0337,  0.0901,  0.3228]],
       dtype=torch.float64)
	q_value: tensor([[-1.1884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8413414114584683, distance: 0.4558148673240108 entropy 0.03264415264129639
epoch: 83, step: 91
	action: tensor([[ 0.6836,  0.0238, -0.3268,  0.6351, -0.2209,  0.0044, -0.2219]],
       dtype=torch.float64)
	q_value: tensor([[-0.8338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9330682823835433, distance: 0.2960551896133099 entropy 0.03264415264129639
epoch: 83, step: 92
	action: tensor([[ 0.6018,  0.1236, -0.1340, -0.1962, -0.2394,  0.2855, -0.3350]],
       dtype=torch.float64)
	q_value: tensor([[-1.8985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.782109562358049, distance: 0.5341654310325121 entropy 0.03264415264129639
epoch: 83, step: 93
	action: tensor([[ 0.6219,  0.1670, -0.2814,  0.5683,  0.5607,  0.3139, -0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-1.7853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9559664795389509, distance: 0.24013109293414076 entropy 0.03264415264129639
epoch: 83, step: 94
	action: tensor([[ 0.5115,  0.0194, -0.2234,  0.4456, -0.2924,  0.1387,  0.1197]],
       dtype=torch.float64)
	q_value: tensor([[-1.5737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8560816499347634, distance: 0.43412498268231253 entropy 0.03264415264129639
epoch: 83, step: 95
	action: tensor([[ 0.1487,  0.1160, -0.4264,  0.1017, -0.5197, -0.0591, -0.1100]],
       dtype=torch.float64)
	q_value: tensor([[-1.0868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5647448581618655, distance: 0.7549681814325078 entropy 0.03264415264129639
epoch: 83, step: 96
	action: tensor([[ 0.7859, -0.5649, -0.1824,  0.9676, -0.0646, -0.4099, -0.0980]],
       dtype=torch.float64)
	q_value: tensor([[-1.2400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7160858806799593, distance: 0.6097477251784599 entropy 0.03264415264129639
epoch: 83, step: 97
	action: tensor([[ 0.6757, -0.0129, -0.3381,  0.0984,  0.1939,  0.2867, -0.1097]],
       dtype=torch.float64)
	q_value: tensor([[-1.3152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8257808291394249, distance: 0.47764437160015716 entropy 0.03264415264129639
epoch: 83, step: 98
	action: tensor([[-0.0233,  0.2450, -0.2011, -0.0076, -0.3421, -0.1732,  0.2306]],
       dtype=torch.float64)
	q_value: tensor([[-1.2657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4624783673767491, distance: 0.8389859091207399 entropy 0.03264415264129639
epoch: 83, step: 99
	action: tensor([[ 0.4545,  0.2836, -0.1462,  0.2534, -0.1542,  0.2604, -0.3052]],
       dtype=torch.float64)
	q_value: tensor([[-0.8411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.874995053314656, distance: 0.404594796382629 entropy 0.03264415264129639
epoch: 83, step: 100
	action: tensor([[ 0.3357, -0.0255, -0.1453,  0.4083,  0.0826, -0.5743, -0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-1.5597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6353810325808154, distance: 0.6909972542887362 entropy 0.03264415264129639
epoch: 83, step: 101
	action: tensor([[ 0.8103,  0.0202, -0.4083,  0.3033, -0.3883, -0.0190,  0.1355]],
       dtype=torch.float64)
	q_value: tensor([[-0.7842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.864299184496807, distance: 0.4215488628088816 entropy 0.03264415264129639
epoch: 83, step: 102
	action: tensor([[ 0.2422,  0.3574, -0.5117,  0.1367, -0.3312, -0.1212, -0.4262]],
       dtype=torch.float64)
	q_value: tensor([[-1.7432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7473113133352842, distance: 0.5752406818173609 entropy 0.03264415264129639
epoch: 83, step: 103
	action: tensor([[ 0.5697,  0.4095, -0.1846,  0.3259, -0.3113,  0.7827, -0.6270]],
       dtype=torch.float64)
	q_value: tensor([[-1.8257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8869833630126969, distance: 0.38470504640342384 entropy 0.03264415264129639
epoch: 83, step: 104
	action: tensor([[ 0.3603,  0.0836, -0.6451,  0.5373,  0.2115,  0.6148,  0.0865]],
       dtype=torch.float64)
	q_value: tensor([[-3.4127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7117710925098757, distance: 0.6143635780535176 entropy 0.03264415264129639
epoch: 83, step: 105
	action: tensor([[ 0.1825, -0.0029,  0.0249,  0.4983, -0.2587,  0.2770, -0.4544]],
       dtype=torch.float64)
	q_value: tensor([[-1.5437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7164609022814551, distance: 0.6093448848216098 entropy 0.03264415264129639
epoch: 83, step: 106
	action: tensor([[ 0.6876, -0.3079, -0.0876,  0.4611,  0.0784,  0.1209, -0.1995]],
       dtype=torch.float64)
	q_value: tensor([[-1.2932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7602792863818725, distance: 0.5602856173800218 entropy 0.03264415264129639
epoch: 83, step: 107
	action: tensor([[ 0.5372,  0.0531, -0.7022,  0.2701, -0.4144,  0.0125, -0.0963]],
       dtype=torch.float64)
	q_value: tensor([[-1.1016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7809923695815327, distance: 0.5355330972111131 entropy 0.03264415264129639
epoch: 83, step: 108
	action: tensor([[ 0.6668,  0.4589,  0.0203, -0.0111,  0.0398,  0.0916, -0.1692]],
       dtype=torch.float64)
	q_value: tensor([[-1.8058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9594089414241005, distance: 0.23055358622692942 entropy 0.03264415264129639
epoch: 83, step: 109
	action: tensor([[ 0.5710, -0.1256,  0.1494,  0.3218, -0.0865,  0.3426, -0.1372]],
       dtype=torch.float64)
	q_value: tensor([[-1.5597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8450943016918593, distance: 0.4503917120691042 entropy 0.03264415264129639
epoch: 83, step: 110
	action: tensor([[ 0.4449,  0.1095, -0.1696,  0.1127,  0.2372, -0.3503, -0.2063]],
       dtype=torch.float64)
	q_value: tensor([[-1.1491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7037517920164542, distance: 0.6228515639114695 entropy 0.03264415264129639
epoch: 83, step: 111
	action: tensor([[ 0.9132,  0.1628, -0.4992, -0.0764, -0.0620,  0.2859,  0.1349]],
       dtype=torch.float64)
	q_value: tensor([[-0.8585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8702960890664299, distance: 0.4121290509599472 entropy 0.03264415264129639
epoch: 83, step: 112
	action: tensor([[ 0.1022,  0.2680, -0.3631, -0.0175, -0.1008, -0.2059, -0.0918]],
       dtype=torch.float64)
	q_value: tensor([[-2.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5838626824803896, distance: 0.738201672693802 entropy 0.03264415264129639
epoch: 83, step: 113
	action: tensor([[ 0.3323, -0.0530,  0.0315, -0.2032, -0.0303,  0.6010,  0.1485]],
       dtype=torch.float64)
	q_value: tensor([[-0.9327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5995964334130248, distance: 0.724111862010435 entropy 0.03264415264129639
epoch: 83, step: 114
	action: tensor([[ 0.2830,  0.2786, -0.3808,  0.2132, -0.3998,  0.1757,  0.1897]],
       dtype=torch.float64)
	q_value: tensor([[-0.9411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7462373490029817, distance: 0.5764618147291029 entropy 0.03264415264129639
epoch: 83, step: 115
	action: tensor([[ 0.6696,  0.3578, -0.6144,  0.3485,  0.0789, -0.2902, -0.0992]],
       dtype=torch.float64)
	q_value: tensor([[-1.2817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.942462664092526, distance: 0.274493185942722 entropy 0.03264415264129639
epoch: 83, step: 116
	action: tensor([[ 1.1234e-02, -2.1892e-04, -3.0271e-03, -1.0766e-01, -3.8910e-01,
          4.4207e-01,  3.0425e-02]], dtype=torch.float64)
	q_value: tensor([[-2.0423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3473022510876336, distance: 0.9245124217183628 entropy 0.03264415264129639
epoch: 83, step: 117
	action: tensor([[ 0.2323,  0.1017, -0.0417,  0.4242, -0.2974,  0.1531, -0.1185]],
       dtype=torch.float64)
	q_value: tensor([[-0.9745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7335915357205964, distance: 0.5906506725180826 entropy 0.03264415264129639
epoch: 83, step: 118
	action: tensor([[ 0.3286,  0.4653,  0.4064,  0.5836, -0.6086,  0.2041, -0.1185]],
       dtype=torch.float64)
	q_value: tensor([[-0.9936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9005976449818668, distance: 0.3607904472897143 entropy 0.03264415264129639
epoch: 83, step: 119
	action: tensor([[ 0.1996,  0.2607, -0.3910,  0.1540, -0.2378,  0.1244,  0.1123]],
       dtype=torch.float64)
	q_value: tensor([[-1.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6821618344990019, distance: 0.6451485358157898 entropy 0.03264415264129639
epoch: 83, step: 120
	action: tensor([[ 0.3078, -0.2525,  0.0207,  0.3638, -0.2786,  0.0236, -0.2052]],
       dtype=torch.float64)
	q_value: tensor([[-1.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5860971668889257, distance: 0.7362170869502687 entropy 0.03264415264129639
epoch: 83, step: 121
	action: tensor([[ 0.4211,  0.1287, -0.2087,  0.1698, -0.4649,  0.6293, -0.1805]],
       dtype=torch.float64)
	q_value: tensor([[-0.7474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8009206299204461, distance: 0.5105870812768719 entropy 0.03264415264129639
epoch: 83, step: 122
	action: tensor([[ 0.6529,  0.5346, -0.0214, -0.0098, -0.2107, -0.1254, -0.2509]],
       dtype=torch.float64)
	q_value: tensor([[-2.0864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9635661189574285, distance: 0.2184285542670249 entropy 0.03264415264129639
epoch: 83, step: 123
	action: tensor([[ 0.5348,  0.2347, -0.3692, -0.2639, -0.1670,  0.2834,  0.2769]],
       dtype=torch.float64)
	q_value: tensor([[-1.9915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.807855065683258, distance: 0.5016157484493126 entropy 0.03264415264129639
epoch: 83, step: 124
	action: tensor([[ 0.8131,  0.2927, -0.5572,  0.0617, -0.1084,  0.2896, -0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-1.4933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9565707240947837, distance: 0.23847781696414436 entropy 0.03264415264129639
epoch: 83, step: 125
	action: tensor([[ 0.7215,  0.0554,  0.2888,  0.1496, -0.0388,  0.1726, -0.0350]],
       dtype=torch.float64)
	q_value: tensor([[-2.4550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8720157625871902, distance: 0.4093878372524178 entropy 0.03264415264129639
epoch: 83, step: 126
	action: tensor([[ 0.6624, -0.0307, -0.0541,  0.4796,  0.3241,  0.0520, -0.3199]],
       dtype=torch.float64)
	q_value: tensor([[-1.1378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9085642859791867, distance: 0.34603069385419866 entropy 0.03264415264129639
epoch: 83, step: 127
	action: tensor([[ 0.2611,  0.4099,  0.5309,  0.0887, -0.3699,  0.1506, -0.2457]],
       dtype=torch.float64)
	q_value: tensor([[-1.2009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8260714015009529, distance: 0.4772458845359484 entropy 0.03264415264129639
LOSS epoch 83 actor 21.31934497960464 critic 309.63962413483137 
epoch: 84, step: 0
	action: tensor([[ 0.7971,  0.3619, -0.4699,  0.2608,  0.1010, -0.1293,  0.3466]],
       dtype=torch.float64)
	q_value: tensor([[-1.4682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9686446170542938, distance: 0.2026342008077438 entropy 0.03264415264129639
epoch: 84, step: 1
	action: tensor([[ 0.2848, -0.2830, -0.5566, -0.1839, -0.3526, -0.2111,  0.4601]],
       dtype=torch.float64)
	q_value: tensor([[-1.7423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26826616327908137, distance: 0.9788885808487505 entropy 0.03264415264129639
epoch: 84, step: 2
	action: tensor([[ 0.1127,  0.1536, -0.5650,  0.2109, -0.1314,  0.1785, -0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-1.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5415084043144216, distance: 0.7748584668556456 entropy 0.03264415264129639
epoch: 84, step: 3
	action: tensor([[ 0.2377, -0.2308, -0.6458,  0.0048, -0.3120,  0.0669, -0.3561]],
       dtype=torch.float64)
	q_value: tensor([[-1.0703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34444933204416095, distance: 0.9265307243786408 entropy 0.03264415264129639
epoch: 84, step: 4
	action: tensor([[ 0.8393,  0.2830, -0.1418,  0.6026, -0.0039,  0.0584, -0.1989]],
       dtype=torch.float64)
	q_value: tensor([[-1.2105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.989736665319277, distance: 0.11593136219737192 entropy 0.03264415264129639
epoch: 84, step: 5
	action: tensor([[ 0.3701,  0.0163, -0.3101,  0.1901, -0.3790,  0.0106, -0.0714]],
       dtype=torch.float64)
	q_value: tensor([[-2.2642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6952996845834893, distance: 0.6316742089646202 entropy 0.03264415264129639
epoch: 84, step: 6
	action: tensor([[ 0.8734,  0.0877, -0.3931,  0.4129, -0.2478, -0.0088,  0.0473]],
       dtype=torch.float64)
	q_value: tensor([[-1.0639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9361036772787701, distance: 0.28926415872916145 entropy 0.03264415264129639
epoch: 84, step: 7
	action: tensor([[ 0.6564, -0.3178, -0.5015,  0.4063,  0.0719, -0.3121, -0.1889]],
       dtype=torch.float64)
	q_value: tensor([[-1.9854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5822222608441525, distance: 0.739655244486328 entropy 0.03264415264129639
epoch: 84, step: 8
	action: tensor([[ 0.4371, -0.2446, -0.2170,  0.5043,  0.1091,  0.0421,  0.1846]],
       dtype=torch.float64)
	q_value: tensor([[-1.1935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6907719346552452, distance: 0.6363501413799318 entropy 0.03264415264129639
epoch: 84, step: 9
	action: tensor([[ 0.6001,  0.4183, -0.1941,  0.2907, -0.0301, -0.0122, -0.6481]],
       dtype=torch.float64)
	q_value: tensor([[-0.6182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.948896150603447, distance: 0.2586922990895992 entropy 0.03264415264129639
epoch: 84, step: 10
	action: tensor([[ 0.4922,  0.1225, -0.2121,  0.3327, -0.1514,  0.0665,  0.1722]],
       dtype=torch.float64)
	q_value: tensor([[-2.4813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8601366383156223, distance: 0.427965415554069 entropy 0.03264415264129639
epoch: 84, step: 11
	action: tensor([[ 0.4785,  0.0092, -0.1824,  0.2126, -0.2301,  0.4500, -0.2913]],
       dtype=torch.float64)
	q_value: tensor([[-0.9477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8188415861962424, distance: 0.4870639068056162 entropy 0.03264415264129639
epoch: 84, step: 12
	action: tensor([[ 0.7087,  0.1803, -0.2953,  0.3715, -0.0651,  0.3588,  0.1133]],
       dtype=torch.float64)
	q_value: tensor([[-1.6192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9607045197564865, distance: 0.22684436404237052 entropy 0.03264415264129639
epoch: 84, step: 13
	action: tensor([[ 0.7350,  0.0083, -0.2256,  0.0739, -0.1542,  0.7089,  0.0787]],
       dtype=torch.float64)
	q_value: tensor([[-1.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9024607842948666, distance: 0.3573932310987707 entropy 0.03264415264129639
epoch: 84, step: 14
	action: tensor([[ 0.1372,  0.1568, -0.4819,  0.6457, -0.1177, -0.0846, -0.1075]],
       dtype=torch.float64)
	q_value: tensor([[-1.9823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5936925449495647, distance: 0.7294307856077685 entropy 0.03264415264129639
epoch: 84, step: 15
	action: tensor([[ 0.6786,  0.0627, -0.0819,  0.1960, -0.4364, -0.1114,  0.1865]],
       dtype=torch.float64)
	q_value: tensor([[-1.0815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8372075264219027, distance: 0.461714862223764 entropy 0.03264415264129639
epoch: 84, step: 16
	action: tensor([[ 0.9969, -0.2275, -0.2669,  0.3263,  0.0806, -0.1047, -0.2829]],
       dtype=torch.float64)
	q_value: tensor([[-1.2456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6974032183569254, distance: 0.6294900148455642 entropy 0.03264415264129639
epoch: 84, step: 17
	action: tensor([[ 0.5943,  0.0700,  0.1113,  0.2024, -0.2147, -0.1157,  0.2229]],
       dtype=torch.float64)
	q_value: tensor([[-1.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8257051414316422, distance: 0.4777481141499912 entropy 0.03264415264129639
epoch: 84, step: 18
	action: tensor([[ 0.6331,  0.3346, -0.1722,  0.2421, -0.0946,  0.3528,  0.1403]],
       dtype=torch.float64)
	q_value: tensor([[-0.8182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9620946412246955, distance: 0.22279579975769084 entropy 0.03264415264129639
epoch: 84, step: 19
	action: tensor([[ 0.3841,  0.0299, -0.1881,  0.6719,  0.0765, -0.0582,  0.0919]],
       dtype=torch.float64)
	q_value: tensor([[-1.5638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8274161975142387, distance: 0.4753972990658074 entropy 0.03264415264129639
epoch: 84, step: 20
	action: tensor([[ 0.3880,  0.3642, -0.0754,  0.2705, -0.1472, -0.1325,  0.1128]],
       dtype=torch.float64)
	q_value: tensor([[-0.8038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8671428039348559, distance: 0.41710868665585693 entropy 0.03264415264129639
epoch: 84, step: 21
	action: tensor([[ 0.5482, -0.1522,  0.0854,  0.3496,  0.2725,  0.3351, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-1.0226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8309257854741923, distance: 0.47053873708416805 entropy 0.03264415264129639
epoch: 84, step: 22
	action: tensor([[ 0.7409, -0.4297,  0.1524,  0.4132, -0.1348,  0.0840, -0.7159]],
       dtype=torch.float64)
	q_value: tensor([[-0.7663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6360167990103136, distance: 0.6903945641312068 entropy 0.03264415264129639
epoch: 84, step: 23
	action: tensor([[ 0.6062, -0.2974, -0.2763,  0.4103, -0.0952, -0.0847,  0.3087]],
       dtype=torch.float64)
	q_value: tensor([[-1.9336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6552433841270353, distance: 0.6719129288261043 entropy 0.03264415264129639
epoch: 84, step: 24
	action: tensor([[ 0.5173,  0.0411, -0.4001,  0.6943, -0.3392,  0.3006,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[-0.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8435061604188817, distance: 0.4526946024090212 entropy 0.03264415264129639
epoch: 84, step: 25
	action: tensor([[ 0.5899,  0.1000, -0.5238, -0.0597, -0.1487,  0.2842,  0.1114]],
       dtype=torch.float64)
	q_value: tensor([[-1.6594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.803977600290741, distance: 0.506651746466609 entropy 0.03264415264129639
epoch: 84, step: 26
	action: tensor([[ 0.5164,  0.1237, -0.0258,  0.2136, -0.5293,  0.7898, -0.1704]],
       dtype=torch.float64)
	q_value: tensor([[-1.5874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8836225225088152, distance: 0.3903832389599492 entropy 0.03264415264129639
epoch: 84, step: 27
	action: tensor([[ 0.3452,  0.3119,  0.2110,  0.4016,  0.4426, -0.1067,  0.6801]],
       dtype=torch.float64)
	q_value: tensor([[-2.4796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8668682230311711, distance: 0.4175394911914245 entropy 0.03264415264129639
epoch: 84, step: 28
	action: tensor([[ 0.5898,  0.2298, -0.2292,  0.5636,  0.4498,  0.3458, -0.1572]],
       dtype=torch.float64)
	q_value: tensor([[-0.5986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9430434609536722, distance: 0.2731042691588832 entropy 0.03264415264129639
epoch: 84, step: 29
	action: tensor([[ 0.2675,  0.7054,  0.1150,  0.4565, -0.0145,  0.5653, -0.3028]],
       dtype=torch.float64)
	q_value: tensor([[-1.4817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 84, step: 30
	action: tensor([[ 0.3908,  0.4226, -0.3631,  0.4373, -0.1366,  0.2210, -0.4842]],
       dtype=torch.float64)
	q_value: tensor([[-4.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8053554296425919, distance: 0.5048679945373062 entropy 0.03264415264129639
epoch: 84, step: 31
	action: tensor([[ 0.7732,  0.0790, -0.0326,  0.5499,  0.0940,  0.2450,  0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-2.2107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9837964281028262, distance: 0.14566730101569333 entropy 0.03264415264129639
epoch: 84, step: 32
	action: tensor([[ 0.3578, -0.1764, -0.7625,  0.5306, -0.0474,  0.1936,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[-1.3661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.544127184074169, distance: 0.772642407139126 entropy 0.03264415264129639
epoch: 84, step: 33
	action: tensor([[ 0.6987,  0.2125, -0.4442,  0.2983,  0.5001,  0.1037, -0.1431]],
       dtype=torch.float64)
	q_value: tensor([[-1.1631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.929674374581765, distance: 0.30346841391194823 entropy 0.03264415264129639
epoch: 84, step: 34
	action: tensor([[ 0.5188, -0.2332,  0.0143,  0.1342, -0.2389,  0.4387,  0.3614]],
       dtype=torch.float64)
	q_value: tensor([[-1.5639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6675214629635697, distance: 0.6598397895071815 entropy 0.03264415264129639
epoch: 84, step: 35
	action: tensor([[ 0.5634,  0.1053, -0.3654,  0.4561, -0.2252, -0.1349,  0.3378]],
       dtype=torch.float64)
	q_value: tensor([[-0.8964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8598632327504673, distance: 0.428383505748345 entropy 0.03264415264129639
epoch: 84, step: 36
	action: tensor([[ 0.3260,  0.0450, -0.2800,  0.2229, -0.2296, -0.1112, -0.5866]],
       dtype=torch.float64)
	q_value: tensor([[-1.1232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6863193706379556, distance: 0.6409151588625053 entropy 0.03264415264129639
epoch: 84, step: 37
	action: tensor([[ 1.1073, -0.1408, -0.2414,  0.5196,  0.0366, -0.0961,  0.1417]],
       dtype=torch.float64)
	q_value: tensor([[-1.4598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8318038294649877, distance: 0.46931533494895705 entropy 0.03264415264129639
epoch: 84, step: 38
	action: tensor([[ 0.8473, -0.3913, -0.6326,  0.4380, -0.1668, -0.1513, -0.1184]],
       dtype=torch.float64)
	q_value: tensor([[-1.8822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6048257564230499, distance: 0.7193678237959272 entropy 0.03264415264129639
epoch: 84, step: 39
	action: tensor([[ 0.2484,  0.0826, -0.4232,  0.0633,  0.0006,  0.1418,  0.0647]],
       dtype=torch.float64)
	q_value: tensor([[-1.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6273750990386617, distance: 0.6985421717565627 entropy 0.03264415264129639
epoch: 84, step: 40
	action: tensor([[ 0.6110, -0.0169,  0.0290,  0.1091, -0.2436, -0.0501,  0.4107]],
       dtype=torch.float64)
	q_value: tensor([[-0.8011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7402192010779443, distance: 0.5832573459545882 entropy 0.03264415264129639
epoch: 84, step: 41
	action: tensor([[ 0.6715,  0.2654, -0.2020,  0.6284,  0.0906, -0.0652, -0.1587]],
       dtype=torch.float64)
	q_value: tensor([[-0.7552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9603897680394514, distance: 0.22775104907204416 entropy 0.03264415264129639
epoch: 84, step: 42
	action: tensor([[ 0.3050,  0.1666, -0.1112,  0.5503, -0.0836,  0.1110, -0.3247]],
       dtype=torch.float64)
	q_value: tensor([[-1.7360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8233374809213022, distance: 0.4809820872916065 entropy 0.03264415264129639
epoch: 84, step: 43
	action: tensor([[ 0.4108, -0.3446, -0.4831,  0.3684, -0.1341,  0.2332, -0.4483]],
       dtype=torch.float64)
	q_value: tensor([[-1.2470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5307849036504573, distance: 0.7838675415481783 entropy 0.03264415264129639
epoch: 84, step: 44
	action: tensor([[ 0.5391,  0.3574, -0.2116,  0.2640, -0.0396,  0.0082, -0.2521]],
       dtype=torch.float64)
	q_value: tensor([[-1.3187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9172128017995245, distance: 0.3292594782734187 entropy 0.03264415264129639
epoch: 84, step: 45
	action: tensor([[ 0.2256,  0.0797,  0.3241,  0.1298,  0.1057, -0.3681, -0.4034]],
       dtype=torch.float64)
	q_value: tensor([[-1.5658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5482717763263184, distance: 0.7691221276440457 entropy 0.03264415264129639
epoch: 84, step: 46
	action: tensor([[0.7725, 0.0164, 0.0112, 0.3161, 0.0040, 0.0518, 0.2512]],
       dtype=torch.float64)
	q_value: tensor([[-0.7516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8947173114214221, distance: 0.3713087440907109 entropy 0.03264415264129639
epoch: 84, step: 47
	action: tensor([[ 0.8374,  0.0709, -0.0017,  0.6477, -0.7252,  0.2977, -0.0877]],
       dtype=torch.float64)
	q_value: tensor([[-0.9922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9898343096131293, distance: 0.11537856480720898 entropy 0.03264415264129639
epoch: 84, step: 48
	action: tensor([[ 0.6350,  0.2626,  0.1515,  0.2130, -0.2304,  0.1468, -0.7819]],
       dtype=torch.float64)
	q_value: tensor([[-2.6620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9547435642614289, distance: 0.243442762103106 entropy 0.03264415264129639
epoch: 84, step: 49
	action: tensor([[ 0.0609, -0.2751, -0.1753,  0.2664,  0.4001,  0.3192,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-2.6204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3812542426373363, distance: 0.900145698141096 entropy 0.03264415264129639
epoch: 84, step: 50
	action: tensor([[ 0.2279, -0.2383, -0.5356, -0.0563, -0.1751,  0.3854,  0.3852]],
       dtype=torch.float64)
	q_value: tensor([[-0.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34932740057905176, distance: 0.9230770479821113 entropy 0.03264415264129639
epoch: 84, step: 51
	action: tensor([[ 0.4753,  0.0770, -0.4053,  0.7523, -0.0671,  0.2143,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-0.9217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8267279470071981, distance: 0.47634427923393113 entropy 0.03264415264129639
epoch: 84, step: 52
	action: tensor([[ 0.5341,  0.0247, -0.2049,  0.4411, -0.2471, -0.1429, -0.2083]],
       dtype=torch.float64)
	q_value: tensor([[-1.4071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8499924731476138, distance: 0.44321374340327935 entropy 0.03264415264129639
epoch: 84, step: 53
	action: tensor([[-0.0720,  0.3648, -0.1414,  0.1753, -0.1685,  0.1075,  0.1880]],
       dtype=torch.float64)
	q_value: tensor([[-1.3109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5213492837307826, distance: 0.7917098529625447 entropy 0.03264415264129639
epoch: 84, step: 54
	action: tensor([[ 0.7230, -0.0571,  0.0592,  0.4463, -0.4092,  0.3099,  0.2628]],
       dtype=torch.float64)
	q_value: tensor([[-0.8351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.923274857266841, distance: 0.3169753906745669 entropy 0.03264415264129639
epoch: 84, step: 55
	action: tensor([[ 0.4454, -0.0203, -0.5089,  0.1631, -0.1762,  0.4886, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-1.3515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7273843176941679, distance: 0.5974920221288025 entropy 0.03264415264129639
epoch: 84, step: 56
	action: tensor([[ 0.3643, -0.3109,  0.2193,  0.7857, -0.0615, -0.1489,  0.1582]],
       dtype=torch.float64)
	q_value: tensor([[-1.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7638266820025534, distance: 0.556124603079326 entropy 0.03264415264129639
epoch: 84, step: 57
	action: tensor([[ 0.3992,  0.1650, -0.4146,  0.6760, -0.5667,  0.2679, -0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-0.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7640872048265744, distance: 0.5558177878756866 entropy 0.03264415264129639
epoch: 84, step: 58
	action: tensor([[ 0.4043, -0.0793, -0.1628,  0.4838,  0.2301,  0.4032, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-2.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8264442962151725, distance: 0.47673401369516516 entropy 0.03264415264129639
epoch: 84, step: 59
	action: tensor([[ 0.4676,  0.5796, -0.5510,  0.5195,  0.1119, -0.0408,  0.5644]],
       dtype=torch.float64)
	q_value: tensor([[-0.8263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8273420253245403, distance: 0.4754994450098455 entropy 0.03264415264129639
epoch: 84, step: 60
	action: tensor([[ 0.5354, -0.3105, -0.2811,  0.3593, -0.0224, -0.1027, -0.1822]],
       dtype=torch.float64)
	q_value: tensor([[-1.6319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5943620928732891, distance: 0.7288295288131387 entropy 0.03264415264129639
epoch: 84, step: 61
	action: tensor([[ 0.4506,  0.3603,  0.0303,  0.5308, -0.1160, -0.0708, -0.0912]],
       dtype=torch.float64)
	q_value: tensor([[-0.8063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9223831406928192, distance: 0.31881204861887136 entropy 0.03264415264129639
epoch: 84, step: 62
	action: tensor([[ 0.7717,  0.5506,  0.4386,  0.1762, -0.0563, -0.2103,  0.1366]],
       dtype=torch.float64)
	q_value: tensor([[-1.2810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9868944292041739, distance: 0.131004032374367 entropy 0.03264415264129639
epoch: 84, step: 63
	action: tensor([[ 0.4301,  0.1807,  0.4673, -0.0916, -0.4516,  0.0149,  0.1943]],
       dtype=torch.float64)
	q_value: tensor([[-1.4704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7091486939857539, distance: 0.6171520876127955 entropy 0.03264415264129639
epoch: 84, step: 64
	action: tensor([[ 0.5433, -0.1844, -0.3615,  0.0343, -0.1371,  0.3773, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-1.0311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.619131414867149, distance: 0.7062269225334387 entropy 0.03264415264129639
epoch: 84, step: 65
	action: tensor([[ 0.3944,  0.4545, -0.2821,  0.4220, -0.0241,  0.5101, -0.1438]],
       dtype=torch.float64)
	q_value: tensor([[-1.1830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8020767173066838, distance: 0.5091023901460661 entropy 0.03264415264129639
epoch: 84, step: 66
	action: tensor([[ 0.5086,  0.3712, -0.1453,  0.3358,  0.0209,  0.1771,  0.2998]],
       dtype=torch.float64)
	q_value: tensor([[-1.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081263489800253, distance: 0.3468583716736711 entropy 0.03264415264129639
epoch: 84, step: 67
	action: tensor([[ 0.3844,  0.0563, -0.1406,  0.2510, -0.1263,  0.2040,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-1.0725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7567341788276597, distance: 0.5644133021627461 entropy 0.03264415264129639
epoch: 84, step: 68
	action: tensor([[ 0.6158, -0.0390, -0.0800,  0.4104, -0.5939,  0.2836, -0.2250]],
       dtype=torch.float64)
	q_value: tensor([[-0.8537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.869246257806902, distance: 0.413793588200365 entropy 0.03264415264129639
epoch: 84, step: 69
	action: tensor([[ 0.2811,  0.0469, -0.0583,  0.1706, -0.1139,  0.4419,  0.0897]],
       dtype=torch.float64)
	q_value: tensor([[-2.0383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7267612574397545, distance: 0.5981744132959032 entropy 0.03264415264129639
epoch: 84, step: 70
	action: tensor([[ 0.7632, -0.0474, -0.3967,  0.4783, -0.0829, -0.5234, -0.1793]],
       dtype=torch.float64)
	q_value: tensor([[-0.8341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8102127221684549, distance: 0.49852878741310797 entropy 0.03264415264129639
epoch: 84, step: 71
	action: tensor([[ 0.6802, -0.1187, -0.1535,  0.0628, -0.1740,  0.2956,  0.0478]],
       dtype=torch.float64)
	q_value: tensor([[-1.7196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7426175472774041, distance: 0.5805587310936942 entropy 0.03264415264129639
epoch: 84, step: 72
	action: tensor([[ 0.9491,  0.0550, -0.5417,  0.7674,  0.0031, -0.0056,  0.1428]],
       dtype=torch.float64)
	q_value: tensor([[-1.2144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9792936927858589, distance: 0.16466754847349593 entropy 0.03264415264129639
epoch: 84, step: 73
	action: tensor([[ 0.5121,  0.0071, -0.4437,  0.5044, -0.2284, -0.1436,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-2.1740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8150518390009518, distance: 0.49213210965509596 entropy 0.03264415264129639
epoch: 84, step: 74
	action: tensor([[ 0.6367,  0.2845, -0.0547,  0.1972, -0.2126,  0.2691,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-1.2138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9527505438284452, distance: 0.24874542418865014 entropy 0.03264415264129639
epoch: 84, step: 75
	action: tensor([[ 0.9810, -0.3368, -0.1859,  0.6253, -0.5962, -0.2949,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[-1.5342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7486257862713077, distance: 0.5737425454511563 entropy 0.03264415264129639
epoch: 84, step: 76
	action: tensor([[ 0.5764,  0.2163, -0.3138,  0.2335,  0.0643,  0.2987, -0.4055]],
       dtype=torch.float64)
	q_value: tensor([[-2.0626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9208520827306337, distance: 0.3219411112949651 entropy 0.03264415264129639
epoch: 84, step: 77
	action: tensor([[ 0.0008,  0.0172, -0.2888, -0.1133, -0.5901, -0.3127,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[-1.8438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3356805112038659, distance: 0.932706900669995 entropy 0.03264415264129639
epoch: 84, step: 78
	action: tensor([[ 0.5036, -0.1364,  0.2097,  0.4985, -0.0648, -0.1193,  0.2066]],
       dtype=torch.float64)
	q_value: tensor([[-1.0708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7970233948754555, distance: 0.5155605585205367 entropy 0.03264415264129639
epoch: 84, step: 79
	action: tensor([[ 0.7386,  0.2374, -0.5035,  0.1180, -0.0681, -0.1502,  0.1996]],
       dtype=torch.float64)
	q_value: tensor([[-0.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8926109142549461, distance: 0.3750047474369952 entropy 0.03264415264129639
epoch: 84, step: 80
	action: tensor([[-0.0297,  0.5011, -0.2337,  0.4567, -0.2232, -0.1031,  0.1677]],
       dtype=torch.float64)
	q_value: tensor([[-1.6484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6018428952690087, distance: 0.7220776921506956 entropy 0.03264415264129639
epoch: 84, step: 81
	action: tensor([[ 0.4587, -0.2323, -0.0025,  0.4900, -0.4634, -0.1769, -0.2942]],
       dtype=torch.float64)
	q_value: tensor([[-1.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7164083876410514, distance: 0.6094013109833534 entropy 0.03264415264129639
epoch: 84, step: 82
	action: tensor([[ 0.3006,  0.0579, -0.1336,  0.2573, -0.2220,  0.0026,  0.5372]],
       dtype=torch.float64)
	q_value: tensor([[-1.2283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7023083072188108, distance: 0.6243671582678827 entropy 0.03264415264129639
epoch: 84, step: 83
	action: tensor([[ 0.8851, -0.0780, -0.2852,  0.1392, -0.4148,  0.5063,  0.1127]],
       dtype=torch.float64)
	q_value: tensor([[-0.6661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8451228087063177, distance: 0.4503502677777002 entropy 0.03264415264129639
epoch: 84, step: 84
	action: tensor([[ 0.5527, -0.1574, -0.2204,  0.4141,  0.2172,  0.0272, -0.1144]],
       dtype=torch.float64)
	q_value: tensor([[-2.2433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7675071305566854, distance: 0.5517743554009376 entropy 0.03264415264129639
epoch: 84, step: 85
	action: tensor([[ 0.1145,  0.2950, -0.0402, -0.0528, -0.5293,  0.3114, -0.5912]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6017017654808795, distance: 0.7222056537539149 entropy 0.03264415264129639
epoch: 84, step: 86
	action: tensor([[ 1.3208e-01,  7.5628e-02,  2.8650e-01, -9.5305e-06, -2.1793e-01,
         -7.1233e-03,  6.2927e-01]], dtype=torch.float64)
	q_value: tensor([[-2.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47834903889912506, distance: 0.8265073103562672 entropy 0.03264415264129639
epoch: 84, step: 87
	action: tensor([[ 0.6416, -0.1251,  0.2447,  0.2138,  0.4430,  0.0234,  0.0686]],
       dtype=torch.float64)
	q_value: tensor([[-0.5383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7504328319934414, distance: 0.5716766036240738 entropy 0.03264415264129639
epoch: 84, step: 88
	action: tensor([[ 0.2702,  0.0056, -0.3104,  0.4494, -0.6652,  0.3154, -0.1438]],
       dtype=torch.float64)
	q_value: tensor([[-0.6391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6539427381604392, distance: 0.6731791817454313 entropy 0.03264415264129639
epoch: 84, step: 89
	action: tensor([[ 0.3356,  0.0169, -0.5370, -0.2185,  0.1628,  0.0810, -0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-1.6989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5753610789057567, distance: 0.7457042051283891 entropy 0.03264415264129639
epoch: 84, step: 90
	action: tensor([[ 0.3060,  0.0412,  0.0538,  0.3969, -0.3534, -0.0780, -0.0768]],
       dtype=torch.float64)
	q_value: tensor([[-0.9798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7618070835525712, distance: 0.5584973467525186 entropy 0.03264415264129639
epoch: 84, step: 91
	action: tensor([[ 0.3831,  0.3129, -0.5779,  0.0404, -0.1543,  0.1241,  0.1250]],
       dtype=torch.float64)
	q_value: tensor([[-0.9115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8118959792992694, distance: 0.49631309337248125 entropy 0.03264415264129639
epoch: 84, step: 92
	action: tensor([[ 0.4531,  0.6070, -0.2429,  0.8609,  0.3216,  0.1695, -0.1526]],
       dtype=torch.float64)
	q_value: tensor([[-1.5120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 84, step: 93
	action: tensor([[ 0.3337, -0.4768,  0.0764,  0.0560, -0.1691, -0.0481,  0.4404]],
       dtype=torch.float64)
	q_value: tensor([[-4.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15664054815765405, distance: 1.0509040157141252 entropy 0.03264415264129639
epoch: 84, step: 94
	action: tensor([[ 0.4466, -0.1406, -0.2411,  0.2917,  0.0087,  0.2523, -0.3689]],
       dtype=torch.float64)
	q_value: tensor([[-0.4242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6889731845865352, distance: 0.6381982516495286 entropy 0.03264415264129639
epoch: 84, step: 95
	action: tensor([[ 0.8878,  0.0532, -0.3088,  0.3869, -0.2086, -0.1402,  0.2085]],
       dtype=torch.float64)
	q_value: tensor([[-1.1297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8993098083912497, distance: 0.36312008961904363 entropy 0.03264415264129639
epoch: 84, step: 96
	action: tensor([[ 0.4624, -0.0759, -0.3802,  0.1711,  0.1420,  0.0008, -0.3025]],
       dtype=torch.float64)
	q_value: tensor([[-1.6695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6733128448811652, distance: 0.6540677300746554 entropy 0.03264415264129639
epoch: 84, step: 97
	action: tensor([[ 0.3887, -0.1150, -0.2336,  0.2908, -0.3815,  0.3501, -0.2785]],
       dtype=torch.float64)
	q_value: tensor([[-0.8995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6939557501409308, distance: 0.6330657314775187 entropy 0.03264415264129639
epoch: 84, step: 98
	action: tensor([[ 0.4522,  0.3098, -0.1864,  0.7781, -0.3226, -0.3128,  0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-1.4546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8985259772103575, distance: 0.36453071893777506 entropy 0.03264415264129639
epoch: 84, step: 99
	action: tensor([[ 0.7335,  0.1607, -0.0640,  0.6557, -0.1054,  0.0513, -0.2189]],
       dtype=torch.float64)
	q_value: tensor([[-1.4948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.985215657528214, distance: 0.1391418266202077 entropy 0.03264415264129639
epoch: 84, step: 100
	action: tensor([[ 0.7102, -0.0471, -0.1274,  0.3966, -0.0630,  0.1009, -0.1744]],
       dtype=torch.float64)
	q_value: tensor([[-1.9608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8899977836509341, distance: 0.37953987569779546 entropy 0.03264415264129639
epoch: 84, step: 101
	action: tensor([[ 0.2984,  0.2564, -0.6272,  0.2646, -0.2528, -0.1468,  0.3348]],
       dtype=torch.float64)
	q_value: tensor([[-1.4218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7442442981181566, distance: 0.5787211516374552 entropy 0.03264415264129639
epoch: 84, step: 102
	action: tensor([[ 0.3652,  0.2997, -0.0272,  0.0961, -0.2294,  0.6006,  0.4211]],
       dtype=torch.float64)
	q_value: tensor([[-1.3172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8512547831833199, distance: 0.44134498676498213 entropy 0.03264415264129639
epoch: 84, step: 103
	action: tensor([[ 0.4216,  0.1675,  0.0780, -0.0066,  0.5179,  0.1006, -0.1265]],
       dtype=torch.float64)
	q_value: tensor([[-1.3256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.758185362757611, distance: 0.5627273014568682 entropy 0.03264415264129639
epoch: 84, step: 104
	action: tensor([[ 0.2722, -0.0127, -0.2153,  0.4704, -0.4308, -0.1290, -0.1883]],
       dtype=torch.float64)
	q_value: tensor([[-0.6103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6727823336483681, distance: 0.6545985889042949 entropy 0.03264415264129639
epoch: 84, step: 105
	action: tensor([[ 0.5707, -0.1163, -0.4191,  0.0725, -0.3096,  0.3594, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-1.1099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6987095691077561, distance: 0.6281297488799 entropy 0.03264415264129639
epoch: 84, step: 106
	action: tensor([[ 0.1862,  0.0946, -0.2753,  0.4893, -0.6022,  0.0462,  0.0769]],
       dtype=torch.float64)
	q_value: tensor([[-1.5068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6624539021808027, distance: 0.6648493356590359 entropy 0.03264415264129639
epoch: 84, step: 107
	action: tensor([[ 0.5583,  0.2028, -0.1774,  0.3260, -0.0244, -0.1226, -0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-1.1751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8954576382896324, distance: 0.3700009564926811 entropy 0.03264415264129639
epoch: 84, step: 108
	action: tensor([[ 0.7623, -0.3142, -0.3508, -0.0502,  0.1253,  0.0332,  0.4630]],
       dtype=torch.float64)
	q_value: tensor([[-1.1161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44106546452195505, distance: 0.8555337871830742 entropy 0.03264415264129639
epoch: 84, step: 109
	action: tensor([[ 0.7263,  0.2408, -0.3402,  0.2331,  0.3108,  0.2718,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[-0.9884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9627916045465625, distance: 0.2207380310497196 entropy 0.03264415264129639
epoch: 84, step: 110
	action: tensor([[ 0.4605,  0.1863,  0.0690,  0.1872,  0.3143, -0.2512, -0.0202]],
       dtype=torch.float64)
	q_value: tensor([[-1.5097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7951582022509169, distance: 0.5179239360973492 entropy 0.03264415264129639
epoch: 84, step: 111
	action: tensor([[ 0.4482,  0.4984, -0.2014, -0.0623,  0.1146, -0.2034, -0.2777]],
       dtype=torch.float64)
	q_value: tensor([[-0.6869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8730844579701242, distance: 0.40767501674936335 entropy 0.03264415264129639
epoch: 84, step: 112
	action: tensor([[ 0.0851,  0.0103, -0.4387,  0.5755, -0.0841,  0.3621,  0.0193]],
       dtype=torch.float64)
	q_value: tensor([[-1.5057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5170193079493792, distance: 0.7952827783213 entropy 0.03264415264129639
epoch: 84, step: 113
	action: tensor([[ 0.7603, -0.1077, -0.1024, -0.0013, -0.2136,  0.3670,  0.7852]],
       dtype=torch.float64)
	q_value: tensor([[-0.9800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7510470891522639, distance: 0.5709726392567023 entropy 0.03264415264129639
epoch: 84, step: 114
	action: tensor([[ 0.2800, -0.1959, -0.5452, -0.0758,  0.2657,  0.1256, -0.0704]],
       dtype=torch.float64)
	q_value: tensor([[-1.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38607391062172536, distance: 0.8966330398133016 entropy 0.03264415264129639
epoch: 84, step: 115
	action: tensor([[ 0.3136,  0.1154, -0.4647,  0.0365, -0.1659,  0.8378,  0.1970]],
       dtype=torch.float64)
	q_value: tensor([[-0.7207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7083144392790761, distance: 0.6180365487694459 entropy 0.03264415264129639
epoch: 84, step: 116
	action: tensor([[ 0.4080,  0.2121, -0.4587,  0.3468,  0.4322,  0.0860,  0.3464]],
       dtype=torch.float64)
	q_value: tensor([[-1.8633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8016675764907064, distance: 0.5096283187548718 entropy 0.03264415264129639
epoch: 84, step: 117
	action: tensor([[ 0.1484,  0.3144, -0.2767,  0.3390, -0.3884,  0.2139,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-1.0689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6666789719510288, distance: 0.6606752680592911 entropy 0.03264415264129639
epoch: 84, step: 118
	action: tensor([[ 0.7488, -0.0330, -0.2742,  0.0577, -0.4498,  0.2250, -0.1622]],
       dtype=torch.float64)
	q_value: tensor([[-1.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7921105588217849, distance: 0.5217625562462023 entropy 0.03264415264129639
epoch: 84, step: 119
	action: tensor([[ 0.3220, -0.1646, -0.5766,  0.6849, -0.2958,  0.4580, -0.3104]],
       dtype=torch.float64)
	q_value: tensor([[-2.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5805330337005907, distance: 0.7411490833809092 entropy 0.03264415264129639
epoch: 84, step: 120
	action: tensor([[ 0.5084,  0.2286, -0.0810,  0.2543, -0.3974, -0.1499,  0.3916]],
       dtype=torch.float64)
	q_value: tensor([[-1.8070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8755230620883196, distance: 0.40373940756277676 entropy 0.03264415264129639
epoch: 84, step: 121
	action: tensor([[ 0.2250,  0.1366, -0.1991,  0.2040,  0.0126, -0.2768,  0.1749]],
       dtype=torch.float64)
	q_value: tensor([[-1.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6342914408737268, distance: 0.6920289384674685 entropy 0.03264415264129639
epoch: 84, step: 122
	action: tensor([[ 0.9145,  0.2830, -0.5941,  0.4107,  0.4580,  0.2196,  0.1105]],
       dtype=torch.float64)
	q_value: tensor([[-0.6659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9876259882943113, distance: 0.12729517853961841 entropy 0.03264415264129639
epoch: 84, step: 123
	action: tensor([[ 0.8857,  0.2667,  0.1379,  0.5668, -0.0764,  0.5189,  0.0449]],
       dtype=torch.float64)
	q_value: tensor([[-2.1745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9808213610627892, distance: 0.1584767604300886 entropy 0.03264415264129639
epoch: 84, step: 124
	action: tensor([[ 0.5438, -0.1780, -0.6234,  0.6737, -0.0145,  0.1987,  0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-2.2077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7416974040368733, distance: 0.5815955550861184 entropy 0.03264415264129639
epoch: 84, step: 125
	action: tensor([[ 0.4794,  0.0656,  0.0243,  0.2616, -0.0962,  0.0590,  0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-1.2373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8197526717121278, distance: 0.4858375872363934 entropy 0.03264415264129639
epoch: 84, step: 126
	action: tensor([[ 1.0379, -0.1790,  0.0340,  0.3315,  0.2352,  0.7428,  0.1857]],
       dtype=torch.float64)
	q_value: tensor([[-0.7459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8893283241190084, distance: 0.3806930396210586 entropy 0.03264415264129639
epoch: 84, step: 127
	action: tensor([[ 0.9930,  0.0661, -0.0762,  0.5117, -0.1764,  0.2289, -0.1228]],
       dtype=torch.float64)
	q_value: tensor([[-1.8248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9721694608961832, distance: 0.1909050877652025 entropy 0.03264415264129639
LOSS epoch 84 actor 20.932407805803027 critic 267.5052423596999 
epoch: 85, step: 0
	action: tensor([[ 0.5963, -0.1355, -0.0820,  0.5596, -0.2577,  0.3314, -0.3374]],
       dtype=torch.float64)
	q_value: tensor([[-2.4372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9011945810263826, distance: 0.3597054974768335 entropy 0.03264415264129639
epoch: 85, step: 1
	action: tensor([[ 0.5970,  0.0672,  0.1871,  0.5333, -0.4713,  0.2322, -0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-1.8265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9639068258317601, distance: 0.21740485175836446 entropy 0.03264415264129639
epoch: 85, step: 2
	action: tensor([[ 1.1306,  0.4727,  0.0469,  0.2213, -0.1979,  0.2382, -0.4388]],
       dtype=torch.float64)
	q_value: tensor([[-1.6282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9452714543148774, distance: 0.26770941627668343 entropy 0.03264415264129639
epoch: 85, step: 3
	action: tensor([[ 0.7171, -0.1330, -0.3161,  0.3089,  0.1350, -0.1833,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[-3.7079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7377512370556352, distance: 0.5860213214250932 entropy 0.03264415264129639
epoch: 85, step: 4
	action: tensor([[ 0.3046, -0.0703, -0.4676,  0.2828,  0.0496,  0.6942, -0.5387]],
       dtype=torch.float64)
	q_value: tensor([[-1.0460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6826732225127671, distance: 0.6446293186822087 entropy 0.03264415264129639
epoch: 85, step: 5
	action: tensor([[ 0.4777,  0.4357, -0.4249,  0.3655, -0.2094, -0.0817,  0.2051]],
       dtype=torch.float64)
	q_value: tensor([[-1.9421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8891994741014385, distance: 0.38091458698362796 entropy 0.03264415264129639
epoch: 85, step: 6
	action: tensor([[ 0.4951,  0.2665, -0.3251, -0.0901, -0.0466, -0.0152, -0.0947]],
       dtype=torch.float64)
	q_value: tensor([[-1.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8233053365570335, distance: 0.4810258434892303 entropy 0.03264415264129639
epoch: 85, step: 7
	action: tensor([[ 0.9101,  0.1039,  0.2043,  0.3444, -0.8407,  0.0541, -0.4909]],
       dtype=torch.float64)
	q_value: tensor([[-1.2925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.931616809495521, distance: 0.299248080274147 entropy 0.03264415264129639
epoch: 85, step: 8
	action: tensor([[ 0.3860,  0.3358, -0.1106, -0.0861, -0.7039,  0.2335,  0.1320]],
       dtype=torch.float64)
	q_value: tensor([[-3.2627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7970697256234373, distance: 0.5155017151146702 entropy 0.03264415264129639
epoch: 85, step: 9
	action: tensor([[ 0.3617,  0.2862, -0.0998,  0.5184, -0.2017,  0.1234,  0.1645]],
       dtype=torch.float64)
	q_value: tensor([[-1.8566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8591365961397261, distance: 0.4294926959942023 entropy 0.03264415264129639
epoch: 85, step: 10
	action: tensor([[ 0.3262,  0.5372, -0.2741,  0.4654, -0.1210, -0.0891, -0.0257]],
       dtype=torch.float64)
	q_value: tensor([[-1.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8192614263100938, distance: 0.4864991867448124 entropy 0.03264415264129639
epoch: 85, step: 11
	action: tensor([[ 0.5630, -0.2791, -0.0944,  0.1693, -0.0905,  0.1199, -0.2487]],
       dtype=torch.float64)
	q_value: tensor([[-1.4945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6015680318111589, distance: 0.7223268884194264 entropy 0.03264415264129639
epoch: 85, step: 12
	action: tensor([[ 0.6676, -0.1974, -0.3699,  0.3495,  0.1632,  0.1402, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[-0.9697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7628989237308973, distance: 0.5572158429740027 entropy 0.03264415264129639
epoch: 85, step: 13
	action: tensor([[ 0.6411,  0.0609, -0.4402,  0.6001,  0.1526, -0.1449, -0.1447]],
       dtype=torch.float64)
	q_value: tensor([[-1.0458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8946258184814351, distance: 0.3714700467306529 entropy 0.03264415264129639
epoch: 85, step: 14
	action: tensor([[ 0.3390,  0.2290, -0.3610,  0.4593, -0.4410, -0.0082,  0.2221]],
       dtype=torch.float64)
	q_value: tensor([[-1.4810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7941135467936726, distance: 0.5192429149608397 entropy 0.03264415264129639
epoch: 85, step: 15
	action: tensor([[ 0.4652,  0.2864, -0.4993,  0.1741, -0.1795,  0.3340,  0.5846]],
       dtype=torch.float64)
	q_value: tensor([[-1.2598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.848066020624145, distance: 0.44605062236926657 entropy 0.03264415264129639
epoch: 85, step: 16
	action: tensor([[ 0.0863,  0.0889, -0.0239,  0.2437, -0.2540,  0.2421, -0.3178]],
       dtype=torch.float64)
	q_value: tensor([[-1.4724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5948309256079055, distance: 0.7284082196536965 entropy 0.03264415264129639
epoch: 85, step: 17
	action: tensor([[ 1.0075,  0.1449, -0.0436,  0.5778, -0.3575,  0.0673, -0.1454]],
       dtype=torch.float64)
	q_value: tensor([[-1.0112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9795360905490867, distance: 0.1637008732020887 entropy 0.03264415264129639
epoch: 85, step: 18
	action: tensor([[ 0.4084, -0.2739, -0.0991,  0.5309,  0.0179, -0.0203, -0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-2.7077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6837992865559448, distance: 0.6434845381761185 entropy 0.03264415264129639
epoch: 85, step: 19
	action: tensor([[ 0.5390,  0.0519, -0.3630,  0.5628,  0.4646, -0.2680, -0.1100]],
       dtype=torch.float64)
	q_value: tensor([[-0.8806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8285044219589726, distance: 0.4738961238286099 entropy 0.03264415264129639
epoch: 85, step: 20
	action: tensor([[ 0.7473,  0.2769, -0.0686,  0.3782, -0.2032,  0.2473,  0.2845]],
       dtype=torch.float64)
	q_value: tensor([[-1.0839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9838402951331928, distance: 0.14546998888478463 entropy 0.03264415264129639
epoch: 85, step: 21
	action: tensor([[ 0.4864, -0.1036, -0.0533,  0.1448, -0.3644,  0.1570,  0.1019]],
       dtype=torch.float64)
	q_value: tensor([[-1.5624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6966501440096495, distance: 0.6302728358019691 entropy 0.03264415264129639
epoch: 85, step: 22
	action: tensor([[ 0.6302,  0.0298, -0.1791,  0.1957,  0.0850,  0.0347, -0.2815]],
       dtype=torch.float64)
	q_value: tensor([[-0.9475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8299597565260485, distance: 0.47188106655011347 entropy 0.03264415264129639
epoch: 85, step: 23
	action: tensor([[ 0.5152, -0.1389, -0.0798,  0.3285,  0.2359, -0.1500,  0.1292]],
       dtype=torch.float64)
	q_value: tensor([[-1.2340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6927465027746524, distance: 0.6343151886429393 entropy 0.03264415264129639
epoch: 85, step: 24
	action: tensor([[ 0.6272,  0.5125, -0.2736,  0.2492, -0.6840, -0.0056, -0.1202]],
       dtype=torch.float64)
	q_value: tensor([[-0.5942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9530298276050471, distance: 0.24800918795538915 entropy 0.03264415264129639
epoch: 85, step: 25
	action: tensor([[ 0.5680,  0.1315, -0.0096,  0.3742, -0.2286,  0.3410, -0.2382]],
       dtype=torch.float64)
	q_value: tensor([[-2.5939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9363922080051389, distance: 0.2886103180419134 entropy 0.03264415264129639
epoch: 85, step: 26
	action: tensor([[ 0.4077,  0.3144, -0.0484,  0.7919,  0.2786, -0.2022,  0.2506]],
       dtype=torch.float64)
	q_value: tensor([[-1.6913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8896347743760621, distance: 0.3801656040720634 entropy 0.03264415264129639
epoch: 85, step: 27
	action: tensor([[ 0.4495,  0.3286, -0.2448,  0.3614, -0.2329, -0.3922, -0.4147]],
       dtype=torch.float64)
	q_value: tensor([[-0.8922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8893080367375945, distance: 0.38072793071187605 entropy 0.03264415264129639
epoch: 85, step: 28
	action: tensor([[ 0.8115, -0.0136, -0.1023,  0.6888, -0.2865,  0.5277, -0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-1.9080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9888889384102287, distance: 0.12062420685838803 entropy 0.03264415264129639
epoch: 85, step: 29
	action: tensor([[ 0.4075,  0.7264, -0.0543,  0.5183,  0.0210,  0.2673,  0.4552]],
       dtype=torch.float64)
	q_value: tensor([[-2.3449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 85, step: 30
	action: tensor([[ 0.5001, -0.3014, -0.2834,  0.3172, -0.3729,  0.0826, -0.1391]],
       dtype=torch.float64)
	q_value: tensor([[-4.7954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.586212338681003, distance: 0.7361146506611514 entropy 0.03264415264129639
epoch: 85, step: 31
	action: tensor([[ 0.6594, -0.0545,  0.0802,  0.3777, -0.1160,  0.0967,  0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-1.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8704581889654873, distance: 0.41187143743399696 entropy 0.03264415264129639
epoch: 85, step: 32
	action: tensor([[ 0.5946,  0.2546, -0.1273,  0.5678, -0.2896,  0.2288,  0.0956]],
       dtype=torch.float64)
	q_value: tensor([[-0.9083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9408307547824499, distance: 0.27835864125930815 entropy 0.03264415264129639
epoch: 85, step: 33
	action: tensor([[ 0.7916, -0.2054, -0.4339,  0.5003, -0.2519,  0.0229,  0.2446]],
       dtype=torch.float64)
	q_value: tensor([[-1.6380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8143967503867601, distance: 0.4930029081723607 entropy 0.03264415264129639
epoch: 85, step: 34
	action: tensor([[ 0.5891,  0.2611, -0.0050,  0.1522, -0.7770,  0.2126,  0.2967]],
       dtype=torch.float64)
	q_value: tensor([[-1.4327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9173998640277509, distance: 0.3288872779513388 entropy 0.03264415264129639
epoch: 85, step: 35
	action: tensor([[ 0.9641,  0.0900, -0.1768,  0.3925,  0.3092,  0.2063,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[-1.8467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9580470230062571, distance: 0.2343894636397497 entropy 0.03264415264129639
epoch: 85, step: 36
	action: tensor([[ 0.7945,  0.0638,  0.1312,  0.0754, -0.4115, -0.2299, -0.0675]],
       dtype=torch.float64)
	q_value: tensor([[-1.7768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7802075576241451, distance: 0.5364917782436296 entropy 0.03264415264129639
epoch: 85, step: 37
	action: tensor([[ 1.0979,  0.4936, -0.5520,  0.6992,  0.2172, -0.2746, -0.1623]],
       dtype=torch.float64)
	q_value: tensor([[-1.6064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.981463084432368, distance: 0.15580286129953033 entropy 0.03264415264129639
epoch: 85, step: 38
	action: tensor([[ 0.8145,  0.1529, -0.1387,  0.0366, -0.8573,  0.4286,  0.1080]],
       dtype=torch.float64)
	q_value: tensor([[-3.4047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.904586238223671, distance: 0.353477848230799 entropy 0.03264415264129639
epoch: 85, step: 39
	action: tensor([[ 0.4644,  0.0140,  0.3048,  0.2077, -0.1447,  0.1659, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-2.7729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8089867006870303, distance: 0.5001364375388766 entropy 0.03264415264129639
epoch: 85, step: 40
	action: tensor([[ 0.5911,  0.1403, -0.6494,  0.0481, -0.3490,  0.3555, -0.2694]],
       dtype=torch.float64)
	q_value: tensor([[-0.8487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8291085330302663, distance: 0.47306071319778537 entropy 0.03264415264129639
epoch: 85, step: 41
	action: tensor([[ 0.4602,  0.0230, -0.6637,  0.5154,  0.2245,  0.1696, -0.0429]],
       dtype=torch.float64)
	q_value: tensor([[-2.5474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7561506868404657, distance: 0.5650897913482958 entropy 0.03264415264129639
epoch: 85, step: 42
	action: tensor([[ 0.5282,  0.1744,  0.2140,  0.2867, -0.3928,  0.0468,  0.3728]],
       dtype=torch.float64)
	q_value: tensor([[-1.3002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8988772926921872, distance: 0.36389914684494945 entropy 0.03264415264129639
epoch: 85, step: 43
	action: tensor([[ 0.7237,  0.4161, -0.2337,  0.1164,  0.0291,  0.4401, -0.0691]],
       dtype=torch.float64)
	q_value: tensor([[-0.9427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9769943591050829, distance: 0.17356967947839302 entropy 0.03264415264129639
epoch: 85, step: 44
	action: tensor([[ 0.6249,  0.3613, -0.2803,  0.3784, -0.5775, -0.1750,  0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-2.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9587519481640294, distance: 0.2324119282953944 entropy 0.03264415264129639
epoch: 85, step: 45
	action: tensor([[ 1.0188, -0.0665,  0.0473,  0.3417,  0.0470, -0.2597, -0.1511]],
       dtype=torch.float64)
	q_value: tensor([[-1.9800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.773905363743062, distance: 0.544128937682897 entropy 0.03264415264129639
epoch: 85, step: 46
	action: tensor([[ 0.5912,  0.0397, -0.0480,  0.2982,  0.4410,  0.2303, -0.1833]],
       dtype=torch.float64)
	q_value: tensor([[-1.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8961162759142083, distance: 0.3688335751788404 entropy 0.03264415264129639
epoch: 85, step: 47
	action: tensor([[ 0.9198, -0.0484, -0.2780,  0.0318, -0.2008,  0.0439,  0.3218]],
       dtype=torch.float64)
	q_value: tensor([[-0.9459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7303195333659537, distance: 0.5942667598060695 entropy 0.03264415264129639
epoch: 85, step: 48
	action: tensor([[ 0.3889, -0.0696, -0.4881,  0.2247,  0.3783, -0.2436,  0.1389]],
       dtype=torch.float64)
	q_value: tensor([[-1.5893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5832806741350287, distance: 0.7387177155933675 entropy 0.03264415264129639
epoch: 85, step: 49
	action: tensor([[ 0.5139,  0.0116,  0.0126,  0.5192,  0.0732, -0.0193, -0.2106]],
       dtype=torch.float64)
	q_value: tensor([[-0.7715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8821527793898901, distance: 0.3928405997517708 entropy 0.03264415264129639
epoch: 85, step: 50
	action: tensor([[ 0.4854,  0.2616, -0.2871,  0.9028,  0.1707, -0.4728, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[-1.0001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8804192807153324, distance: 0.3957193380867435 entropy 0.03264415264129639
epoch: 85, step: 51
	action: tensor([[ 0.6447,  0.3312, -0.5899,  0.3810, -0.0881, -0.1643, -0.0302]],
       dtype=torch.float64)
	q_value: tensor([[-1.4173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9369888333123777, distance: 0.28725358249750255 entropy 0.03264415264129639
epoch: 85, step: 52
	action: tensor([[ 0.4359,  0.1947, -0.3120, -0.0527, -0.3789,  0.3046,  0.4733]],
       dtype=torch.float64)
	q_value: tensor([[-1.9816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7808178240761825, distance: 0.5357464602748397 entropy 0.03264415264129639
epoch: 85, step: 53
	action: tensor([[ 0.6984,  0.1600, -0.4465,  0.5213, -0.1201,  0.0624,  0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-1.3407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9373895585113067, distance: 0.2863387181010416 entropy 0.03264415264129639
epoch: 85, step: 54
	action: tensor([[ 0.6894, -0.0139, -0.0071,  0.1564, -0.2610,  0.3370,  0.1788]],
       dtype=torch.float64)
	q_value: tensor([[-1.6637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8578436603786889, distance: 0.4314592750500093 entropy 0.03264415264129639
epoch: 85, step: 55
	action: tensor([[ 0.0876,  0.0179,  0.0669,  0.2052, -0.3503,  0.3643,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[-1.2953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5686309181678751, distance: 0.7515903585195397 entropy 0.03264415264129639
epoch: 85, step: 56
	action: tensor([[ 0.4712,  0.0025, -0.0863,  0.4587,  0.0404,  0.2143,  0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-0.8054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8575157118737667, distance: 0.43195666725929405 entropy 0.03264415264129639
epoch: 85, step: 57
	action: tensor([[ 0.8017,  0.1156, -0.5318,  0.2243,  0.3719, -0.0545, -0.0583]],
       dtype=torch.float64)
	q_value: tensor([[-0.8623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8764770405335203, distance: 0.4021893232887937 entropy 0.03264415264129639
epoch: 85, step: 58
	action: tensor([[ 0.3987, -0.4659, -0.2918,  0.0551, -0.3069,  0.1967,  0.0995]],
       dtype=torch.float64)
	q_value: tensor([[-1.7084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27894901299426966, distance: 0.9717167324199684 entropy 0.03264415264129639
epoch: 85, step: 59
	action: tensor([[ 0.1535,  0.0233, -0.3775,  0.2167,  0.0035,  0.1544,  0.2082]],
       dtype=torch.float64)
	q_value: tensor([[-0.7887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5486686079202847, distance: 0.7687842264420219 entropy 0.03264415264129639
epoch: 85, step: 60
	action: tensor([[ 0.3786,  0.3532, -0.3403, -0.0445,  0.0138,  0.4783,  0.3172]],
       dtype=torch.float64)
	q_value: tensor([[-0.6920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8380094081901526, distance: 0.46057630286492507 entropy 0.03264415264129639
epoch: 85, step: 61
	action: tensor([[ 0.6738, -0.0640, -0.1895,  0.5741, -0.2437,  0.1290, -0.1922]],
       dtype=torch.float64)
	q_value: tensor([[-1.3494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9114768701393099, distance: 0.3404748785073109 entropy 0.03264415264129639
epoch: 85, step: 62
	action: tensor([[ 0.0973, -0.3510,  0.3144,  0.3127, -0.2203,  0.0158, -0.0382]],
       dtype=torch.float64)
	q_value: tensor([[-1.7167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34527671207039523, distance: 0.9259458458274199 entropy 0.03264415264129639
epoch: 85, step: 63
	action: tensor([[ 0.9423, -0.1743, -0.2733,  0.5276,  0.0214, -0.0266,  0.3616]],
       dtype=torch.float64)
	q_value: tensor([[-0.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8548724568416559, distance: 0.4359449135944504 entropy 0.03264415264129639
epoch: 85, step: 64
	action: tensor([[ 0.6413,  0.0461,  0.0678,  0.3793, -0.2196, -0.2455,  0.1000]],
       dtype=torch.float64)
	q_value: tensor([[-1.3608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8608960374641349, distance: 0.42680199820667325 entropy 0.03264415264129639
epoch: 85, step: 65
	action: tensor([[ 0.7018, -0.3346,  0.0098, -0.2675, -0.2017,  0.1732,  0.3149]],
       dtype=torch.float64)
	q_value: tensor([[-1.0234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061887563519995, distance: 0.9531853527011412 entropy 0.03264415264129639
epoch: 85, step: 66
	action: tensor([[ 0.4474,  0.1242,  0.6839,  0.2798,  0.0721,  0.1258, -0.4166]],
       dtype=torch.float64)
	q_value: tensor([[-0.9973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.875472748227146, distance: 0.40382099551211154 entropy 0.03264415264129639
epoch: 85, step: 67
	action: tensor([[ 0.5987,  0.4851, -0.1580,  0.0640,  0.0059, -0.1370, -0.1496]],
       dtype=torch.float64)
	q_value: tensor([[-1.2068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9393907711695871, distance: 0.28172544424594825 entropy 0.03264415264129639
epoch: 85, step: 68
	action: tensor([[ 0.5407,  0.2326,  0.1259,  0.2511, -0.1447,  0.1416,  0.1998]],
       dtype=torch.float64)
	q_value: tensor([[-1.6257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9208273689279075, distance: 0.3219913701545358 entropy 0.03264415264129639
epoch: 85, step: 69
	action: tensor([[ 0.9868, -0.1725, -0.2365,  0.4029, -0.2204,  0.1682,  0.1166]],
       dtype=torch.float64)
	q_value: tensor([[-0.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8359091065255252, distance: 0.4635525058914207 entropy 0.03264415264129639
epoch: 85, step: 70
	action: tensor([[ 0.4195,  0.2965, -0.0058,  0.5825,  0.1502, -0.0734,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[-1.8964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9046791980529464, distance: 0.3533056128674295 entropy 0.03264415264129639
epoch: 85, step: 71
	action: tensor([[ 0.8380,  0.1248, -0.4665,  0.2029,  0.0211,  0.2446, -0.3283]],
       dtype=torch.float64)
	q_value: tensor([[-0.9199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9296202963504641, distance: 0.303585070403094 entropy 0.03264415264129639
epoch: 85, step: 72
	action: tensor([[ 0.7297,  0.2363, -0.7148,  0.4107, -0.0389, -0.1481,  0.3394]],
       dtype=torch.float64)
	q_value: tensor([[-2.4636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9322809429452628, distance: 0.2977913951204248 entropy 0.03264415264129639
epoch: 85, step: 73
	action: tensor([[ 0.9615,  0.1028, -0.3280,  0.5522,  0.0821, -0.1282, -0.3398]],
       dtype=torch.float64)
	q_value: tensor([[-1.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9634050438743063, distance: 0.21891086078482674 entropy 0.03264415264129639
epoch: 85, step: 74
	action: tensor([[ 0.9208,  0.2613, -0.3222,  0.5315, -0.1950, -0.2082, -0.5422]],
       dtype=torch.float64)
	q_value: tensor([[-2.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9889859781148648, distance: 0.12009630918641542 entropy 0.03264415264129639
epoch: 85, step: 75
	action: tensor([[ 0.5761,  0.2060, -0.3103,  0.5903,  0.1569,  0.4446, -0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-3.2230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9267972328794102, distance: 0.3096138945459482 entropy 0.03264415264129639
epoch: 85, step: 76
	action: tensor([[ 0.5718, -0.0125,  0.2294,  0.5643,  0.2605, -0.2485, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[-1.7768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8836367085123685, distance: 0.3903594450656764 entropy 0.03264415264129639
epoch: 85, step: 77
	action: tensor([[ 0.7406,  0.5401, -0.2319,  0.1997, -0.2002,  0.1946,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-0.7701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9713278357584503, distance: 0.1937701748446594 entropy 0.03264415264129639
epoch: 85, step: 78
	action: tensor([[ 0.6063,  0.2029, -0.0595,  0.0067,  0.1178,  0.4017, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[-2.2651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.905753545978456, distance: 0.35130894087133396 entropy 0.03264415264129639
epoch: 85, step: 79
	action: tensor([[ 0.5653,  0.2511,  0.1189,  0.5559, -0.0678,  0.4045,  0.2118]],
       dtype=torch.float64)
	q_value: tensor([[-1.2445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.969104124368914, distance: 0.2011439374807129 entropy 0.03264415264129639
epoch: 85, step: 80
	action: tensor([[ 0.5753,  0.1370, -0.0875,  0.3587,  0.0476,  0.2809,  0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-1.2948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9285343826039985, distance: 0.3059181668953704 entropy 0.03264415264129639
epoch: 85, step: 81
	action: tensor([[ 0.5329, -0.4036, -0.3489,  0.1508,  0.0613,  0.2065, -0.0949]],
       dtype=torch.float64)
	q_value: tensor([[-1.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4737484752307711, distance: 0.83014389208083 entropy 0.03264415264129639
epoch: 85, step: 82
	action: tensor([[ 0.6171,  0.2450, -0.3359,  0.6884,  0.5368,  0.5129,  0.1863]],
       dtype=torch.float64)
	q_value: tensor([[-0.8572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9394337094395555, distance: 0.28162563315568306 entropy 0.03264415264129639
epoch: 85, step: 83
	action: tensor([[ 1.0582,  0.4593, -0.0125,  0.5928, -0.6064,  0.0055, -0.4341]],
       dtype=torch.float64)
	q_value: tensor([[-1.5519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9367391996483732, distance: 0.2878220316009022 entropy 0.03264415264129639
epoch: 85, step: 84
	action: tensor([[ 0.5152, -0.3277, -0.3408,  0.2210,  0.3693, -0.2182,  0.3304]],
       dtype=torch.float64)
	q_value: tensor([[-3.9732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43404493951796785, distance: 0.8608900077211306 entropy 0.03264415264129639
epoch: 85, step: 85
	action: tensor([[ 0.3235, -0.1470, -0.4098,  0.0398, -0.3567, -0.1786, -0.1439]],
       dtype=torch.float64)
	q_value: tensor([[-0.6429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48141233335285905, distance: 0.8240769849750588 entropy 0.03264415264129639
epoch: 85, step: 86
	action: tensor([[ 0.2841,  0.1196,  0.0108,  0.4334, -0.2831,  0.2729, -0.2967]],
       dtype=torch.float64)
	q_value: tensor([[-0.9563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7898310148305822, distance: 0.5246153659068733 entropy 0.03264415264129639
epoch: 85, step: 87
	action: tensor([[ 0.5685,  0.4119, -0.3999,  0.6025, -0.2764,  0.3608, -0.3518]],
       dtype=torch.float64)
	q_value: tensor([[-1.3307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8440638994080254, distance: 0.4518871878971508 entropy 0.03264415264129639
epoch: 85, step: 88
	action: tensor([[ 0.4837,  0.1927, -0.2454,  0.4803, -0.0801,  0.1277, -0.2686]],
       dtype=torch.float64)
	q_value: tensor([[-2.8276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8915865723379954, distance: 0.3767890134954073 entropy 0.03264415264129639
epoch: 85, step: 89
	action: tensor([[ 0.7968,  0.4201, -0.2483,  0.3503, -0.4100, -0.2052,  0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-1.5256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9889418936522317, distance: 0.12033641648303355 entropy 0.03264415264129639
epoch: 85, step: 90
	action: tensor([[ 0.6487,  0.5695, -0.5200,  0.3793, -0.4857,  0.1702, -0.0523]],
       dtype=torch.float64)
	q_value: tensor([[-2.2180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.902598705890072, distance: 0.35714046260214244 entropy 0.03264415264129639
epoch: 85, step: 91
	action: tensor([[-0.0095,  0.3206, -0.2752,  0.0198, -0.1917,  0.1288, -0.1910]],
       dtype=torch.float64)
	q_value: tensor([[-2.8828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5243429194833451, distance: 0.7892301654753787 entropy 0.03264415264129639
epoch: 85, step: 92
	action: tensor([[ 0.5126,  0.0118, -0.1935,  0.4432, -0.4120,  0.2388,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[-1.1086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.850506389774083, distance: 0.4424538804243147 entropy 0.03264415264129639
epoch: 85, step: 93
	action: tensor([[ 0.2922, -0.1672, -0.4085,  0.1987,  0.1263, -0.1078, -0.3897]],
       dtype=torch.float64)
	q_value: tensor([[-1.3592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4864953705514672, distance: 0.8200283648275586 entropy 0.03264415264129639
epoch: 85, step: 94
	action: tensor([[ 0.5564, -0.1271, -0.2766,  0.5769, -0.3862, -0.2268,  0.1437]],
       dtype=torch.float64)
	q_value: tensor([[-0.7319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8000606339403276, distance: 0.5116887264209901 entropy 0.03264415264129639
epoch: 85, step: 95
	action: tensor([[ 0.2385,  0.3640, -0.1345,  0.1755, -0.3207,  0.1046, -0.2760]],
       dtype=torch.float64)
	q_value: tensor([[-1.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7699364973398057, distance: 0.5488839820267449 entropy 0.03264415264129639
epoch: 85, step: 96
	action: tensor([[ 0.3566, -0.0168, -0.3957,  0.6027, -0.5455,  0.0442, -0.0373]],
       dtype=torch.float64)
	q_value: tensor([[-1.4397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7331166231944048, distance: 0.5911768992345452 entropy 0.03264415264129639
epoch: 85, step: 97
	action: tensor([[ 0.6439,  0.2616,  0.0025,  0.5003, -0.3373,  0.1801,  0.3506]],
       dtype=torch.float64)
	q_value: tensor([[-1.4105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.969141573348524, distance: 0.201021996946158 entropy 0.03264415264129639
epoch: 85, step: 98
	action: tensor([[ 0.2745,  0.3745, -0.1210,  0.4424, -0.3533,  0.0446, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-1.3641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8144715220566038, distance: 0.4929035931977955 entropy 0.03264415264129639
epoch: 85, step: 99
	action: tensor([[ 0.5117,  0.2763, -0.5211,  0.6023, -0.0144, -0.3134, -0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-1.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8755442129398127, distance: 0.4037051048430288 entropy 0.03264415264129639
epoch: 85, step: 100
	action: tensor([[ 0.7285, -0.2150, -0.2679,  0.2836,  0.0977,  0.1912,  0.1299]],
       dtype=torch.float64)
	q_value: tensor([[-1.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7545410267521512, distance: 0.5669518149155229 entropy 0.03264415264129639
epoch: 85, step: 101
	action: tensor([[ 0.7928, -0.3594, -0.3847,  0.7589, -0.1601, -0.1381, -0.0125]],
       dtype=torch.float64)
	q_value: tensor([[-1.0486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8010195368014154, distance: 0.5104602302383848 entropy 0.03264415264129639
epoch: 85, step: 102
	action: tensor([[ 0.9475,  0.2529, -0.5107,  0.0388, -0.0345, -0.2834,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[-1.4915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.864486247878742, distance: 0.4212582103532343 entropy 0.03264415264129639
epoch: 85, step: 103
	action: tensor([[ 0.7717, -0.1486, -0.0412,  0.2798, -0.4244, -0.2014,  0.1414]],
       dtype=torch.float64)
	q_value: tensor([[-2.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.735946523254594, distance: 0.5880342717804158 entropy 0.03264415264129639
epoch: 85, step: 104
	action: tensor([[ 0.6064,  0.4879, -0.3736,  0.3953, -0.2139,  0.1023, -0.2641]],
       dtype=torch.float64)
	q_value: tensor([[-1.2708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9225356020789204, distance: 0.31849877634804424 entropy 0.03264415264129639
epoch: 85, step: 105
	action: tensor([[ 0.1846, -0.0212,  0.1058,  0.6479,  0.1629,  0.0730, -0.0680]],
       dtype=torch.float64)
	q_value: tensor([[-2.4676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7715392370116116, distance: 0.5469687377040109 entropy 0.03264415264129639
epoch: 85, step: 106
	action: tensor([[ 0.4188, -0.2725,  0.1548,  1.1261,  0.1762,  0.2781,  0.0855]],
       dtype=torch.float64)
	q_value: tensor([[-0.6345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9541758572768919, distance: 0.24496490404133342 entropy 0.03264415264129639
epoch: 85, step: 107
	action: tensor([[ 0.7817,  0.0662, -0.2234,  0.1361, -0.3045,  0.1210, -0.0204]],
       dtype=torch.float64)
	q_value: tensor([[-0.9742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8661704531970704, distance: 0.4186322645106297 entropy 0.03264415264129639
epoch: 85, step: 108
	action: tensor([[ 0.0639, -0.0673,  0.1071, -0.0993,  0.0836,  0.1220,  0.4435]],
       dtype=torch.float64)
	q_value: tensor([[-1.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937291589969019, distance: 0.9617060130444471 entropy 0.03264415264129639
epoch: 85, step: 109
	action: tensor([[ 0.8570, -0.0132,  0.0016,  0.5598,  0.0998, -0.5254, -0.1425]],
       dtype=torch.float64)
	q_value: tensor([[-0.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8722270070693563, distance: 0.40904984001852757 entropy 0.03264415264129639
epoch: 85, step: 110
	action: tensor([[ 0.2816,  0.1049, -0.0041,  0.2216, -0.3949, -0.0332, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-1.5820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7133677691175218, distance: 0.6126595466205926 entropy 0.03264415264129639
epoch: 85, step: 111
	action: tensor([[ 0.4469,  0.0096, -0.3659,  0.0278, -0.2850,  0.5442, -0.0756]],
       dtype=torch.float64)
	q_value: tensor([[-0.9681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7321742968829452, distance: 0.5922196593275938 entropy 0.03264415264129639
epoch: 85, step: 112
	action: tensor([[ 0.3172, -0.1127,  0.0950,  0.7318,  0.2238,  0.0467,  0.2810]],
       dtype=torch.float64)
	q_value: tensor([[-1.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8266859404406139, distance: 0.4764020161128637 entropy 0.03264415264129639
epoch: 85, step: 113
	action: tensor([[ 0.6021,  0.1652, -0.3536,  0.0622, -0.1273,  0.3625, -0.2336]],
       dtype=torch.float64)
	q_value: tensor([[-0.5589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8720914172482215, distance: 0.4092668197070885 entropy 0.03264415264129639
epoch: 85, step: 114
	action: tensor([[ 0.6849,  0.2288,  0.1616,  0.7646, -0.8464, -0.0097,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[-1.8960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9898792254344891, distance: 0.11512338980926896 entropy 0.03264415264129639
epoch: 85, step: 115
	action: tensor([[ 0.9882,  0.4345, -0.4099,  0.3090,  0.2236,  0.3470, -0.1909]],
       dtype=torch.float64)
	q_value: tensor([[-2.3204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9885735258391448, distance: 0.1223243211300631 entropy 0.03264415264129639
epoch: 85, step: 116
	action: tensor([[ 0.6368, -0.1572, -0.0632,  0.0846,  0.2561,  0.5993,  0.4563]],
       dtype=torch.float64)
	q_value: tensor([[-3.0081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8021471123531089, distance: 0.5090118462921431 entropy 0.03264415264129639
epoch: 85, step: 117
	action: tensor([[0.6234, 0.2987, 0.1017, 0.7743, 0.0367, 0.4671, 0.0233]],
       dtype=torch.float64)
	q_value: tensor([[-0.9878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9400638851660523, distance: 0.28015668322595266 entropy 0.03264415264129639
epoch: 85, step: 118
	action: tensor([[ 0.4049, -0.0666,  0.3118,  0.0177,  0.3891,  0.1327,  0.2625]],
       dtype=torch.float64)
	q_value: tensor([[-1.7952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6284125718453404, distance: 0.6975690434785475 entropy 0.03264415264129639
epoch: 85, step: 119
	action: tensor([[ 0.7664, -0.0808,  0.0190,  0.5951, -0.1454,  0.0010,  0.2024]],
       dtype=torch.float64)
	q_value: tensor([[-0.4381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9270840570679012, distance: 0.309006732293639 entropy 0.03264415264129639
epoch: 85, step: 120
	action: tensor([[ 1.6802e-01, -1.2643e-04, -3.0561e-01,  4.2532e-01, -2.6660e-01,
         -8.6035e-02, -4.4881e-01]], dtype=torch.float64)
	q_value: tensor([[-1.1583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5898727931300152, distance: 0.7328515039228061 entropy 0.03264415264129639
epoch: 85, step: 121
	action: tensor([[ 0.1570, -0.1701, -0.3344,  0.2115, -0.1706, -0.0924, -0.2043]],
       dtype=torch.float64)
	q_value: tensor([[-1.1823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40011322497369395, distance: 0.8863216076589195 entropy 0.03264415264129639
epoch: 85, step: 122
	action: tensor([[ 0.8923, -0.0497, -0.1728,  0.7393,  0.1643,  0.3965,  0.3348]],
       dtype=torch.float64)
	q_value: tensor([[-0.6594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07525840464015253 entropy 0.03264415264129639
epoch: 85, step: 123
	action: tensor([[ 1.0756,  0.2668, -0.0163,  0.5861,  0.2072,  0.1371, -0.2029]],
       dtype=torch.float64)
	q_value: tensor([[-4.7954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9703566560503474, distance: 0.19702452531004153 entropy 0.03264415264129639
epoch: 85, step: 124
	action: tensor([[ 0.2485,  0.1412, -0.4281,  0.4449, -0.0358, -0.0696, -0.3366]],
       dtype=torch.float64)
	q_value: tensor([[-2.6367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6896250682727453, distance: 0.6375290983359613 entropy 0.03264415264129639
epoch: 85, step: 125
	action: tensor([[ 0.6182, -0.0113, -0.5208,  0.3548, -0.3689, -0.1050,  0.2903]],
       dtype=torch.float64)
	q_value: tensor([[-1.2063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8166838438711701, distance: 0.48995598188057166 entropy 0.03264415264129639
epoch: 85, step: 126
	action: tensor([[ 0.5563,  0.0075, -0.2537,  0.3128,  0.2290,  0.2752, -0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-1.3870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8631789362116016, distance: 0.42328528854084013 entropy 0.03264415264129639
epoch: 85, step: 127
	action: tensor([[ 0.2682, -0.0810, -0.2802,  0.0846,  0.0878,  0.2011,  0.1509]],
       dtype=torch.float64)
	q_value: tensor([[-0.9957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5604950699127424, distance: 0.7586449457501631 entropy 0.03264415264129639
LOSS epoch 85 actor 51.662651726214285 critic 1078.3357696668802 
epoch: 86, step: 0
	action: tensor([[ 0.8784,  0.0696,  0.0769,  0.3693, -0.4527, -0.3279,  0.1202]],
       dtype=torch.float64)
	q_value: tensor([[-0.5991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8766563581412553, distance: 0.40189728923879425 entropy 0.03264415264129639
epoch: 86, step: 1
	action: tensor([[ 0.3270,  0.0089, -0.2457,  0.3422, -0.1823,  0.6059,  0.4418]],
       dtype=torch.float64)
	q_value: tensor([[-1.6834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7530867306878628, distance: 0.5686288734343576 entropy 0.03264415264129639
epoch: 86, step: 2
	action: tensor([[ 0.7977,  0.4568, -0.6890,  0.3450,  0.1315,  0.1928,  0.1687]],
       dtype=torch.float64)
	q_value: tensor([[-1.0673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9660864805654995, distance: 0.2107381310542239 entropy 0.03264415264129639
epoch: 86, step: 3
	action: tensor([[ 0.8813,  0.2096, -0.0528, -0.0799, -0.0340, -0.1428, -0.1130]],
       dtype=torch.float64)
	q_value: tensor([[-2.3940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8250928387719854, distance: 0.47858654960429675 entropy 0.03264415264129639
epoch: 86, step: 4
	action: tensor([[ 0.8182,  0.2074, -0.1763,  0.3655, -0.5309,  0.8205,  0.1874]],
       dtype=torch.float64)
	q_value: tensor([[-1.7144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9778650476109119, distance: 0.17025347672848132 entropy 0.03264415264129639
epoch: 86, step: 5
	action: tensor([[ 0.1294, -0.1487, -0.2490,  0.1649,  0.0928,  0.2341,  0.1538]],
       dtype=torch.float64)
	q_value: tensor([[-2.8375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43943372542490944, distance: 0.856781688733219 entropy 0.03264415264129639
epoch: 86, step: 6
	action: tensor([[ 0.4977,  0.0712,  0.0307,  0.5848, -0.3480, -0.0743, -0.1158]],
       dtype=torch.float64)
	q_value: tensor([[-0.5126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9008644498013578, distance: 0.3603059250164982 entropy 0.03264415264129639
epoch: 86, step: 7
	action: tensor([[ 0.5092,  0.2017,  0.1406,  0.4585, -0.2203, -0.2227, -0.2075]],
       dtype=torch.float64)
	q_value: tensor([[-1.2966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9159046486620226, distance: 0.3318506616889859 entropy 0.03264415264129639
epoch: 86, step: 8
	action: tensor([[0.8068, 0.3497, 0.1037, 0.8152, 0.0017, 0.7074, 0.1918]],
       dtype=torch.float64)
	q_value: tensor([[-1.2710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8854139530586677, distance: 0.3873669479583228 entropy 0.03264415264129639
epoch: 86, step: 9
	action: tensor([[ 0.4902, -0.0028, -0.6050, -0.0130, -0.3605,  0.2446,  0.1281]],
       dtype=torch.float64)
	q_value: tensor([[-2.2970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6891476369197701, distance: 0.6380192465209196 entropy 0.03264415264129639
epoch: 86, step: 10
	action: tensor([[ 0.1946, -0.3175, -0.3751, -0.2703, -0.1833,  0.2490,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[-1.5298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16150239588748294, distance: 1.0478704810555828 entropy 0.03264415264129639
epoch: 86, step: 11
	action: tensor([[ 0.3914,  0.1354, -0.0159, -0.0772, -0.1511,  0.2267,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-0.7975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7157579515906192, distance: 0.6100997617588596 entropy 0.03264415264129639
epoch: 86, step: 12
	action: tensor([[ 0.6748, -0.1972, -0.0043,  0.2266, -0.0563,  0.4161, -0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-0.8975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7874847869917445, distance: 0.5275355184954825 entropy 0.03264415264129639
epoch: 86, step: 13
	action: tensor([[ 0.3577, -0.1805, -0.0349, -0.2630, -0.0763,  0.0365, -0.2280]],
       dtype=torch.float64)
	q_value: tensor([[-1.2184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32418396807362215, distance: 0.9407428761773478 entropy 0.03264415264129639
epoch: 86, step: 14
	action: tensor([[ 0.6680,  0.6601, -0.2497,  0.3505, -0.0021,  0.2738,  0.1045]],
       dtype=torch.float64)
	q_value: tensor([[-0.6731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 86, step: 15
	action: tensor([[ 0.7146, -0.0071, -0.0584,  0.2093,  0.0362,  0.2048, -0.2972]],
       dtype=torch.float64)
	q_value: tensor([[-4.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8530897616613661, distance: 0.43861423757681556 entropy 0.03264415264129639
epoch: 86, step: 16
	action: tensor([[ 0.9510, -0.0162, -0.7460,  0.4447,  0.3019,  0.3609, -0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-1.4600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9217389767356275, distance: 0.32013227009995787 entropy 0.03264415264129639
epoch: 86, step: 17
	action: tensor([[ 0.2424,  0.4571, -0.1763, -0.0537,  0.0371,  0.2833, -0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-2.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7855337059465266, distance: 0.5299516112660203 entropy 0.03264415264129639
epoch: 86, step: 18
	action: tensor([[ 0.2250, -0.0237, -0.2938,  0.0113, -0.1287, -0.0431,  0.2581]],
       dtype=torch.float64)
	q_value: tensor([[-1.1774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49932651515347193, distance: 0.809718364771228 entropy 0.03264415264129639
epoch: 86, step: 19
	action: tensor([[ 0.0693,  0.1497,  0.2495,  0.4422, -0.0925,  0.1979, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-0.6620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7248175034991463, distance: 0.6002982764537873 entropy 0.03264415264129639
epoch: 86, step: 20
	action: tensor([[ 1.0097, -0.0858, -0.5778,  0.4720, -0.1237,  0.3059,  0.3879]],
       dtype=torch.float64)
	q_value: tensor([[-0.7023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.908639186280136, distance: 0.34588893790438047 entropy 0.03264415264129639
epoch: 86, step: 21
	action: tensor([[ 0.4340,  0.2081, -0.2890,  0.5014,  0.1510, -0.0273,  0.1550]],
       dtype=torch.float64)
	q_value: tensor([[-2.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8575453303487894, distance: 0.4319117691064157 entropy 0.03264415264129639
epoch: 86, step: 22
	action: tensor([[ 0.5593,  0.0646,  0.0466, -0.0769,  0.0309, -0.0879, -0.3275]],
       dtype=torch.float64)
	q_value: tensor([[-0.9108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6915526210859122, distance: 0.63554635943815 entropy 0.03264415264129639
epoch: 86, step: 23
	action: tensor([[ 0.2688, -0.4337, -0.6507, -0.0529,  0.5101,  0.0073, -0.0970]],
       dtype=torch.float64)
	q_value: tensor([[-1.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14878355666012721, distance: 1.055787936748587 entropy 0.03264415264129639
epoch: 86, step: 24
	action: tensor([[ 0.7884, -0.1113, -0.5128,  0.8214,  0.2040,  0.1920,  0.2156]],
       dtype=torch.float64)
	q_value: tensor([[-0.6444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9268369177973058, distance: 0.3095299587318629 entropy 0.03264415264129639
epoch: 86, step: 25
	action: tensor([[-0.0842,  0.4172,  0.1311,  0.1863,  0.1736,  0.1317, -0.0846]],
       dtype=torch.float64)
	q_value: tensor([[-1.5023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5786072034587633, distance: 0.7428484934885592 entropy 0.03264415264129639
epoch: 86, step: 26
	action: tensor([[ 0.6029,  0.4950, -0.4620,  0.5225, -0.2087,  0.3280, -0.3120]],
       dtype=torch.float64)
	q_value: tensor([[-0.7021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8564982394860587, distance: 0.4334962129805411 entropy 0.03264415264129639
epoch: 86, step: 27
	action: tensor([[ 0.3710, -0.1361, -0.4281,  0.5078, -0.1156,  0.1055, -0.2440]],
       dtype=torch.float64)
	q_value: tensor([[-2.8516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6748378042278057, distance: 0.6525393666360421 entropy 0.03264415264129639
epoch: 86, step: 28
	action: tensor([[ 0.0852,  0.3503, -0.0024,  0.2520, -0.0232,  0.4896,  0.1447]],
       dtype=torch.float64)
	q_value: tensor([[-1.0651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7164681292883475, distance: 0.6093371191064411 entropy 0.03264415264129639
epoch: 86, step: 29
	action: tensor([[ 0.5104,  0.1545, -0.5657,  0.4335, -0.4096,  0.4468,  0.3007]],
       dtype=torch.float64)
	q_value: tensor([[-0.9778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8041344178756109, distance: 0.5064490456526333 entropy 0.03264415264129639
epoch: 86, step: 30
	action: tensor([[ 0.7832,  0.3331, -0.3472,  0.5800, -0.2639, -0.3045, -0.4646]],
       dtype=torch.float64)
	q_value: tensor([[-1.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9850585779351915, distance: 0.13987904552260177 entropy 0.03264415264129639
epoch: 86, step: 31
	action: tensor([[ 0.6828,  0.1938, -0.5684,  0.2837,  0.0586,  0.0553, -0.0869]],
       dtype=torch.float64)
	q_value: tensor([[-2.9019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.91639808472512, distance: 0.33087564946516707 entropy 0.03264415264129639
epoch: 86, step: 32
	action: tensor([[ 0.7405, -0.1521,  0.2037,  0.6317,  0.0690,  0.0378, -0.2709]],
       dtype=torch.float64)
	q_value: tensor([[-1.7668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9143316981743109, distance: 0.3349398120837904 entropy 0.03264415264129639
epoch: 86, step: 33
	action: tensor([[ 1.0722,  0.3795, -0.0207,  0.2940,  0.0651,  0.0757, -0.2996]],
       dtype=torch.float64)
	q_value: tensor([[-1.3983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9628762518812861, distance: 0.2204868038107465 entropy 0.03264415264129639
epoch: 86, step: 34
	action: tensor([[ 0.5132,  0.1089, -0.0512,  0.6830,  0.1691,  0.2895, -0.2660]],
       dtype=torch.float64)
	q_value: tensor([[-2.8306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9469941174761164, distance: 0.2634624512202682 entropy 0.03264415264129639
epoch: 86, step: 35
	action: tensor([[ 0.6414, -0.2106, -0.0614,  0.7403,  0.0996,  0.4115, -0.1701]],
       dtype=torch.float64)
	q_value: tensor([[-1.3984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9476164459037035, distance: 0.2619112623261652 entropy 0.03264415264129639
epoch: 86, step: 36
	action: tensor([[ 0.3313,  0.1887, -0.7460,  0.1943,  0.1727,  0.0710, -0.0486]],
       dtype=torch.float64)
	q_value: tensor([[-1.4424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7187671963641442, distance: 0.6068616331030774 entropy 0.03264415264129639
epoch: 86, step: 37
	action: tensor([[ 0.7274,  0.0326, -0.5345,  0.2503,  0.0148,  0.0300,  0.0832]],
       dtype=torch.float64)
	q_value: tensor([[-1.3380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8479366491503504, distance: 0.4462404875583561 entropy 0.03264415264129639
epoch: 86, step: 38
	action: tensor([[ 0.2895,  0.2895, -0.2790,  0.2237, -0.2483, -0.1109, -0.3602]],
       dtype=torch.float64)
	q_value: tensor([[-1.4362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7745870084320792, distance: 0.543308081018332 entropy 0.03264415264129639
epoch: 86, step: 39
	action: tensor([[ 0.5729,  0.3272, -0.0154,  0.0156, -0.6420, -0.0952, -0.1426]],
       dtype=torch.float64)
	q_value: tensor([[-1.4115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8990894613033401, distance: 0.3635171924883012 entropy 0.03264415264129639
epoch: 86, step: 40
	action: tensor([[ 0.3963,  0.1327, -0.2643,  0.6179,  0.1487,  0.3319, -0.4176]],
       dtype=torch.float64)
	q_value: tensor([[-1.8902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8512045169552248, distance: 0.44141955344454853 entropy 0.03264415264129639
epoch: 86, step: 41
	action: tensor([[ 0.9797,  0.0206, -0.2118,  0.4090, -0.1993,  0.2207, -0.2327]],
       dtype=torch.float64)
	q_value: tensor([[-1.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9453099955740986, distance: 0.2676151357230427 entropy 0.03264415264129639
epoch: 86, step: 42
	action: tensor([[ 0.5156,  0.1674, -0.7999,  0.5285, -0.1382,  0.1199, -0.0424]],
       dtype=torch.float64)
	q_value: tensor([[-2.5347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7848761467321608, distance: 0.5307634121771575 entropy 0.03264415264129639
epoch: 86, step: 43
	action: tensor([[ 0.7939, -0.0675, -0.1252,  0.4757, -0.2207, -0.6292,  0.1710]],
       dtype=torch.float64)
	q_value: tensor([[-1.9455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7954079630445526, distance: 0.5176080910168191 entropy 0.03264415264129639
epoch: 86, step: 44
	action: tensor([[ 0.7280, -0.0308,  0.2322,  0.7085, -0.2365,  0.2104,  0.2267]],
       dtype=torch.float64)
	q_value: tensor([[-1.3390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9851137701880344, distance: 0.13962045627057912 entropy 0.03264415264129639
epoch: 86, step: 45
	action: tensor([[ 0.5526,  0.1509, -0.1656,  0.4127, -0.1355,  0.2081,  0.1887]],
       dtype=torch.float64)
	q_value: tensor([[-1.2988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9193001017053518, distance: 0.3250821975864156 entropy 0.03264415264129639
epoch: 86, step: 46
	action: tensor([[ 0.5151,  0.1684, -0.3203,  0.6535, -0.2935, -0.0063, -0.0149]],
       dtype=torch.float64)
	q_value: tensor([[-1.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8921997092477398, distance: 0.3757220293411713 entropy 0.03264415264129639
epoch: 86, step: 47
	action: tensor([[ 0.0364,  0.0242,  0.0519,  0.3511, -0.3587, -0.0594,  0.4200]],
       dtype=torch.float64)
	q_value: tensor([[-1.5553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5285782563759887, distance: 0.7857085844983346 entropy 0.03264415264129639
epoch: 86, step: 48
	action: tensor([[ 0.3606,  0.2071, -0.1660,  0.3985, -0.1768,  0.4632,  0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-0.5464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8376560359992774, distance: 0.46107838817810437 entropy 0.03264415264129639
epoch: 86, step: 49
	action: tensor([[ 0.1856, -0.1391, -0.3162,  0.1654, -0.4946,  0.4327, -0.3630]],
       dtype=torch.float64)
	q_value: tensor([[-1.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45313221380824686, distance: 0.8462484067782458 entropy 0.03264415264129639
epoch: 86, step: 50
	action: tensor([[ 0.5809,  0.1827, -0.7385,  0.3494, -0.0977,  0.1460,  0.2676]],
       dtype=torch.float64)
	q_value: tensor([[-1.5818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8474982769439199, distance: 0.4468832415312487 entropy 0.03264415264129639
epoch: 86, step: 51
	action: tensor([[ 0.6166, -0.1680,  0.0665,  0.3900, -0.0962,  0.2918, -0.1739]],
       dtype=torch.float64)
	q_value: tensor([[-1.6830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8409909188422571, distance: 0.4563180597854159 entropy 0.03264415264129639
epoch: 86, step: 52
	action: tensor([[ 0.8139,  0.3838,  0.0128, -0.0066, -0.1633, -0.2556,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-1.2493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9225805200371813, distance: 0.31840642173342587 entropy 0.03264415264129639
epoch: 86, step: 53
	action: tensor([[ 0.5466, -0.0188,  0.0760,  0.5162, -0.2909, -0.0317,  0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-1.6294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8876791988388805, distance: 0.3835189163166512 entropy 0.03264415264129639
epoch: 86, step: 54
	action: tensor([[ 0.0918,  0.1807, -0.5297, -0.0795,  0.0471,  0.0711, -0.1667]],
       dtype=torch.float64)
	q_value: tensor([[-0.9289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5319183839054948, distance: 0.7829201768725231 entropy 0.03264415264129639
epoch: 86, step: 55
	action: tensor([[ 0.2323,  0.2173, -0.0502,  0.6414,  0.0324,  0.6014,  0.2180]],
       dtype=torch.float64)
	q_value: tensor([[-0.9403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8171469225472395, distance: 0.4893367466746105 entropy 0.03264415264129639
epoch: 86, step: 56
	action: tensor([[ 0.9195, -0.0448, -0.0802,  0.3876,  0.0270,  0.2891,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[-1.1014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.925709299133033, distance: 0.3119061378023597 entropy 0.03264415264129639
epoch: 86, step: 57
	action: tensor([[ 0.4158,  0.1607, -0.2431,  0.6009,  0.1330,  0.0782, -0.0945]],
       dtype=torch.float64)
	q_value: tensor([[-1.7257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8564100575922995, distance: 0.43362938433018877 entropy 0.03264415264129639
epoch: 86, step: 58
	action: tensor([[ 0.8468,  0.0110, -0.2961,  0.5087, -0.1405, -0.0384, -0.1913]],
       dtype=torch.float64)
	q_value: tensor([[-1.0503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9331974272318309, distance: 0.2957694322409816 entropy 0.03264415264129639
epoch: 86, step: 59
	action: tensor([[ 0.6394, -0.2199,  0.0066,  0.5563, -0.1820,  0.1000,  0.0538]],
       dtype=torch.float64)
	q_value: tensor([[-2.0360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8508592916391502, distance: 0.44193133281219626 entropy 0.03264415264129639
epoch: 86, step: 60
	action: tensor([[ 0.8686, -0.0542, -0.5332,  0.8778, -0.1956,  0.0633, -0.4256]],
       dtype=torch.float64)
	q_value: tensor([[-1.0291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9541125836689275, distance: 0.24513396848606675 entropy 0.03264415264129639
epoch: 86, step: 61
	action: tensor([[ 0.7150,  0.1415, -0.4956,  0.2010,  0.0312,  0.3334,  0.0876]],
       dtype=torch.float64)
	q_value: tensor([[-2.9584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9221005254911723, distance: 0.31939194365129414 entropy 0.03264415264129639
epoch: 86, step: 62
	action: tensor([[ 0.2952,  0.0994, -0.1862,  0.2575, -0.3527,  0.1414,  0.3147]],
       dtype=torch.float64)
	q_value: tensor([[-1.7200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7235468434274587, distance: 0.6016826242155954 entropy 0.03264415264129639
epoch: 86, step: 63
	action: tensor([[ 0.5802,  0.2790,  0.0173,  0.3771, -0.0583,  0.0855, -0.2685]],
       dtype=torch.float64)
	q_value: tensor([[-0.8272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9494754707374079, distance: 0.2572218344570264 entropy 0.03264415264129639
epoch: 86, step: 64
	action: tensor([[ 1.0257,  0.0249, -0.2130,  0.2458, -0.4323, -0.5984, -0.0635]],
       dtype=torch.float64)
	q_value: tensor([[-1.5016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7352872980181933, distance: 0.5887678455098161 entropy 0.03264415264129639
epoch: 86, step: 65
	action: tensor([[ 0.4346, -0.1215, -0.2509,  0.0303, -0.1550,  0.1128,  0.3284]],
       dtype=torch.float64)
	q_value: tensor([[-2.3318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5873636839275074, distance: 0.7350898346109138 entropy 0.03264415264129639
epoch: 86, step: 66
	action: tensor([[ 0.5756, -0.0586,  0.0964,  0.3879, -0.1764,  0.0622,  0.1275]],
       dtype=torch.float64)
	q_value: tensor([[-0.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8478788172213287, distance: 0.4463253354024778 entropy 0.03264415264129639
epoch: 86, step: 67
	action: tensor([[-0.1103, -0.3951, -0.1558,  0.9287, -0.1770,  0.0738, -0.4395]],
       dtype=torch.float64)
	q_value: tensor([[-0.8427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35376375794990733, distance: 0.9199248453404453 entropy 0.03264415264129639
epoch: 86, step: 68
	action: tensor([[ 0.6344, -0.0625,  0.5490,  0.2739, -0.1532,  0.1476,  0.1179]],
       dtype=torch.float64)
	q_value: tensor([[-0.9405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8365899187244675, distance: 0.4625898679820632 entropy 0.03264415264129639
epoch: 86, step: 69
	action: tensor([[ 0.7828,  0.0272, -0.0467,  0.7520,  0.0301,  0.0138,  0.3562]],
       dtype=torch.float64)
	q_value: tensor([[-0.9307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9865304909455209, distance: 0.13281055040721682 entropy 0.03264415264129639
epoch: 86, step: 70
	action: tensor([[ 0.3726,  0.4386, -0.2812,  0.2218,  0.0699, -0.0684, -0.1556]],
       dtype=torch.float64)
	q_value: tensor([[-1.1582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.852013589667197, distance: 0.4402178122405469 entropy 0.03264415264129639
epoch: 86, step: 71
	action: tensor([[ 0.1252,  0.2677, -0.4444,  0.1234, -0.4010,  0.2905, -0.3013]],
       dtype=torch.float64)
	q_value: tensor([[-1.2659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5910856235400989, distance: 0.7317671054023395 entropy 0.03264415264129639
epoch: 86, step: 72
	action: tensor([[-0.0671,  0.9181,  0.0100,  0.0057, -0.1113,  0.3341, -0.1235]],
       dtype=torch.float64)
	q_value: tensor([[-1.6925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 86, step: 73
	action: tensor([[ 0.1716, -0.0575, -0.1653,  0.5920,  0.2119,  0.4220, -0.3070]],
       dtype=torch.float64)
	q_value: tensor([[-4.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6761950504240111, distance: 0.6511760736174488 entropy 0.03264415264129639
epoch: 86, step: 74
	action: tensor([[ 0.3367,  0.0029, -0.2893,  0.9588,  0.1319, -0.0191, -0.1295]],
       dtype=torch.float64)
	q_value: tensor([[-0.9461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7610754500493427, distance: 0.5593544294937027 entropy 0.03264415264129639
epoch: 86, step: 75
	action: tensor([[ 0.6118, -0.1045,  0.0380,  0.2844, -0.2113,  0.1013,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-1.1042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7821641613828021, distance: 0.5340985012049047 entropy 0.03264415264129639
epoch: 86, step: 76
	action: tensor([[ 0.4812, -0.3719, -0.3811,  0.5143, -0.2381,  0.3189, -0.0898]],
       dtype=torch.float64)
	q_value: tensor([[-0.9921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6344280009226395, distance: 0.6918997203685667 entropy 0.03264415264129639
epoch: 86, step: 77
	action: tensor([[ 0.8055,  0.6894, -0.3129,  0.2736, -0.3742,  0.3165,  0.0733]],
       dtype=torch.float64)
	q_value: tensor([[-1.1743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.926055820241288, distance: 0.31117786098972705 entropy 0.03264415264129639
epoch: 86, step: 78
	action: tensor([[ 0.2464, -0.0808, -0.4467,  0.1450,  0.0146, -0.0804, -0.1984]],
       dtype=torch.float64)
	q_value: tensor([[-2.9361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 86, step: 79
	action: tensor([[ 0.5151,  0.3339, -0.1930,  0.0581,  0.1346, -0.0287, -0.3906]],
       dtype=torch.float64)
	q_value: tensor([[-4.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8649305290935985, distance: 0.4205670968797393 entropy 0.03264415264129639
epoch: 86, step: 80
	action: tensor([[ 0.3992, -0.3277, -0.4933,  0.6106, -0.1352,  0.3492, -0.4236]],
       dtype=torch.float64)
	q_value: tensor([[-1.4105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6119008385432994, distance: 0.7128990655695623 entropy 0.03264415264129639
epoch: 86, step: 81
	action: tensor([[ 0.7489,  0.1553, -0.2114,  0.0908, -0.1542, -0.1787,  0.2640]],
       dtype=torch.float64)
	q_value: tensor([[-1.5210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8483597104454653, distance: 0.4456193037936034 entropy 0.03264415264129639
epoch: 86, step: 82
	action: tensor([[ 0.7575, -0.5491, -0.1420,  0.1513, -0.5209,  0.1523, -0.0693]],
       dtype=torch.float64)
	q_value: tensor([[-1.2442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.373986862660167, distance: 0.9054165255393403 entropy 0.03264415264129639
epoch: 86, step: 83
	action: tensor([[ 0.6627, -0.1330, -0.6042,  0.3514, -0.0019,  0.0511, -0.4347]],
       dtype=torch.float64)
	q_value: tensor([[-1.3808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.760778313897235, distance: 0.5597021392212389 entropy 0.03264415264129639
epoch: 86, step: 84
	action: tensor([[ 1.0420,  0.3771, -0.0664,  0.4959, -0.3976,  0.1167,  0.0983]],
       dtype=torch.float64)
	q_value: tensor([[-1.8631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9798241991360653, distance: 0.162544427291918 entropy 0.03264415264129639
epoch: 86, step: 85
	action: tensor([[ 0.0744,  0.1572, -0.3854,  0.0845, -0.2937, -0.3394,  0.2569]],
       dtype=torch.float64)
	q_value: tensor([[-2.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5266990935777108, distance: 0.7872730073548475 entropy 0.03264415264129639
epoch: 86, step: 86
	action: tensor([[ 0.7413, -0.0254, -0.4865,  0.2802,  0.0981, -0.0479, -0.2841]],
       dtype=torch.float64)
	q_value: tensor([[-0.9426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.814021928157085, distance: 0.49350046209331194 entropy 0.03264415264129639
epoch: 86, step: 87
	action: tensor([[ 0.6517,  0.1116, -0.3427,  0.3012, -0.2507,  0.2446, -0.1444]],
       dtype=torch.float64)
	q_value: tensor([[-1.6786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9033707703246628, distance: 0.35572218555681917 entropy 0.03264415264129639
epoch: 86, step: 88
	action: tensor([[ 0.4578,  0.3461, -0.0545,  0.6240,  0.1150,  0.2655,  0.5818]],
       dtype=torch.float64)
	q_value: tensor([[-1.8320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9083817195957518, distance: 0.34637597507366563 entropy 0.03264415264129639
epoch: 86, step: 89
	action: tensor([[ 0.8098, -0.0660, -0.3473,  0.7692, -0.1578,  0.1354, -0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-0.9705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9615340397582494, distance: 0.22443727260954072 entropy 0.03264415264129639
epoch: 86, step: 90
	action: tensor([[ 0.5326,  0.0088, -0.1302,  0.5475,  0.0843,  0.1200, -0.1240]],
       dtype=torch.float64)
	q_value: tensor([[-2.0067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9002524791680047, distance: 0.3614163107746106 entropy 0.03264415264129639
epoch: 86, step: 91
	action: tensor([[ 0.6578,  0.1016,  0.2020,  0.3593, -0.1845,  0.0241, -0.3313]],
       dtype=torch.float64)
	q_value: tensor([[-1.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9301048376647502, distance: 0.3025382233739089 entropy 0.03264415264129639
epoch: 86, step: 92
	action: tensor([[ 0.5625,  0.2517, -0.4624, -0.0550,  0.3685,  0.0478,  0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-1.5858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8543967303537081, distance: 0.43665884032462937 entropy 0.03264415264129639
epoch: 86, step: 93
	action: tensor([[ 0.2785,  0.1431, -0.0400,  0.4545, -0.1530,  0.5051,  0.5268]],
       dtype=torch.float64)
	q_value: tensor([[-1.2659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8196267257564462, distance: 0.48600729458989606 entropy 0.03264415264129639
epoch: 86, step: 94
	action: tensor([[ 0.4556, -0.2917, -0.1618,  0.0891, -0.2239,  0.4739,  0.4410]],
       dtype=torch.float64)
	q_value: tensor([[-0.8868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5550713944857982, distance: 0.7633115914314644 entropy 0.03264415264129639
epoch: 86, step: 95
	action: tensor([[ 0.6878, -0.4778, -0.4029,  0.1445,  0.0453,  0.3758, -0.4543]],
       dtype=torch.float64)
	q_value: tensor([[-0.8900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4758868376116365, distance: 0.8284555782834321 entropy 0.03264415264129639
epoch: 86, step: 96
	action: tensor([[ 0.5521,  0.0362, -0.2444,  0.2655,  0.3710,  0.1806, -0.1433]],
       dtype=torch.float64)
	q_value: tensor([[-1.5406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8394350728715971, distance: 0.45854507844916553 entropy 0.03264415264129639
epoch: 86, step: 97
	action: tensor([[ 0.2563,  0.2927,  0.2517,  0.2395,  0.0476, -0.0392, -0.1660]],
       dtype=torch.float64)
	q_value: tensor([[-0.9190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7867047837334387, distance: 0.5285027493618397 entropy 0.03264415264129639
epoch: 86, step: 98
	action: tensor([[ 0.4927, -0.0437, -0.0086,  0.5902, -0.2138, -0.0078,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-0.7775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8639904695806186, distance: 0.42202809534067376 entropy 0.03264415264129639
epoch: 86, step: 99
	action: tensor([[ 0.2139, -0.0076, -0.2979,  0.2616, -0.2945, -0.1558, -0.0604]],
       dtype=torch.float64)
	q_value: tensor([[-0.9056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5807778227694474, distance: 0.7409327949899713 entropy 0.03264415264129639
epoch: 86, step: 100
	action: tensor([[ 0.6568, -0.0751, -0.4022,  0.1148,  0.0687,  0.1000, -0.1074]],
       dtype=torch.float64)
	q_value: tensor([[-0.7910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7391264332812142, distance: 0.5844827944573868 entropy 0.03264415264129639
epoch: 86, step: 101
	action: tensor([[ 0.2475, -0.0346, -0.1901,  0.3438, -0.0792,  0.0745,  0.0654]],
       dtype=torch.float64)
	q_value: tensor([[-1.1668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6461438120280293, distance: 0.6807224764169916 entropy 0.03264415264129639
epoch: 86, step: 102
	action: tensor([[ 0.7538,  0.3413,  0.0438,  0.2784, -0.0771,  0.3644,  0.1254]],
       dtype=torch.float64)
	q_value: tensor([[-0.6353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9886365826136168, distance: 0.12198633199992298 entropy 0.03264415264129639
epoch: 86, step: 103
	action: tensor([[ 0.6840, -0.3417,  0.0520,  0.3715,  0.1270, -0.0862,  0.4662]],
       dtype=torch.float64)
	q_value: tensor([[-1.6323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6423533689665023, distance: 0.6843586524575295 entropy 0.03264415264129639
epoch: 86, step: 104
	action: tensor([[ 0.4638,  0.0476,  0.0177,  0.6015,  0.2954,  0.1146, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-0.5768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9087949560257221, distance: 0.34559394262612503 entropy 0.03264415264129639
epoch: 86, step: 105
	action: tensor([[ 0.6116,  0.6366, -0.0627,  0.3635, -0.0996,  0.2830, -0.2249]],
       dtype=torch.float64)
	q_value: tensor([[-0.7763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 86, step: 106
	action: tensor([[ 0.4359,  0.0492, -0.1753,  0.4043,  0.3238,  0.3116, -0.1176]],
       dtype=torch.float64)
	q_value: tensor([[-4.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8398406516374781, distance: 0.45796558160493744 entropy 0.03264415264129639
epoch: 86, step: 107
	action: tensor([[ 0.9201, -0.3772,  0.0818,  0.3574, -0.3514,  0.0450, -0.2511]],
       dtype=torch.float64)
	q_value: tensor([[-0.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6297200831362342, distance: 0.6963406876683517 entropy 0.03264415264129639
epoch: 86, step: 108
	action: tensor([[ 0.8091,  0.5300,  0.0491,  0.1107, -0.3906,  0.0163,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[-1.8317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9866737073984628, distance: 0.13210260008738398 entropy 0.03264415264129639
epoch: 86, step: 109
	action: tensor([[ 0.6490,  0.0926,  0.0410,  0.4789, -0.1316, -0.1260,  0.1778]],
       dtype=torch.float64)
	q_value: tensor([[-2.1475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9274271905528906, distance: 0.3082788010021862 entropy 0.03264415264129639
epoch: 86, step: 110
	action: tensor([[ 0.3389,  0.3526,  0.4071, -0.0689, -0.2404, -0.4535, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-0.9652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6938989500839439, distance: 0.6331244754359302 entropy 0.03264415264129639
epoch: 86, step: 111
	action: tensor([[ 0.7641,  0.2359,  0.2153,  0.8851,  0.0168, -0.0289,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[-1.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9822451125540431, distance: 0.15248097315297465 entropy 0.03264415264129639
epoch: 86, step: 112
	action: tensor([[ 0.6346, -0.0343, -0.6838,  0.3896, -0.2244, -0.0047, -0.0200]],
       dtype=torch.float64)
	q_value: tensor([[-1.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7970427354091325, distance: 0.515535995458514 entropy 0.03264415264129639
epoch: 86, step: 113
	action: tensor([[ 0.3773,  0.2995, -0.0021,  0.7685,  0.2149,  0.1001,  0.2610]],
       dtype=torch.float64)
	q_value: tensor([[-1.6195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8885890512823179, distance: 0.3819624140793328 entropy 0.03264415264129639
epoch: 86, step: 114
	action: tensor([[ 0.5784,  0.0778, -0.7214,  0.2863, -0.2462, -0.1000, -0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-0.8833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8130759808902054, distance: 0.4947539252184166 entropy 0.03264415264129639
epoch: 86, step: 115
	action: tensor([[ 0.7161, -0.0139,  0.0410,  0.9671, -0.0400, -0.1269,  0.1966]],
       dtype=torch.float64)
	q_value: tensor([[-1.7021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9919361321440494, distance: 0.10276101716563057 entropy 0.03264415264129639
epoch: 86, step: 116
	action: tensor([[ 0.2974, -0.0660, -0.1820,  0.3315,  0.0113,  0.2940,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[-1.2409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6999197415914074, distance: 0.6268669968923016 entropy 0.03264415264129639
epoch: 86, step: 117
	action: tensor([[ 2.4658e-01,  2.8367e-01, -3.9149e-04, -5.3856e-02,  3.9569e-01,
         -1.6621e-02,  2.1365e-01]], dtype=torch.float64)
	q_value: tensor([[-0.6934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6719303965571585, distance: 0.65545018458186 entropy 0.03264415264129639
epoch: 86, step: 118
	action: tensor([[ 1.1056,  0.2020,  0.0351,  0.2824,  0.3781, -0.2786,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-0.5839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8988157168651375, distance: 0.36400992305672575 entropy 0.03264415264129639
epoch: 86, step: 119
	action: tensor([[ 0.4827,  0.1728, -0.3664,  0.2762,  0.2910,  0.3951,  0.2340]],
       dtype=torch.float64)
	q_value: tensor([[-1.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8815309033732068, distance: 0.39387573959334543 entropy 0.03264415264129639
epoch: 86, step: 120
	action: tensor([[ 0.2930, -0.1909, -0.3677,  0.1251, -0.1760,  0.5684, -0.1720]],
       dtype=torch.float64)
	q_value: tensor([[-1.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5473349478514488, distance: 0.7699192466678023 entropy 0.03264415264129639
epoch: 86, step: 121
	action: tensor([[ 0.7787,  0.3187, -0.1040,  0.0117,  0.1254,  0.2516,  0.1714]],
       dtype=torch.float64)
	q_value: tensor([[-1.2738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9558624453618071, distance: 0.24041459402827436 entropy 0.03264415264129639
epoch: 86, step: 122
	action: tensor([[ 0.5167,  0.0809, -0.2173,  0.3427,  0.2209,  0.2393, -0.1935]],
       dtype=torch.float64)
	q_value: tensor([[-1.4305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8698176841723064, distance: 0.41288840771627533 entropy 0.03264415264129639
epoch: 86, step: 123
	action: tensor([[ 0.3435,  0.1759, -0.4081, -0.1358, -0.2276,  0.5540,  0.0527]],
       dtype=torch.float64)
	q_value: tensor([[-1.0442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7238834923562838, distance: 0.6013161651809837 entropy 0.03264415264129639
epoch: 86, step: 124
	action: tensor([[ 0.2409, -0.0686, -0.3595,  0.1578, -0.0642, -0.0880, -0.1444]],
       dtype=torch.float64)
	q_value: tensor([[-1.6130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5200298529451521, distance: 0.7928003009944062 entropy 0.03264415264129639
epoch: 86, step: 125
	action: tensor([[ 0.7033,  0.0148, -0.4639,  0.0357,  0.2060, -0.0132,  0.1210]],
       dtype=torch.float64)
	q_value: tensor([[-0.6556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7540803058674572, distance: 0.5674836432767147 entropy 0.03264415264129639
epoch: 86, step: 126
	action: tensor([[ 0.6349, -0.2049, -0.1201,  0.4196, -0.0462, -0.0082,  0.1388]],
       dtype=torch.float64)
	q_value: tensor([[-1.2008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7628783576625345, distance: 0.5572400088082685 entropy 0.03264415264129639
epoch: 86, step: 127
	action: tensor([[ 0.7158,  0.0017, -0.7534,  0.5352,  0.0733, -0.2263, -0.0559]],
       dtype=torch.float64)
	q_value: tensor([[-0.7852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8387407898651174, distance: 0.4595353839164055 entropy 0.03264415264129639
LOSS epoch 86 actor 39.62505277978645 critic 444.751385697871 
epoch: 87, step: 0
	action: tensor([[ 0.7773,  0.2626, -0.0600,  0.7214, -0.0545, -0.0902,  0.4136]],
       dtype=torch.float64)
	q_value: tensor([[-1.7739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9889779363626215, distance: 0.12014014460590378 entropy 0.03264415264129639
epoch: 87, step: 1
	action: tensor([[ 0.2754,  0.1073, -0.4044,  0.2632, -0.0781,  0.0563,  0.1110]],
       dtype=torch.float64)
	q_value: tensor([[-1.2908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6838723129933282, distance: 0.6434102276277703 entropy 0.03264415264129639
epoch: 87, step: 2
	action: tensor([[ 0.2137,  0.0670, -0.4160,  0.5602, -0.5154,  0.1896,  0.3509]],
       dtype=torch.float64)
	q_value: tensor([[-0.7982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6257291399421333, distance: 0.7000832723944086 entropy 0.03264415264129639
epoch: 87, step: 3
	action: tensor([[ 0.7737,  0.0920, -0.2297, -0.0143,  0.2098,  0.1250,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[-1.0554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8251484150887711, distance: 0.4785105087186421 entropy 0.03264415264129639
epoch: 87, step: 4
	action: tensor([[ 0.1630,  0.4786, -0.0500,  0.3841,  0.2860, -0.0093,  0.0616]],
       dtype=torch.float64)
	q_value: tensor([[-1.2199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.763903194173768, distance: 0.5560345129963705 entropy 0.03264415264129639
epoch: 87, step: 5
	action: tensor([[ 0.7166, -0.1464, -0.5095,  0.7583, -0.2311, -0.0545,  0.4555]],
       dtype=torch.float64)
	q_value: tensor([[-0.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8628769702259407, distance: 0.4237521293534214 entropy 0.03264415264129639
epoch: 87, step: 6
	action: tensor([[ 0.5878,  0.1452, -0.4291,  0.1601, -0.2987, -0.2694, -0.3102]],
       dtype=torch.float64)
	q_value: tensor([[-1.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8404047160059451, distance: 0.45715841700169213 entropy 0.03264415264129639
epoch: 87, step: 7
	action: tensor([[ 0.3612,  0.1612, -0.0237,  0.3879, -0.2262,  0.0155,  0.1716]],
       dtype=torch.float64)
	q_value: tensor([[-1.7464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8356579905013325, distance: 0.46390706841062623 entropy 0.03264415264129639
epoch: 87, step: 8
	action: tensor([[ 0.5451,  0.4776, -0.5074,  0.1961, -0.1387, -0.0263,  0.0666]],
       dtype=torch.float64)
	q_value: tensor([[-0.7874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9259336746053786, distance: 0.31143476649849183 entropy 0.03264415264129639
epoch: 87, step: 9
	action: tensor([[ 0.0301,  0.3668, -0.1354,  0.6306, -0.3737,  0.5778,  0.1168]],
       dtype=torch.float64)
	q_value: tensor([[-1.8051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.608245208061286, distance: 0.7162487086945195 entropy 0.03264415264129639
epoch: 87, step: 10
	action: tensor([[ 0.5363, -0.1157, -0.1452,  0.2046, -0.3135,  0.0655, -0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-1.5249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7190714447738058, distance: 0.6065332812035887 entropy 0.03264415264129639
epoch: 87, step: 11
	action: tensor([[ 0.7502,  0.2753, -0.1411, -0.0508, -0.0458,  0.3457, -0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-1.0711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9366697284830969, distance: 0.28798002711515464 entropy 0.03264415264129639
epoch: 87, step: 12
	action: tensor([[ 1.1879,  0.2193, -0.4173, -0.0089, -0.2209,  0.0225,  0.2248]],
       dtype=torch.float64)
	q_value: tensor([[-1.7312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8079641230430381, distance: 0.5014733750532099 entropy 0.03264415264129639
epoch: 87, step: 13
	action: tensor([[ 0.1575,  0.2478, -0.2151,  0.4389, -0.3263,  0.1769, -0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-2.7133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6914905086254047, distance: 0.6356103466264824 entropy 0.03264415264129639
epoch: 87, step: 14
	action: tensor([[ 0.0593, -0.2889, -0.0110, -0.1961,  0.1338,  0.0407, -0.3017]],
       dtype=torch.float64)
	q_value: tensor([[-1.1060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03750895833060641, distance: 1.122677557929514 entropy 0.03264415264129639
epoch: 87, step: 15
	action: tensor([[ 0.4178, -0.0632, -0.5833,  0.1331, -0.0557, -0.1822,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[-0.4082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5894333072664263, distance: 0.7332440548600433 entropy 0.03264415264129639
epoch: 87, step: 16
	action: tensor([[-0.0430,  0.0336, -0.3579,  0.0805, -0.0671,  0.1585,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[-0.9622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33720044423983897, distance: 0.931639293994991 entropy 0.03264415264129639
epoch: 87, step: 17
	action: tensor([[ 2.1985e-02, -3.9329e-01, -2.3625e-04,  5.1507e-01,  2.1533e-01,
         -4.5413e-01, -4.2647e-02]], dtype=torch.float64)
	q_value: tensor([[-0.6496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16879377869221923, distance: 1.0433045125736602 entropy 0.03264415264129639
epoch: 87, step: 18
	action: tensor([[ 0.4805,  0.3226,  0.0319,  0.0839, -0.1315,  0.6410,  0.1304]],
       dtype=torch.float64)
	q_value: tensor([[-0.3836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9136887399073902, distance: 0.3361943587472056 entropy 0.03264415264129639
epoch: 87, step: 19
	action: tensor([[ 0.9106,  0.1831,  0.1120,  0.4592, -0.0823,  0.0681,  0.0941]],
       dtype=torch.float64)
	q_value: tensor([[-1.5070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9798923240203143, distance: 0.1622697744137575 entropy 0.03264415264129639
epoch: 87, step: 20
	action: tensor([[ 0.5650,  0.1372, -0.1860,  0.1146, -0.3116, -0.2514, -0.4149]],
       dtype=torch.float64)
	q_value: tensor([[-1.6338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8159914498697826, distance: 0.4908804035452309 entropy 0.03264415264129639
epoch: 87, step: 21
	action: tensor([[ 0.3719, -0.2924, -0.2263,  0.2094, -0.4292, -0.2515,  0.2303]],
       dtype=torch.float64)
	q_value: tensor([[-1.6777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44839622346659924, distance: 0.8499048522684597 entropy 0.03264415264129639
epoch: 87, step: 22
	action: tensor([[ 0.0908,  0.3870, -0.0673,  0.4486, -0.0109,  0.0354, -0.3201]],
       dtype=torch.float64)
	q_value: tensor([[-0.6904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7081972973583632, distance: 0.6181606391098349 entropy 0.03264415264129639
epoch: 87, step: 23
	action: tensor([[ 0.2796,  0.0600, -0.1444,  0.4875, -0.1407, -0.2572, -0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-1.0750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.732915315276673, distance: 0.5913998171222794 entropy 0.03264415264129639
epoch: 87, step: 24
	action: tensor([[ 0.9108, -0.1495, -0.4403,  0.0639, -0.2191,  0.1330,  0.3736]],
       dtype=torch.float64)
	q_value: tensor([[-0.7977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.684901079809181, distance: 0.6423624571962172 entropy 0.03264415264129639
epoch: 87, step: 25
	action: tensor([[ 0.2335,  0.3694,  0.2114,  0.2342, -0.0026, -0.0975,  0.5023]],
       dtype=torch.float64)
	q_value: tensor([[-1.6209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7849645589649321, distance: 0.5306543336159795 entropy 0.03264415264129639
epoch: 87, step: 26
	action: tensor([[ 0.4877,  0.1986, -0.0807,  0.5859, -0.3902,  0.1795,  0.1453]],
       dtype=torch.float64)
	q_value: tensor([[-0.6009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9166121724688986, distance: 0.3304517247939499 entropy 0.03264415264129639
epoch: 87, step: 27
	action: tensor([[ 0.5308,  0.2951, -0.4018,  0.7184, -0.4852,  0.0353, -0.2003]],
       dtype=torch.float64)
	q_value: tensor([[-1.3117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8656850219903973, distance: 0.4193908160581764 entropy 0.03264415264129639
epoch: 87, step: 28
	action: tensor([[ 0.4988,  0.0541, -0.3380,  0.1622, -0.0468,  0.3496, -0.3493]],
       dtype=torch.float64)
	q_value: tensor([[-2.2875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8172934012263078, distance: 0.4891407101579363 entropy 0.03264415264129639
epoch: 87, step: 29
	action: tensor([[ 0.9993,  0.1125, -0.1214,  0.4952, -0.1514, -0.1917,  0.1027]],
       dtype=torch.float64)
	q_value: tensor([[-1.4880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9402256522558007, distance: 0.2797783574665287 entropy 0.03264415264129639
epoch: 87, step: 30
	action: tensor([[ 0.2964,  0.0411, -0.5337,  0.5230,  0.2216, -0.1388,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[-1.8870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.653580744566362, distance: 0.6735311796435873 entropy 0.03264415264129639
epoch: 87, step: 31
	action: tensor([[ 0.1954,  0.3299, -0.0188,  0.4044,  0.0784,  0.1931,  0.0779]],
       dtype=torch.float64)
	q_value: tensor([[-0.8240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7848584250154478, distance: 0.5307852736420345 entropy 0.03264415264129639
epoch: 87, step: 32
	action: tensor([[ 0.4915,  0.1694, -0.1572, -0.1015, -0.4556,  0.5208, -0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-0.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8033989247613389, distance: 0.5073990358283081 entropy 0.03264415264129639
epoch: 87, step: 33
	action: tensor([[ 0.5457,  0.2384, -0.2362,  0.1305,  0.0795,  0.0875, -0.3105]],
       dtype=torch.float64)
	q_value: tensor([[-1.8404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8869004811085056, distance: 0.3848460842363343 entropy 0.03264415264129639
epoch: 87, step: 34
	action: tensor([[ 0.5248, -0.0336, -0.2513, -0.2649,  0.0236, -0.0827,  0.1468]],
       dtype=torch.float64)
	q_value: tensor([[-1.3211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5238928267975683, distance: 0.7896034835078745 entropy 0.03264415264129639
epoch: 87, step: 35
	action: tensor([[ 0.5903, -0.0481, -0.5592,  0.8096, -0.0608,  0.3217,  0.3533]],
       dtype=torch.float64)
	q_value: tensor([[-0.8542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8329053950462905, distance: 0.467775969138529 entropy 0.03264415264129639
epoch: 87, step: 36
	action: tensor([[ 0.0897, -0.2271, -0.2541,  0.5024, -0.2206, -0.3890,  0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-1.4333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38820506590054205, distance: 0.8950754211719403 entropy 0.03264415264129639
epoch: 87, step: 37
	action: tensor([[ 0.9179, -0.1751, -0.0960,  0.4699, -0.2309,  0.0056, -0.0607]],
       dtype=torch.float64)
	q_value: tensor([[-0.5215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8344322230901351, distance: 0.465633911656823 entropy 0.03264415264129639
epoch: 87, step: 38
	action: tensor([[ 0.3560,  0.4318, -0.4651,  0.5335,  0.1403,  0.1094,  0.1701]],
       dtype=torch.float64)
	q_value: tensor([[-1.7288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7841254910768742, distance: 0.5316886317487614 entropy 0.03264415264129639
epoch: 87, step: 39
	action: tensor([[ 0.8996,  0.0020,  0.0080, -0.1735,  0.0173,  0.0226, -0.3694]],
       dtype=torch.float64)
	q_value: tensor([[-1.2975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6627336743478776, distance: 0.6645737512291823 entropy 0.03264415264129639
epoch: 87, step: 40
	action: tensor([[ 0.5492, -0.1553, -0.5953,  0.3888, -0.1525,  0.0441, -0.0559]],
       dtype=torch.float64)
	q_value: tensor([[-1.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.702473499505681, distance: 0.6241939002401091 entropy 0.03264415264129639
epoch: 87, step: 41
	action: tensor([[ 0.1132, -0.2502, -0.0655,  0.4504, -0.2047,  0.1515,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[-1.2129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48525074532308266, distance: 0.8210215499273698 entropy 0.03264415264129639
epoch: 87, step: 42
	action: tensor([[ 0.4861,  0.1166, -0.5676,  0.0636, -0.2213, -0.2424, -0.1571]],
       dtype=torch.float64)
	q_value: tensor([[-0.5160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7584165934177802, distance: 0.5624581884537732 entropy 0.03264415264129639
epoch: 87, step: 43
	action: tensor([[ 0.6799,  0.2839,  0.1138,  0.4572, -0.3992,  0.2091,  0.1253]],
       dtype=torch.float64)
	q_value: tensor([[-1.4113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9810822243012454, distance: 0.1573952889527014 entropy 0.03264415264129639
epoch: 87, step: 44
	action: tensor([[ 0.7451,  0.2475, -0.0787,  0.3361, -0.0642,  0.3055, -0.6569]],
       dtype=torch.float64)
	q_value: tensor([[-1.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9823617825754117, distance: 0.15197915977941068 entropy 0.03264415264129639
epoch: 87, step: 45
	action: tensor([[ 0.3160,  0.0991, -0.1413,  0.5149, -0.0431,  0.3584,  0.0952]],
       dtype=torch.float64)
	q_value: tensor([[-2.6472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8200705368458409, distance: 0.48540901230086386 entropy 0.03264415264129639
epoch: 87, step: 46
	action: tensor([[ 0.4263,  0.2019,  0.0030,  0.5597, -0.0345,  0.2704,  0.0788]],
       dtype=torch.float64)
	q_value: tensor([[-0.9237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9123996581253718, distance: 0.33869563045692697 entropy 0.03264415264129639
epoch: 87, step: 47
	action: tensor([[ 0.7205,  0.0421, -0.4552,  0.8477, -0.1393,  0.0510,  0.3461]],
       dtype=torch.float64)
	q_value: tensor([[-1.0334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9346582566622036, distance: 0.2925176339688514 entropy 0.03264415264129639
epoch: 87, step: 48
	action: tensor([[ 0.5550,  0.0011, -0.0880,  0.1005, -0.0339, -0.1731, -0.0610]],
       dtype=torch.float64)
	q_value: tensor([[-1.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7129092811480753, distance: 0.6131493463752217 entropy 0.03264415264129639
epoch: 87, step: 49
	action: tensor([[ 0.7109,  0.1936,  0.0178,  0.1375, -0.4252,  0.5399,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-0.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9558669559668721, distance: 0.24040230921774197 entropy 0.03264415264129639
epoch: 87, step: 50
	action: tensor([[ 0.1352, -0.1412, -0.1221, -0.3301,  0.0257,  0.0702, -0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-2.0205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19368810425729177, distance: 1.027562453016646 entropy 0.03264415264129639
epoch: 87, step: 51
	action: tensor([[ 0.3776,  0.1616, -0.2463,  0.5508,  0.3647,  0.3936, -0.2154]],
       dtype=torch.float64)
	q_value: tensor([[-0.5520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8515306887603985, distance: 0.4409354742257233 entropy 0.03264415264129639
epoch: 87, step: 52
	action: tensor([[ 0.8186,  0.4164, -0.0620,  0.1138, -0.0236,  0.2777, -0.4959]],
       dtype=torch.float64)
	q_value: tensor([[-1.1550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9875282580167424, distance: 0.1277968802479976 entropy 0.03264415264129639
epoch: 87, step: 53
	action: tensor([[ 0.7068,  0.1897, -0.7888,  0.1305, -0.3371,  0.1174, -0.1150]],
       dtype=torch.float64)
	q_value: tensor([[-2.6618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8890558846309137, distance: 0.3811613259525691 entropy 0.03264415264129639
epoch: 87, step: 54
	action: tensor([[ 0.1158, -0.0037, -0.3181,  0.7332,  0.1319,  0.4601,  0.1881]],
       dtype=torch.float64)
	q_value: tensor([[-2.4916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6372244522654145, distance: 0.689248288743932 entropy 0.03264415264129639
epoch: 87, step: 55
	action: tensor([[ 0.6907,  0.1771, -0.1957,  0.5709,  0.2127,  0.3367, -0.5711]],
       dtype=torch.float64)
	q_value: tensor([[-0.8245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.97191749299412, distance: 0.19176733426756334 entropy 0.03264415264129639
epoch: 87, step: 56
	action: tensor([[ 0.5052,  0.0631, -0.2005,  1.0303, -0.3974, -0.4402,  0.2815]],
       dtype=torch.float64)
	q_value: tensor([[-2.2876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9315952784461361, distance: 0.29929518701235497 entropy 0.03264415264129639
epoch: 87, step: 57
	action: tensor([[ 0.4647,  0.1831,  0.3125,  0.2380, -0.1930,  0.1363, -0.0901]],
       dtype=torch.float64)
	q_value: tensor([[-1.2351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8917240361122875, distance: 0.3765500612278058 entropy 0.03264415264129639
epoch: 87, step: 58
	action: tensor([[ 0.7459, -0.1661, -0.3361,  0.1819, -0.2867,  0.3140, -0.4926]],
       dtype=torch.float64)
	q_value: tensor([[-1.0298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7702805333489834, distance: 0.5484734290676349 entropy 0.03264415264129639
epoch: 87, step: 59
	action: tensor([[ 0.3604, -0.3180, -0.0599,  0.6839,  0.1120, -0.2034, -0.5087]],
       dtype=torch.float64)
	q_value: tensor([[-2.2618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6603806867303129, distance: 0.666887969633923 entropy 0.03264415264129639
epoch: 87, step: 60
	action: tensor([[ 1.0195,  0.1157, -0.3365,  0.5743, -0.2052,  0.0663, -0.5336]],
       dtype=torch.float64)
	q_value: tensor([[-0.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9781534935360617, distance: 0.1691405319322147 entropy 0.03264415264129639
epoch: 87, step: 61
	action: tensor([[ 0.9173, -0.2651, -0.1819,  0.3004,  0.1049, -0.0052, -0.0495]],
       dtype=torch.float64)
	q_value: tensor([[-3.3506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6894668973385629, distance: 0.6376915240328679 entropy 0.03264415264129639
epoch: 87, step: 62
	action: tensor([[ 0.5186, -0.0365, -0.3615,  0.2235, -0.1095,  0.2315,  0.1282]],
       dtype=torch.float64)
	q_value: tensor([[-1.3012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.772025992302514, distance: 0.5463857450566402 entropy 0.03264415264129639
epoch: 87, step: 63
	action: tensor([[ 0.1360,  0.4189, -0.0765,  0.6391, -0.0237,  0.0940, -0.2871]],
       dtype=torch.float64)
	q_value: tensor([[-0.9923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7407109968740213, distance: 0.5827049968244853 entropy 0.03264415264129639
epoch: 87, step: 64
	action: tensor([[ 0.5629,  0.4864, -0.2413, -0.0529, -0.5619,  0.3018, -0.0711]],
       dtype=torch.float64)
	q_value: tensor([[-1.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9253350956263309, distance: 0.3126906893181355 entropy 0.03264415264129639
epoch: 87, step: 65
	action: tensor([[ 0.2345,  0.2816, -0.4046,  0.3569, -0.0993,  0.2465, -0.0676]],
       dtype=torch.float64)
	q_value: tensor([[-2.4078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7164186773055239, distance: 0.6093902553109957 entropy 0.03264415264129639
epoch: 87, step: 66
	action: tensor([[ 0.1424, -0.2471, -0.0016, -0.1945, -0.4612,  0.2700,  0.4197]],
       dtype=torch.float64)
	q_value: tensor([[-1.1659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17437989494374706, distance: 1.039792841836255 entropy 0.03264415264129639
epoch: 87, step: 67
	action: tensor([[ 0.5989,  0.3240,  0.2440,  0.2955,  0.0862,  0.1740, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-0.7027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9665535976337111, distance: 0.20928176933324813 entropy 0.03264415264129639
epoch: 87, step: 68
	action: tensor([[ 0.3311,  0.2081, -0.2260,  0.1446, -0.6034, -0.1940, -0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-1.0374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.761794318039022, distance: 0.5585123123737321 entropy 0.03264415264129639
epoch: 87, step: 69
	action: tensor([[ 0.2983,  0.0876, -0.4108,  0.5340, -0.0823,  0.0470, -0.1932]],
       dtype=torch.float64)
	q_value: tensor([[-1.3466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7191079690988152, distance: 0.6064938513585671 entropy 0.03264415264129639
epoch: 87, step: 70
	action: tensor([[ 0.8379,  0.3372, -0.2850,  0.1956, -0.2826,  0.1062, -0.0918]],
       dtype=torch.float64)
	q_value: tensor([[-1.0735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9765042933061455, distance: 0.17540862706155463 entropy 0.03264415264129639
epoch: 87, step: 71
	action: tensor([[ 0.5727,  0.3390, -0.3240,  0.0241, -0.4605,  0.2924,  0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-2.3307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9063963766462798, distance: 0.35010879722661825 entropy 0.03264415264129639
epoch: 87, step: 72
	action: tensor([[ 0.5666, -0.0766, -0.6121,  0.2362, -0.4504, -0.5711, -0.5507]],
       dtype=torch.float64)
	q_value: tensor([[-1.9320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7151014852170512, distance: 0.6108038781963502 entropy 0.03264415264129639
