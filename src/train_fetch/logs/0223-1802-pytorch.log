epoch: 0, step: 0
	action: tensor([[-5.0474, -8.5445, -3.6635, -2.3454, -4.4477, -0.9223, -3.8232]],
       dtype=torch.float64)
	q_value: tensor([[-5.9570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.9970545804369806
epoch: 0, step: 1
	action: tensor([[-3.3518, -7.4121,  1.9076,  0.1186, -9.3468, -1.3621, 14.4511]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 2
	action: tensor([[-16.0481,  -4.3320,  -3.4657,  -4.1128,  -0.5336,   7.0495,   2.7861]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 3
	action: tensor([[ -8.0900, -12.6717,   0.3452,  -5.5709,   1.1165,  -2.8087,  -1.2646]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 4
	action: tensor([[ -5.2839, -14.8494,  -1.6637,  -1.9567,  -3.7746,  -9.7725,   6.0427]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 5
	action: tensor([[-14.0887,  -6.9401,  -4.2057,  -1.3256,  -5.7289,   7.3980,   8.7044]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5789799701592644, distance: 1.8377256535092004 entropy 2.998170092159314
epoch: 0, step: 6
	action: tensor([[-16.7859,  -9.0034,  -3.2728,  -8.0901,  -0.7266,   1.5690,   7.7147]],
       dtype=torch.float64)
	q_value: tensor([[-4.6956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.2294817726976306
epoch: 0, step: 7
	action: tensor([[-12.3700, -11.3709,   4.6740,   7.0045,  -4.8339, -18.6869,   7.4543]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8718901997499238, distance: 1.565657917463193 entropy 2.998170092159314
epoch: 0, step: 8
	action: tensor([[-4.9900, -5.4229, -5.2515,  5.8295, -7.6247, 11.2484,  8.4708]],
       dtype=torch.float64)
	q_value: tensor([[-4.7068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.026732214601098
epoch: 0, step: 9
	action: tensor([[ -7.4480, -10.5059,  -1.8131,  -8.5990,  -3.9274,  -2.9755,  15.9701]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 10
	action: tensor([[-9.0863, -4.7688, -1.6963, -3.8042, -9.0360, 10.9934,  3.5803]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 11
	action: tensor([[-10.0536,   0.9367,  -2.2182,   8.3365,  -0.8915,   7.7164,   9.5729]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 12
	action: tensor([[-14.3488,  -3.1116,  -5.3244, -10.5305,   1.6187,   5.2847,   2.3029]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 13
	action: tensor([[-6.8733, -9.8110,  2.0887, -2.6757,  1.6639,  3.9579,  5.1341]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 14
	action: tensor([[-16.7444,   3.3037,  -5.1536,  -6.3839,  -4.9499,  -7.2529,  13.8123]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 15
	action: tensor([[-10.6248,  -4.4201,   0.0407,   0.3907,  -6.4613,  -6.8006,   0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 16
	action: tensor([[-10.3450,   0.1613,   4.0831,   5.1982,  -0.5273,  11.6045,   2.6969]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 17
	action: tensor([[ -9.9379, -10.7856,  -7.8538,  -1.6690,  -0.3035,  -1.7246,   1.1386]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 18
	action: tensor([[-12.8637,   0.0814,  -3.2862,   3.6449,  -1.9824,  12.3523,  -1.5650]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 19
	action: tensor([[-3.7539, -3.6322,  5.0387,  7.1283, -0.2212, -2.6181, 16.9379]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 20
	action: tensor([[-10.1194,   2.6953,   2.0458,   0.2846,   4.6868,  -6.5316,   0.7539]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 21
	action: tensor([[-6.8298, -3.2054, -0.7687, -3.7467,  2.2223, -2.3468,  9.3734]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 22
	action: tensor([[-6.8599, -7.5452,  1.8209,  7.3061, -3.7673, 16.1723, 11.1161]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5283776487694773, distance: 1.41472501325938 entropy 2.998170092159314
epoch: 0, step: 23
	action: tensor([[-14.6064,   2.2545,   0.5830,   5.6910,  -0.1685,   7.0727,  12.2113]],
       dtype=torch.float64)
	q_value: tensor([[-4.2198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.233424426859519
epoch: 0, step: 24
	action: tensor([[-1.1499, -6.1211, -2.8089,  4.4037, -1.7347, 14.9687,  4.4730]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 25
	action: tensor([[ -7.0108,   0.8960,   1.2738,   3.1845,  -5.3308, -11.9800,   5.5223]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 26
	action: tensor([[-10.3295, -10.1425,  -6.3650,  -0.4836,  -1.1144,   8.0650,   2.6369]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 27
	action: tensor([[-9.2982, -5.3442, -7.3681, -4.9965, -2.5883,  7.4562, -1.2916]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 28
	action: tensor([[-14.2521, -11.4075,  -4.2111,   4.4858, -10.5297,  -0.3723,   9.9557]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 29
	action: tensor([[-3.6894, -1.4084, -2.2556, -0.7841, -3.8787, -8.2773, -3.2722]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 30
	action: tensor([[-12.4367,  -9.1812,   3.4634,   6.1911,  -5.2338,   0.4222,   6.2639]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 31
	action: tensor([[ -5.9271,  -8.8522,  -5.4396,   0.2437,  -3.4328,  -1.5797, -11.5971]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 32
	action: tensor([[-14.2935,  -2.6663,  -2.1518,   2.8506,  -2.8727,  14.6857,  12.8247]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 33
	action: tensor([[-9.2054, -5.9880,  7.1425, -1.1543, -7.7184,  0.2780,  4.4742]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 34
	action: tensor([[-12.1088,  -1.8824,  -1.1502,   3.2835,  -2.9859,   7.3048,   4.5740]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 35
	action: tensor([[-6.0189, -6.6951, -6.4669,  0.0489, -8.7315,  0.5972, 16.5833]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5229241568456826, distance: 1.4121987770243156 entropy 2.998170092159314
epoch: 0, step: 36
	action: tensor([[-21.5140, -16.0230,  12.3464,   3.0791,  -9.9394, -10.4056,   0.0391]],
       dtype=torch.float64)
	q_value: tensor([[-6.1789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3591962201222105
epoch: 0, step: 37
	action: tensor([[-7.4001, -3.8077,  8.6991, 13.5204, -0.8177,  5.1494,  1.2497]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 38
	action: tensor([[-14.0158,  -1.2509,  -0.3164,  10.7839,  -5.9850,  -6.4571,   0.0424]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4597039778157199, distance: 1.3825762396846177 entropy 2.998170092159314
epoch: 0, step: 39
	action: tensor([[-8.8754, -0.5595, -0.7339,  8.1417, -3.7627,  7.6688, -5.5353]],
       dtype=torch.float64)
	q_value: tensor([[-3.6628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.929087579689376
epoch: 0, step: 40
	action: tensor([[-17.8789,  -9.2300,   7.1001,   1.0852,  -0.3186, -20.6490,   4.8057]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 41
	action: tensor([[-10.2499,  -3.6969,  -1.6998,   1.2737,  -3.1851,  12.5023,  -0.0608]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 42
	action: tensor([[-9.9429, -5.0602, -3.7269, -2.2759,  0.0414, 11.2374, -3.0637]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 43
	action: tensor([[-4.7033, -7.2368,  0.2589,  4.8745, -7.7356, -1.8039, -6.7240]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 44
	action: tensor([[-19.1929, -11.2343,  -4.2524,   1.2966,  -8.0031,  -0.8043,  16.3668]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 45
	action: tensor([[-17.9048,  -1.6374,  -3.4487,   4.8903,  -7.4443,  -0.4472,  -2.6942]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 46
	action: tensor([[-11.0978,  -3.3104,  -1.1550,  13.4069,  -0.4398,   6.0116,  -3.3244]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 47
	action: tensor([[  2.2137, -10.3977,   3.3164,   2.7753,   2.6180,  -1.6207,   7.1851]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 48
	action: tensor([[-10.4062, -11.6306,  -3.8656,   3.1175,  -5.1758,   0.4162,   6.6711]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 49
	action: tensor([[-21.9230,  -5.0740,   5.7009,  -2.7712,  -3.8329,  11.3223,   4.7697]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 50
	action: tensor([[-10.0199,  -6.0926,  -1.3087,  -5.2988,  -2.1620,  -0.0834,   4.6351]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 51
	action: tensor([[-8.2712, -3.6751, -1.7002, -2.6694, -7.8121, -3.8915, -4.7515]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 52
	action: tensor([[-13.3592, -11.6100,   4.5632,  -6.6630,   2.5617,  -0.6374,   1.0189]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 53
	action: tensor([[-14.6078,   0.6800,  -1.5525,   2.7687,   1.9656,   6.3749,   3.0900]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 54
	action: tensor([[-10.7912,  -5.7554,  -3.5699,   4.5590,  -5.7867,  -0.4939,   3.9361]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 55
	action: tensor([[-6.9529, -7.5675, -4.0249,  6.5801,  2.3147,  5.1031, -1.3130]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 56
	action: tensor([[-8.6283, -7.4423,  0.3529, -1.0416, -0.2979,  2.2411, -6.5704]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 57
	action: tensor([[-5.0248, -0.6957,  6.9360, -3.2517, -2.1948, -1.3846,  3.0235]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 58
	action: tensor([[-5.5922,  2.5957,  0.4950, 10.2586, -5.7122, -1.6366, -3.7933]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 59
	action: tensor([[-11.8902, -10.4148,   6.4957,   2.0185,   0.5499,  -0.9010,   2.6323]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.345525576890221, distance: 0.9257698499127975 entropy 2.998170092159314
epoch: 0, step: 60
	action: tensor([[  0.8426, -13.4720,   9.9655,  -2.9314,  -4.2963,  -0.3158,  -3.1145]],
       dtype=torch.float64)
	q_value: tensor([[-5.4461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.1882127601741206
epoch: 0, step: 61
	action: tensor([[-12.9690, -12.0268,  -5.8110,  -1.2514,  -3.5265,   0.1000,   1.9227]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4842715205004515, distance: 1.3941624151071692 entropy 2.998170092159314
epoch: 0, step: 62
	action: tensor([[-8.8027, -3.0960, -0.5515, -7.7251, -5.6319,  6.6457, -4.8190]],
       dtype=torch.float64)
	q_value: tensor([[-4.4858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.040311368362538
epoch: 0, step: 63
	action: tensor([[-9.7822, -4.7776,  1.7022,  6.6494,  5.1122,  1.2604, -0.0245]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 64
	action: tensor([[ -8.1739, -11.5337,  -7.4684,  -3.5541,  -0.6003,  -1.6255,   3.0635]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 65
	action: tensor([[-11.4859,  -2.9103,  -4.9600,  -2.0911,  -8.4792, -11.0369,  12.8627]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 66
	action: tensor([[ -9.6701, -10.6304,  -3.1977,  -0.2832,   2.7339,  -0.1714,  -0.3921]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 67
	action: tensor([[-10.5847,  -4.1588,   1.2775,   1.3298,   0.5808, -11.4987,   2.4441]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 68
	action: tensor([[-12.7300,  -7.0530,  -8.3213,   3.7283,  -3.7498,   2.8768,   8.4068]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 69
	action: tensor([[-15.5728,  -2.6441,  -0.4130,   2.1199,  -2.6502,   4.1170,   7.7128]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 70
	action: tensor([[-9.8324, -9.5926,  4.6153,  5.3024, -3.3606,  2.5994, 12.4736]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 71
	action: tensor([[-9.6950, -8.4206,  4.3341,  0.3701, -4.2018, 14.1549,  2.3079]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 72
	action: tensor([[-7.9556,  4.0595, -0.3120, -5.0938, -9.0249, -6.9315,  6.7764]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 73
	action: tensor([[-13.1032,  -2.4488,  -4.6512,   0.2832,  -1.3187,  -3.7641,  -7.3626]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 74
	action: tensor([[-12.1565, -11.5469,   1.6938,  -1.5931,  -1.1904,   3.4441,   4.0293]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 75
	action: tensor([[-5.1766, -8.1853, -1.4321,  0.3648, -6.1040,  6.6011,  0.4531]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6522504477339304, distance: 1.4709388967758361 entropy 2.998170092159314
epoch: 0, step: 76
	action: tensor([[-7.0982, -5.6629, -2.8732,  0.8944, -0.5274,  3.3143,  3.7186]],
       dtype=torch.float64)
	q_value: tensor([[-3.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.7815736377918867
epoch: 0, step: 77
	action: tensor([[-3.9249, -5.8049, -4.7348, 10.8772,  5.4000, -9.2583,  9.5432]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 78
	action: tensor([[-16.3357,  -3.7835,  -2.2345,   0.3560,  -4.2745,  -1.3587,  -0.6539]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 79
	action: tensor([[-12.0106,   0.9241,   7.1820,  10.9927,  -3.6567,  -2.5678,   4.1280]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 80
	action: tensor([[-14.1310,  -8.5466,  -3.6824,  -2.9207,  -6.6333,   1.8210,   0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 81
	action: tensor([[-9.0356, -6.6433, -3.8380, -0.3663, -2.6108,  0.8507,  2.6954]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 82
	action: tensor([[-8.4086,  1.8222,  5.4178,  6.6627,  0.6326,  1.6535,  4.5713]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 83
	action: tensor([[-18.4468,  -5.2998,   4.1695,   4.2066,  -3.1977,   4.4318,   5.9053]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 84
	action: tensor([[-18.4417,  -7.9107,  -0.6724,  -1.1591,  -2.5210,  15.4597,   0.5382]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20459467731362002, distance: 1.2559639187336096 entropy 2.998170092159314
epoch: 0, step: 85
	action: tensor([[-12.3289,  -2.1339,   0.7080,   6.5670,  -7.3119,   0.3775,   8.0476]],
       dtype=torch.float64)
	q_value: tensor([[-6.8943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.301005272620586
epoch: 0, step: 86
	action: tensor([[-15.9205,   2.0245,  -7.2030,  -6.1167, -12.0652,  -5.7159,  -2.2601]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 87
	action: tensor([[-13.8452,  -7.4349,  -5.2677,  -1.4241,  -6.9992, -10.8054,  -1.9515]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 88
	action: tensor([[-9.7366, -0.8248,  2.4429,  7.7394,  4.0519,  6.0122,  6.3664]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 89
	action: tensor([[-17.6969,  -1.3652,   3.3808,  -1.1780,  -1.0907,  -1.8856,  -2.4548]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19251458847113123, distance: 1.2496504317838837 entropy 2.998170092159314
epoch: 0, step: 90
	action: tensor([[ -5.4459,   8.3207,  -5.1261,  -6.9074,  -6.7445, -18.0628,   7.2566]],
       dtype=torch.float64)
	q_value: tensor([[-8.1774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07929486822232823, distance: 1.0980370015417547 entropy 3.573429968307273
epoch: 0, step: 91
	action: tensor([[-9.1308, -2.3105,  2.4904,  7.3074,  0.2904, -0.0620,  2.3341]],
       dtype=torch.float64)
	q_value: tensor([[-3.8342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.6962076966828303
epoch: 0, step: 92
	action: tensor([[-3.3853, -5.1849, -4.1400,  3.8699,  2.7595, -4.0603, -2.0639]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 93
	action: tensor([[ -9.1538, -18.6189,   0.9685,  -1.2550,  -0.9325, -16.4624,  -3.6602]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 94
	action: tensor([[-7.1092, -9.5697,  7.2217,  0.9593,  1.6010,  5.3087,  7.3301]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 95
	action: tensor([[-9.0846, -0.6009, -4.5727,  6.2574, -7.8237, -4.0492,  3.4084]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 96
	action: tensor([[-14.4605,  -6.1180,   3.7239,  10.1181,   2.6264,  10.9641,   6.8042]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 97
	action: tensor([[ -9.7747, -10.0050, -11.2615,  -3.4920,  -3.9567,   4.2351,   4.9371]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 98
	action: tensor([[-6.9497, -1.7276,  0.2739,  3.5883, -6.2495, -2.6197, -2.5383]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 99
	action: tensor([[-7.5224, -8.5626, -0.1557,  4.4558, -5.9340,  7.4001,  0.2730]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 100
	action: tensor([[-18.0919,  -9.4992,  -3.4572,  -1.1876,  -6.4046,  -3.3310,   5.1174]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 101
	action: tensor([[-7.4352, -4.2434,  2.5685,  2.2758, -8.6987,  9.8855,  5.3719]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 102
	action: tensor([[-10.3824,   0.9508,  -4.3746,   5.5678,  -7.6831,  -2.3508,   2.9619]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 103
	action: tensor([[-10.8212, -12.6163,   4.5319,   7.2558,  -3.3910,  20.4782,  -9.6079]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 104
	action: tensor([[-11.1801,  -6.4809,   1.0345,   7.0244,  -3.7818, -11.4266,   3.7850]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 105
	action: tensor([[ -5.4906,  -2.4581,  -2.1854,   6.0222, -12.4471,   5.9916,   5.5803]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 106
	action: tensor([[-11.1605,   2.1185, -11.1450,   1.0361,  -4.5220,   8.0241,   3.7764]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 107
	action: tensor([[-6.3168, -0.1533,  3.6563,  8.1339,  0.1840, -4.1616, -5.7205]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 108
	action: tensor([[-11.7076,   0.7363,   1.3504,  10.2281,  -2.6938,   5.3606,   6.0044]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 109
	action: tensor([[-11.9906,  -3.9250,   0.4294,  -0.9734,  -5.3263,  -6.8461,   3.4184]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 110
	action: tensor([[-17.6610,  -3.6724,   7.3682,  -7.2776,  -7.1450,   0.9898,   1.2694]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 111
	action: tensor([[-11.0252,   0.2893,   2.4915,  10.1616,  -5.5038,  -4.6380,  12.9801]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 112
	action: tensor([[ -8.2268,  -5.5660,  -0.1846,   1.5484,  -5.5824, -10.1351,   0.4958]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 113
	action: tensor([[-6.3427, -7.2584,  3.2530, -3.5224, -2.6621, -3.9462, 12.7091]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 114
	action: tensor([[-13.7476, -12.7784,  -4.0780,   0.1001,   0.0973,  -1.1093,   9.5188]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0697836876801965, distance: 1.6463386471856705 entropy 2.998170092159314
epoch: 0, step: 115
	action: tensor([[-10.6211, -10.2082,   1.0372,   9.2576, -10.9570,  -4.0832,  15.1849]],
       dtype=torch.float64)
	q_value: tensor([[-6.5058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.309777004530834
epoch: 0, step: 116
	action: tensor([[-12.4266,   1.7179,  -0.2313,  -7.8641,   5.0343,   4.6403,   2.6887]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 117
	action: tensor([[ -5.5988, -11.4096,  -2.9116,   1.5175,  -2.7634,  -5.6844,   3.4346]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 118
	action: tensor([[-9.3568, -5.7988, -6.9168, -2.9800, -8.2150, -7.0383,  5.9948]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 119
	action: tensor([[-12.0747,   1.4131,  -3.0489,   5.8966,  -5.9222,  -1.5675,   2.5679]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 120
	action: tensor([[-16.8652,   1.6954,  -7.3226,  -0.9405,  -0.2364,   9.2838,   5.3854]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 121
	action: tensor([[-9.4447, -2.9739, -3.6145,  1.5110, -5.9704,  6.1869,  1.4678]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 122
	action: tensor([[-8.7572,  7.8855,  1.9196, -6.2374,  0.9603, -5.1355, -2.1605]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 123
	action: tensor([[-10.7587,  -1.5213,   5.1189,   5.9048,  -3.8963, -10.1613,   4.0368]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 124
	action: tensor([[-8.6887, -2.0404,  0.9805,  6.7791, -4.0939, -1.6251,  8.7289]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 125
	action: tensor([[-12.0174,  -7.8081,   5.5638,  -1.1823,  -2.3479,  -3.6283,   7.2943]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 126
	action: tensor([[-12.1314, -11.5543,   5.7200,  -5.1446,  -6.1581,   1.1928,  -0.4051]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 127
	action: tensor([[ -6.4695, -12.4225,   1.7134,  -3.0082,   6.5241,   3.3409,   7.6493]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
LOSS epoch 0 actor 658.3818015804583 critic 1926.2399249446732
epoch: 1, step: 0
	action: tensor([[-19.3555,   3.4578,   2.9841,   9.6065,   0.4227,   7.8702,   2.2970]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 1
	action: tensor([[-20.4278, -13.6458,  -5.4917,  15.2193,  -6.4470,   4.5555,   5.5579]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 2
	action: tensor([[-7.0106, -6.3965, -6.0286, -3.2225, -7.1706,  3.7348,  3.0123]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 3
	action: tensor([[-6.4199, 12.2629, -5.9277, -3.8588, -8.4197,  3.6756,  6.3064]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 4
	action: tensor([[-19.1114, -17.2895,   1.8665,   3.5537,   1.2089,  -3.6270,  -4.1585]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 5
	action: tensor([[-21.3120,   2.8772,   7.2733,   1.9951,  -2.6442,   8.7828, -17.1170]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 6
	action: tensor([[-28.5368, -12.4006,  -8.9179,   7.3704,  -7.1240,   6.9727,  16.1069]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4688655291533745, distance: 1.798065098784035 entropy 3.3119940110059054
epoch: 1, step: 7
	action: tensor([[-22.8766,   1.1003,   6.0729,  -0.5001,  -5.6250,  -3.5085,  15.3309]],
       dtype=torch.float64)
	q_value: tensor([[-10.3994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.6841512224207054
epoch: 1, step: 8
	action: tensor([[-16.6604,  -1.3217,  -7.3481,  -2.9226,  -7.8014,   9.0943,  -2.5175]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 9
	action: tensor([[ -6.1497,  -7.4478,  -0.8000,  11.0403,  -2.0235,  -4.7271, -10.6322]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 10
	action: tensor([[-4.0003,  0.7032,  1.0398, -0.3608, -2.3870,  4.5977,  7.5421]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 11
	action: tensor([[ -8.0762,  -6.0883, -14.6232,  -4.5942,   4.1601,  -6.7787,   5.0285]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 12
	action: tensor([[-7.2231, -5.5306, -3.1037, -2.5515, -5.6075,  2.9446, -0.6607]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 13
	action: tensor([[-12.4552, -11.6774,  -4.0307,   2.9862,  -3.4950, -26.8406,   5.1020]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 14
	action: tensor([[-7.0943, -6.0470,  4.4904,  5.4861,  3.9977,  6.6043,  7.6458]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 15
	action: tensor([[ -8.9387,  -5.4351,  -1.4337,   7.5084, -11.2033,   7.8337,  14.2272]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 16
	action: tensor([[-16.5762,  -2.7229,   5.3317,   1.6712,  -7.5625,   0.8543,   3.8987]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 17
	action: tensor([[-11.8688,  -3.5456,   5.6761,   0.4793,  -6.3426,  18.8394,  -3.1869]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 18
	action: tensor([[-12.5205, -13.6720,  -4.3764,   1.6032,  -3.4536,   1.6129,   9.9624]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3285512676131388, distance: 1.319003209553401 entropy 3.3119940110059054
epoch: 1, step: 19
	action: tensor([[-14.4414, -17.8856,  -1.8635,  -3.7104,  -8.9627,  24.2609, -10.0907]],
       dtype=torch.float64)
	q_value: tensor([[-7.5305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.676422394809348
epoch: 1, step: 20
	action: tensor([[-13.7031,  -9.3349,   7.1908,  -1.8737,  -3.8822,   1.3814,   9.8856]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 21
	action: tensor([[-11.4306, -12.6827,  -1.9159,   3.5911,  -7.8288, -13.5976,  12.3439]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 22
	action: tensor([[-23.1015,  -3.9657,   6.5631,  -4.8506,  -4.5797,  -3.5646,   0.2756]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 23
	action: tensor([[-13.1958,  -7.5740,   1.5583,   9.6081,  -5.0287,  -0.4891,  10.1853]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 24
	action: tensor([[-16.7208,  -7.6912,   7.4681,   2.3106,  -3.1195,   6.3332,   6.5843]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 25
	action: tensor([[-11.0325,  -9.3400,   2.6667,  -2.9724,  -3.8668,  -3.2162,   5.4510]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 26
	action: tensor([[ -7.2150, -11.1193,  -3.1537,   5.8263, -12.9524,   0.0611,  -1.3157]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 27
	action: tensor([[ -9.1927, -16.9408, -11.8126,   4.3800,   0.6700,  -4.8288,   6.6387]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 28
	action: tensor([[-12.8506,  -4.9674,   7.0226,  12.1704, -16.9946,  10.1836,   2.7413]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 29
	action: tensor([[-1.1654e+01, -7.8157e+00, -1.5874e+01, -3.6645e+00, -1.6994e+00,
          1.3274e+00, -1.2158e-02]], dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 30
	action: tensor([[-13.3355,   4.7252,  -1.4759,  -3.8659,  -5.8434,  18.9575,  20.7247]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 31
	action: tensor([[-16.5114,  -9.6500,   5.6822,  14.9557,  -3.5576,   0.9320,  -4.1317]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 32
	action: tensor([[-20.9131,   0.8839,  -1.6611,  -2.6563,  -3.5074,  13.8917,   4.3846]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 33
	action: tensor([[-24.9985,  -6.2111,  -6.5872,  -6.0034,   2.7301,  -4.1670,   2.2331]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 34
	action: tensor([[-14.1912, -14.1786,   0.9870,  10.2338,  -4.3918,   2.9155,   6.9390]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 35
	action: tensor([[-14.4934,  -8.7970,  11.4858,  -5.0206, -15.3857,  -4.5830,  -0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 36
	action: tensor([[-17.9651, -15.3738,  -1.0964,  -2.8948,  -6.6215,  12.7453,  14.3802]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 37
	action: tensor([[ -8.6855, -10.6066,   1.2257,   1.2651,  -8.9359,  -3.1931,  -3.4722]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 38
	action: tensor([[-24.7567,  -6.5760,   1.0189,   1.1801,   0.5105,  -8.6386,  21.7545]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11109647894302832, distance: 1.078906989290033 entropy 3.3119940110059054
epoch: 1, step: 39
	action: tensor([[-14.0691, -10.1668,   6.3287,  -6.0173, -12.1007,   3.4832,   4.5832]],
       dtype=torch.float64)
	q_value: tensor([[-8.6656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.5407209868366
epoch: 1, step: 40
	action: tensor([[-15.0085,  -5.2799,  -1.3215,   4.3137,  -1.8598, -11.2687,   7.5433]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 41
	action: tensor([[-14.0779, -13.5869,   4.0056,   4.2597,  -8.4124,  -0.9223,   3.2894]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 42
	action: tensor([[-16.7505, -14.5118,   0.4554,   4.6449,  -0.6669,  -6.5113,  -0.9158]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 43
	action: tensor([[-13.8495,  -2.5745,  -0.5544,   3.5691, -11.3856,   8.1442, -11.2546]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 44
	action: tensor([[-21.0492,  -6.8675,  -9.4698,  -0.0910,  -2.2611,  11.0079,  11.6622]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 45
	action: tensor([[-16.9252, -16.1160,   9.3998,  12.2819,   0.1075,  -3.1159,   1.1571]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 46
	action: tensor([[-24.0073,   0.6456,  -2.1659,  -5.1386, -10.1527,   7.3122,  -4.6473]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 47
	action: tensor([[ -7.7809,  -7.4381,   6.5360,   1.1906,   0.8832,   1.3610, -11.3471]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11328515072203893, distance: 1.2074242707142058 entropy 3.3119940110059054
epoch: 1, step: 48
	action: tensor([[-25.2667, -13.0653, -14.6543,  -0.0586,  -1.7421,   3.9384,  -7.1865]],
       dtype=torch.float64)
	q_value: tensor([[-5.8692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3269352132070162
epoch: 1, step: 49
	action: tensor([[ -3.2814,  -8.0475,  -0.5865,  -0.8942, -13.3949,   5.9180,  -1.6679]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 50
	action: tensor([[-11.5135, -14.7062,   3.6496,  -6.0165,  -7.4777,  -2.3155,   8.3579]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14415950918194365, distance: 1.0586517215202633 entropy 3.3119940110059054
epoch: 1, step: 51
	action: tensor([[-17.8321, -25.8347,   1.8187,  10.2430,  -0.1145,   4.4124,   6.0354]],
       dtype=torch.float64)
	q_value: tensor([[-9.7490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8038999107609897
epoch: 1, step: 52
	action: tensor([[-11.0283, -10.5958,   1.2332,   6.0047,  -4.7319,   2.5716,   0.0305]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 53
	action: tensor([[-13.5388, -23.2718,  -3.6047,  -0.4840,   2.3753,  -3.6450,   5.6664]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 54
	action: tensor([[-13.0344,  -0.5236,   1.3204,   3.6942,   4.6527,  -0.4483,   6.3006]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 55
	action: tensor([[-23.5156, -19.6311,  -6.2413,  -2.0897,  -2.5961,   5.1283,   6.3198]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 56
	action: tensor([[  0.1945,  -1.0958,   9.6322,   5.5147,  -3.5098, -13.4230, -19.9097]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17933968729635752, distance: 1.242728190466432 entropy 3.3119940110059054
epoch: 1, step: 57
	action: tensor([[-23.5219,   1.0538,   8.4996,   8.8143,  -0.4680,   3.4097,  11.5184]],
       dtype=torch.float64)
	q_value: tensor([[-9.5483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.7346975972384375
epoch: 1, step: 58
	action: tensor([[-21.7228,  -4.9827,   4.8017, -11.0592,   4.0362,   9.6805,   3.4080]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 59
	action: tensor([[-12.0281, -17.4876,  -6.8917,  12.6430,   1.7111,   8.5559,   5.4077]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7235069984523731, distance: 0.6017259826740252 entropy 3.3119940110059054
epoch: 1, step: 60
	action: tensor([[-11.3138,  -5.6628,  -0.2837,  14.1767, -16.1168, -12.6740,   5.1270]],
       dtype=torch.float64)
	q_value: tensor([[-6.9945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20193841833264348, distance: 1.022291843087985 entropy 3.2820407200399138
epoch: 1, step: 61
	action: tensor([[-20.5359,  -9.4387,  -8.3599,   1.4822, -14.4502, -16.0130,  10.8165]],
       dtype=torch.float64)
	q_value: tensor([[-6.9643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.332473439393015
epoch: 1, step: 62
	action: tensor([[-19.8780,  -5.7565,  -2.8976,   4.3647, -10.3851,  -8.2018,   4.1211]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 63
	action: tensor([[-16.8034,  -0.8538,  -1.7740,  -6.2814,  -3.2731,   4.0657,   4.5019]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 64
	action: tensor([[-20.5112,   1.4464,   1.1212,  10.3025,  -3.5527,  13.8744,   9.9234]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 65
	action: tensor([[-15.9998, -12.1833,  -2.5901,   8.3120,  -5.4484,   1.9194,  -6.4065]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 66
	action: tensor([[-4.8013, -9.4952,  1.3842,  8.4157, -8.2542, -4.7772,  6.9529]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 67
	action: tensor([[ -7.8152,  -2.9916,   1.1883,   1.1008, -17.6228,  -8.1955,  -4.7740]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 68
	action: tensor([[-16.7178,  -8.3053,  -2.2961,   6.2553,  -7.0611,  13.5817,   3.1802]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 69
	action: tensor([[-27.0962,  -0.4929,  -2.0121,   9.0924,  -6.8318,  -7.5234,  -6.8424]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2896021677301093, distance: 1.7315566598564696 entropy 3.3119940110059054
epoch: 1, step: 70
	action: tensor([[ -8.1893, -13.4379,   2.9825,  -2.4089,   9.8055,  23.4061,  24.8149]],
       dtype=torch.float64)
	q_value: tensor([[-8.5931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0250794658956615, distance: 1.6284623729446772 entropy 3.663886312583628
epoch: 1, step: 71
	action: tensor([[-20.5296, -23.7507,  -7.0531,  -5.9498,  10.4850,  16.2421,  32.6873]],
       dtype=torch.float64)
	q_value: tensor([[-13.0590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6569324941889871, distance: 1.4730215508202995 entropy 3.8728599485330677
epoch: 1, step: 72
	action: tensor([[ -9.0433,  -7.4964,   6.1757, -17.5283,  -3.8735,  -9.0008,  -4.6400]],
       dtype=torch.float64)
	q_value: tensor([[-10.3012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603491656091648
epoch: 1, step: 73
	action: tensor([[-17.6142,  -4.1120,  -5.0443,  20.2933, -10.6819,   1.2472,   5.7651]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 74
	action: tensor([[-20.5578,  -7.1982,  14.5814,  -7.5171,  -6.6262,   4.7601,   8.2471]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 75
	action: tensor([[-31.9521, -11.6298, -18.1529,  -1.8028,  -3.3795,  -3.7418,  -6.4796]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 76
	action: tensor([[-4.4951,  8.6564, -4.6045,  4.7645, -8.2613, -4.0297, 11.7166]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 77
	action: tensor([[-15.5684,  -8.0891,  -1.6722,  -4.7428,  -3.7781,   5.8746,   2.6754]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 78
	action: tensor([[-13.4040,   1.0457,  -4.4803,  -7.6391,  -9.4899,  11.7134,  16.6865]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 79
	action: tensor([[-17.4269, -14.1222,  22.4385,   5.6760,  -2.3373,  -1.7920,   6.9522]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 80
	action: tensor([[-12.5634, -14.6139,  -2.7988,   8.4326,  -7.9696,  -4.8733,  -0.7916]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 81
	action: tensor([[-3.0574, -7.3806,  3.7530,  0.8872,  6.1869, 10.9410, -5.3152]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 82
	action: tensor([[ -8.9891,  -8.5487,   3.1931,   2.6611,  -4.2665, -14.0157,  10.1114]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 83
	action: tensor([[ -0.9091,  -8.6921,   0.0263,   6.1679,  -9.6061,   0.0562, -11.1573]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 84
	action: tensor([[-11.3258, -15.9660,  -8.8017,  -3.1075,  -2.4608,   8.9285,   6.4707]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 85
	action: tensor([[ -9.0252, -11.4070,  -2.8214,   7.2115,  -3.2896, -14.4321,  -5.5833]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 86
	action: tensor([[-12.2978, -19.0785,  -6.0023,  15.6405,  -7.4948,  -3.2575, -10.5707]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 87
	action: tensor([[-15.0638, -12.4266,   0.4164,   2.8187,   5.2540,  -4.7236,  10.7045]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 88
	action: tensor([[-17.0553,   4.8487,   4.6052,  -3.4992, -10.0146,  -3.6143,  19.4639]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 89
	action: tensor([[-10.8503,  -7.3054,  -2.9074,   6.6615,  -8.7775, -13.6221,   5.1740]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 90
	action: tensor([[-13.2601,  -7.4109,  -0.1915,  -1.4156,  -7.1601,  -0.8982,   2.4681]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02691453728929938, distance: 1.1596417541837907 entropy 3.3119940110059054
epoch: 1, step: 91
	action: tensor([[-18.9917,   2.8475,   2.3348,  11.5386,  -9.8610,  -8.4455,   5.0726]],
       dtype=torch.float64)
	q_value: tensor([[-6.8980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.4977368051638504
epoch: 1, step: 92
	action: tensor([[ -8.0596, -16.7105,   2.0978,  -7.2945, -16.4042,   8.1463,   0.5539]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1334187052349498, distance: 1.0652740518632637 entropy 3.3119940110059054
epoch: 1, step: 93
	action: tensor([[ -9.5663,   1.1897, -13.8496, -11.4183,  -5.4080,  -4.5716,  -5.9785]],
       dtype=torch.float64)
	q_value: tensor([[-8.4037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.5527269767728593
epoch: 1, step: 94
	action: tensor([[-8.1225, -7.1708,  1.1311,  7.0609, -1.5397,  7.1298,  3.1389]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5185204671225878, distance: 1.816056833118493 entropy 3.3119940110059054
epoch: 1, step: 95
	action: tensor([[-26.4816,  -9.0296,  11.2774,   5.9087, -13.1343, -22.7748,  28.8556]],
       dtype=torch.float64)
	q_value: tensor([[-9.2495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.7380596214004833
epoch: 1, step: 96
	action: tensor([[ -9.1141,  -8.7939,   0.3389,   7.3274,  -8.2039, -14.6663,  -4.1195]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 97
	action: tensor([[ -9.4505, -15.2682,   6.1992,  -2.4247,  -4.1084,  15.9105,   7.3626]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 98
	action: tensor([[-21.7314,  -2.7808,   2.8617,   9.1229,   2.6234,   1.2859,   6.7055]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 99
	action: tensor([[ -6.4910, -19.5053,   3.4823,   5.0469,  -3.0714,  -3.8357,  -6.0260]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 100
	action: tensor([[ -7.6524,  -4.5743,   5.4503,  -9.8228,  -7.8982,  13.4074, -11.4346]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 101
	action: tensor([[-21.3866,  -6.8925,   1.7862,  -2.0613,   2.9437,   4.5117,  -3.6323]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 102
	action: tensor([[-17.3724,  -6.1527,   3.4167,   5.8196,  -1.9519, -15.6700,  -1.6103]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 103
	action: tensor([[-13.5480, -12.0013,  14.2849,  -8.7790,  -3.8746, -12.4519,   6.7502]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 104
	action: tensor([[-6.3979, -5.6767,  2.3423,  1.9634, -1.0282, 12.0647,  2.8522]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4988355624131413, distance: 0.8101152662010619 entropy 3.3119940110059054
epoch: 1, step: 105
	action: tensor([[-17.9950, -10.9028,   1.4577,   8.4682, -17.2847,  -0.5079,  13.0933]],
       dtype=torch.float64)
	q_value: tensor([[-7.9107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.5615457030438464
epoch: 1, step: 106
	action: tensor([[-10.1637,   0.3865,  -0.5014,   7.2368,  -5.3206,   7.9418,   9.5231]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 107
	action: tensor([[-14.8410,  -3.9044,  -4.8348,   1.1993,  -0.9424,  -1.1731,  -0.6887]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 108
	action: tensor([[-10.7012,   0.6921,  -2.3247,  -1.5545, -11.3890, -10.8751,   3.4770]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 109
	action: tensor([[-14.3007,  -7.5588,  -0.8668, -15.2377,  -4.1792,   8.5010,   3.6262]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011496400320261113, distance: 1.1509033761227527 entropy 3.3119940110059054
epoch: 1, step: 110
	action: tensor([[-31.0239,   5.1647, -10.5134,  21.5727,  -9.4973,  -9.8743,   9.5236]],
       dtype=torch.float64)
	q_value: tensor([[-8.8560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.7390097193284872
epoch: 1, step: 111
	action: tensor([[-15.5437, -12.5569,   1.2954,   7.5316,  -3.1979,   4.7402,   4.0222]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 112
	action: tensor([[-13.6310,  -3.2813,  -7.3645,  13.8059, -11.6853,  20.4077,  -2.6236]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 113
	action: tensor([[-24.1301,  -3.7383,   4.3832,   9.1950,  -5.0461,  -9.6165,  -0.7143]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 114
	action: tensor([[-20.6672,  -5.8367, -13.6973,  -0.2357,   1.9231,  -8.5850,   5.1163]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36968426825518463, distance: 1.3392662576772232 entropy 3.3119940110059054
epoch: 1, step: 115
	action: tensor([[-12.3821, -12.1971,   9.3045, -17.0802,  -3.7039,   6.6553,  10.8711]],
       dtype=torch.float64)
	q_value: tensor([[-8.4629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.48553770292422
epoch: 1, step: 116
	action: tensor([[-26.8678,  -3.9619,   6.7225,   8.3137, -12.4911,   5.3489,   6.5272]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 117
	action: tensor([[-8.4030, -2.3462,  3.3711, 15.8305, -2.1898, -2.1519, 17.7801]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 118
	action: tensor([[ -5.8203,   1.3111,   0.6860,   1.1501,  -4.2803,   0.8606, -10.3924]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 119
	action: tensor([[ -9.0701,  -0.3390,  -2.9195,  -1.4925, -13.8391,   1.1581,   1.5781]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 120
	action: tensor([[-22.9386,  -6.3719,  -3.0607,  -3.2424, -13.2409,  -5.5869,  -3.1278]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 121
	action: tensor([[-20.1170,   3.1107,   2.1502,   1.0510,  -0.8548,  11.9494,  13.0632]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 122
	action: tensor([[ -7.1772, -12.2928,   1.7899,   3.8263,   0.5798, -11.6576,  -0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 123
	action: tensor([[-4.3992, -9.5968, -7.2694,  0.6122, -3.4705, 10.8392, 11.9491]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 124
	action: tensor([[-16.8920,  -9.4168,   0.4390,   8.5188,  -1.5287,   8.8036,   8.8737]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 125
	action: tensor([[-14.1166,  -6.6607,  -8.3856,   7.3951,  -5.3632,   4.4860,  -5.8946]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 126
	action: tensor([[ -9.8731, -16.9169,  -0.1506,   7.2886,  -5.2859,   6.3426,  -6.3479]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
epoch: 1, step: 127
	action: tensor([[-7.0470, -1.8091,  2.0898,  0.0452, -1.4989,  6.5832, -5.5037]],
       dtype=torch.float64)
	q_value: tensor([[-8.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3119940110059054
LOSS epoch 1 actor 491.30323034293195 critic 1670.9386713011818
epoch: 2, step: 0
	action: tensor([[-14.6355,   0.9049,   4.4517, -17.0985, -14.4795,   8.5032,   1.3797]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 1
	action: tensor([[-20.1189, -18.4693,  -8.0707, -20.5719,  -8.8482,   4.1026,   6.4292]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 2
	action: tensor([[-25.6744, -18.1416, -11.0138,  -4.0738,   1.2397,  10.9522,  -3.9637]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 3
	action: tensor([[-12.9088,  -4.6800, -17.0427,  14.3743,   6.0899,  -1.8766,  -0.9438]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 4
	action: tensor([[-18.3039,  -5.3910,  10.8809,   5.3286,   1.2230,   4.7203,   8.0680]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 5
	action: tensor([[-19.4721, -32.0050,  -3.0482,  -5.2845, -15.8363,  19.9277,  10.1768]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 6
	action: tensor([[-13.0585, -13.1034,  -5.8507,   0.1204,  -0.0389, -12.5872,   3.8840]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8373627788701838, distance: 1.5511512615148002 entropy 3.604142754539228
epoch: 2, step: 7
	action: tensor([[-5.4954, -6.1819, -4.0079, -5.2337,  1.2005,  6.9058,  8.7208]],
       dtype=torch.float64)
	q_value: tensor([[-10.4922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.589915377654517
epoch: 2, step: 8
	action: tensor([[-19.8848, -16.7211,  -9.2493,   3.1530,  -2.7092,   6.1371,   1.1363]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 9
	action: tensor([[-21.3344, -25.8168,   1.0957,  16.8035,  -2.2193,   3.6191,   4.6777]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 10
	action: tensor([[-29.6113, -11.0150,   6.1499,  -2.7180,  -7.4839, -19.0712,   7.9029]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 11
	action: tensor([[-10.7911, -29.3518,   3.9434,   0.2659, -12.1909, -16.4589,  -6.3879]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 12
	action: tensor([[-22.6224,  10.4856,  -2.3061,  -5.2076,  -3.2784,   2.7002,   6.9288]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 13
	action: tensor([[-11.5208,  -2.6845,   2.5128,  10.1098,  -0.7112,  -3.7186,  -1.3368]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 14
	action: tensor([[-10.2079,  -2.0589,  -1.0116, -14.4756,  -4.1479,   3.7951,   5.7258]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 15
	action: tensor([[ -2.2017, -16.8583,   2.8896,  10.6979,  -2.2094,  -9.5906,  11.6804]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 16
	action: tensor([[-14.0316,  -9.8673, -11.6196,   6.4313,  -3.9593,  -0.2723,  -6.0491]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 17
	action: tensor([[-24.0934, -12.1538,   1.4527,   0.7840,  -7.0683,  -4.3017,  11.2119]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 18
	action: tensor([[ -4.8796, -21.3957,  -5.1039,  -0.5775,  -5.9802,  -0.8404,  -7.3778]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4116088921301875, distance: 1.3596085468012817 entropy 3.604142754539228
epoch: 2, step: 19
	action: tensor([[ -9.9818, -11.3347,  -0.5275,  -1.8041, -11.8123,  -5.1526,   8.7729]],
       dtype=torch.float64)
	q_value: tensor([[-8.6719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.386207016922603
epoch: 2, step: 20
	action: tensor([[ -8.8828, -10.5760, -19.1668,   4.1583,  -2.9399,   0.0716,  -5.6218]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 21
	action: tensor([[-10.1706,  -4.8365,   2.0056,   5.5426, -15.3134,   0.1095,  -3.2653]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 22
	action: tensor([[-15.3405,  -7.2297,  15.0031,   6.1708,   4.2079, -23.5029,  -7.8630]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8260462575696024, distance: 1.9237397833662777 entropy 3.604142754539228
epoch: 2, step: 23
	action: tensor([[-10.8157, -26.4282,  -0.7129,  -6.6424,  -3.7562,  24.9934,   9.4859]],
       dtype=torch.float64)
	q_value: tensor([[-15.1129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.027333074325112
epoch: 2, step: 24
	action: tensor([[-16.4126, -16.8526,   6.1688, -14.7770,  -3.1460,   7.6987,   5.4506]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 25
	action: tensor([[-10.3007, -25.0000,  -0.0943, -10.0606, -21.8343, -13.3704,  -2.9065]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 26
	action: tensor([[-13.1555, -16.1963,  -0.1268,  -0.2913,  -1.4800, -11.7747,  -1.9077]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 27
	action: tensor([[-15.4997,  -0.9316,   0.5046,   3.9189,  -6.1347, -26.4967,   0.3530]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 28
	action: tensor([[-29.9669,  -8.3905, -10.3151,  -2.3842, -12.9881,   4.7319,  -9.0198]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 29
	action: tensor([[-11.6602, -19.3894,  -8.1264,   9.2250, -10.9003,   4.8411,  -8.0114]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 30
	action: tensor([[-17.5252, -13.2214,   2.2568,   9.9971,  -9.8137, -11.6690,   4.8161]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 31
	action: tensor([[-24.0235,  -6.2134,  -2.7855,  -6.0476, -10.9628,  20.8863,  15.1284]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02778253822911203, distance: 1.1283358885609653 entropy 3.604142754539228
epoch: 2, step: 32
	action: tensor([[-37.5424,  -7.3647,  15.8260,  -8.4644,  14.3583,   4.0220,  -8.0006]],
       dtype=torch.float64)
	q_value: tensor([[-13.5669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.9495889653054
epoch: 2, step: 33
	action: tensor([[-18.3187,  -5.9036, -22.5238,  -6.8327,   4.4090,  -6.1413,   7.7963]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 34
	action: tensor([[-25.1695,  -0.7997, -10.0024,   1.9254, -17.9056,  -0.1441,  -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12411422101933345, distance: 1.0709777069746067 entropy 3.604142754539228
epoch: 2, step: 35
	action: tensor([[-22.1155,   1.1304,  -8.5421,  -3.3475, -19.5603,  -1.4309,  13.1672]],
       dtype=torch.float64)
	q_value: tensor([[-8.8445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.7091705659546466
epoch: 2, step: 36
	action: tensor([[ -7.7528, -19.9068,  18.5475,  -1.8708,   0.4696,  -0.6692,  -3.9530]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0936727580923774, distance: 1.0894296985963012 entropy 3.604142754539228
epoch: 2, step: 37
	action: tensor([[-13.8392,   4.6404,   4.5282,   7.8356,  -5.7511, -11.1733,   3.2893]],
       dtype=torch.float64)
	q_value: tensor([[-10.0778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8032374495565264
epoch: 2, step: 38
	action: tensor([[-22.3796, -15.5889,   1.1043,   6.1170,  -2.7119,  -5.3737,  -5.8032]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 39
	action: tensor([[-14.3887, -15.3026,   6.9717,   3.0910, -12.8977,   4.0975,   6.7165]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 40
	action: tensor([[ -3.2060, -13.1393,  -2.7226,  10.8383,  -6.3068,   9.8759,   7.4589]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 41
	action: tensor([[-20.1394,   1.9156, -20.4286,  16.3932,   7.1304,  -5.7939,  -7.3231]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 42
	action: tensor([[-9.6298, -4.8311, 12.0515,  2.5562, -0.6436,  0.8164, -3.4222]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 43
	action: tensor([[ -4.3565, -13.6898,  -3.1660,  -9.5750,   9.5216,  14.6788,  14.8746]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 44
	action: tensor([[ -9.3275,  -7.3933,  -9.9285, -11.3960,  -6.9263,   3.9755,   2.4693]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 45
	action: tensor([[-18.6260, -17.7238,   0.3805,  16.8750,  -3.4484,  -8.2062, -15.8313]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1614474785936827, distance: 1.047904795565592 entropy 3.604142754539228
epoch: 2, step: 46
	action: tensor([[-14.9510, -17.3969,   1.8835,  43.2001,   3.6389, -12.5512,  -7.4142]],
       dtype=torch.float64)
	q_value: tensor([[-13.0546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8867256004259687
epoch: 2, step: 47
	action: tensor([[-16.4512, -24.0935,  -5.0382,  -4.8975,  -2.2635,  -3.2343,  29.6191]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 48
	action: tensor([[-25.4288, -14.9758,   4.0597,   5.9416,   7.1676,   3.2009,   5.5252]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 49
	action: tensor([[-11.1395, -12.2140,  -0.8753,   8.3410,   0.7801,   5.2357,   4.1264]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 50
	action: tensor([[ -8.3281,  -9.3303,  18.9752,  -3.5275,   3.1356,  -5.3047, -20.1027]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 51
	action: tensor([[  2.6768, -20.6569,  19.9089, -12.3630,   0.5050,  13.8018,   9.9109]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 52
	action: tensor([[-19.6610, -18.6414,  -7.6371,  -4.7535,   6.5639,   3.2050,  23.5961]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 53
	action: tensor([[-12.3568,  -3.8481, -17.9165,   0.0744, -16.8198,  15.7358,   8.7381]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 54
	action: tensor([[-23.5446,   0.1372, -13.0772,   8.7993,  -2.7896,  13.8101,   8.9646]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 55
	action: tensor([[-10.5724,  -4.1548,   8.6080,   7.3981,   1.2252,  -1.9367,  19.2004]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 56
	action: tensor([[-19.4859, -11.2593,  -1.9305,  15.5640,   1.3760,  -7.4386,   9.3863]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1953378432326529, distance: 1.2511288186599012 entropy 3.604142754539228
epoch: 2, step: 57
	action: tensor([[ -1.4607,  -8.6433,  12.7257,  15.4161,   4.7087,  -2.8591, -23.7089]],
       dtype=torch.float64)
	q_value: tensor([[-13.6270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.925706918157567
epoch: 2, step: 58
	action: tensor([[-28.7196, -13.4758,   8.9156,   1.6278, -20.0890,   6.5967,  -5.8882]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37118431504978866, distance: 1.3399994238390158 entropy 3.604142754539228
epoch: 2, step: 59
	action: tensor([[-26.2287, -11.4415,  -7.1559,   2.3982,   2.7986,   6.8306,   6.6284]],
       dtype=torch.float64)
	q_value: tensor([[-10.7353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8741807428587984
epoch: 2, step: 60
	action: tensor([[-26.8670,  -5.6908,   0.5508,  15.6704,  -6.6021,   4.5162,  10.8916]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 61
	action: tensor([[-24.9035,  -7.1920,   3.2312,  -4.3761,   3.6913, -15.4167,  14.2731]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 62
	action: tensor([[-16.6640, -17.6563, -14.1595,  14.0097,  -5.7936,  -9.9603,  -3.9803]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 63
	action: tensor([[-16.3964,  -7.7775,   8.3740,   7.5303,  15.0344,   4.8408,   0.4162]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 64
	action: tensor([[-2.9805, -5.9459, 12.5886,  5.1259,  0.1228, -0.5678, -0.6476]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 65
	action: tensor([[-18.6337,  -2.5572, -13.0144,   1.4718,   4.1770,  -0.6272,  -0.9742]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 66
	action: tensor([[-13.8697,  -6.2547,   4.6593,  -5.6076,  -1.8235,   2.1947,  11.3671]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5541146870776812, distance: 1.4265868801508526 entropy 3.604142754539228
epoch: 2, step: 67
	action: tensor([[-19.5850,  -8.6879, -16.9234,  -5.0513,  -0.7490,  -7.6528,  13.8176]],
       dtype=torch.float64)
	q_value: tensor([[-14.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.934234628081229
epoch: 2, step: 68
	action: tensor([[ -8.4319, -14.4152,  -2.6667,   7.9460,  -7.9871, -11.5588,   9.0742]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 69
	action: tensor([[-10.7156,  -4.9182,  14.3653,   8.6697, -10.4326,  10.8645,  10.1139]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 70
	action: tensor([[-30.2316, -12.4629,  -8.0028,  16.7205, -14.1391, -14.9224,   7.3785]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18382255700574657, distance: 1.2450878602852429 entropy 3.604142754539228
epoch: 2, step: 71
	action: tensor([[-32.5551, -30.4923,   3.4531,   5.7057,   1.3271,   4.7675,  12.3376]],
       dtype=torch.float64)
	q_value: tensor([[-13.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10866111328841344, distance: 1.2049141390859932 entropy 3.9240688232936805
epoch: 2, step: 72
	action: tensor([[-11.7657,  -6.3212,   0.6493,  -1.0533,  11.3737,   3.5852,   7.4748]],
       dtype=torch.float64)
	q_value: tensor([[-14.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8957839250385597
epoch: 2, step: 73
	action: tensor([[-22.8757,  -4.8621,   7.5966,  -4.5224,  -2.5110,   5.6130,   0.1933]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 74
	action: tensor([[-10.8620, -16.7221,  -3.4625,  -4.9817,   2.3899,   7.7589,   0.2023]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 75
	action: tensor([[-16.8842,   0.4558,   3.7164,   5.5877,  -2.7066,  10.4789,   0.1157]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 76
	action: tensor([[-33.3624,   9.4506, -10.6342,  -8.9442,  -8.1619,   9.9741,   6.9774]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 77
	action: tensor([[-22.8664,  -7.1504,  -4.4156,   2.5885,  -2.9189,   9.8695,  15.5093]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 78
	action: tensor([[-17.2027,  -8.1995,  -2.6092,   2.6661,  -6.0767,  -2.1819, -12.5467]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 79
	action: tensor([[-23.5768,  -7.4338,   7.8156,  13.5759,  -1.2906,  -0.3981,   0.2015]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 80
	action: tensor([[-14.3146, -24.8626,   1.4295, -10.6183, -12.7577,  -2.5753,  12.5253]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 81
	action: tensor([[-26.4164,  -3.5247,  -4.5900,  -5.5670,  -1.1540,  -4.9556,   1.3012]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 82
	action: tensor([[-12.8305,   1.1369,   5.6218,  -9.4431,  -2.0474,   0.5036,   4.9029]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 83
	action: tensor([[-15.6493, -16.7646,  -6.0981,   0.3351,  10.1059, -24.7550,  13.7444]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 84
	action: tensor([[-23.1059, -11.4534,  -4.0079,  16.5180,  -5.3145,   1.0497,   4.2255]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 85
	action: tensor([[-11.6464,  -1.8659,   4.0874,   4.0181, -22.5768,  -2.8720,   1.5571]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 86
	action: tensor([[-24.8682,   5.6827,   3.6159,  13.6245,  -1.2689,   2.4432,  -5.0830]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 87
	action: tensor([[-19.3713,  -3.8127,  13.5433,  -0.0911,   0.1211, -13.9770,   2.6959]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 88
	action: tensor([[-5.1750, -4.6722, -8.3103,  1.9583,  2.8682,  1.3665, 18.7554]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 89
	action: tensor([[-19.0581, -11.6217,  -0.7354,  -5.0789, -17.8550, -15.8790,  -4.8200]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 90
	action: tensor([[-19.6129, -18.1885,  -0.6417,  10.2008,  -5.9174,  14.9107,   2.1792]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 91
	action: tensor([[-16.0440, -10.7035,  -8.8205,   1.0153,  -4.0078,  16.6063,  -5.8379]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 92
	action: tensor([[-11.1569, -14.3731,  12.2177,  -5.7747,  -1.6090,  -4.6501,  -4.1507]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 93
	action: tensor([[-18.5529, -28.1105,   3.5243,  -6.9146,  -9.0343,  -8.5178,   4.7610]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7627262589047215, distance: 1.5193195644662767 entropy 3.604142754539228
epoch: 2, step: 94
	action: tensor([[-11.0789, -16.3885,   4.7680, -14.3172,  12.0736, -11.2979,  12.8386]],
       dtype=torch.float64)
	q_value: tensor([[-19.0131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.204690522969984
epoch: 2, step: 95
	action: tensor([[-23.2726, -12.5938, -20.1606,  -9.1565, -28.4024, -13.8737,   5.4807]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 96
	action: tensor([[  4.2309, -21.5074,  -5.9458,   2.7522,   0.4713,   9.1226,  11.2699]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 97
	action: tensor([[ -4.6470, -12.2058,   8.9300,  -7.0659,  -5.0822,   1.3367,   9.6626]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 98
	action: tensor([[-23.5287, -17.7316,  -6.0078, -12.0602,  -4.7514,  -1.9630, -12.3281]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 99
	action: tensor([[-20.7581,  -7.7276,  -9.7409,   7.5236, -10.5889,  11.5682,   4.9080]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 100
	action: tensor([[-21.1900,  -6.1283,  -1.3074,   0.7760,   3.2019, -13.5357,  20.2247]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0290929061199918, distance: 1.6300752729480061 entropy 3.604142754539228
epoch: 2, step: 101
	action: tensor([[-19.1754, -15.3794,   2.1874,  -2.0951, -10.8041,  19.0861,  37.3366]],
       dtype=torch.float64)
	q_value: tensor([[-15.3295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.0611525234006
epoch: 2, step: 102
	action: tensor([[-26.5491, -14.4712, -13.1598,   0.6686,  11.8163,   7.6453,  -9.9610]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2304846657361188, distance: 1.7090560822017626 entropy 3.604142754539228
epoch: 2, step: 103
	action: tensor([[-25.0601, -10.9043,  27.6461,  -5.9645, -16.1629,  -0.5460,   6.8498]],
       dtype=torch.float64)
	q_value: tensor([[-9.4945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.7449305609084824
epoch: 2, step: 104
	action: tensor([[-15.8103,   5.1981,  -8.3042,   2.2430, -12.7037,  -6.7232,  -8.1535]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 105
	action: tensor([[-27.1420,  -9.1678, -14.8458,   0.0435, -12.7663,   3.5746,  -1.9312]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 106
	action: tensor([[ -9.6677,   5.4484, -11.2199,  -0.4670,  22.1456,  15.5135,  15.7324]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 107
	action: tensor([[ -1.0448,  -8.7078,   3.8750,  -3.8331, -11.6027,   4.6786,   3.1309]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 108
	action: tensor([[-25.9786, -23.7177, -11.1668,   8.5660, -18.2028,   9.8629,  11.6479]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 109
	action: tensor([[ -9.8003,  -5.7649, -10.5044,  12.1540,  -9.4506,   5.4757,  14.2713]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 110
	action: tensor([[-21.2281, -11.3935,   3.1590,   9.9691,   1.2675,  13.3201,   5.6284]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 111
	action: tensor([[-35.2859, -17.6812,  -2.8722,  -3.0145,  -0.2853, -19.8978,  -0.8239]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 112
	action: tensor([[-10.2067,  -0.3119, -17.7400,   0.5570,  -5.6735, -13.4800,  -0.5416]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 113
	action: tensor([[-9.4908, -8.4210, -9.5865, -6.5422,  7.7751, -2.8125, -3.4808]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 114
	action: tensor([[-22.5024, -17.6965,   2.2613,  -7.1385,  -8.4307,  -3.9744, -20.7867]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 115
	action: tensor([[-14.1333, -20.1432,   4.4696,   7.8267,  -2.9507,  -1.3634,   4.4105]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3719810521987439, distance: 1.7624317217379046 entropy 3.604142754539228
epoch: 2, step: 116
	action: tensor([[-29.5206, -16.2963,  -5.0599,   5.3059,  -2.9558, -17.3340,   1.9708]],
       dtype=torch.float64)
	q_value: tensor([[-16.8071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.173824148883691
epoch: 2, step: 117
	action: tensor([[-28.8824, -14.8384, -14.7137,   8.6068, -14.6371,  14.2604,   7.7906]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7647592467880455, distance: 1.5201954430041924 entropy 3.604142754539228
epoch: 2, step: 118
	action: tensor([[-21.4029, -40.7913,  -7.5873, -21.1475,   7.5167, -12.2929,   2.5906]],
       dtype=torch.float64)
	q_value: tensor([[-15.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5962307733404373, distance: 1.8438616852713872 entropy 4.122643023368978
epoch: 2, step: 119
	action: tensor([[-21.1864,  -7.3057,  11.7869,   2.3223, -15.9980,  -4.4505,  12.4689]],
       dtype=torch.float64)
	q_value: tensor([[-11.9272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.96036523493616
epoch: 2, step: 120
	action: tensor([[-17.5110,  -5.6005,  -4.7996,  -2.3214,  -0.8215,   8.6329,  -6.1795]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23371367669035392, distance: 1.0017335754950416 entropy 3.604142754539228
epoch: 2, step: 121
	action: tensor([[-32.9570,   5.8274,   1.6079,   3.9708,   4.0507,   4.5056,  -7.0160]],
       dtype=torch.float64)
	q_value: tensor([[-12.0182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.674889480681011
epoch: 2, step: 122
	action: tensor([[-30.1720,  -5.7738,   3.0892,  15.0906,   2.2076,  21.5916, -22.9698]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5148571105392281, distance: 1.408453548017671 entropy 3.604142754539228
epoch: 2, step: 123
	action: tensor([[-39.0003, -20.1470, -16.8608,   9.9982,   2.8349, -14.1543,   9.9394]],
       dtype=torch.float64)
	q_value: tensor([[-12.4001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.923273742828966
epoch: 2, step: 124
	action: tensor([[-19.2324,  -4.2168,   1.4123,  -2.5648,  -2.6052,  -2.0802,   8.3761]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 125
	action: tensor([[-10.0171, -16.0630, -12.9265,  10.5433, -18.8318,   0.6393,   8.8978]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 126
	action: tensor([[-10.8919, -15.1393,  21.8483,  19.5767,  -8.6529,  10.9593,  -4.9354]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.604142754539228
epoch: 2, step: 127
	action: tensor([[-18.2825,  -5.8405,  -1.2957,   5.6677, -17.2223,   0.8910,  -8.2019]],
       dtype=torch.float64)
	q_value: tensor([[-12.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15465198491068932, distance: 1.229652073652626 entropy 3.604142754539228
LOSS epoch 2 actor 319.2616123872921 critic 1354.1184724742034
epoch: 3, step: 0
	action: tensor([[-17.4086, -16.6010,  17.6998,   7.6994, -15.1172,  14.2159,  -2.4994]],
       dtype=torch.float64)
	q_value: tensor([[-14.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.015452999289527, distance: 1.6245872131833232 entropy 3.8651062843105373
epoch: 3, step: 1
	action: tensor([[-52.6488, -25.2672,  39.9952,  34.8152, -27.7224,  -5.9203, -11.3229]],
       dtype=torch.float64)
	q_value: tensor([[-18.7342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48487236764603114, distance: 1.3944445716190108 entropy 4.382598961214127
epoch: 3, step: 2
	action: tensor([[-69.8967, -15.1456,  -6.5009,   5.3004,   0.9583, -31.3930, -33.6565]],
       dtype=torch.float64)
	q_value: tensor([[-22.5408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.505437004176825
epoch: 3, step: 3
	action: tensor([[-30.2330,   3.9519,   4.7506, -10.0976,  -5.8615,   1.7013,  20.8460]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 4
	action: tensor([[-34.6733,   2.0432,   3.1037, -11.4427, -13.3469, -21.1292,  -7.8642]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 5
	action: tensor([[-27.3505,  -8.2018,  16.4092,   0.5294,   6.1758,  18.5612,  10.4127]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6204029272387674, distance: 1.8524254167318035 entropy 3.8777897468041735
epoch: 3, step: 6
	action: tensor([[  1.7233, -15.2634,   6.9988,  -3.4087,   6.2419, -18.9923,   6.6214]],
       dtype=torch.float64)
	q_value: tensor([[-18.8915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.288524099060558
epoch: 3, step: 7
	action: tensor([[-19.0262,   0.5232,  -8.5775,  14.4834,  -9.1691,  18.4521,   3.2075]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4134954628692349, distance: 1.3605167804054454 entropy 3.8777897468041735
epoch: 3, step: 8
	action: tensor([[-47.5823,  -1.2254,   0.6404,  15.3087,  -2.3782,   2.9389,  40.9982]],
       dtype=torch.float64)
	q_value: tensor([[-22.5332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.524406848093961
epoch: 3, step: 9
	action: tensor([[-14.8973, -17.0210,  -6.9926,  10.0325,   9.3898,  10.6400,  14.1167]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 10
	action: tensor([[-39.6901, -10.7333,  -2.3533,   9.7479,   5.8325, -12.1058,  -2.7908]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 11
	action: tensor([[-15.0275, -23.6932,  13.6227,  -8.0461,  -4.6477, -37.2479,   9.3175]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.916024071118277, distance: 1.5840072816144344 entropy 3.8777897468041735
epoch: 3, step: 12
	action: tensor([[-25.1678, -22.0748,  -5.6401,  -4.7958,   4.7649, -11.2697, -12.2416]],
       dtype=torch.float64)
	q_value: tensor([[-16.4731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.112550552635287
epoch: 3, step: 13
	action: tensor([[ -9.2120, -39.9061,  12.0699,  -6.3036, -20.4172,   9.6626, -13.4729]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 14
	action: tensor([[ -9.4309,  -2.0999,  -0.9283,   0.0725,  -4.3774, -26.1979, -17.4261]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 15
	action: tensor([[-20.7636, -10.5945,   8.1547,  -9.9338,  -5.0118,  -0.1406, -15.8115]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 16
	action: tensor([[-29.9889, -22.2883,  13.8870,  15.2949,   0.9188,   8.4039,   0.7328]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 17
	action: tensor([[-20.3343, -17.1062,   1.4071, -13.7515,  -8.2290,  -9.8304,  19.7953]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 18
	action: tensor([[-2.3342, -7.5087, 17.9433, -4.6889, -2.6623,  4.2685, 12.4551]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 19
	action: tensor([[ -8.7838, -22.4998,   1.9735, -14.0495, -25.6320,   4.3318,  24.7953]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 20
	action: tensor([[-27.0439, -15.3554,  -5.0420,   2.6906,   1.5467,  -5.5321,  15.5889]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 21
	action: tensor([[-28.0616, -31.9714,   1.7888,  -6.2007, -16.6458, -35.5686,  -4.1763]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 22
	action: tensor([[-17.3124, -24.9954,  -4.3811, -13.8664, -21.2303,  -2.8170,   9.1051]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 23
	action: tensor([[-17.3731,  -9.2287, -13.4221, -13.6608, -20.3259,  12.7283,   9.8428]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 24
	action: tensor([[-24.0727, -17.6498,   5.6654,   3.8081, -11.0285,  -2.7954, -31.9251]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 25
	action: tensor([[-11.1013, -18.5009,   5.3200,  -4.6308, -13.1455, -10.7924,  -4.1870]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 26
	action: tensor([[  1.0627,  -0.4260,   2.6048,  -7.8260, -15.7164,  26.6533,  13.2732]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 27
	action: tensor([[-26.4187, -17.2125,  18.7857,  -3.8210, -15.1136,  -1.3210,  -9.5821]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 28
	action: tensor([[-28.5929,  -5.9070, -16.3596,   0.5802,  -7.5030,  -7.0025,   1.1894]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1711458848241194, distance: 1.6861693710671783 entropy 3.8777897468041735
epoch: 3, step: 29
	action: tensor([[-30.7346, -25.6201,  -2.5858, -18.4934,  13.9158,   7.8642, -24.0510]],
       dtype=torch.float64)
	q_value: tensor([[-20.7558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1307037665632693, distance: 1.6703913411790967 entropy 4.309578612030008
epoch: 3, step: 30
	action: tensor([[-26.0693,   2.4915, -29.3072,  15.5516,   0.9977, -30.0966,   4.7414]],
       dtype=torch.float64)
	q_value: tensor([[-16.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.072887131686479
epoch: 3, step: 31
	action: tensor([[-14.3846,  -6.2969, -13.3344,  17.2016,  -2.1345, -11.5545,   8.9795]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7396073359818027, distance: 1.8940910568106304 entropy 3.8777897468041735
epoch: 3, step: 32
	action: tensor([[-18.7056, -37.7542,  -5.4462,  17.7844, -15.7042,  15.8347,  29.8622]],
       dtype=torch.float64)
	q_value: tensor([[-19.0581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8869513337167563, distance: 1.5719439008669962 entropy 4.1467285749213465
epoch: 3, step: 33
	action: tensor([[-47.3108,   4.7124,   0.6206,   7.1130,  -8.8048,   6.2571,  -4.0527]],
       dtype=torch.float64)
	q_value: tensor([[-22.8742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.422193402710999
epoch: 3, step: 34
	action: tensor([[ -7.9776, -20.3653,   1.6396,  -9.2030,   7.0287, -14.8422,  29.2768]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 35
	action: tensor([[-23.6859,  -2.8081,  -5.3010,  -0.5329,  -9.7728,  11.6052,  16.6330]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 36
	action: tensor([[-26.0622, -17.2569,   2.5473,  16.1648,   3.7024, -10.6038, -14.9255]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 37
	action: tensor([[ -6.2692, -21.9600,  10.4672,  -2.8118,  18.0390, -11.7890,  27.2039]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 38
	action: tensor([[-20.4034, -10.3410,  -0.6482,  10.6711, -15.8607,  16.0617,  -0.1302]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031705134598551066, distance: 1.126057344997245 entropy 3.8777897468041735
epoch: 3, step: 39
	action: tensor([[-18.5448,  -1.4549, -16.1317,  -0.3147,  -2.8975,  42.7817,   3.4799]],
       dtype=torch.float64)
	q_value: tensor([[-18.8354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6979322138005561, distance: 1.4911346675079014 entropy 4.163313536938857
epoch: 3, step: 40
	action: tensor([[-32.4224, -17.9002,   7.6426,   0.9274, -10.1272, -56.8126,  25.3677]],
       dtype=torch.float64)
	q_value: tensor([[-25.6404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9978265531383208, distance: 1.6174675771074418 entropy 4.567164096105372
epoch: 3, step: 41
	action: tensor([[-17.0696, -13.5513, -10.0768,   0.4570, -13.2397,  -9.2636,   7.8214]],
       dtype=torch.float64)
	q_value: tensor([[-12.2681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.990873126415489
epoch: 3, step: 42
	action: tensor([[-24.2849,   4.3181, -29.9046, -11.7443,   1.4717,  12.3246,  16.7708]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 43
	action: tensor([[-24.6149,  -4.9055, -13.7754,  11.4251,  -0.9197, -18.5158,  -3.8141]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 44
	action: tensor([[ -6.5529,  -8.4815,  -1.9118,  10.7590, -18.6271, -18.9072,  -4.5331]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 45
	action: tensor([[-36.6494, -27.1186,  -6.7898,  20.1402, -16.6922,   7.0184,  15.4870]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1563569350597729, distance: 1.680416813368262 entropy 3.8777897468041735
epoch: 3, step: 46
	action: tensor([[-24.2118, -12.5691,  -2.1655,  -0.7532, -20.3607,   9.4686,  13.3755]],
       dtype=torch.float64)
	q_value: tensor([[-14.3415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.9609795893818514
epoch: 3, step: 47
	action: tensor([[-12.3067,  -2.5883,   6.8469,  14.1293,   4.7590, -11.8954,   6.0778]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 48
	action: tensor([[ -4.5853, -11.4472,  -9.8889,   8.3733, -27.6214, -16.3123,  -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 49
	action: tensor([[-27.0504, -12.6105,  -3.0097,  -1.0913,   8.5229, -20.0051,  20.4501]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 50
	action: tensor([[ -7.8315, -28.8510, -18.8165,   8.7719, -13.8995, -13.4493,  -3.9100]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7282653494610287, distance: 0.5965257617783754 entropy 3.8777897468041735
epoch: 3, step: 51
	action: tensor([[-35.4117, -31.1748,  -3.0391,  -7.5899, -18.0652,  12.0943,  24.8167]],
       dtype=torch.float64)
	q_value: tensor([[-18.1764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1528550445806599, distance: 1.0532599190920837 entropy 4.250744075104974
epoch: 3, step: 52
	action: tensor([[-19.0727, -12.8415, -17.8334,  -2.0083,  -1.7782,  -1.3376,  35.9974]],
       dtype=torch.float64)
	q_value: tensor([[-20.8749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7659557147219331, distance: 1.5207106852166754 entropy 4.171727062552152
epoch: 3, step: 53
	action: tensor([[ -9.9915, -23.9830,   2.4649,  18.1432,   5.2074,   3.8950,  -9.9319]],
       dtype=torch.float64)
	q_value: tensor([[-23.7870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.243861197181863
epoch: 3, step: 54
	action: tensor([[ -7.8959, -11.7585,  -9.5283,   5.8347, -16.9311,  10.7197,  16.0808]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 55
	action: tensor([[-19.1072,  -1.1808,  -3.4218,  10.8124,   5.4883,  15.5538,  -0.2189]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07337401532810439, distance: 1.185583733423043 entropy 3.8777897468041735
epoch: 3, step: 56
	action: tensor([[-29.5855,  -4.1783,  -7.5173, -15.7614,  -2.5917,   8.2228,  27.9859]],
       dtype=torch.float64)
	q_value: tensor([[-16.4654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.117565339134125
epoch: 3, step: 57
	action: tensor([[-23.1528,  -5.4068,  -6.3512, -16.0676, -18.5717,  -6.1538,  16.4828]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 58
	action: tensor([[-22.9643, -21.7444,  -0.2690, -28.6035, -12.3271, -11.3461, -11.2486]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 59
	action: tensor([[-11.5625, -17.6518, -11.6036, -17.5847,  -1.1853,  10.8853,   3.2706]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 60
	action: tensor([[ -9.9169, -15.8152, -11.3948,  18.4066,   2.0267,  18.5333,  20.6310]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 61
	action: tensor([[-22.6751, -16.7759,   8.7213, -12.4526,  -5.4447,  -2.8616,  -6.6167]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 62
	action: tensor([[-27.0763, -15.2628,  36.3526,  -3.6327,  -7.1110,   1.3534, -19.9773]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 63
	action: tensor([[-10.4969,  -5.1162, -30.1489,  14.6142,   1.5544,  -3.8016,  33.3150]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 64
	action: tensor([[-25.2109,  -9.2021,  -3.0372, -16.0977,  -1.7853, -17.7709,  -9.2211]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 65
	action: tensor([[-37.6448,  -5.5777,  -3.0399,   4.2823, -14.6245,  -8.5718,  18.1640]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 66
	action: tensor([[-26.2571, -20.7651,   1.2498,  -0.3126,  -0.4810,  -0.8518,   6.0008]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1736271948141015, distance: 1.687132621444393 entropy 3.8777897468041735
epoch: 3, step: 67
	action: tensor([[-17.7221,  -1.4371,  -4.5049,  10.2170,  -7.7173,   7.9824,  -6.8521]],
       dtype=torch.float64)
	q_value: tensor([[-11.7010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.781491078521744
epoch: 3, step: 68
	action: tensor([[-37.3626, -10.4711,   4.1161,  20.2257,  -9.8495,  20.0477,  12.2476]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21334653598298536, distance: 1.2605181998426092 entropy 3.8777897468041735
epoch: 3, step: 69
	action: tensor([[-36.2305, -20.8212,  -2.5919,  26.9983, -11.3522,  25.8189,  10.3818]],
       dtype=torch.float64)
	q_value: tensor([[-17.6484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5933760221225168, distance: 1.4444942984196834 entropy 4.10976251513636
epoch: 3, step: 70
	action: tensor([[ -4.2101, -13.3825,  26.0231,   4.3311,  11.4760,   3.3127,   2.2802]],
       dtype=torch.float64)
	q_value: tensor([[-17.3725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.288565082162886
epoch: 3, step: 71
	action: tensor([[-20.2377, -15.1426, -20.2871, -12.4953,  12.0220,   6.1024,  -9.7854]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6928049695591088, distance: 1.488881576360794 entropy 3.8777897468041735
epoch: 3, step: 72
	action: tensor([[-34.0177,  -8.9185,   5.4038, -11.9741,   0.7798,  -5.5759,  21.9455]],
       dtype=torch.float64)
	q_value: tensor([[-15.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.125546754328629
epoch: 3, step: 73
	action: tensor([[-39.8640, -28.0488,   1.0290,  11.9142, -20.5583,   6.6046,   1.0768]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8936894490263922, distance: 1.9466266003807557 entropy 3.8777897468041735
epoch: 3, step: 74
	action: tensor([[-39.2768,  10.0714,  -0.1065,   7.3737,   6.5279, -33.6002, -18.8405]],
       dtype=torch.float64)
	q_value: tensor([[-14.8644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.9703851230328238
epoch: 3, step: 75
	action: tensor([[-29.7423, -24.0511, -14.3570,   5.7199,   1.2304, -23.3873,  13.1789]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 76
	action: tensor([[-22.5540,  -0.7384,  -3.2119,  14.4280, -15.7997,   2.4028,  -0.4691]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 77
	action: tensor([[ -2.5650, -13.7088,   9.6297,   7.8823,  -2.8503,  -8.8444, -22.0371]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 78
	action: tensor([[-15.8681, -10.8768,  13.6328,  -5.9334,  -6.2117,  32.6766,   6.4029]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 79
	action: tensor([[  3.9668, -30.0319,   5.1576,  -2.2597, -19.5596,  -7.3547,   5.0332]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 80
	action: tensor([[-18.0347,  -4.0986, -18.1564,   9.1726, -13.9947,  -0.2407,  -2.4285]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 81
	action: tensor([[-21.5306, -18.3135,  -1.8861,  -2.1153,  -3.9490,  -5.6103,  24.9693]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.56762245241861, distance: 0.7524683892969196 entropy 3.8777897468041735
epoch: 3, step: 82
	action: tensor([[-18.7975,   5.7679,  -8.8405,   2.8839, -13.3954,  -3.8531,   5.6650]],
       dtype=torch.float64)
	q_value: tensor([[-19.1504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.207727365615034
epoch: 3, step: 83
	action: tensor([[-42.5156, -19.8313,   3.2793, -15.0119,  -1.9801,  -0.8874, -13.1075]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 84
	action: tensor([[-12.4795, -20.7979,  -7.3788,   4.1466,  -1.2806,  -1.7271,  17.3973]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 85
	action: tensor([[-34.7607, -23.4215,  11.6469,   5.5353, -14.4532,  -2.4166,  10.4008]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 86
	action: tensor([[-22.6082, -34.6524, -24.9138,   6.4216,  -1.5228, -19.4681,   9.9469]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 87
	action: tensor([[-20.3864,  -0.1173,  -8.4898,  11.9677,  -1.0556,  11.6967,  -4.3612]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.280384015423237, distance: 1.7280674398635707 entropy 3.8777897468041735
epoch: 3, step: 88
	action: tensor([[-30.1076, -35.2660, -37.5228,  11.6840,  -5.6377,  -5.1203,  32.5333]],
       dtype=torch.float64)
	q_value: tensor([[-18.3602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.174172406059971
epoch: 3, step: 89
	action: tensor([[ -7.5495, -34.0159, -16.0446,  -3.2914,  -0.1048,  -5.0632,  -3.2267]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 90
	action: tensor([[-21.1995, -16.7798,  23.6592,  16.3815,  -8.7608,  18.6900, -21.1979]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16467129129879365, distance: 1.0458885223562948 entropy 3.8777897468041735
epoch: 3, step: 91
	action: tensor([[-19.8580,   2.5841,   1.4346,   3.4929,  14.9397,   4.7873,  -2.6042]],
       dtype=torch.float64)
	q_value: tensor([[-27.6364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.368080282215021
epoch: 3, step: 92
	action: tensor([[-30.1370, -22.8193,   4.6569,  24.0702,  -2.1553,   2.8193,  -2.2043]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 93
	action: tensor([[-23.9326,   0.8302,   9.1447,  -0.2715, -11.6301,   5.4882,  -7.7868]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 94
	action: tensor([[-23.6087,  11.3159,  27.0243,  -7.7697,  -8.1937,  13.1047,   8.1784]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 95
	action: tensor([[-43.4453, -21.5721,  14.1168, -11.2455,  -5.9172, -20.9427, -13.4582]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 96
	action: tensor([[-33.5674,  -6.8715, -28.8578, -13.7064,   1.8580,   7.1400, -15.0612]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8867976279261123, distance: 1.5718798764811062 entropy 3.8777897468041735
epoch: 3, step: 97
	action: tensor([[-37.3117, -27.0296,  -2.3575,  13.8633, -17.6775,  -4.6045, -19.9454]],
       dtype=torch.float64)
	q_value: tensor([[-19.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.211160767495447
epoch: 3, step: 98
	action: tensor([[-16.7069, -27.8755,  -1.8948,   7.0580,  -4.2653,  14.6490,  -3.7333]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 99
	action: tensor([[-14.5259, -17.0383,  21.9149,  18.0007,  10.2098,   8.3057, -10.7791]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5873192045521891, distance: 0.7351294523739533 entropy 3.8777897468041735
epoch: 3, step: 100
	action: tensor([[-60.8417,  17.6097,  -9.4274,   0.5003,  -6.2720,  31.4063, -12.5792]],
       dtype=torch.float64)
	q_value: tensor([[-23.7110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5842666288003175, distance: 1.8396082688248994 entropy 4.34217945339143
epoch: 3, step: 101
	action: tensor([[-15.3940, -22.7209,  13.5784,  -3.4816,  34.2513,  13.8662,  -2.2997]],
       dtype=torch.float64)
	q_value: tensor([[-20.0197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.2107841261052545
epoch: 3, step: 102
	action: tensor([[-18.3761,  -8.3379,  -3.3847,  -2.9771,   0.8131,  -6.7293,  -1.4868]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 103
	action: tensor([[-14.2969,  -9.3714,  -2.1439,   8.6042,   2.6871,   6.0358,   5.6841]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 104
	action: tensor([[-22.1233, -11.8955, -27.1235, -10.4153,  -2.5605,   3.8900,   7.8875]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 105
	action: tensor([[-21.7751, -13.9399, -16.9963,  -1.8517, -20.8181,  19.3110,   9.6122]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4946735548070713, distance: 1.3990391527386306 entropy 3.8777897468041735
epoch: 3, step: 106
	action: tensor([[-13.3960, -20.2109,  -5.5988, -24.3044, -12.3632,  -3.6283, -12.7508]],
       dtype=torch.float64)
	q_value: tensor([[-24.3721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.345969552131593
epoch: 3, step: 107
	action: tensor([[  4.0831, -11.8777,  -0.2746, -10.5818,  -5.6825,  -5.8076,  -9.7321]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 108
	action: tensor([[-10.6223,   1.4216,   9.9345,   0.5458,  -0.8445,   4.5131,  -5.3631]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 109
	action: tensor([[ -8.5100, -12.4487, -24.1044,   4.9185,  -7.4704,  -6.7515, -14.2786]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7393873650108036, distance: 1.894015014268475 entropy 3.8777897468041735
epoch: 3, step: 110
	action: tensor([[-38.4217,   1.1597, -22.0931,  -3.4604, -17.5840,  -5.7626,  -3.2651]],
       dtype=torch.float64)
	q_value: tensor([[-17.5717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.974099580616069
epoch: 3, step: 111
	action: tensor([[  5.4291, -10.8826, -24.7686,   3.1204,   3.0254,   5.0180,  -4.8560]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 112
	action: tensor([[ -5.4235,  -9.3740,  -4.9826,   2.9329, -12.7937,  15.9114,  -9.7062]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 113
	action: tensor([[-26.1218, -24.1845,  -3.5742,  -8.8659, -37.7762,   5.4213,   3.8682]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2744032648083926, distance: 0.9747749347281862 entropy 3.8777897468041735
epoch: 3, step: 114
	action: tensor([[-32.4428,  -6.4304,   9.5699,  -3.8425,  -9.3214,  23.7549, -13.0667]],
       dtype=torch.float64)
	q_value: tensor([[-23.8933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.2191114214176
epoch: 3, step: 115
	action: tensor([[ 2.3991,  0.3750,  3.7889, 13.2892, -9.0468,  5.7159, 26.9727]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 116
	action: tensor([[-26.7700, -16.1840,  -8.0919,  -6.2763,  -4.2027,  10.8044,   2.1763]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 117
	action: tensor([[-14.3875, -21.6472,  25.8627,  -4.7042,   9.4161,   2.1332,  11.9842]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 118
	action: tensor([[-17.5035, -24.0432, -11.2405,   7.6409,  -0.6167, -27.6567,  16.2845]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 119
	action: tensor([[-45.7000, -30.7992,  -9.6759,  11.6055,  -1.9553,   4.9650,  16.8409]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 120
	action: tensor([[-24.2705,   1.1736,  -8.6898, -11.3263,  10.5567,   1.4632,  -7.4446]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 121
	action: tensor([[ -5.5804,   1.8646, -13.5037,  19.0545,  -9.6109,   2.1386, -22.0335]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8777897468041735
epoch: 3, step: 122
	action: tensor([[-25.8541,  -5.4883,   7.4136,  15.3493,  10.9789,  25.1995,  -1.9454]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.504301136590385, distance: 0.8056856987815598 entropy 3.8777897468041735
epoch: 3, step: 123
	action: tensor([[-28.0328,  11.8096,  18.9838,  28.4544,   1.1377,   6.1899,  -3.4062]],
       dtype=torch.float64)
	q_value: tensor([[-14.9152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6739192361629512, distance: 0.653460413281925 entropy 4.167930726082803
epoch: 3, step: 124
	action: tensor([[-32.8536,  13.0584,   0.0714,  23.6794,  -4.2993,  -4.4613,   2.4220]],
       dtype=torch.float64)
	q_value: tensor([[-18.1375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.236763710068435
epoch: 3, step: 125
	action: tensor([[-22.5487,   6.2451,  16.1381,  14.1420, -16.9891,   7.3278,   6.6607]],
       dtype=torch.float64)
	q_value: tensor([[-18.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0823258443649153, distance: 1.651319228236534 entropy 3.8777897468041735
epoch: 3, step: 126
	action: tensor([[-18.5895, -24.2553,  -0.9791,  19.2285,   7.0496,   6.6243,  -6.1092]],
       dtype=torch.float64)
	q_value: tensor([[-16.0682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9306979926415149, distance: 1.5900612935562406 entropy 4.014512899517157
epoch: 3, step: 127
	action: tensor([[-15.5537, -20.4370, -12.1708, -10.4159,  -6.5683,   1.4463,  19.0249]],
       dtype=torch.float64)
	q_value: tensor([[-11.4296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.6599956685067894
LOSS epoch 3 actor 112.26667040370819 critic 970.0986056519172
epoch: 4, step: 0
	action: tensor([[-11.7377,  -2.4210, -17.6016, -10.0884, -17.4114,  -3.0584,  -3.0941]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 1
	action: tensor([[-26.5669,  -1.8384, -14.5446, -19.2492,   9.1669,  20.2340,  -3.5332]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 2
	action: tensor([[-3.9391e+01, -5.0284e+01, -1.5915e+00, -2.4536e+01, -3.1581e+00,
         -1.4183e-01,  2.4852e-03]], dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4754898701107195, distance: 1.800475723640973 entropy 4.134791455937981
epoch: 4, step: 3
	action: tensor([[-16.0707,  -7.8024,  -9.8510,  25.3715,  -5.0106,  -2.3523,   6.1282]],
       dtype=torch.float64)
	q_value: tensor([[-23.8184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.427948261311611
epoch: 4, step: 4
	action: tensor([[ -5.9944, -35.5800,  19.4843, -34.2025,  -7.8489,  27.7056,   9.6290]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1636971607980109, distance: 1.0464981835837608 entropy 4.134791455937981
epoch: 4, step: 5
	action: tensor([[-23.9067, -27.4852,  -5.3490,  -8.6505,  27.4894,  12.0324,  -2.3786]],
       dtype=torch.float64)
	q_value: tensor([[-32.3140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4824091746961747, distance: 1.393287498542816 entropy 4.422734517786202
epoch: 4, step: 6
	action: tensor([[-19.8749, -24.9964,  -3.8720,   5.9170,   0.6849, -36.1801,   4.2546]],
       dtype=torch.float64)
	q_value: tensor([[-25.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22529211303807473, distance: 1.0072231103739187 entropy 4.446617086896791
epoch: 4, step: 7
	action: tensor([[-21.0685, -24.0433,  22.9854,   8.1540, -63.2054,  43.9555,   9.6096]],
       dtype=torch.float64)
	q_value: tensor([[-27.6236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5419679082959399, distance: 1.8244909982560997 entropy 4.400698518500452
epoch: 4, step: 8
	action: tensor([[ -9.3009, -33.8948,  15.5995,   1.9502,   1.5096,  -7.8293, -18.6742]],
       dtype=torch.float64)
	q_value: tensor([[-28.6412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.64824280538017
epoch: 4, step: 9
	action: tensor([[-21.9310, -23.4606,  -4.1165,  17.3489, -26.4387,   5.2457,   5.3636]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 10
	action: tensor([[-27.3496, -43.8038,  13.7036,   9.3636,   0.5290,  22.8396,   6.3284]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08237015569279593, distance: 1.0962016669881198 entropy 4.134791455937981
epoch: 4, step: 11
	action: tensor([[-26.8020, -37.8494,  31.6295,  13.7357,  16.2637,  -1.4985,  -0.6121]],
       dtype=torch.float64)
	q_value: tensor([[-21.1525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16243188448459478, distance: 1.0472895290483195 entropy 4.331497923529603
epoch: 4, step: 12
	action: tensor([[-18.9391, -48.1248, -25.8290, -31.3622, -25.0867, -20.1860,  31.8873]],
       dtype=torch.float64)
	q_value: tensor([[-22.6899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0829197054613648, distance: 1.6515546823422376 entropy 4.385294744622366
epoch: 4, step: 13
	action: tensor([[-12.8794, -18.2926,  26.0449,  20.1504,   1.2959,  12.6429,   4.6771]],
       dtype=torch.float64)
	q_value: tensor([[-15.8055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27131596270856795, distance: 1.2902784484216065 entropy 3.948943564586661
epoch: 4, step: 14
	action: tensor([[-29.9445, -38.7269,  -1.0673, -18.9143,  -5.9428,  11.1477,   7.1264]],
       dtype=torch.float64)
	q_value: tensor([[-14.0471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.051670752506369
epoch: 4, step: 15
	action: tensor([[-11.4765, -13.3649,  22.2431,   4.6879,  14.3778, -16.3356,  -4.9085]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45485820283923395, distance: 0.8449119139348269 entropy 4.134791455937981
epoch: 4, step: 16
	action: tensor([[-15.2145, -27.6677, -30.8478, -29.2268,   4.5579,  -9.0687,  52.5071]],
       dtype=torch.float64)
	q_value: tensor([[-29.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3754680576309586, distance: 1.3420909502081522 entropy 4.623663331064064
epoch: 4, step: 17
	action: tensor([[ 19.9145, -39.4201, -15.4953, -41.0187,  27.1354,  -5.7226,  12.5758]],
       dtype=torch.float64)
	q_value: tensor([[-31.9881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.718785181953173
epoch: 4, step: 18
	action: tensor([[-2.8489e+01, -2.4303e-03,  1.6017e+01,  8.2485e+00, -3.8520e+01,
          1.0640e+01,  3.0801e+01]], dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 19
	action: tensor([[-22.2612, -18.3206,   7.4931, -10.8258,  -5.8411, -11.4740,  12.4706]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 20
	action: tensor([[ -6.4230,  -9.5575,  -8.5767,  28.2095, -33.7972, -10.2486,   2.7445]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 21
	action: tensor([[-22.1233, -19.9181, -15.1219,  20.9176,  -5.0639,  17.9449,  -7.3670]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.315045502458109, distance: 1.741151089925852 entropy 4.134791455937981
epoch: 4, step: 22
	action: tensor([[-42.0731, -18.0990, -23.8769,  37.8609,   3.1982, -29.1728,  19.7961]],
       dtype=torch.float64)
	q_value: tensor([[-23.3184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2558699576369654, distance: 0.9871453690732953 entropy 4.454676304791289
epoch: 4, step: 23
	action: tensor([[-75.3694, -21.2002, -33.1684,  -5.8432, -68.0042, -24.2612,   7.7584]],
       dtype=torch.float64)
	q_value: tensor([[-29.2888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7119103013066481, distance: 1.4972599088052516 entropy 4.720745189336741
epoch: 4, step: 24
	action: tensor([[-62.1010,  -0.0867, -22.6438,  65.9822,   3.3935,  21.7298,  23.6691]],
       dtype=torch.float64)
	q_value: tensor([[-25.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6524246813144003, distance: 1.4710164516631217 entropy 4.514333309757432
epoch: 4, step: 25
	action: tensor([[-32.0073, -17.8791,   1.4829, -20.3266, -19.3035,  40.5314,  17.6131]],
       dtype=torch.float64)
	q_value: tensor([[-27.9933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3232099109679285, distance: 1.3163490545111283 entropy 4.660629133253145
epoch: 4, step: 26
	action: tensor([[-27.5970, -23.0035,  -5.6948,  25.0163, -21.9294,  13.2954,  15.1302]],
       dtype=torch.float64)
	q_value: tensor([[-20.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.306108580193082
epoch: 4, step: 27
	action: tensor([[-33.3602, -32.8179,   7.5327,   6.6759, -11.6733,  15.5652,  21.2109]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4670414037780788, distance: 1.3860467496664688 entropy 4.134791455937981
epoch: 4, step: 28
	action: tensor([[-17.8932,   3.3032, -20.5502,   5.1546, -39.1537,  23.2438,  12.8888]],
       dtype=torch.float64)
	q_value: tensor([[-19.9071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.272079443534999
epoch: 4, step: 29
	action: tensor([[-25.9420,   3.2620, -10.9180,  13.0624, -20.9538,   8.8213,  13.7191]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 30
	action: tensor([[-13.4454, -14.7394,  -7.9541,   6.9942,  -0.8524,   6.9735, -14.4901]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5036917610340934, distance: 1.8107025923631095 entropy 4.134791455937981
epoch: 4, step: 31
	action: tensor([[-12.9479, -13.6194,   5.5927, -14.4757,   2.4718,  -7.9418,   0.6076]],
       dtype=torch.float64)
	q_value: tensor([[-20.8504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4426804831120268, distance: 1.3744905920191652 entropy 4.344684371595103
epoch: 4, step: 32
	action: tensor([[-35.1183,  -4.4102,  22.6330,   0.1700,  19.4664,  10.4495,  42.9862]],
       dtype=torch.float64)
	q_value: tensor([[-24.9194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.415824396263555
epoch: 4, step: 33
	action: tensor([[-37.4421, -17.1821,  -0.8713,  -1.5330, -15.8315,  10.8828,  28.1467]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 34
	action: tensor([[-21.1499, -16.5759, -12.4382, -10.8934,  -7.5577,  19.3337,  29.4404]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 35
	action: tensor([[-25.6739,  -3.6556,   3.8221, -16.7633, -16.7274, -20.9531,  -5.5457]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 36
	action: tensor([[-31.5581,   4.5695,  14.6233,   1.3548, -18.9561,  36.6228,  17.8823]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 37
	action: tensor([[ -2.0315,   2.7262, -14.4447,   7.3442,  -3.2408,  35.4162,  20.8827]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 38
	action: tensor([[-19.2445, -31.1586,  10.8090,  -1.9909,  10.0959, -16.7522, -15.6107]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 39
	action: tensor([[-2.4656e-02, -5.8564e+01,  2.8180e-01, -2.0900e+01,  8.4166e-01,
          2.5545e+01,  1.8740e+01]], dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 40
	action: tensor([[-12.4878, -24.4132,   2.3598, -21.5917,  -5.9414, -27.6335, -60.1658]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49948145004233535, distance: 0.809593070207351 entropy 4.134791455937981
epoch: 4, step: 41
	action: tensor([[-31.5080, -37.9812, -25.1438,  11.3338, -41.5152, -44.8006,  -1.6291]],
       dtype=torch.float64)
	q_value: tensor([[-31.0659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.540420818220938, distance: 1.4202878862486312 entropy 4.5637148963776335
epoch: 4, step: 42
	action: tensor([[-25.8211, -28.3521,  28.9127, -25.5865,   9.5341, -31.3015,  -3.6284]],
       dtype=torch.float64)
	q_value: tensor([[-22.5191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7920436782704989, distance: 1.5319020192405552 entropy 4.260424414961277
epoch: 4, step: 43
	action: tensor([[-59.1479, -27.8804, -39.8034, -43.6014, -33.0294,  28.8253,  43.1588]],
       dtype=torch.float64)
	q_value: tensor([[-33.4168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1839391710929733, distance: 1.033755800262622 entropy 4.763545432668485
epoch: 4, step: 44
	action: tensor([[-37.2884,   5.6659,  -9.8912, -13.3825, -25.4734,  -9.9701,  -6.3881]],
       dtype=torch.float64)
	q_value: tensor([[-31.9514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.5970891707932475
epoch: 4, step: 45
	action: tensor([[-54.4536,  -9.7588, -22.0475,  14.6752,  10.6032,  20.3986,   5.2070]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 46
	action: tensor([[-16.8924,  -4.1882,  -0.7339,  23.3495,  -2.5781,  21.4197, -16.9938]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 47
	action: tensor([[ -8.1283,  -7.7043,  27.5096,  -4.6513, -13.1387,  29.8816,  27.6430]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 48
	action: tensor([[-51.6017, -15.3177, -10.9709, -11.9318, -17.7466,  -3.0411,  13.0126]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 49
	action: tensor([[-26.2215, -46.1552,  -7.1932,   9.9845,  -5.9296,  -9.8490, -39.7478]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 50
	action: tensor([[  0.4616, -33.5692,  -1.7213,  -1.9191,  -1.5354, -42.5082,   7.0750]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 51
	action: tensor([[-30.6801,  -4.5610,  -0.2671,   9.2470, -30.4018,  -0.1633,  21.9848]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3002874959850743, distance: 1.3048974618675497 entropy 4.134791455937981
epoch: 4, step: 52
	action: tensor([[ -8.8680, -38.2094, -18.3772,  29.7126,  -8.3403,  -2.7621,  29.9743]],
       dtype=torch.float64)
	q_value: tensor([[-23.0226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.457602485010256
epoch: 4, step: 53
	action: tensor([[-36.5036,  -7.8665,  39.7119,   0.4129,  -2.1434, -18.6469,  -1.5189]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6398570812090418, distance: 1.859289004333554 entropy 4.134791455937981
epoch: 4, step: 54
	action: tensor([[-38.3094, -36.9194,  29.8636,  15.7810,   5.3413,  17.9262,  54.7141]],
       dtype=torch.float64)
	q_value: tensor([[-25.1685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3532840878138175, distance: 1.331224129440728 entropy 4.471412670260174
epoch: 4, step: 55
	action: tensor([[-33.4516,   3.7898, -41.5025, -17.2064,  -6.6744, -15.3263,  29.7063]],
       dtype=torch.float64)
	q_value: tensor([[-20.8301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.465874338926026
epoch: 4, step: 56
	action: tensor([[-25.4235,  -1.3941,  16.1390,  14.4175,   1.3979, -22.6563, -23.3922]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4263959001425641, distance: 1.3667111395752465 entropy 4.134791455937981
epoch: 4, step: 57
	action: tensor([[-14.3812, -28.6580,  15.2019,  40.7738,   1.2228,   2.8146, -32.6721]],
       dtype=torch.float64)
	q_value: tensor([[-27.7808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.619435189645237
epoch: 4, step: 58
	action: tensor([[-20.6720,   9.7609,  20.6279,   2.5462,  -3.3774,  -5.2318,  20.6807]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 59
	action: tensor([[-35.2795,  -9.5519,   0.3275, -13.9754,  -4.6849,   6.5261, -11.0767]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 60
	action: tensor([[-25.3802, -42.5244, -23.0433,  -5.8618,  -4.1147,  16.4806,   7.9651]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 61
	action: tensor([[-21.2549,  -9.6263,  16.0357,  20.2869, -18.1089,  12.8497,  28.6930]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 62
	action: tensor([[-29.1932,  -6.2416,  -6.2430,   4.3777, -10.7594, -10.6509,  21.5261]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 63
	action: tensor([[-32.8992, -28.2710,   1.8590, -11.5559,  13.2235,  -6.9626,  18.7444]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 64
	action: tensor([[-19.5098,  -8.2239, -24.9569,  -9.3914, -13.1295,  -7.5396,  33.5640]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 65
	action: tensor([[-31.3141,  -2.0282,   3.9950,  16.8666, -15.2048,  35.1487,  16.3835]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 66
	action: tensor([[ -6.2362, -26.2740, -17.3424,   1.0072,  -2.6245,  26.0673, -18.1470]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03870558679248903, distance: 1.166280265486705 entropy 4.134791455937981
epoch: 4, step: 67
	action: tensor([[-34.0021, -36.0411,   3.6637,   4.8139,  -0.0968, -37.5355,  51.7753]],
       dtype=torch.float64)
	q_value: tensor([[-20.8431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.331384106502743
epoch: 4, step: 68
	action: tensor([[-33.1462,   4.4380,  -7.2496,   3.6554, -19.3808,  14.7973,  25.9749]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 69
	action: tensor([[-34.3636,   5.0918,   0.7412,   4.2632,  -4.7828, -29.6295,  38.2770]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 70
	action: tensor([[-26.3022, -19.0805, -28.5632,   9.4503,   5.6611,   1.0268,  -1.8413]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938324422507207, distance: 0.9616356917511725 entropy 4.134791455937981
epoch: 4, step: 71
	action: tensor([[-24.1791,  -3.9996,  -8.0356,  40.7513,   8.8586,  -9.6852,  66.4283]],
       dtype=torch.float64)
	q_value: tensor([[-23.6851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.529033367048955
epoch: 4, step: 72
	action: tensor([[ -7.6188, -11.2263,   0.3019,  20.4635, -11.7387, -12.3043,   6.7361]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7811400134193027, distance: 0.5353525521497288 entropy 4.134791455937981
epoch: 4, step: 73
	action: tensor([[-19.9271,  -3.6691,  15.6014, -19.2098,  -5.7136,   3.1744,   3.1166]],
       dtype=torch.float64)
	q_value: tensor([[-17.0285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.070347160311509
epoch: 4, step: 74
	action: tensor([[ -2.8535,  -9.4203,  -1.8945, -11.5059,   6.8014, -13.5284, -13.7806]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 75
	action: tensor([[-21.2969, -18.9251,   0.8770,  17.6868,  -5.6455,  16.9873,  10.4428]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 76
	action: tensor([[-33.7908, -31.5002,  17.0290, -32.8662, -25.5054,   8.2547,  11.0751]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48119617556565686, distance: 1.392717344761412 entropy 4.134791455937981
epoch: 4, step: 77
	action: tensor([[-49.2695, -14.8129, -10.4342,  17.1425,  15.4620,  18.7059, -13.1059]],
       dtype=torch.float64)
	q_value: tensor([[-26.9850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8652189497260525, distance: 1.5628654945395068 entropy 4.319963445457953
epoch: 4, step: 78
	action: tensor([[-45.9893, -32.9782,  -0.1247,   0.1339, -28.8025,  64.3260,  31.0880]],
       dtype=torch.float64)
	q_value: tensor([[-30.6723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8729215094037226, distance: 1.5660891542382822 entropy 4.592519597119531
epoch: 4, step: 79
	action: tensor([[-25.2483, -38.1884,   8.2408,  13.5192, -19.8408,  -8.4794,  13.0520]],
       dtype=torch.float64)
	q_value: tensor([[-18.7886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8566761550238153, distance: 1.5592823866391883 entropy 4.343218687552137
epoch: 4, step: 80
	action: tensor([[-16.9210,  -6.7587,  11.1968,  -6.3662,   0.1097, -31.5832,   6.6429]],
       dtype=torch.float64)
	q_value: tensor([[-17.3159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.18468660745396
epoch: 4, step: 81
	action: tensor([[-20.8761, -24.7430,  11.4766,  32.0464,  -3.8096,  -0.4538, -27.6072]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2137458314335898, distance: 1.7026311375872791 entropy 4.134791455937981
epoch: 4, step: 82
	action: tensor([[-21.8736,  -9.9034, -18.9604,  34.6968,  12.7005, -13.3696, -13.5070]],
       dtype=torch.float64)
	q_value: tensor([[-23.9391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.458722668862964
epoch: 4, step: 83
	action: tensor([[-27.4358,   1.2903,   3.2396,   1.6174, -22.1997, -21.9158,  -2.5722]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.051393987266271424, distance: 1.173382043372292 entropy 4.134791455937981
epoch: 4, step: 84
	action: tensor([[-25.1271, -69.2593,   0.5835,  33.4707, -21.3306,  30.7113,   4.9082]],
       dtype=torch.float64)
	q_value: tensor([[-35.0961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7874298462784546, distance: 0.5276037049191209 entropy 4.8032372677287425
epoch: 4, step: 85
	action: tensor([[-29.8694, -32.1564,   1.6474, -21.9190, -12.4764, -21.8183,   2.5007]],
       dtype=torch.float64)
	q_value: tensor([[-22.4429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4242432141067808, distance: 0.8683128455938844 entropy 4.487269522198241
epoch: 4, step: 86
	action: tensor([[-36.3691,  -8.3087, -20.8100,  14.4252,  19.8568,  34.7841, -22.4869]],
       dtype=torch.float64)
	q_value: tensor([[-28.4235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.551602490155419
epoch: 4, step: 87
	action: tensor([[-29.5078, -21.2691,   5.8690,  12.3904,   0.4542,  13.9105,  18.9201]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.309774536169137, distance: 1.7391678073995451 entropy 4.134791455937981
epoch: 4, step: 88
	action: tensor([[-28.0952,  -4.6805,  38.1877,  25.7725, -15.3302,   4.4409,  28.9851]],
       dtype=torch.float64)
	q_value: tensor([[-16.8832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.9625321507510827
epoch: 4, step: 89
	action: tensor([[ -5.3576, -19.3931,   4.3641, -15.9209,  -5.8674,  11.3719,  30.9401]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 90
	action: tensor([[-41.2765, -11.9452, -13.2719,  22.5973, -24.9593,   3.5147,   0.8441]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 91
	action: tensor([[-24.6337, -17.9921,  -7.4204,  -9.6175,   3.2843, -27.1124,  -1.4763]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 92
	action: tensor([[-24.8163,  -9.0198,  -5.0661,  19.1667, -21.8400, -21.6810,  34.6160]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 93
	action: tensor([[-23.0967, -23.3443, -44.8744,  -1.0239, -18.7053, -24.3742, -34.6183]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 94
	action: tensor([[ -4.4308, -48.9882, -19.6757,   2.2238, -38.3728, -18.7363,   5.2268]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 95
	action: tensor([[-40.0683,  10.7070,  19.5175,   8.3006, -25.5309,  -6.0309,   4.8254]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 96
	action: tensor([[-35.2316, -23.0703,   1.8184,  13.6532, -18.4049, -19.7140, -20.2621]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 97
	action: tensor([[-11.6263, -14.6811,   6.8373,  10.1735,   8.8047,   7.8201,  27.5609]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 98
	action: tensor([[-30.8004,   3.3068,  -1.3044,  24.6590,  -7.4444,   9.8040,   7.8929]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 99
	action: tensor([[-16.6914, -12.7812,   2.2644,   2.7698,  -8.0453, -24.5088,   5.4470]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 100
	action: tensor([[-30.8305, -14.2930,  -2.4967,  -7.9141, -32.2920, -20.5109,   9.1524]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 101
	action: tensor([[-30.1627, -42.2854,   8.3584,  20.0139, -24.9154,  15.6444,  -1.3989]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 102
	action: tensor([[-28.5228, -20.2300, -10.9600,  22.3263,  13.8755,  -7.1731,  12.9253]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8358339273200652, distance: 1.550505778335164 entropy 4.134791455937981
epoch: 4, step: 103
	action: tensor([[-2.9227e+01, -1.7548e+01, -1.6636e+01,  7.3659e+00, -1.5899e-02,
          3.6196e+01,  2.0177e+01]], dtype=torch.float64)
	q_value: tensor([[-20.5124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1085723642437808, distance: 1.2048659110397644 entropy 4.344469088781599
epoch: 4, step: 104
	action: tensor([[-45.3128,  15.9922, -11.7727, -15.6214, -20.6122,  44.5465,  45.0068]],
       dtype=torch.float64)
	q_value: tensor([[-22.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.406150021852166
epoch: 4, step: 105
	action: tensor([[-16.5524, -16.9579,  -0.5949,  24.5492,   8.1334, -12.2104,  11.4760]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 106
	action: tensor([[-18.3739, -33.7186,  -2.7859,   7.8692, -20.6730,  -8.3506,  27.5765]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5991472302426957, distance: 1.4471079131811702 entropy 4.134791455937981
epoch: 4, step: 107
	action: tensor([[-77.4218, -45.0133,  33.9152,  55.4725, -33.6211,  35.9993, -20.8741]],
       dtype=torch.float64)
	q_value: tensor([[-32.2029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20638415398026555, distance: 1.0194404425620653 entropy 4.680230092476337
epoch: 4, step: 108
	action: tensor([[-26.7182, -19.6523,   9.4847,   1.2891,  -2.3796,  27.1804,  23.7594]],
       dtype=torch.float64)
	q_value: tensor([[-25.2504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.081259315964354, distance: 1.6508962866425057 entropy 4.624662511268279
epoch: 4, step: 109
	action: tensor([[-55.4493, -26.5831,  34.9406, -25.7342, -26.8593,  14.2104,   2.0466]],
       dtype=torch.float64)
	q_value: tensor([[-31.3989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06262256641276709, distance: 1.107934128142447 entropy 4.726780198063921
epoch: 4, step: 110
	action: tensor([[-27.8314,   5.7013, -36.9010,  15.4727,   7.3356,   5.9209,  25.0793]],
       dtype=torch.float64)
	q_value: tensor([[-29.1066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2393892955403878, distance: 1.273973996930302 entropy 4.630397282966273
epoch: 4, step: 111
	action: tensor([[-14.9984,  18.9890, -13.0942,  14.9299, -26.7360, -40.4986,   0.3622]],
       dtype=torch.float64)
	q_value: tensor([[-16.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.193248472377168
epoch: 4, step: 112
	action: tensor([[-50.1967,  -3.7073,   5.2268,   5.2826, -20.3955, -15.1677,  -8.2613]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 113
	action: tensor([[-33.4866,  11.9391,  -7.5517,   5.8968,  -2.5158,  16.6798,   4.9130]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 114
	action: tensor([[-29.9088,   1.8829,   8.4796,   8.4333, -26.7142, -30.2123,   1.6109]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 115
	action: tensor([[-34.9543, -13.2148,  -1.8810,  18.8218,   4.5747, -26.9834,  18.1034]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9941410474192356, distance: 1.615974970611883 entropy 4.134791455937981
epoch: 4, step: 116
	action: tensor([[-38.7204, -46.2732, -22.3152,  22.4819, -38.8333, -30.7656, -24.3808]],
       dtype=torch.float64)
	q_value: tensor([[-31.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6440587952333834, distance: 1.4672879998042123 entropy 4.56711470484621
epoch: 4, step: 117
	action: tensor([[-51.6106, -23.7196, -10.4828,  -7.9924, -13.1783,  43.2364, -11.6834]],
       dtype=torch.float64)
	q_value: tensor([[-24.8908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.511908070773127, distance: 1.8136712301045892 entropy 4.518154250481664
epoch: 4, step: 118
	action: tensor([[-18.0384,  -8.3443,   9.3489,  10.8355, -37.4544,   7.3450, -21.2958]],
       dtype=torch.float64)
	q_value: tensor([[-19.2056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.2291884759732215
epoch: 4, step: 119
	action: tensor([[-16.2199,   4.0231, -46.1005, -29.5616, -30.5479, -11.8723,  -8.9381]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 120
	action: tensor([[-24.6674,  10.7468, -15.5743,  -4.6257, -11.2328, -29.3908,  14.4221]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 121
	action: tensor([[-39.8988, -22.9780,  -4.4518,  -1.2164,  -5.6973, -33.0819, -18.2011]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 122
	action: tensor([[ -5.8804, -33.1426,   6.8132,  18.3375,  26.3544,   9.5348,  -1.7019]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 123
	action: tensor([[-27.1940, -55.8600,   8.8061, -25.5448, -28.6655, -14.6594,  14.0955]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 124
	action: tensor([[-30.0443,  -8.9595,   4.7659, -15.8440,   6.6566,  22.0312, -12.9783]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 125
	action: tensor([[ -6.8563,   7.6525,  13.0357,   3.0046,  -8.0280, -27.1587,   2.7714]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.134791455937981
epoch: 4, step: 126
	action: tensor([[-34.8223, -13.1094,   6.5299,  25.2721,   0.3069,  -1.4893,  -1.6184]],
       dtype=torch.float64)
	q_value: tensor([[-26.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5245809643402342, distance: 1.818240574371547 entropy 4.134791455937981
epoch: 4, step: 127
	action: tensor([[-19.9616, -37.5318,   9.9498, -10.6861,  21.6091, -20.2987,  25.8360]],
       dtype=torch.float64)
	q_value: tensor([[-20.4339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.148056793393343
LOSS epoch 4 actor 6.954356702559437 critic 566.9277638410124
epoch: 5, step: 0
	action: tensor([[-18.2476, -33.8843, -11.7224,  -4.4021,  -7.4205,  23.1857,  -7.8313]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 5, step: 1
	action: tensor([[ -3.8624,  -2.6351, -12.1135,   6.4405, -10.6030, -22.6119, -13.2799]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 5, step: 2
	action: tensor([[-23.9615, -23.7624, -26.6251,  -2.5620, -18.2661,  -7.6982, -12.4530]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 5, step: 3
	action: tensor([[-23.1918, -11.0772,  13.9019,  -4.3441,   3.8155, -41.3065,  35.6047]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 5, step: 4
	action: tensor([[-21.0258, -12.7696, -46.7026,  11.8554,  29.8910, -15.0943,  17.1326]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6902608354729571, distance: 1.4877623279148084 entropy 4.376282187494627
