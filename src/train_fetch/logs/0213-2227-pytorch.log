epoch: 0, step: 0
	action: tensor([[ 0.0512,  0.0493, -0.0045,  0.0346,  0.0334,  0.0586,  0.0358]],
       dtype=torch.float64)
	q_value: tensor([[0.0865]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.40983668022772335, distance: 0.8791091493126667 entropy tensor([[-21.6069,  -2.2225,  -4.3514,  -3.7118,  -2.3418, -21.6069,  -1.5828]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 1
	action: tensor([[-0.0024,  0.0295,  0.0313, -0.0094, -0.0108,  0.0012,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[0.0351]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3016829244028255, distance: 0.9562754888903929 entropy tensor([[-21.6069,  -2.7368,  -2.0540,  -2.7737,  -4.3815, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 2
	action: tensor([[-0.0017,  0.0366, -0.0100, -0.0122, -0.0101,  0.0017,  0.0555]],
       dtype=torch.float64)
	q_value: tensor([[0.0297]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3065994095443578, distance: 0.9529032251423523 entropy tensor([[-21.6069,  -2.6499,  -2.1294,  -2.9862,  -5.9595, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 3
	action: tensor([[-0.0016,  0.0163,  0.0214, -0.0213, -0.0084,  0.0007,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[0.0296]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2871006043573756, distance: 0.9662084171637463 entropy tensor([[-21.6069,  -2.6299,  -2.1169,  -2.9378,  -5.6117, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 4
	action: tensor([[-0.0015, -0.0032,  0.0471, -0.0012, -0.0093,  0.0016,  0.0555]],
       dtype=torch.float64)
	q_value: tensor([[0.0301]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2827524280734166, distance: 0.9691505278362575 entropy tensor([[-21.6069,  -2.6546,  -2.1361,  -3.0071,  -5.8222, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 5
	action: tensor([[-0.0018,  0.0317, -0.0017,  0.0106, -0.0102,  0.0019,  0.0561]],
       dtype=torch.float64)
	q_value: tensor([[0.0327]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.31345116228019543, distance: 0.9481835388792411 entropy tensor([[-21.6069,  -2.6460,  -2.1372,  -2.9893, -10.0768, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 6
	action: tensor([[-0.0018,  0.0318, -0.0126,  0.0094, -0.0099,  0.0006,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.0314]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3126482837416499, distance: 0.9487377993595199 entropy tensor([[-21.6069,  -2.6044,  -2.0964,  -2.9004,  -6.0280, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 7
	action: tensor([[-0.0018,  0.0103,  0.0240, -0.0154, -0.0099,  0.0003,  0.0552]],
       dtype=torch.float64)
	q_value: tensor([[0.0316]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2851681286567318, distance: 0.9675170945602075 entropy tensor([[-21.6069,  -2.5980,  -2.0935,  -2.8880,  -5.8984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 8
	action: tensor([[-1.5180e-03,  1.9422e-02, -9.9746e-05, -3.3812e-02, -9.1733e-03,
          1.4631e-03,  5.5691e-02]], dtype=torch.float64)
	q_value: tensor([[0.0310]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.28409339790511146, distance: 0.9682441395297002 entropy tensor([[-21.6069,  -2.6464,  -2.1338,  -2.9899,  -6.0008, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 9
	action: tensor([[-0.0014,  0.0016,  0.0237,  0.0015, -0.0093,  0.0013,  0.0553]],
       dtype=torch.float64)
	q_value: tensor([[0.0290]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2876172849187225, distance: 0.9658582193679526 entropy tensor([[-21.6069,  -2.6580,  -2.1391,  -3.0029,  -5.4716, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 10
	action: tensor([[-0.0016,  0.0393,  0.0200, -0.0330, -0.0108,  0.0011,  0.0561]],
       dtype=torch.float64)
	q_value: tensor([[0.0328]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29857913485302623, distance: 0.9583982976040081 entropy tensor([[-21.6069,  -2.6271,  -2.1199,  -2.9545,  -6.5594, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 11
	action: tensor([[-0.0016,  0.0402,  0.0099, -0.0052, -0.0091,  0.0020,  0.0550]],
       dtype=torch.float64)
	q_value: tensor([[0.0273]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.31244041767992115, distance: 0.9488812451949402 entropy tensor([[-21.6069,  -2.6753,  -2.1424,  -3.0295,  -5.3887, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 12
	action: tensor([[-0.0018,  0.0252,  0.0105, -0.0160, -0.0097,  0.0012,  0.0552]],
       dtype=torch.float64)
	q_value: tensor([[0.0294]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2962577039161908, distance: 0.959982950810232 entropy tensor([[-21.6069,  -2.6343,  -2.1175,  -2.9448,  -5.7696, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 13
	action: tensor([[-0.0015,  0.0107,  0.0477, -0.0106, -0.0101,  0.0013,  0.0553]],
       dtype=torch.float64)
	q_value: tensor([[0.0296]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.28819640727130846, distance: 0.9654655485480668 entropy tensor([[-21.6069,  -2.6421,  -2.1256,  -2.9817,  -5.7189, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 14
	action: tensor([[-0.0017,  0.0321,  0.0148, -0.0107, -0.0097,  0.0021,  0.0557]],
       dtype=torch.float64)
	q_value: tensor([[0.0309]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30403665484227305, distance: 0.9546625292164855 entropy tensor([[-21.6069,  -2.6601,  -2.1418,  -3.0102,  -6.5915, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 15
	action: tensor([[-0.0017,  0.0266, -0.0279, -0.0126, -0.0093,  0.0013,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.0294]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2992686244148498, distance: 0.9579271339088059 entropy tensor([[-21.6069,  -2.6412,  -2.1239,  -2.9675,  -5.7171, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 16
	action: tensor([[-0.0015,  0.0196,  0.0479, -0.0036, -0.0109,  0.0003,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[0.0305]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2979338796445796, distance: 0.958839023968315 entropy tensor([[-21.6069,  -2.6207,  -2.1102,  -2.9182,  -5.5214, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 17
	action: tensor([[-0.0019, -0.0295,  0.0161, -0.0076, -0.0097,  0.0021,  0.0556]],
       dtype=torch.float64)
	q_value: tensor([[0.0306]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2595278579361472, distance: 0.9847161379233618 entropy tensor([[-21.6069,  -2.6517,  -2.1362,  -2.9973,  -6.8782, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 18
	action: tensor([[-0.0012,  0.0004, -0.0276,  0.0036, -0.0115,  0.0012,  0.0565]],
       dtype=torch.float64)
	q_value: tensor([[0.0344]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2878564108412871, distance: 0.9656961006655318 entropy tensor([[-21.6069,  -2.6299,  -2.1282,  -2.9590,  -6.2813, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 19
	action: tensor([[-0.0014,  0.0311,  0.0202, -0.0037, -0.0094, -0.0002,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[0.0336]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30629983308278796, distance: 0.9531090488420266 entropy tensor([[-21.6069,  -2.5883,  -2.0958,  -2.8782,  -5.7608, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 20
	action: tensor([[-0.0017, -0.0004,  0.0080, -0.0072, -0.0084,  0.0015,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.0301]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.28171509489959334, distance: 0.9698511009714598 entropy tensor([[-21.6069,  -2.6353,  -2.1201,  -2.9628,  -6.1899, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 21
	action: tensor([[-0.0015,  0.0094, -0.0113, -0.0063, -0.0098,  0.0008,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[0.0324]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.28955554498119596, distance: 0.9645433646514595 entropy tensor([[-21.6069,  -2.6238,  -2.1220,  -2.9488,  -5.9042, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 22
	action: tensor([[-0.0014,  0.0116, -0.0214,  0.0229, -0.0108,  0.0004,  0.0556]],
       dtype=torch.float64)
	q_value: tensor([[0.0318]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3045026395653686, distance: 0.9543428768623423 entropy tensor([[-21.6069,  -2.6107,  -2.1096,  -2.9250,  -5.7663, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 23
	action: tensor([[-0.0019,  0.0033,  0.0370,  0.0092, -0.0109, -0.0004,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[0.0344]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29183628559549823, distance: 0.9629938827644403 entropy tensor([[-21.6069,  -2.5784,  -2.0831,  -2.8418,  -6.0617, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 24
	action: tensor([[-0.0018,  0.0319, -0.0164,  0.0045, -0.0104,  0.0015,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[0.0330]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.31064728852157586, distance: 0.9501177625648387 entropy tensor([[-21.6069,  -2.6242,  -2.1215,  -2.9594, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 25
	action: tensor([[-0.0017,  0.0210, -0.0017, -0.0090, -0.0096,  0.0002,  0.0552]],
       dtype=torch.float64)
	q_value: tensor([[0.0313]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29641142661794195, distance: 0.9598780976322634 entropy tensor([[-21.6069,  -2.6027,  -2.0964,  -2.8945,  -5.8050, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 26
	action: tensor([[-0.0014,  0.0208, -0.0213, -0.0179, -0.0106,  0.0008,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.0307]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2926774672504685, distance: 0.9624217738955527 entropy tensor([[-21.6069,  -2.6227,  -2.1147,  -2.9472,  -5.9068, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 27
	action: tensor([[-0.0015,  0.0286,  0.0151, -0.0054, -0.0093,  0.0005,  0.0552]],
       dtype=torch.float64)
	q_value: tensor([[0.0305]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30378603298324014, distance: 0.9548344044765663 entropy tensor([[-21.6069,  -2.6282,  -2.1189,  -2.9343,  -5.5266, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 28
	action: tensor([[-0.0017,  0.0233, -0.0075,  0.0087, -0.0093,  0.0013,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.0301]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3065585430502936, distance: 0.9529313050436207 entropy tensor([[-21.6069,  -2.6330,  -2.1190,  -2.9614,  -6.0532, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 29
	action: tensor([[-0.0018,  0.0065,  0.0261,  0.0132, -0.0104,  0.0002,  0.0555]],
       dtype=torch.float64)
	q_value: tensor([[0.0321]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2963320263148348, distance: 0.9599322574528523 entropy tensor([[-21.6069,  -2.6016,  -2.0985,  -2.8923,  -5.8994, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 30
	action: tensor([[-0.0018,  0.0086,  0.0245, -0.0078, -0.0105,  0.0011,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[0.0332]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.28787546846683343, distance: 0.9656831791156604 entropy tensor([[-21.6069,  -2.6128,  -2.1095,  -2.9325,  -7.2013, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 31
	action: tensor([[-0.0016,  0.0067,  0.0526,  0.0041, -0.0098,  0.0013,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[0.0315]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2925314105573782, distance: 0.9625211351341199 entropy tensor([[-21.6069,  -2.6387,  -2.1274,  -2.9740,  -6.1107, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 32
	action: tensor([[-0.0020,  0.0228,  0.0151,  0.0029, -0.0102,  0.0019,  0.0561]],
       dtype=torch.float64)
	q_value: tensor([[0.0323]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3033185590687315, distance: 0.9551549131820452 entropy tensor([[-21.6069,  -2.6438,  -2.1360,  -2.9788, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 33
	action: tensor([[-0.0017,  0.0265,  0.0479, -0.0019, -0.0092,  0.0011,  0.0556]],
       dtype=torch.float64)
	q_value: tensor([[0.0310]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.303847691018472, distance: 0.9547921225634766 entropy tensor([[-21.6069,  -2.6220,  -2.1124,  -2.9447,  -6.1702, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 34
	action: tensor([[-0.0020, -0.0043,  0.0017, -0.0164, -0.0094,  0.0020,  0.0557]],
       dtype=torch.float64)
	q_value: tensor([[0.0303]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.27422215325403865, distance: 0.9748965808023377 entropy tensor([[-21.6069,  -2.6521,  -2.1333,  -2.9869,  -6.5912, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 35
	action: tensor([[-0.0014, -0.0218,  0.0160, -0.0003, -0.0099,  0.0008,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[0.0321]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.26925658991571366, distance: 0.9782258770301074 entropy tensor([[-21.6069,  -2.6287,  -2.1277,  -2.9567,  -5.7242, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 36
	action: tensor([[-0.0013,  0.0255,  0.0058, -0.0048, -0.0109,  0.0010,  0.0565]],
       dtype=torch.float64)
	q_value: tensor([[0.0347]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30211295848661557, distance: 0.9559809991821028 entropy tensor([[-21.6069,  -2.6211,  -2.1213,  -2.9425,  -6.7022, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 37
	action: tensor([[-0.0016,  0.0242,  0.0291, -0.0232, -0.0095,  0.0009,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.0306]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29198271856076174, distance: 0.9628943144461728 entropy tensor([[-21.6069,  -2.6237,  -2.1145,  -2.9420,  -5.8622, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 38
	action: tensor([[-0.0016,  0.0213,  0.0457, -0.0006, -0.0098,  0.0018,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.0293]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3009837394258754, distance: 0.9567541011151537 entropy tensor([[-21.6069,  -2.6641,  -2.1405,  -3.0161,  -5.6978, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 39
	action: tensor([[-0.0020,  0.0167,  0.0468,  0.0003, -0.0100,  0.0019,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[0.0309]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2976613238731155, distance: 0.9590251259202478 entropy tensor([[-21.6069,  -2.6483,  -2.1322,  -2.9815,  -6.6213, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 40
	action: tensor([[-0.0020,  0.0171,  0.0359, -0.0014, -0.0098,  0.0020,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[0.0311]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2972021343199619, distance: 0.9593385803676932 entropy tensor([[-21.6069,  -2.6474,  -2.1336,  -2.9877,  -6.9997, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 41
	action: tensor([[-0.0018,  0.0196,  0.0112, -0.0221, -0.0106,  0.0017,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[0.0310]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2893333615624938, distance: 0.9646941778416713 entropy tensor([[-21.6069,  -2.6426,  -2.1268,  -2.9789,  -6.2814, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 42
	action: tensor([[-0.0015,  0.0422,  0.0501, -0.0082, -0.0111,  0.0013,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.0297]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.31230839115731224, distance: 0.948972343829127 entropy tensor([[-21.6069,  -2.6477,  -2.1321,  -2.9915,  -5.6038, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 43
	action: tensor([[-0.0020,  0.0007, -0.0025, -0.0147, -0.0099,  0.0024,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.0285]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2788980822963776, distance: 0.9717510499220146 entropy tensor([[-21.6069,  -2.6705,  -2.1350,  -3.0071,  -6.1660, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 44
	action: tensor([[-0.0013,  0.0175,  0.0133, -0.0139, -0.0104,  0.0008,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[0.0317]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29188959295672723, distance: 0.9629576371705033 entropy tensor([[-21.6069,  -2.6240,  -2.1208,  -2.9549,  -5.6875, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 45
	action: tensor([[-0.0014,  0.0070, -0.0083, -0.0096, -0.0104,  0.0013,  0.0555]],
       dtype=torch.float64)
	q_value: tensor([[0.0305]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2864057556323375, distance: 0.9666791744530441 entropy tensor([[-21.6069,  -2.6395,  -2.1245,  -2.9810,  -5.9437, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 46
	action: tensor([[-0.0014, -0.0098,  0.0215,  0.0096, -0.0113,  0.0006,  0.0556]],
       dtype=torch.float64)
	q_value: tensor([[0.0316]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.28302717804757394, distance: 0.9689648878999042 entropy tensor([[-21.6069,  -2.6155,  -2.1131,  -2.9366,  -5.6986, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 47
	action: tensor([[-0.0015,  0.0088,  0.0046,  0.0042, -0.0110,  0.0011,  0.0562]],
       dtype=torch.float64)
	q_value: tensor([[0.0342]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29399315378927715, distance: 0.9615262596880211 entropy tensor([[-21.6069,  -2.6129,  -2.1116,  -2.9395,  -7.1934, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 48
	action: tensor([[-0.0015,  0.0338,  0.0435, -0.0125, -0.0108,  0.0007,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[0.0323]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3040452664771097, distance: 0.9546566228490145 entropy tensor([[-21.6069,  -2.6070,  -2.1064,  -2.9274,  -6.0970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 49
	action: tensor([[-0.0018,  0.0253,  0.0294, -0.0082, -0.0095,  0.0023,  0.0553]],
       dtype=torch.float64)
	q_value: tensor([[0.0289]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30007572466398036, distance: 0.9573753061973074 entropy tensor([[-21.6069,  -2.6672,  -2.1367,  -3.0154,  -6.0859, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 0 actor 0.10968465870699791 critic 13.455579792092099
epoch: 1, step: 0
	action: tensor([[-0.2979, -0.0161, -0.0103, -0.0057, -0.0684, -0.1608, -0.0105]],
       dtype=torch.float64)
	q_value: tensor([[0.3763]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.10003235403325794, distance: 1.2002160293900255 entropy tensor([[ -3.1833,  -1.8071,  -2.2231,  -1.6401,  -2.0428, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 1
	action: tensor([[-0.3279,  0.0009,  0.0391,  0.1128, -0.0531, -0.1696, -0.0236]],
       dtype=torch.float64)
	q_value: tensor([[0.3761]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.07179926837816186, distance: 1.1847137293000438 entropy tensor([[ -3.1113,  -1.6925,  -2.1556,  -1.6405,  -2.0830, -21.6069,  -3.9652]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 2
	action: tensor([[-0.3370,  0.0864, -0.0512,  0.0824, -0.0538, -0.1728, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[0.3685]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.03688433330875007, distance: 1.1652573462255278 entropy tensor([[ -3.0482,  -1.6386,  -2.1554,  -1.5997,  -2.0682, -21.6069,  -4.0453]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 3
	action: tensor([[-0.3035,  0.0145, -0.0564,  0.0510, -0.0103, -0.1716, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[0.3742]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06267406129833497, distance: 1.179659673295497 entropy tensor([[ -3.0452,  -1.6476,  -2.1911,  -1.6398,  -2.1562, -21.6069,  -4.3482]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 4
	action: tensor([[-0.3183,  0.0983, -0.0580, -0.0226, -0.1080, -0.1707, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.3774]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.0434728811115237, distance: 1.1689536102433784 entropy tensor([[ -3.0606,  -1.6654,  -2.1719,  -1.6431,  -2.1632, -21.6069,  -4.3131]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 5
	action: tensor([[-0.3557,  0.0659, -0.0818,  0.0006, -0.0596, -0.1694, -0.0240]],
       dtype=torch.float64)
	q_value: tensor([[0.3766]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.10654187787781266, distance: 1.2037619754010378 entropy tensor([[ -2.9905,  -1.6915,  -2.2216,  -1.6687,  -2.1493, -21.6069,  -3.9915]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 6
	action: tensor([[-0.3151, -0.0195, -0.0596,  0.0150, -0.0375, -0.1717, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[0.3792]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.11611803129967124, distance: 1.2089595088208798 entropy tensor([[ -3.0056,  -1.6560,  -2.1788,  -1.6775,  -2.1700, -21.6069,  -4.0821]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 7
	action: tensor([[-0.3535,  0.0389, -0.0364,  0.0285, -0.0433, -0.1705, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.3813]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.11581655736431751, distance: 1.2087962221497635 entropy tensor([[ -3.1018,  -1.6759,  -2.1462,  -1.6496,  -2.1425, -21.6069,  -4.0421]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 8
	action: tensor([[-0.3261, -0.0032, -0.0331,  0.0139, -0.0224, -0.1724, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[0.3758]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.12060480011544872, distance: 1.2113870662965818 entropy tensor([[ -3.0246,  -1.6542,  -2.1692,  -1.6576,  -2.1492, -21.6069,  -4.0495]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 9
	action: tensor([[-0.3231,  0.0535, -0.0406, -0.0218, -0.0989, -0.1714, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[0.3771]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.0841972413573937, distance: 1.1915460799854765 entropy tensor([[ -3.0485,  -1.6645,  -2.1481,  -1.6564,  -2.1479, -21.6069,  -4.0054]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 10
	action: tensor([[-0.3082,  0.0618, -0.0247,  0.0655, -0.0743, -0.1699, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[0.3761]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.02405749261945589, distance: 1.1580274738254688 entropy tensor([[ -3.0262,  -1.6903,  -2.1845,  -1.6601,  -2.1308, -21.6069,  -3.9249]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 11
	action: tensor([[-0.3353,  0.0157, -0.0124,  0.0360, -0.0405, -0.1703, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[0.3726]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1075857362153061, distance: 1.2043296271073816 entropy tensor([[ -3.0653,  -1.6694,  -2.1806,  -1.6253,  -2.1197, -21.6069,  -4.2919]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 12
	action: tensor([[-0.3056,  0.1234, -0.0346,  0.0694, -0.0832, -0.1719, -0.0200]],
       dtype=torch.float64)
	q_value: tensor([[0.3738]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.024896036034449565, distance: 1.130009655116066 entropy tensor([[ -3.0413,  -1.6551,  -2.1509,  -1.6410,  -2.1256, -21.6069,  -4.0718]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 13
	action: tensor([[-0.3284,  0.0425,  0.0039,  0.0742, -0.0353, -0.1702, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[0.3705]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06230326960556254, distance: 1.1794538499931304 entropy tensor([[ -3.0333,  -1.6720,  -2.2180,  -1.6358,  -2.1421, -21.6069,  -4.3732]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 14
	action: tensor([[-0.3348,  0.0052, -0.0070,  0.0828, -0.1035, -0.1722, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[0.3701]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.08968551483711784, distance: 1.1945581129923486 entropy tensor([[ -3.0180,  -1.6492,  -2.1738,  -1.6297,  -2.1292, -21.6069,  -4.1701]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 15
	action: tensor([[-0.3484,  0.0199, -0.0359,  0.0729, -0.0544, -0.1718, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[0.3747]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1034501572193749, distance: 1.2020791204092365 entropy tensor([[ -3.1181,  -1.6564,  -2.1380,  -1.6003,  -2.0624, -21.6069,  -4.0210]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 16
	action: tensor([[-0.3426, -0.0341, -0.0340,  0.0299, -0.0615, -0.1724, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[0.3763]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.15512118107065143, distance: 1.2299018846083807 entropy tensor([[ -3.0867,  -1.6424,  -2.1517,  -1.6242,  -2.1134, -21.6069,  -4.1551]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 17
	action: tensor([[-0.3260,  0.0020, -0.0468, -0.0589, -0.0090, -0.1717, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[0.3802]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.14707069600240708, distance: 1.2256085607537848 entropy tensor([[ -3.1328,  -1.6552,  -2.1250,  -1.6245,  -2.1015, -21.6069,  -3.9436]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 18
	action: tensor([[-0.3587,  0.0928, -0.0115,  0.0783, -0.0883, -0.1710, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[0.3779]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.05935322196216086, distance: 1.1778150222963901 entropy tensor([[ -2.9912,  -1.6764,  -2.1584,  -1.6965,  -2.1864, -21.6069,  -3.7520]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 19
	action: tensor([[-0.3588,  0.0196, -0.0313,  0.0402, -0.1055, -0.1727, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[0.3698]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.12830150448766475, distance: 1.2155400617542462 entropy tensor([[ -3.0025,  -1.6438,  -2.1767,  -1.6342,  -2.1175, -21.6069,  -4.1838]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 20
	action: tensor([[-0.3472,  0.0887, -0.0576,  0.0689, -0.0489, -0.1719, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[0.3773]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.05393593373081518, distance: 1.1747996250553323 entropy tensor([[ -3.0857,  -1.6535,  -2.1408,  -1.6253,  -2.0947, -21.6069,  -3.9492]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 21
	action: tensor([[-0.3455,  0.0907, -0.0194,  0.0205, -0.0804, -0.1720, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[0.3749]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06992511504031573, distance: 1.1836774780506585 entropy tensor([[ -3.0237,  -1.6469,  -2.1942,  -1.6525,  -2.1695, -21.6069,  -4.2995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 22
	action: tensor([[-0.3360,  0.0228, -0.0563, -0.0231, -0.0752, -0.1717, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[0.3709]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.12409341977838118, distance: 1.2132712206137337 entropy tensor([[ -2.9896,  -1.6685,  -2.2021,  -1.6630,  -2.1473, -21.6069,  -4.0543]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 23
	action: tensor([[-0.3334,  0.0276, -0.1076, -0.0230, -0.0849, -0.1704, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[0.3793]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.11211124582463139, distance: 1.2067875178095375 entropy tensor([[ -3.0355,  -1.6796,  -2.1535,  -1.6641,  -2.1423, -21.6069,  -3.9235]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 24
	action: tensor([[-0.3242,  0.0019, -0.0136,  0.0055, -0.1288, -0.1699, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[0.3860]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1119120431506413, distance: 1.2066794323696481 entropy tensor([[ -3.0383,  -1.6802,  -2.1610,  -1.6707,  -2.1665, -21.6069,  -3.9783]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 25
	action: tensor([[-0.3371,  0.0169, -0.0557,  0.0754, -0.0497, -0.1704, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[0.3768]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.09015477362593849, distance: 1.1948152957067728 entropy tensor([[ -3.1069,  -1.6836,  -2.1489,  -1.6193,  -2.0737, -21.6069,  -3.8833]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 26
	action: tensor([[-0.3344, -0.0477, -0.0266,  0.0895, -0.0785, -0.1717, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[0.3787]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.12693226574157457, distance: 1.2148022847535438 entropy tensor([[ -3.1190,  -1.6505,  -2.1645,  -1.6250,  -2.1242, -21.6069,  -4.2468]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 27
	action: tensor([[-0.3206,  0.0381, -0.0165, -0.0062, -0.0999, -0.1721, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[0.3797]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.08755725265125403, distance: 1.1933909985378308 entropy tensor([[ -3.2135,  -1.6446,  -2.1321,  -1.5929,  -2.0644, -21.6069,  -4.0569]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 28
	action: tensor([[-0.3246, -0.0134, -0.0280,  0.0126, -0.0434, -0.1703, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[0.3739]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1255626278681492, distance: 1.2140638441567273 entropy tensor([[ -3.0446,  -1.6839,  -2.1674,  -1.6449,  -2.1120, -21.6069,  -3.9329]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 29
	action: tensor([[-0.3308,  0.0887, -0.0417,  0.0508, -0.0619, -0.1711, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[0.3779]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.0400444641997586, distance: 1.1670316831414222 entropy tensor([[ -3.0783,  -1.6669,  -2.1400,  -1.6459,  -2.1254, -21.6069,  -3.9960]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 30
	action: tensor([[-3.3309e-01, -3.2347e-04, -4.8543e-02,  1.2343e-01, -1.0496e-01,
         -1.7115e-01, -1.4034e-02]], dtype=torch.float64)
	q_value: tensor([[0.3728]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.07322286022382807, distance: 1.1855002521109232 entropy tensor([[ -3.0187,  -1.6608,  -2.1926,  -1.6491,  -2.1503, -21.6069,  -4.2644]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 31
	action: tensor([[-0.3107,  0.0737, -0.0322,  0.0287, -0.0294, -0.1717, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[0.3806]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.03742260534015385, distance: 1.1655597637844677 entropy tensor([[ -3.2128,  -1.6467,  -2.1334,  -1.5882,  -2.0618, -21.6069,  -4.2543]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 32
	action: tensor([[-0.3068,  0.0478, -0.0268, -0.0625, -0.0842, -0.1707, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[0.3721]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.08717376993886083, distance: 1.1931805796539012 entropy tensor([[ -2.9950,  -1.6686,  -2.1962,  -1.6603,  -2.1824, -21.6069,  -4.2283]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 33
	action: tensor([[-0.3280,  0.0648, -0.0024,  0.0153, -0.0656, -0.1691, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[0.3746]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.07025528489805288, distance: 1.1838601004011244 entropy tensor([[ -2.9950,  -1.7002,  -2.1878,  -1.6740,  -2.1460, -21.6069,  -3.8193]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 34
	action: tensor([[-0.3349,  0.0155, -0.0452,  0.1350, -0.0624, -0.1714, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[0.3701]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06158732041993842, distance: 1.1790563310485809 entropy tensor([[ -2.9942,  -1.6704,  -2.1819,  -1.6549,  -2.1392, -21.6069,  -4.0536]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 35
	action: tensor([[-0.3407, -0.0275, -0.0470,  0.0318, -0.0963, -0.1723, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.3776]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.14407364885050278, distance: 1.2240063887507904 entropy tensor([[ -3.1457,  -1.6365,  -2.1567,  -1.5990,  -2.0867, -21.6069,  -4.3523]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 36
	action: tensor([[-0.3333,  0.0727, -0.0570,  0.0437, -0.0762, -0.1711, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[0.3816]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.05629286541376621, distance: 1.1761125018879608 entropy tensor([[ -3.1709,  -1.6613,  -2.1277,  -1.6148,  -2.0896, -21.6069,  -3.9440]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 37
	action: tensor([[-0.3053, -0.0461, -0.0671,  0.0174, -0.0998, -0.1709, -0.0150]],
       dtype=torch.float64)
	q_value: tensor([[0.3764]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.11851634897519059, distance: 1.2102577196516924 entropy tensor([[ -3.0462,  -1.6615,  -2.1748,  -1.6441,  -2.1437, -21.6069,  -4.2187]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 38
	action: tensor([[-0.3216, -0.0233, -0.0232, -0.0221, -0.0774, -0.1695, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[0.3853]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.14259255490776424, distance: 1.2232138455628554 entropy tensor([[ -3.2225,  -1.6838,  -2.1265,  -1.6133,  -2.0951, -21.6069,  -3.9968]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 39
	action: tensor([[-0.3241,  0.0234, -0.0079,  0.0389, -0.1101, -0.1702, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[0.3783]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.08217170455738287, distance: 1.1904325147351962 entropy tensor([[ -3.0841,  -1.6791,  -2.1314,  -1.6441,  -2.0996, -21.6069,  -3.8536]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 40
	action: tensor([[-0.3362,  0.0726, -0.0615,  0.0599, -0.1272, -0.1707, -0.0224]],
       dtype=torch.float64)
	q_value: tensor([[0.3741]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.049836951869459245, distance: 1.1725128761950074 entropy tensor([[ -3.0838,  -1.6704,  -2.1496,  -1.6157,  -2.0819, -21.6069,  -4.0295]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 41
	action: tensor([[-0.3360,  0.0403, -0.0103,  0.0238, -0.0848, -0.1706, -0.0205]],
       dtype=torch.float64)
	q_value: tensor([[0.3788]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.09316997072706168, distance: 1.1964664906026548 entropy tensor([[ -3.1041,  -1.6635,  -2.1619,  -1.6240,  -2.1123, -21.6069,  -4.2151]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 42
	action: tensor([[-0.3444, -0.0436,  0.0049,  0.0006, -0.0034, -0.1714, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[0.3727]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.18334984270482457, distance: 1.244839246334812 entropy tensor([[ -3.0375,  -1.6705,  -2.1652,  -1.6401,  -2.1097, -21.6069,  -3.9948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 43
	action: tensor([[-0.3129, -0.0140, -0.0088,  0.0730, -0.0777, -0.1730, -0.0309]],
       dtype=torch.float64)
	q_value: tensor([[0.3763]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.08257959821637528, distance: 1.190656843335444 entropy tensor([[ -3.0577,  -1.6495,  -2.1279,  -1.6575,  -2.1306, -21.6069,  -3.7654]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 44
	action: tensor([[-0.3265,  0.1139, -0.0179,  0.0007, -0.0826, -0.1709, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[0.3750]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.0374607539549785, distance: 1.165581193856212 entropy tensor([[ -3.1374,  -1.6573,  -2.1399,  -1.5986,  -2.0753, -21.6069,  -4.1066]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 45
	action: tensor([[-0.3346, -0.0429, -0.0402,  0.0642, -0.0846, -0.1708, -0.0154]],
       dtype=torch.float64)
	q_value: tensor([[0.3690]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.13447051103652696, distance: 1.2188585243795917 entropy tensor([[ -2.9705,  -1.6800,  -2.2294,  -1.6684,  -2.1513, -21.6069,  -4.0550]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 46
	action: tensor([[-0.3451,  0.0940, -0.0677,  0.0274, -0.0961, -0.1715, -0.0237]],
       dtype=torch.float64)
	q_value: tensor([[0.3815]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06082593517684809, distance: 1.1786334373801384 entropy tensor([[ -3.2235,  -1.6528,  -2.1288,  -1.6010,  -2.0778, -21.6069,  -4.0412]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 47
	action: tensor([[-0.3255,  0.0275, -0.0379,  0.0766, -0.1166, -0.1710, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[0.3773]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06312867371346509, distance: 1.1799119757694405 entropy tensor([[ -3.0303,  -1.6624,  -2.1861,  -1.6536,  -2.1455, -21.6069,  -4.1504]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 48
	action: tensor([[-0.3515, -0.0447, -0.0607,  0.0306, -0.0567, -0.1707, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[0.3778]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.17298671953996125, distance: 1.239376453335452 entropy tensor([[ -3.1442,  -1.6655,  -2.1437,  -1.6021,  -2.0763, -21.6069,  -4.1453]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 49
	action: tensor([[-0.3157,  0.0745,  0.0025,  0.0135, -0.0601, -0.1720, -0.0261]],
       dtype=torch.float64)
	q_value: tensor([[0.3837]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.04950706995952392, distance: 1.1723286470270131 entropy tensor([[ -3.1485,  -1.6533,  -2.1224,  -1.6279,  -2.1108, -21.6069,  -3.9592]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 1 actor 0.18490385621044378 critic 1.3878033264587335
epoch: 2, step: 0
	action: tensor([[-0.4136,  0.0048, -0.0513,  0.1822, -0.0318,  0.0687, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[0.3819]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.10238985147072133, distance: 1.2015014424214192 entropy tensor([[-1.7593, -1.3139, -1.7365, -1.2439, -1.2247, -3.2586, -2.5868]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 1
	action: tensor([[-0.4381,  0.1095, -0.0287, -0.0274, -0.0753,  0.0663, -0.0727]],
       dtype=torch.float64)
	q_value: tensor([[0.3922]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.15487526208229951, distance: 1.2297709579438345 entropy tensor([[-1.7797, -1.2852, -1.6453, -1.2295, -1.1495, -2.7516, -3.0661]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 2
	action: tensor([[-0.4878,  0.0904, -0.0103,  0.0378, -0.0532,  0.0437, -0.0957]],
       dtype=torch.float64)
	q_value: tensor([[0.3852]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.20821176809194974, distance: 1.2578481750669157 entropy tensor([[-1.7711, -1.3155, -1.7614, -1.2277, -1.1265, -2.6889, -2.7067]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 3
	action: tensor([[-0.5254,  0.0742,  0.0048,  0.1994, -0.0834,  0.0144, -0.0822]],
       dtype=torch.float64)
	q_value: tensor([[0.3815]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.19495923335976673, distance: 1.2509306624479988 entropy tensor([[-1.7631, -1.2832, -1.7259, -1.2261, -1.1234, -2.6454, -2.7446]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 4
	action: tensor([[-0.4165,  0.1206, -0.0181,  0.1665, -0.1310,  0.0622, -0.0791]],
       dtype=torch.float64)
	q_value: tensor([[0.3788]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.03263064205504529, distance: 1.1628647269216308 entropy tensor([[-1.7343, -1.2247, -1.6323, -1.2069, -1.1124, -2.7387, -2.8778]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 5
	action: tensor([[-0.4447,  0.1276, -0.0201,  0.0317, -0.0113,  0.0346, -0.0525]],
       dtype=torch.float64)
	q_value: tensor([[0.3849]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.12963449900871726, distance: 1.2162578796218881 entropy tensor([[-1.7431, -1.2842, -1.6727, -1.2030, -1.1127, -2.8628, -2.9663]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 6
	action: tensor([[-0.4510,  0.0937, -0.0891,  0.0831, -0.1238,  0.0647, -0.0889]],
       dtype=torch.float64)
	q_value: tensor([[0.3806]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1403125783678627, distance: 1.2219928103302549 entropy tensor([[-1.7822, -1.2958, -1.7383, -1.2403, -1.1464, -2.6761, -2.8113]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 7
	action: tensor([[-0.4445,  0.0568, -0.0924, -0.0251, -0.2262,  0.0908, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[0.3921]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.20172030062267332, distance: 1.2544645473488585 entropy tensor([[-1.7695, -1.2994, -1.7239, -1.2185, -1.1263, -2.6899, -2.8789]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 8
	action: tensor([[-0.5126,  0.0199, -0.1092,  0.0207, -0.0535,  0.0643, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.3996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.30021204456694783, distance: 1.3048596018586565 entropy tensor([[-1.7680, -1.3313, -1.7448, -1.2222, -1.1131, -2.8276, -2.6887]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 9
	action: tensor([[-0.4879,  0.1377, -0.0292,  0.0872, -0.0027,  0.0565, -0.0826]],
       dtype=torch.float64)
	q_value: tensor([[0.3976]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.14921569894532904, distance: 1.2267539592505936 entropy tensor([[-1.7762, -1.2899, -1.7344, -1.2399, -1.1563, -2.5386, -2.7328]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 10
	action: tensor([[-0.4957, -0.0123, -0.0225,  0.1330, -0.0412,  0.0679, -0.0928]],
       dtype=torch.float64)
	q_value: tensor([[0.3796]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.24208143223129275, distance: 1.2753568762694918 entropy tensor([[-1.7583, -1.2721, -1.7158, -1.2293, -1.1269, -2.5858, -2.8413]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 11
	action: tensor([[-0.4554,  0.0019, -0.0816,  0.0656, -0.1248,  0.0535, -0.0580]],
       dtype=torch.float64)
	q_value: tensor([[0.3889]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.22057652412954942, distance: 1.2642681572677603 entropy tensor([[-1.7562, -1.2675, -1.6446, -1.2208, -1.1306, -2.6309, -2.8802]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 12
	action: tensor([[-0.4589,  0.1500, -0.0208, -0.0565,  0.0433,  0.0469, -0.0895]],
       dtype=torch.float64)
	q_value: tensor([[0.3962]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.16770356813407483, distance: 1.236582217536328 entropy tensor([[-1.7769, -1.2979, -1.6992, -1.2231, -1.1403, -2.7345, -2.8090]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 13
	action: tensor([[-0.4456,  0.0107,  0.0097, -0.0234,  0.0159,  0.0718, -0.0719]],
       dtype=torch.float64)
	q_value: tensor([[0.3790]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2308443255610333, distance: 1.2695746939579649 entropy tensor([[-1.8018, -1.2812, -1.7657, -1.2501, -1.1430, -2.5980, -2.5978]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 14
	action: tensor([[-0.4402,  0.0293, -0.0406, -0.1096, -0.1454,  0.0455, -0.0804]],
       dtype=torch.float64)
	q_value: tensor([[0.3853]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.25655428857818596, distance: 1.2827656493989825 entropy tensor([[-1.7828, -1.3056, -1.7156, -1.2496, -1.1465, -2.6077, -2.7350]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 15
	action: tensor([[-0.5070,  0.0352,  0.0034,  0.0325, -0.1020,  0.0661, -0.0719]],
       dtype=torch.float64)
	q_value: tensor([[0.3918]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2718939941052305, distance: 1.290571741638552 entropy tensor([[-1.7653, -1.3281, -1.7591, -1.2284, -1.1297, -2.8070, -2.5228]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 16
	action: tensor([[-0.5094,  0.0619,  0.0336, -0.0278, -0.0143,  0.0569, -0.0456]],
       dtype=torch.float64)
	q_value: tensor([[0.3860]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2846226704129522, distance: 1.2970134635086592 entropy tensor([[-1.7518, -1.2797, -1.7025, -1.2182, -1.1145, -2.6550, -2.7067]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 17
	action: tensor([[-0.5273,  0.0157, -0.0286,  0.1809, -0.1595,  0.0769, -0.0752]],
       dtype=torch.float64)
	q_value: tensor([[0.3806]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.24222601340010375, distance: 1.2754311013635224 entropy tensor([[-1.7600, -1.2747, -1.7236, -1.2395, -1.1270, -2.6073, -2.6605]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 18
	action: tensor([[-0.5130,  0.0848, -0.0045,  0.1136, -0.2346,  0.0367, -0.0793]],
       dtype=torch.float64)
	q_value: tensor([[0.3908]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.21290279918839783, distance: 1.260287684718683 entropy tensor([[-1.7455, -1.2470, -1.6079, -1.1818, -1.0967, -2.7443, -2.8910]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 19
	action: tensor([[-0.4519,  0.0440, -0.0882,  0.0666, -0.1293,  0.0814, -0.0709]],
       dtype=torch.float64)
	q_value: tensor([[0.3839]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.18095129188080672, distance: 1.2435770140581683 entropy tensor([[-1.7250, -1.2524, -1.6670, -1.1867, -1.0890, -2.8280, -2.7492]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 20
	action: tensor([[-0.4721,  0.0645, -0.0999,  0.0696, -0.0372,  0.0429, -0.0719]],
       dtype=torch.float64)
	q_value: tensor([[0.3953]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.19724637747995732, distance: 1.2521272267015033 entropy tensor([[-1.7772, -1.3076, -1.7138, -1.2195, -1.1294, -2.6926, -2.8622]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 21
	action: tensor([[-0.5572,  0.0841,  0.0684,  0.0027, -0.0296,  0.0813, -0.0673]],
       dtype=torch.float64)
	q_value: tensor([[0.3916]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.31019921454224364, distance: 1.309861449565189 entropy tensor([[-1.7838, -1.2937, -1.7283, -1.2403, -1.1552, -2.5851, -2.8406]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 22
	action: tensor([[-0.4532, -0.1118, -0.0336,  0.2081, -0.0909,  0.0582, -0.0557]],
       dtype=torch.float64)
	q_value: tensor([[0.3757]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2256793262422354, distance: 1.2669081316930093 entropy tensor([[-1.7260, -1.2570, -1.6847, -1.2175, -1.0918, -2.5704, -2.6530]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 23
	action: tensor([[-0.3892,  0.0730, -0.0439, -0.0796, -0.0871,  0.0718, -0.0752]],
       dtype=torch.float64)
	q_value: tensor([[0.3997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.13891317126854918, distance: 1.2212427570454818 entropy tensor([[-1.7844, -1.2602, -1.5665, -1.2005, -1.1366, -2.8877, -3.0345]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 24
	action: tensor([[-0.4904,  0.0108, -0.0109,  0.0626, -0.0681,  0.0457, -0.0258]],
       dtype=torch.float64)
	q_value: tensor([[0.3897]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2574212348938325, distance: 1.28320808838126 entropy tensor([[-1.7797, -1.3428, -1.7719, -1.2338, -1.1408, -2.7435, -2.6451]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 25
	action: tensor([[-0.4696,  0.0997, -0.0106, -0.0159, -0.0861,  0.0727, -0.0822]],
       dtype=torch.float64)
	q_value: tensor([[0.3894]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.19701193293466845, distance: 1.2520046250481687 entropy tensor([[-1.7667, -1.2800, -1.6921, -1.2337, -1.1434, -2.6679, -2.7855]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 26
	action: tensor([[-0.4976,  0.1070, -0.0257,  0.0033,  0.0030,  0.0426, -0.0619]],
       dtype=torch.float64)
	q_value: tensor([[0.3837]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.22500874318863984, distance: 1.2665615143767452 entropy tensor([[-1.7594, -1.3036, -1.7466, -1.2234, -1.1102, -2.6723, -2.6842]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 27
	action: tensor([[-0.5444,  0.0869, -0.0580,  0.0785, -0.0775,  0.0395, -0.0601]],
       dtype=torch.float64)
	q_value: tensor([[0.3817]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2715455526540904, distance: 1.2903949503747543 entropy tensor([[-1.7731, -1.2796, -1.7423, -1.2446, -1.1416, -2.5790, -2.7065]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 28
	action: tensor([[-0.4615, -0.0074,  0.0075, -0.0454, -0.0896,  0.0532, -0.0617]],
       dtype=torch.float64)
	q_value: tensor([[0.3865]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2827695158329684, distance: 1.2960776113133323 entropy tensor([[-1.7508, -1.2600, -1.7189, -1.2248, -1.1254, -2.5710, -2.7759]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 29
	action: tensor([[-0.4706,  0.0454, -0.0630,  0.0783, -0.1221,  0.0513, -0.0866]],
       dtype=torch.float64)
	q_value: tensor([[0.3882]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2024545290101687, distance: 1.25484771589322 entropy tensor([[-1.7615, -1.3009, -1.7227, -1.2314, -1.1324, -2.7190, -2.6139]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 30
	action: tensor([[-0.4303,  0.0481, -0.0822,  0.0085,  0.0076,  0.0860, -0.0565]],
       dtype=torch.float64)
	q_value: tensor([[0.3914]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.16809477843860177, distance: 1.2367893433889863 entropy tensor([[-1.7667, -1.2882, -1.7041, -1.2201, -1.1279, -2.7029, -2.8273]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 31
	action: tensor([[-0.4520,  0.0189, -0.0665, -0.0393, -0.1194,  0.0425, -0.0791]],
       dtype=torch.float64)
	q_value: tensor([[0.3925]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.24936417718306192, distance: 1.279090336717925 entropy tensor([[-1.7966, -1.3235, -1.7388, -1.2467, -1.1616, -2.5449, -2.8000]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 32
	action: tensor([[-0.4887,  0.1135,  0.0007,  0.0163, -0.2084,  0.0485, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[0.3930]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2043569870678159, distance: 1.2558399994161327 entropy tensor([[-1.7721, -1.3101, -1.7387, -1.2308, -1.1408, -2.7176, -2.6458]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 33
	action: tensor([[-0.4714,  0.1087,  0.0401, -0.0098, -0.0971,  0.0703, -0.0670]],
       dtype=torch.float64)
	q_value: tensor([[0.3839]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.18986954168943182, distance: 1.2482637742159515 entropy tensor([[-1.7349, -1.2919, -1.7274, -1.2049, -1.0922, -2.8103, -2.6728]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 34
	action: tensor([[-0.5427,  0.0464, -0.0643,  0.0732, -0.0112,  0.0706, -0.0526]],
       dtype=torch.float64)
	q_value: tensor([[0.3791]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.29354655497308935, distance: 1.3015106467062814 entropy tensor([[-1.7447, -1.2994, -1.7340, -1.2163, -1.0994, -2.7373, -2.6762]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 35
	action: tensor([[-0.4950,  0.1197, -0.1088, -0.0708, -0.0492,  0.0562, -0.0612]],
       dtype=torch.float64)
	q_value: tensor([[0.3902]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.24015302636397107, distance: 1.2743664577048486 entropy tensor([[-1.7658, -1.2696, -1.7057, -1.2331, -1.1398, -2.4793, -2.7879]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 36
	action: tensor([[-0.5783,  0.0555, -0.0151,  0.0520, -0.1110,  0.0279, -0.0894]],
       dtype=torch.float64)
	q_value: tensor([[0.3917]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.35232632976666767, distance: 1.330752973273019 entropy tensor([[-1.7773, -1.2978, -1.7851, -1.2473, -1.1481, -2.5517, -2.5977]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 37
	action: tensor([[-0.4412,  0.0611, -0.0082, -0.0539, -0.0758,  0.0679, -0.0566]],
       dtype=torch.float64)
	q_value: tensor([[0.3834]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.20563647035699262, distance: 1.2565069112090606 entropy tensor([[-1.7402, -1.2434, -1.6979, -1.2136, -1.1086, -2.5897, -2.6517]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 38
	action: tensor([[-0.5044,  0.0375, -0.0687,  0.0370, -0.1184,  0.0552, -0.0698]],
       dtype=torch.float64)
	q_value: tensor([[0.3868]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.27079354025861035, distance: 1.2900133138243957 entropy tensor([[-1.7724, -1.3146, -1.7554, -1.2338, -1.1327, -2.7040, -2.6641]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 39
	action: tensor([[-0.4940,  0.0723, -0.0088,  0.0434, -0.1753,  0.0773, -0.0674]],
       dtype=torch.float64)
	q_value: tensor([[0.3928]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.22280101442360145, distance: 1.2654196918023335 entropy tensor([[-1.7656, -1.2847, -1.7221, -1.2258, -1.1286, -2.6361, -2.7408]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 40
	action: tensor([[-0.4952, -0.0356, -0.0687,  0.1981,  0.0433,  0.0797, -0.0559]],
       dtype=torch.float64)
	q_value: tensor([[0.3868]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.22843381592131973, distance: 1.2683309047182572 entropy tensor([[-1.7476, -1.2888, -1.7010, -1.2039, -1.0947, -2.7295, -2.7404]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 41
	action: tensor([[-0.5170,  0.1504, -0.0071,  0.0803,  0.0238,  0.0819, -0.0562]],
       dtype=torch.float64)
	q_value: tensor([[0.3945]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.17260110307593268, distance: 1.2391727156276378 entropy tensor([[-1.7826, -1.2655, -1.6175, -1.2285, -1.1472, -2.5349, -3.0305]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 42
	action: tensor([[-0.5415,  0.1404, -0.0046,  0.0330, -0.2022,  0.0283, -0.0610]],
       dtype=torch.float64)
	q_value: tensor([[0.3772]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.25042007535399513, distance: 1.2796307331572132 entropy tensor([[-1.7504, -1.2629, -1.7056, -1.2289, -1.1177, -2.5222, -2.8251]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 43
	action: tensor([[-0.4929,  0.0693, -0.0395,  0.1103, -0.0332,  0.0591, -0.0943]],
       dtype=torch.float64)
	q_value: tensor([[0.3795]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.19457725374873336, distance: 1.2507307107733665 entropy tensor([[-1.7215, -1.2631, -1.7216, -1.2035, -1.0834, -2.7447, -2.6433]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 44
	action: tensor([[-0.4536, -0.1434, -0.0616, -0.0125, -0.1503,  0.0703, -0.0599]],
       dtype=torch.float64)
	q_value: tensor([[0.3854]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.35715554161779695, distance: 1.3331269420398155 entropy tensor([[-1.7583, -1.2732, -1.6934, -1.2230, -1.1313, -2.5959, -2.8716]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 45
	action: tensor([[-0.5245,  0.1239, -0.0758,  0.0884, -0.0377,  0.0571, -0.0850]],
       dtype=torch.float64)
	q_value: tensor([[0.4018]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.21125728465587068, distance: 1.259432494362686 entropy tensor([[-1.7684, -1.2955, -1.6622, -1.2038, -1.1470, -2.7826, -2.6295]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 46
	action: tensor([[-0.4734,  0.0342,  0.0124, -0.1001, -0.0269,  0.0551, -0.0752]],
       dtype=torch.float64)
	q_value: tensor([[0.3848]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2917389656202545, distance: 1.3006009696532064 entropy tensor([[-1.7607, -1.2653, -1.7283, -1.2286, -1.1262, -2.5186, -2.8102]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 47
	action: tensor([[-0.4888,  0.0889, -0.0424,  0.1125, -0.1526,  0.0434, -0.0489]],
       dtype=torch.float64)
	q_value: tensor([[0.3843]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.179978884939489, distance: 1.2430649218316974 entropy tensor([[-1.7734, -1.2987, -1.7542, -1.2427, -1.1345, -2.6363, -2.5337]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 48
	action: tensor([[-0.5555, -0.0135, -0.0515,  0.0381, -0.0485,  0.0367, -0.0422]],
       dtype=torch.float64)
	q_value: tensor([[0.3882]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3780950222827628, distance: 1.343371948229334 entropy tensor([[-1.7509, -1.2726, -1.6928, -1.2101, -1.1128, -2.7499, -2.8453]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 49
	action: tensor([[-0.5282,  0.1486,  0.0451,  0.0189,  0.0039,  0.0597, -0.0901]],
       dtype=torch.float64)
	q_value: tensor([[0.3934]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2219338803416051, distance: 1.2649709339378195 entropy tensor([[-1.7653, -1.2602, -1.6957, -1.2361, -1.1468, -2.5278, -2.6822]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 2 actor 0.2569747271592532 critic 8.576319765437113
epoch: 3, step: 0
	action: tensor([[-0.4928,  0.1358, -0.1148,  0.2102, -0.0598,  0.2868, -0.1418]],
       dtype=torch.float64)
	q_value: tensor([[0.1542]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06953806204750079, distance: 1.1834633568257966 entropy tensor([[-1.1305, -0.8645, -1.2807, -0.9068, -0.6982, -1.3702, -1.4333]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 1
	action: tensor([[-0.5139,  0.2847, -0.0509, -0.0894, -0.1103,  0.2312, -0.1539]],
       dtype=torch.float64)
	q_value: tensor([[0.1674]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.126449241673247, distance: 1.2145419134565385 entropy tensor([[-1.1154, -0.8564, -1.2154, -0.8212, -0.6594, -1.2908, -1.4239]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 2
	action: tensor([[-0.4173,  0.1106, -0.0954, -0.0109, -0.2060,  0.1489, -0.0719]],
       dtype=torch.float64)
	q_value: tensor([[0.1546]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.11337742677894602, distance: 1.2074743091262052 entropy tensor([[-1.1192, -0.8909, -1.3072, -0.8672, -0.6598, -1.3221, -1.3317]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 3
	action: tensor([[-0.4119,  0.2339, -0.0268,  0.0450, -0.1639,  0.1329, -0.1240]],
       dtype=torch.float64)
	q_value: tensor([[0.1722]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.004329965187007234, distance: 1.1418640809308076 entropy tensor([[-1.1399, -0.9230, -1.3238, -0.8686, -0.7048, -1.4254, -1.4170]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 4
	action: tensor([[-0.6831,  0.2857, -0.0175,  0.1123,  0.0393,  0.2614, -0.1322]],
       dtype=torch.float64)
	q_value: tensor([[0.1627]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.22318262717962556, distance: 1.2656171330132568 entropy tensor([[-1.1280, -0.9047, -1.3156, -0.8712, -0.6986, -1.4463, -1.4292]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 5
	action: tensor([[-0.5856,  0.0644,  0.0406,  0.1792,  0.0481,  0.1189, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[0.1502]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.25470959463583465, distance: 1.2818237167056326 entropy tensor([[-1.0757, -0.8019, -1.2303, -0.8324, -0.6006, -1.1829, -1.3472]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 6
	action: tensor([[-0.5197,  0.0136, -0.1047,  0.0390,  0.0405,  0.2679, -0.0669]],
       dtype=torch.float64)
	q_value: tensor([[0.1581]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.253086942663153, distance: 1.280994589909606 entropy tensor([[-1.1171, -0.8278, -1.2141, -0.8668, -0.6823, -1.2902, -1.4414]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 7
	action: tensor([[-0.5058,  0.1466,  0.0188,  0.1202, -0.2547,  0.1810, -0.0509]],
       dtype=torch.float64)
	q_value: tensor([[0.1766]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1301110212545522, distance: 1.2165143841940294 entropy tensor([[-1.1279, -0.8812, -1.2636, -0.8606, -0.6801, -1.2429, -1.4104]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 8
	action: tensor([[-0.6517,  0.1479,  0.0025, -0.0590,  0.1503,  0.1813, -0.1015]],
       dtype=torch.float64)
	q_value: tensor([[0.1554]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3851561253431166, distance: 1.3468091452025306 entropy tensor([[-1.1027, -0.8512, -1.2578, -0.8233, -0.6635, -1.3996, -1.3886]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 9
	action: tensor([[-0.6050,  0.0996, -0.0255, -0.0354, -0.0266,  0.2421, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[0.1617]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3428113694555326, distance: 1.3260631241238559 entropy tensor([[-1.1033, -0.8190, -1.2813, -0.8925, -0.6506, -1.1946, -1.3432]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 10
	action: tensor([[-0.6105,  0.1558, -0.0660, -0.0241, -0.0272,  0.2580, -0.1477]],
       dtype=torch.float64)
	q_value: tensor([[0.1647]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3027262853090231, distance: 1.3061206064457302 entropy tensor([[-1.1070, -0.8600, -1.2703, -0.8612, -0.6489, -1.2387, -1.3374]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 11
	action: tensor([[-0.5638,  0.2416, -0.1476, -0.1066, -0.0131,  0.1293, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.1573]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.24022735668330153, distance: 1.2744046476073925 entropy tensor([[-1.1122, -0.8596, -1.2590, -0.8510, -0.6464, -1.2250, -1.3360]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 12
	action: tensor([[-0.7161,  0.1970, -0.0359, -0.1395, -0.3496,  0.2461, -0.1015]],
       dtype=torch.float64)
	q_value: tensor([[0.1697]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.500055015459816, distance: 1.4015554578919318 entropy tensor([[-1.1272, -0.8715, -1.3616, -0.9052, -0.6838, -1.2766, -1.3265]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 13
	action: tensor([[-0.5955, -0.0522, -0.0641,  0.1348, -0.1738,  0.2900, -0.2009]],
       dtype=torch.float64)
	q_value: tensor([[0.1402]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.36682740832684946, distance: 1.3378688212841678 entropy tensor([[-1.0436, -0.8363, -1.2505, -0.7807, -0.5960, -1.2352, -1.1913]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 14
	action: tensor([[-0.5127,  0.0167,  0.1056, -0.0323,  0.0795,  0.1880, -0.0956]],
       dtype=torch.float64)
	q_value: tensor([[0.1501]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.28036738140337825, distance: 1.294863514973449 entropy tensor([[-1.0972, -0.8452, -1.1776, -0.7858, -0.6395, -1.2486, -1.3429]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 15
	action: tensor([[-0.4540,  0.1468,  0.0843,  0.0343,  0.1885,  0.2293, -0.0743]],
       dtype=torch.float64)
	q_value: tensor([[0.1624]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.05427128187992314, distance: 1.1749865128601218 entropy tensor([[-1.1414, -0.8810, -1.2446, -0.8906, -0.6857, -1.2945, -1.4480]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 16
	action: tensor([[-0.5728, -0.0581, -0.0120,  0.0183, -0.0765,  0.2041, -0.0960]],
       dtype=torch.float64)
	q_value: tensor([[0.1680]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4032919468927192, distance: 1.3555973454676762 entropy tensor([[-1.1446, -0.8823, -1.2529, -0.8942, -0.6799, -1.2737, -1.5146]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 17
	action: tensor([[-0.6145, -0.0099,  0.0413,  0.2291, -0.1613,  0.1939, -0.0767]],
       dtype=torch.float64)
	q_value: tensor([[0.1607]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3079926519648919, distance: 1.3087579879062112 entropy tensor([[-1.1184, -0.8753, -1.2360, -0.8540, -0.6746, -1.2935, -1.3696]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 18
	action: tensor([[-0.4909,  0.1267, -0.1011, -0.1325, -0.0767,  0.1853, -0.0839]],
       dtype=torch.float64)
	q_value: tensor([[0.1501]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.23043680044240444, distance: 1.2693645023034423 entropy tensor([[-1.0964, -0.8156, -1.1647, -0.8023, -0.6589, -1.2941, -1.3521]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 19
	action: tensor([[-0.5716, -0.1131, -0.0327, -0.0236, -0.1726,  0.3199, -0.1043]],
       dtype=torch.float64)
	q_value: tensor([[0.1687]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4447466221956671, distance: 1.3754744802411945 entropy tensor([[-1.1378, -0.9147, -1.3359, -0.8930, -0.6938, -1.3191, -1.3564]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 20
	action: tensor([[-0.3294,  0.1561,  0.1282,  0.2187, -0.1884,  0.1688,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[0.1593]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3754744802411945 entropy tensor([[-1.1057, -0.8791, -1.2194, -0.8043, -0.6403, -1.2561, -1.3231]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 21
	action: tensor([[-1.2868,  0.2194, -0.0378,  0.2064,  0.0347,  0.1897, -0.5094]],
       dtype=torch.float64)
	q_value: tensor([[0.4292]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.004642185611941, distance: 1.620224242423542 entropy tensor([[-0.4312, -0.1796, -0.9412, -0.5095, -0.0352, -0.4073, -0.2778]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 22
	action: tensor([[-0.7920, -0.0180, -0.0246,  0.2673,  0.1413,  0.3419, -0.2097]],
       dtype=torch.float64)
	q_value: tensor([[0.1394]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.501237770641352, distance: 1.4021078944121743 entropy tensor([[-0.8680, -0.5658, -1.1004, -0.6472, -0.4239, -0.8226, -0.9520]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 23
	action: tensor([[-5.5172e-01,  1.1257e-01, -4.0245e-04, -3.0239e-03, -6.8695e-02,
          1.0599e-01, -6.7531e-02]], dtype=torch.float64)
	q_value: tensor([[0.1639]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.32224832210951426, distance: 1.3158706660776665 entropy tensor([[-1.0256, -0.7698, -1.1401, -0.7518, -0.5949, -1.0558, -1.3100]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 24
	action: tensor([[-0.6368,  0.1355, -0.0569, -0.1323,  0.0185,  0.2680, -0.1145]],
       dtype=torch.float64)
	q_value: tensor([[0.1582]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.43301012362083635, distance: 1.3698762067847432 entropy tensor([[-1.1252, -0.8708, -1.2847, -0.8864, -0.6851, -1.3453, -1.3922]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 25
	action: tensor([[-0.6144,  0.1331, -0.0422,  0.0262, -0.1977,  0.2747, -0.1043]],
       dtype=torch.float64)
	q_value: tensor([[0.1638]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.352872527479821, distance: 1.3310216883148007 entropy tensor([[-1.1018, -0.8491, -1.2809, -0.8665, -0.6428, -1.1916, -1.2911]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 26
	action: tensor([[-0.6815,  0.1727, -0.1248, -0.0225, -0.1492,  0.2206, -0.1400]],
       dtype=torch.float64)
	q_value: tensor([[0.1508]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.449711579650091, distance: 1.3778359030039269 entropy tensor([[-1.0934, -0.8541, -1.2281, -0.8104, -0.6345, -1.2669, -1.3068]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 27
	action: tensor([[-0.5983,  0.1570,  0.0418, -0.0213,  0.0451,  0.2551, -0.1151]],
       dtype=torch.float64)
	q_value: tensor([[0.1526]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3094193234324829, distance: 1.3094715465062532 entropy tensor([[-1.0908, -0.8403, -1.2578, -0.8266, -0.6283, -1.2233, -1.2739]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 28
	action: tensor([[-0.5281, -0.0048,  0.0255,  0.1640,  0.1400,  0.1226, -0.1261]],
       dtype=torch.float64)
	q_value: tensor([[0.1545]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2734717996390885, distance: 1.2913719813321078 entropy tensor([[-1.1122, -0.8528, -1.2372, -0.8656, -0.6408, -1.2376, -1.3675]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 29
	action: tensor([[-0.5602,  0.1612,  0.0973,  0.1531,  0.0094,  0.2292, -0.1196]],
       dtype=torch.float64)
	q_value: tensor([[0.1630]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1628759715691388, distance: 1.2340233904854805 entropy tensor([[-1.1317, -0.8485, -1.1983, -0.8793, -0.7034, -1.2807, -1.4967]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 30
	action: tensor([[-0.5315,  0.1367, -0.0823, -0.0371, -0.2032,  0.2796, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[0.1462]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.27242265998278925, distance: 1.290839928438029 entropy tensor([[-1.1066, -0.8493, -1.1981, -0.8431, -0.6459, -1.2926, -1.4253]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 31
	action: tensor([[-0.6485,  0.0817, -0.0753,  0.0445, -0.0145,  0.1290, -0.0553]],
       dtype=torch.float64)
	q_value: tensor([[0.1619]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.45074133610040024, distance: 1.3783251670623118 entropy tensor([[-1.1096, -0.8963, -1.2833, -0.8306, -0.6534, -1.3070, -1.3286]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 32
	action: tensor([[-0.6302,  0.0365, -0.0647, -0.1485,  0.1469,  0.2453, -0.0768]],
       dtype=torch.float64)
	q_value: tensor([[0.1610]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.49860671469566564, distance: 1.4008786947011116 entropy tensor([[-1.1128, -0.8380, -1.2653, -0.8692, -0.6619, -1.2436, -1.3541]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 33
	action: tensor([[-0.6225, -0.0441, -0.0731, -0.0715, -0.1613,  0.3920, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[0.1775]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5109036644156424, distance: 1.406614469300221 entropy tensor([[-1.1006, -0.8360, -1.3109, -0.8841, -0.6656, -1.1553, -1.3106]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 34
	action: tensor([[-0.6105,  0.0241, -0.0275, -0.1911,  0.0333,  0.3043, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[0.1650]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4935090050151918, distance: 1.398494027609655 entropy tensor([[-1.0792, -0.8641, -1.2348, -0.7928, -0.6175, -1.1913, -1.2636]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 35
	action: tensor([[-0.7910,  0.0772,  0.0701, -0.0088, -0.1813,  0.1999, -0.0568]],
       dtype=torch.float64)
	q_value: tensor([[0.1720]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6568778346956861, distance: 1.4729972543339025 entropy tensor([[-1.1021, -0.8601, -1.2884, -0.8641, -0.6482, -1.1848, -1.2818]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 36
	action: tensor([[-0.6310,  0.1683,  0.0645,  0.2885, -0.2565,  0.3648, -0.1451]],
       dtype=torch.float64)
	q_value: tensor([[0.1370]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1648809228240038, distance: 1.2350867417289166 entropy tensor([[-1.0620, -0.7898, -1.1901, -0.7945, -0.6002, -1.1959, -1.2234]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 37
	action: tensor([[-0.5882,  0.2067,  0.0209,  0.1326, -0.3161,  0.2838, -0.2450]],
       dtype=torch.float64)
	q_value: tensor([[0.1376]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2128367865297398, distance: 1.260253388453225 entropy tensor([[-1.0451, -0.7903, -1.1337, -0.7373, -0.5778, -1.2301, -1.2903]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 38
	action: tensor([[-0.5826, -0.0760, -0.0431, -0.0098, -0.3340,  0.2593, -0.1264]],
       dtype=torch.float64)
	q_value: tensor([[0.1363]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.49922439749240666, distance: 1.4011673660106312 entropy tensor([[-1.0621, -0.8251, -1.2061, -0.7587, -0.6076, -1.2864, -1.2645]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 39
	action: tensor([[-0.6064,  0.0310, -0.0875, -0.0151, -0.2034,  0.1984, -0.1467]],
       dtype=torch.float64)
	q_value: tensor([[0.1494]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.45657878505890026, distance: 1.3810954146380958 entropy tensor([[-1.0908, -0.8649, -1.2197, -0.7723, -0.6395, -1.2951, -1.2912]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 40
	action: tensor([[-0.7862, -0.0608,  0.0190,  0.1115, -0.0966,  0.2513, -0.0755]],
       dtype=torch.float64)
	q_value: tensor([[0.1517]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6694099014921986, distance: 1.4785573883530239 entropy tensor([[-1.1101, -0.8702, -1.2521, -0.8257, -0.6533, -1.2849, -1.3163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 41
	action: tensor([[-0.5774,  0.0562, -0.1107,  0.1322,  0.0447,  0.4804, -0.0257]],
       dtype=torch.float64)
	q_value: tensor([[0.1492]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.23015978777892276, distance: 1.269221605961774 entropy tensor([[-1.0569, -0.7836, -1.1710, -0.7833, -0.6077, -1.1504, -1.2579]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 42
	action: tensor([[-0.6005, -0.0205, -0.0604,  0.2582, -0.1000,  0.2020, -0.1138]],
       dtype=torch.float64)
	q_value: tensor([[0.1810]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.34045877465910435, distance: 1.3249009888856251 entropy tensor([[-1.0722, -0.8330, -1.2146, -0.7797, -0.6043, -1.1222, -1.3763]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 43
	action: tensor([[-0.6016, -0.0560,  0.0316,  0.0135,  0.0228,  0.1933, -0.1047]],
       dtype=torch.float64)
	q_value: tensor([[0.1603]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4749282398925989, distance: 1.3897674595543514 entropy tensor([[-1.1044, -0.8184, -1.1652, -0.8104, -0.6643, -1.2782, -1.3849]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 44
	action: tensor([[-0.5006, -0.0588, -0.0510, -0.0347, -0.3144,  0.2001, -0.0352]],
       dtype=torch.float64)
	q_value: tensor([[0.1620]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3943082851082711, distance: 1.351251214384541 entropy tensor([[-1.1128, -0.8605, -1.2237, -0.8653, -0.6777, -1.2530, -1.3809]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 45
	action: tensor([[-0.6046,  0.1138, -0.0593,  0.1912, -0.0785,  0.2043, -0.0940]],
       dtype=torch.float64)
	q_value: tensor([[0.1640]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2807759354573416, distance: 1.295070088317644 entropy tensor([[-1.1143, -0.8911, -1.2686, -0.8189, -0.6755, -1.3797, -1.3381]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 46
	action: tensor([[-0.5533, -0.0197,  0.0501,  0.0997, -0.0175,  0.1812, -0.0553]],
       dtype=torch.float64)
	q_value: tensor([[0.1571]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3395558092241897, distance: 1.3244546710431795 entropy tensor([[-1.1062, -0.8237, -1.2104, -0.8304, -0.6556, -1.2781, -1.3881]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 47
	action: tensor([[-0.4608,  0.0530, -0.0737,  0.0523, -0.1817,  0.1783, -0.1353]],
       dtype=torch.float64)
	q_value: tensor([[0.1627]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2158018694857078, distance: 1.2617929504176297 entropy tensor([[-1.1244, -0.8611, -1.2125, -0.8616, -0.6850, -1.3125, -1.4306]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 48
	action: tensor([[-0.4975,  0.1720, -0.1035,  0.1308, -0.1132,  0.1468,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[0.1628]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.15215094702148102, distance: 1.2283196056680263 entropy tensor([[-1.1396, -0.9000, -1.2679, -0.8474, -0.6913, -1.3893, -1.4134]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 49
	action: tensor([[-0.6244,  0.0631, -0.0161,  0.1523, -0.1184,  0.1978, -0.1147]],
       dtype=torch.float64)
	q_value: tensor([[0.1726]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.35941145004525277, distance: 1.3342344656412073 entropy tensor([[-1.1176, -0.8611, -1.2966, -0.8594, -0.6925, -1.3547, -1.4263]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 3 actor 25.362695508633035 critic 455.00175226283
epoch: 4, step: 0
	action: tensor([[-0.5884,  0.3141,  0.0326,  0.0579, -0.1964,  0.2998, -0.1643]],
       dtype=torch.float64)
	q_value: tensor([[-0.1081]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.15789721584213923, distance: 1.2313788731692097 entropy tensor([[-0.6860, -0.4269, -0.6204, -0.3908, -0.3495, -0.6472, -0.6476]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 1
	action: tensor([[-0.7832,  0.4758, -0.2405,  0.1857, -0.1739,  0.2247, -0.3261]],
       dtype=torch.float64)
	q_value: tensor([[-0.1086]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2873094132157923, distance: 1.2983690839246718 entropy tensor([[-0.6540, -0.4312, -0.6207, -0.3730, -0.3014, -0.6042, -0.5973]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 2
	action: tensor([[-0.7872,  0.1901, -0.2047,  0.1741,  0.2942,  0.0954,  0.1004]],
       dtype=torch.float64)
	q_value: tensor([[-0.1112]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5114135364952512, distance: 1.4068517885170055 entropy tensor([[-0.5726, -0.3187, -0.5525, -0.3228, -0.2181, -0.4956, -0.4552]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 3
	action: tensor([[-0.6434, -0.0093, -0.0200, -0.1438, -0.0315,  0.3114, -0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-0.0861]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5473185067885395, distance: 1.4234642134502031 entropy tensor([[-0.6376, -0.3431, -0.6732, -0.3818, -0.2971, -0.5040, -0.5783]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 4
	action: tensor([[-0.6421, -0.0061, -0.1076,  0.2569, -0.1522,  0.2961,  0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-0.1183]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.38160370261547083, distance: 1.3450809981530196 entropy tensor([[-0.6782, -0.4314, -0.6364, -0.3994, -0.3224, -0.5801, -0.5929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 5
	action: tensor([[-0.7195,  0.0246, -0.0047, -0.1355, -0.0663,  0.3985,  0.1104]],
       dtype=torch.float64)
	q_value: tensor([[-0.1086]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6004936700645291, distance: 1.4477169983616844 entropy tensor([[-0.6580, -0.3943, -0.5753, -0.3414, -0.3091, -0.5840, -0.6102]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 6
	action: tensor([[-0.5376,  0.3076,  0.0722,  0.2936,  0.0181,  0.0917, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[-0.1219]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.005987155387112919, distance: 1.147764825200215 entropy tensor([[-0.6321, -0.3701, -0.5720, -0.3463, -0.2629, -0.4888, -0.5224]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 7
	action: tensor([[-0.8581, -0.0877, -0.1008,  0.0786,  0.2449,  0.2968, -0.1019]],
       dtype=torch.float64)
	q_value: tensor([[-0.0851]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.7760204949747471, distance: 1.5250380510432495 entropy tensor([[-0.7026, -0.4255, -0.6680, -0.4206, -0.3651, -0.6915, -0.6922]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 8
	action: tensor([[-0.6334,  0.2456, -0.0494,  0.0869, -0.1250,  0.0348,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[-0.1185]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3111650680902358, distance: 1.3103441628969312 entropy tensor([[-0.5832, -0.3133, -0.5812, -0.3062, -0.2603, -0.4314, -0.5129]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 9
	action: tensor([[-0.7061,  0.1554,  0.0630, -0.0249,  0.1034,  0.2789, -0.1262]],
       dtype=torch.float64)
	q_value: tensor([[-0.0939]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4414496204933185, distance: 1.373904124635466 entropy tensor([[-0.6823, -0.4401, -0.6903, -0.4363, -0.3640, -0.6813, -0.6545]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 10
	action: tensor([[-0.6752,  0.1071,  0.0696,  0.1864,  0.0963, -0.0284,  0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-0.1089]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4202358446183121, distance: 1.3637567964616337 entropy tensor([[-0.6559, -0.3970, -0.6185, -0.3815, -0.2937, -0.5433, -0.5889]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 11
	action: tensor([[-0.6927,  0.2845, -0.0230, -0.0538, -0.2299,  0.3408, -0.1407]],
       dtype=torch.float64)
	q_value: tensor([[-0.0978]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3719000477224037, distance: 1.3403491055832581 entropy tensor([[-0.6838, -0.3986, -0.6718, -0.4207, -0.3643, -0.6378, -0.6748]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 12
	action: tensor([[-0.5991,  0.0426, -0.1309,  0.2625, -0.0640,  0.4727,  0.1189]],
       dtype=torch.float64)
	q_value: tensor([[-0.1217]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2355096045318359, distance: 1.2719784578569577 entropy tensor([[-0.6197, -0.3954, -0.5801, -0.3495, -0.2725, -0.5396, -0.5363]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 13
	action: tensor([[-0.3973,  0.1123, -0.0220,  0.1168, -0.1845,  0.4613, -0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-0.1008]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.004200198868205884, distance: 1.1419384884481254 entropy tensor([[-0.6321, -0.3780, -0.5490, -0.3129, -0.2720, -0.5204, -0.5895]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 14
	action: tensor([[-0.5341,  0.3028, -0.0809, -0.0919, -0.1417,  0.3209, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-0.1018]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1662014756916863, distance: 1.235786613848248 entropy tensor([[-0.7202, -0.4937, -0.6091, -0.3786, -0.3219, -0.6432, -0.6747]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 15
	action: tensor([[-0.9091,  0.0315,  0.0877,  0.4367, -0.3201,  0.2342, -0.0920]],
       dtype=torch.float64)
	q_value: tensor([[-0.0964]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5418361949056907, distance: 1.4209402342249111 entropy tensor([[-0.6733, -0.4717, -0.6718, -0.4260, -0.3329, -0.6310, -0.6317]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 16
	action: tensor([[-0.6100,  0.0984,  0.2062, -0.0921,  0.0673,  0.6633, -0.2167]],
       dtype=torch.float64)
	q_value: tensor([[-0.1576]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2265894517233582, distance: 1.2673784143272278 entropy tensor([[-0.5379, -0.2575, -0.4480, -0.2112, -0.1998, -0.4188, -0.4199]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 17
	action: tensor([[-0.6418, -0.0149, -0.2103,  0.0184,  0.2070,  0.6729,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[-0.1327]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.33677490484722883, distance: 1.323079179512604 entropy tensor([[-0.6201, -0.3788, -0.4972, -0.2890, -0.1941, -0.4338, -0.4985]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 18
	action: tensor([[-0.7403, -0.0381,  0.0331,  0.0162, -0.0688,  0.3045, -0.1140]],
       dtype=torch.float64)
	q_value: tensor([[-0.1084]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6224079393364279, distance: 1.4575945128517522 entropy tensor([[-0.5692, -0.3259, -0.5489, -0.2835, -0.2163, -0.4051, -0.5074]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 19
	action: tensor([[-0.7024,  0.1248, -0.0910,  0.1237, -0.2644,  0.1097, -0.3684]],
       dtype=torch.float64)
	q_value: tensor([[-0.1282]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4731718412637884, distance: 1.3889397200307911 entropy tensor([[-0.6395, -0.3738, -0.5670, -0.3436, -0.2902, -0.5314, -0.5532]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 20
	action: tensor([[-0.5973, -0.0896,  0.0812,  0.0840, -0.1883,  0.0586, -0.5061]],
       dtype=torch.float64)
	q_value: tensor([[-0.1266]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4969952017584145, distance: 1.4001252810973086 entropy tensor([[-0.6480, -0.3850, -0.6107, -0.3556, -0.3074, -0.6081, -0.5455]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 21
	action: tensor([[-0.5205,  0.2687,  0.1701,  0.0174, -0.0905,  0.1907, -0.1389]],
       dtype=torch.float64)
	q_value: tensor([[-0.1365]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.12731967365762276, distance: 1.21501107440131 entropy tensor([[-0.6687, -0.3960, -0.6139, -0.3734, -0.3408, -0.6746, -0.5750]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 22
	action: tensor([[-0.4469,  0.1559, -0.0833,  0.1442, -0.0296,  0.1308, -0.3392]],
       dtype=torch.float64)
	q_value: tensor([[-0.1014]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.08789220842617951, distance: 1.1935747600970965 entropy tensor([[-0.6942, -0.4658, -0.6847, -0.4217, -0.3433, -0.6868, -0.6612]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 23
	action: tensor([[-0.5607,  0.1780, -0.1095,  0.2553,  0.0603,  0.2172, -0.2538]],
       dtype=torch.float64)
	q_value: tensor([[-0.0884]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.14781150234308793, distance: 1.2260042608842288 entropy tensor([[-0.7552, -0.4920, -0.6854, -0.4509, -0.4061, -0.7640, -0.7119]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 24
	action: tensor([[-0.5343,  0.1182,  0.0927, -0.0598, -0.0792,  0.2357, -0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-0.0904]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.28677271442853836, distance: 1.2980984008430407 entropy tensor([[-0.7024, -0.4288, -0.6281, -0.4007, -0.3487, -0.6500, -0.6628]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 25
	action: tensor([[-0.4273,  0.1329, -0.0178, -0.0529, -0.0285,  0.3451, -0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-0.1057]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.10262924076784308, distance: 1.2016318912695407 entropy tensor([[-0.7114, -0.4781, -0.6706, -0.4321, -0.3558, -0.6751, -0.6712]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 26
	action: tensor([[-0.5809,  0.1101, -0.0817,  0.0720, -0.0474,  0.4613,  0.0048]],
       dtype=torch.float64)
	q_value: tensor([[-0.0961]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.23996301038247103, distance: 1.2742688248868186 entropy tensor([[-0.7407, -0.5182, -0.6868, -0.4540, -0.3734, -0.6911, -0.7128]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 27
	action: tensor([[-0.6588, -0.1110,  0.0769,  0.2208,  0.1943,  0.1547, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-0.1054]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.47012130005844965, distance: 1.3875009151904671 entropy tensor([[-0.6654, -0.4210, -0.5907, -0.3624, -0.2933, -0.5509, -0.6074]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 28
	action: tensor([[-0.6420,  0.0719, -0.0399,  0.2001, -0.3350,  0.3692, -0.0558]],
       dtype=torch.float64)
	q_value: tensor([[-0.1099]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3441874898100288, distance: 1.3267424284607778 entropy tensor([[-0.6789, -0.3929, -0.6065, -0.3771, -0.3530, -0.5792, -0.6676]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 29
	action: tensor([[-0.6091,  0.2660,  0.0083,  0.5678, -0.2329,  0.2353, -0.2363]],
       dtype=torch.float64)
	q_value: tensor([[-0.1293]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.030128587951326113, distance: 1.1269736773917662 entropy tensor([[-0.6322, -0.3800, -0.5317, -0.2963, -0.2652, -0.5425, -0.5597]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 30
	action: tensor([[-0.7605,  0.0535, -0.1598,  0.2955, -0.0378,  0.3522,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[-0.1209]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4572671499149381, distance: 1.3814217221677574 entropy tensor([[-0.6236, -0.3404, -0.5112, -0.2835, -0.2524, -0.5547, -0.5311]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 31
	action: tensor([[-0.6757,  0.2679, -0.0378, -0.0358, -0.1618,  0.4390, -0.1103]],
       dtype=torch.float64)
	q_value: tensor([[-0.1099]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.31866698302625807, distance: 1.3140874252422372 entropy tensor([[-0.5992, -0.3287, -0.5411, -0.2941, -0.2521, -0.4921, -0.5353]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 32
	action: tensor([[-0.6257, -0.0196, -0.0549, -0.0326, -0.2770,  0.2206, -0.1814]],
       dtype=torch.float64)
	q_value: tensor([[-0.1185]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5287746154586499, distance: 1.4149087251331032 entropy tensor([[-0.6217, -0.3903, -0.5642, -0.3445, -0.2578, -0.5111, -0.5372]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 33
	action: tensor([[-0.4831,  0.2934, -0.1289, -0.0780, -0.3340,  0.2184, -0.1398]],
       dtype=torch.float64)
	q_value: tensor([[-0.1284]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.13919184469008927, distance: 1.221392156969287 entropy tensor([[-0.6715, -0.4291, -0.6209, -0.3722, -0.3355, -0.6349, -0.6032]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 34
	action: tensor([[-0.6637,  0.1605, -0.1252,  0.0201, -0.0779,  0.3220,  0.0467]],
       dtype=torch.float64)
	q_value: tensor([[-0.1039]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.38687762931625924, distance: 1.3476458080582647 entropy tensor([[-0.6817, -0.4912, -0.6864, -0.4300, -0.3533, -0.7049, -0.6417]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 35
	action: tensor([[-0.6946, -0.1450, -0.1769,  0.3846,  0.1721,  0.2314,  0.1322]],
       dtype=torch.float64)
	q_value: tensor([[-0.1023]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.49792552927181744, distance: 1.400560276732869 entropy tensor([[-0.6503, -0.4056, -0.6232, -0.3842, -0.3058, -0.5620, -0.5900]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 36
	action: tensor([[-0.5164,  0.1643, -0.1779,  0.0357,  0.1008,  0.4652, -0.2442]],
       dtype=torch.float64)
	q_value: tensor([[-0.1017]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.13103378695266077, distance: 1.2170109410224408 entropy tensor([[-0.6278, -0.3333, -0.5753, -0.3229, -0.3087, -0.5214, -0.6169]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 37
	action: tensor([[-0.6203,  0.2027, -0.1447,  0.0659, -0.5646,  0.1929, -0.2782]],
       dtype=torch.float64)
	q_value: tensor([[-0.0955]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.35337457746867385, distance: 1.3312686359857457 entropy tensor([[-0.6846, -0.4360, -0.6313, -0.3867, -0.3176, -0.5797, -0.6278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 38
	action: tensor([[-0.7379,  0.2468,  0.3332,  0.3558, -0.0067,  0.2907, -0.1596]],
       dtype=torch.float64)
	q_value: tensor([[-0.1352]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.0994732592447587, distance: 1.1999109839019753 entropy tensor([[-0.6064, -0.3859, -0.5840, -0.3160, -0.2763, -0.6036, -0.5326]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 39
	action: tensor([[-0.5818,  0.0791,  0.0781,  0.1572, -0.1551,  0.2614, -0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-0.1328]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2562793946562316, distance: 1.282625327659059 entropy tensor([[-0.5806, -0.3223, -0.4979, -0.2768, -0.2188, -0.4739, -0.5057]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 40
	action: tensor([[-0.6709, -0.1562,  0.0718,  0.0333,  0.0550,  0.0378, -0.0594]],
       dtype=torch.float64)
	q_value: tensor([[-0.1123]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6764008898759393, distance: 1.4816500315962746 entropy tensor([[-0.6876, -0.4344, -0.6018, -0.3706, -0.3358, -0.6394, -0.6431]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 41
	action: tensor([[-0.5802,  0.0966,  0.1366,  0.1450,  0.0351,  0.3181,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[-0.1140]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1971236539321839, distance: 1.252063050506661 entropy tensor([[-0.6823, -0.4090, -0.6620, -0.4173, -0.3824, -0.6424, -0.6579]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 42
	action: tensor([[-0.6510,  0.0231, -0.1032, -0.0664, -0.0762,  0.1931, -0.1620]],
       dtype=torch.float64)
	q_value: tensor([[-0.1027]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5370527481647165, distance: 1.4187343345465904 entropy tensor([[-0.6848, -0.4224, -0.5946, -0.3762, -0.3212, -0.5869, -0.6531]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 43
	action: tensor([[-0.3379,  0.1008, -0.1532,  0.0246, -0.3183,  0.2243, -0.1134]],
       dtype=torch.float64)
	q_value: tensor([[-0.1118]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.04685609374909827, distance: 1.1708471037135986 entropy tensor([[-0.6893, -0.4427, -0.6583, -0.4186, -0.3496, -0.6314, -0.6209]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 44
	action: tensor([[-0.5864, -0.0844,  0.3189, -0.1731, -0.3042,  0.1236, -0.1733]],
       dtype=torch.float64)
	q_value: tensor([[-0.0946]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6224435164857234, distance: 1.4576104942741963 entropy tensor([[-0.7564, -0.5420, -0.7207, -0.4478, -0.4135, -0.7984, -0.7427]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 45
	action: tensor([[-0.9132,  0.1600, -0.0239,  0.3168,  0.0428,  0.2536, -0.1599]],
       dtype=torch.float64)
	q_value: tensor([[-0.1427]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5254452351472287, distance: 1.4133671843491595 entropy tensor([[-0.6438, -0.4114, -0.6108, -0.3759, -0.3220, -0.6482, -0.5779]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 46
	action: tensor([[-0.7331,  0.4746, -0.1696,  0.0065, -0.2761,  0.4366,  0.0762]],
       dtype=torch.float64)
	q_value: tensor([[-0.1301]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.26407414621613445, distance: 1.2865982836979097 entropy tensor([[-0.5693, -0.2932, -0.5112, -0.2744, -0.2127, -0.4374, -0.4798]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 47
	action: tensor([[-0.6630,  0.1060,  0.0989,  0.0869, -0.0760,  0.3546, -0.2604]],
       dtype=torch.float64)
	q_value: tensor([[-0.1108]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.34180047397338953, distance: 1.3255638866145183 entropy tensor([[-0.5387, -0.3368, -0.5476, -0.3122, -0.2059, -0.4614, -0.4621]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 48
	action: tensor([[-0.6765,  0.0406, -0.1608, -0.0347,  0.0133,  0.2540, -0.1534]],
       dtype=torch.float64)
	q_value: tensor([[-0.1243]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5261605850820019, distance: 1.4136985412525946 entropy tensor([[-0.6546, -0.3957, -0.5649, -0.3347, -0.2865, -0.5477, -0.5639]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 49
	action: tensor([[-0.5877, -0.0870, -0.1801,  0.1938, -0.1727,  0.2352, -0.2097]],
       dtype=torch.float64)
	q_value: tensor([[-0.1047]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4345906955810128, distance: 1.370631467050728 entropy tensor([[-0.6775, -0.4198, -0.6543, -0.4042, -0.3356, -0.5865, -0.5988]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 4 actor 0.09338840103597482 critic 17.078386087714513
epoch: 5, step: 0
	action: tensor([[-0.7298, -0.0721, -0.6970,  0.2101,  0.0528,  0.2353, -0.0734]],
       dtype=torch.float64)
	q_value: tensor([[-0.2755]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.7328300815828264, distance: 1.5063804911393166 entropy tensor([[-0.3599, -0.1299, -0.0053, -0.0044, -0.0120, -0.2108, -0.1063]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 1
	action: tensor([[-0.9709,  0.0882,  0.2750,  0.1152, -0.1767,  0.1502,  0.1405]],
       dtype=torch.float64)
	q_value: tensor([[-0.2963]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.7978927901630741, distance: 1.534399995965763 entropy tensor([[-0.2365, -0.0320,  0.0461,  0.0504,  0.0609, -0.0731,  0.0215]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 2
	action: tensor([[-0.6532,  0.5196, -0.4062,  0.1701, -0.4957,  0.0326, -0.2301]],
       dtype=torch.float64)
	q_value: tensor([[-0.3241]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.16768942325494773, distance: 1.236574727896215 entropy tensor([[-0.1855,  0.0372,  0.1589,  0.1502,  0.1691,  0.0179,  0.1111]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 3
	action: tensor([[-0.4671, -0.0315, -0.5576,  0.5018, -0.0018,  0.4750, -0.5769]],
       dtype=torch.float64)
	q_value: tensor([[-0.2844]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.24146417803060816, distance: 1.2750399416484646 entropy tensor([[-0.2415, -0.0784,  0.0422,  0.0599,  0.0833, -0.1379,  0.0392]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 4
	action: tensor([[-0.6788,  0.2421, -0.5404, -0.2463,  0.1566, -0.1834, -0.6003]],
       dtype=torch.float64)
	q_value: tensor([[-0.3130]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.41450028176633213, distance: 1.3610002733272186 entropy tensor([[-0.2350, -0.0345,  0.1119,  0.1039,  0.0885, -0.0653,  0.0185]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 5
	action: tensor([[-0.6969,  0.4517, -0.2957, -0.3860,  0.5667,  0.4975,  0.0852]],
       dtype=torch.float64)
	q_value: tensor([[-0.2737]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.18275973705344506, distance: 1.244528823224737 entropy tensor([[-0.2833, -0.0273, -0.0125, -0.0293,  0.0104, -0.1340,  0.0365]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 6
	action: tensor([[-0.7531, -0.4082,  0.0394,  0.3407, -0.3838,  0.3139, -0.1836]],
       dtype=torch.float64)
	q_value: tensor([[-0.2636]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.7455211875095347, distance: 1.5118867315327706 entropy tensor([[-0.2276,  0.0173,  0.0604,  0.0546,  0.1566,  0.0343,  0.1022]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 7
	action: tensor([[-0.8564, -0.0090,  0.0541, -0.0786,  0.2125,  0.1760, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[-0.3486]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.8313426155979977, distance: 1.5486079843474712 entropy tensor([[-0.1999,  0.0212,  0.1832,  0.1844,  0.1516, -0.0187,  0.0851]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 8
	action: tensor([[-0.4634, -0.4022, -0.1781,  0.3667, -0.2767,  0.1104, -0.0682]],
       dtype=torch.float64)
	q_value: tensor([[-0.3016]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4476337383461988, distance: 1.3768481372415249 entropy tensor([[-0.2847, -0.0209,  0.0350,  0.0498,  0.0928, -0.0521,  0.0103]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 9
	action: tensor([[-0.6765,  0.4451,  0.0239,  0.0376, -0.4296,  0.6220,  0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-0.2850]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.16173815595263386, distance: 1.2334195278443172 entropy tensor([[-0.3397, -0.1214,  0.0197,  0.0117, -0.0345, -0.2423, -0.1234]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 10
	action: tensor([[-0.4801, -0.0928,  0.0076, -0.5563,  0.3314,  0.3916,  0.1756]],
       dtype=torch.float64)
	q_value: tensor([[-0.3041]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.48602164563215555, distance: 1.3949841110782608 entropy tensor([[-0.1806, -0.0189,  0.1762,  0.1620,  0.2059,  0.0398,  0.1199]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 11
	action: tensor([[-0.7710, -0.2051,  0.0894,  0.0711, -0.4061, -0.0494, -0.2042]],
       dtype=torch.float64)
	q_value: tensor([[-0.2971]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.8412524612779322, distance: 1.552792280917226 entropy tensor([[-0.3749, -0.1057, -0.0818, -0.0417,  0.0307, -0.1410, -0.0437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 12
	action: tensor([[-0.4876,  0.2380,  0.2029,  0.1254, -0.2242,  0.6937, -0.5070]],
       dtype=torch.float64)
	q_value: tensor([[-0.3177]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.09528324616459327, distance: 1.0884613432589518 entropy tensor([[-0.2500, -0.0219,  0.0525,  0.0794,  0.0504, -0.1347,  0.0046]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 13
	action: tensor([[-0.8735,  0.2868, -0.2501,  0.7321, -0.2016,  0.1308, -0.0058]],
       dtype=torch.float64)
	q_value: tensor([[-0.3229]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.29329987481208386, distance: 1.3013865413300416 entropy tensor([[-0.2648, -0.0580,  0.1402,  0.1522,  0.1850, -0.0129,  0.0578]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 14
	action: tensor([[-1.0481,  0.0986, -0.2152,  0.1543,  0.2051, -0.0838, -0.7695]],
       dtype=torch.float64)
	q_value: tensor([[-0.3214]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.9876611279047558, distance: 1.61334729584222 entropy tensor([[-0.1525,  0.0453,  0.1828,  0.1690,  0.1907,  0.0078,  0.1328]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 15
	action: tensor([[-0.7182, -0.1464, -0.3411,  0.0990, -0.0732,  0.1036,  0.3214]],
       dtype=torch.float64)
	q_value: tensor([[-0.3560]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.7336574440655768, distance: 1.5067400687988861 entropy tensor([[-0.1359,  0.1200,  0.1568,  0.1829,  0.2001,  0.0774,  0.1991]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 16
	action: tensor([[-1.0225,  0.0516, -0.0836, -0.0999, -0.1401, -0.0076, -0.3530]],
       dtype=torch.float64)
	q_value: tensor([[-0.2688]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.0689742334081358, distance: 1.6460166893460728 entropy tensor([[-0.2822, -0.0667,  0.0032,  0.0168,  0.0174, -0.1254, -0.0282]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 17
	action: tensor([[-0.7815,  0.4907,  0.1996, -0.2596,  0.3266,  0.4377, -0.3086]],
       dtype=torch.float64)
	q_value: tensor([[-0.3228]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.33158003628765265, distance: 1.3205058543566994 entropy tensor([[-0.2126,  0.0303,  0.0891,  0.1052,  0.1466, -0.0234,  0.1022]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 18
	action: tensor([[-0.4897,  0.1761,  0.3055, -0.0528, -0.0541,  0.2531, -0.5045]],
       dtype=torch.float64)
	q_value: tensor([[-0.2903]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.16522500294396147, distance: 1.2352691369458693 entropy tensor([[-0.2392,  0.0109,  0.0960,  0.1008,  0.1948,  0.0318,  0.0981]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 19
	action: tensor([[-0.6410, -0.0940, -0.1317, -0.0204,  0.0158,  0.2567,  0.2418]],
       dtype=torch.float64)
	q_value: tensor([[-0.2793]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5683880424045842, distance: 1.433122962177535 entropy tensor([[-0.3703, -0.1262, -0.0353,  0.0143,  0.0468, -0.1940, -0.0767]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 20
	action: tensor([[-0.6198,  0.1462, -0.8424, -0.3152,  0.3664,  0.2833, -0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-0.2713]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3045600563690245, distance: 1.3070395576940361 entropy tensor([[-0.3353, -0.1075, -0.0182, -0.0076,  0.0108, -0.1359, -0.0598]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 21
	action: tensor([[-0.9354, -0.1540, -0.2812,  0.1642, -0.0769,  0.3984, -0.3208]],
       dtype=torch.float64)
	q_value: tensor([[-0.2731]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.9361072565709911, distance: 1.5922871842003228 entropy tensor([[-0.2189, -0.0064,  0.0113, -0.0080,  0.0412, -0.0494,  0.0388]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 22
	action: tensor([[-0.7286,  0.2335,  0.6828,  0.0702,  0.2380,  0.5286, -0.4240]],
       dtype=torch.float64)
	q_value: tensor([[-0.3492]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.15753918591945215, distance: 1.231188482967879 entropy tensor([[-0.1757,  0.0466,  0.1537,  0.1690,  0.1714,  0.0338,  0.1287]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 23
	action: tensor([[-0.7261, -0.2311, -0.0128, -0.1128,  0.0213, -0.1819,  0.4174]],
       dtype=torch.float64)
	q_value: tensor([[-0.3450]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.9002184292701005, distance: 1.5774603654231616 entropy tensor([[-0.1601,  0.0643,  0.1918,  0.2259,  0.2658,  0.0846,  0.1523]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 24
	action: tensor([[-0.5422, -0.1798,  0.3443,  0.0014,  0.2122,  0.2759,  0.1656]],
       dtype=torch.float64)
	q_value: tensor([[-0.2921]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.42494103513207415, distance: 1.3660139672504346 entropy tensor([[-0.2931, -0.0670, -0.0422, -0.0088,  0.0028, -0.1529, -0.0520]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 25
	action: tensor([[-0.7224, -0.2119,  0.1572,  0.3978, -0.1155, -0.0627, -0.1268]],
       dtype=torch.float64)
	q_value: tensor([[-0.2985]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5734265598515176, distance: 1.4354231024653357 entropy tensor([[-0.3647, -0.1041, -0.0058,  0.0162,  0.0405, -0.1265, -0.0848]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 26
	action: tensor([[-1.0875, -0.1815,  0.0183,  0.0621,  0.1674,  0.2024, -0.2882]],
       dtype=torch.float64)
	q_value: tensor([[-0.2995]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.181468044185693, distance: 1.690172849439949 entropy tensor([[-0.2895, -0.0466,  0.0542,  0.0412,  0.0343, -0.1549, -0.0463]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 27
	action: tensor([[-0.6504,  0.4529, -0.3593,  0.0615,  0.0018,  0.2310, -0.4012]],
       dtype=torch.float64)
	q_value: tensor([[-0.3576]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.192414399527983, distance: 1.2495979360798164 entropy tensor([[-0.1554,  0.0987,  0.1642,  0.1876,  0.2026,  0.0846,  0.1518]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 28
	action: tensor([[-0.5918, -0.2151, -0.0754, -0.1892,  0.1264,  0.4955, -0.1168]],
       dtype=torch.float64)
	q_value: tensor([[-0.2632]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5624343622385348, distance: 1.4304002723063896 entropy tensor([[-0.3101, -0.1013,  0.0191,  0.0108,  0.0601, -0.1206,  0.0018]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 29
	action: tensor([[-0.7912,  0.5218, -0.1371,  0.1771, -0.1001,  0.3647, -0.2159]],
       dtype=torch.float64)
	q_value: tensor([[-0.3152]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.19819524799161403, distance: 1.2526233114302328 entropy tensor([[-0.3287, -0.0802,  0.0182,  0.0255,  0.0435, -0.1090, -0.0234]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 30
	action: tensor([[-0.4536, -0.1648, -0.0624, -0.0550,  0.3210,  0.0164,  0.2562]],
       dtype=torch.float64)
	q_value: tensor([[-0.2832]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.43568425887498874, distance: 1.3711537716824143 entropy tensor([[-0.2333, -0.0433,  0.1057,  0.0984,  0.1555, -0.0287,  0.0758]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 31
	action: tensor([[-0.4605,  0.1510,  0.0947,  0.0927,  0.0725,  0.2462, -0.3244]],
       dtype=torch.float64)
	q_value: tensor([[-0.2571]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06727848748670295, distance: 1.1822125656362197 entropy tensor([[-0.4253, -0.1811, -0.1529, -0.1092, -0.1151, -0.2539, -0.2024]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 32
	action: tensor([[-0.5449,  0.0117, -0.0320,  0.1290, -0.0589,  0.0033,  0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-0.2482]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.34816629862457193, distance: 1.3287045631755572 entropy tensor([[-0.4274, -0.1866, -0.0709, -0.0547, -0.0302, -0.2609, -0.1654]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 33
	action: tensor([[-0.6437,  0.3205,  0.3143,  0.2154,  0.0461,  0.4954, -0.4716]],
       dtype=torch.float64)
	q_value: tensor([[-0.2426]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.03845006100257331, distance: 1.1221285589263208 entropy tensor([[-0.4026, -0.1734, -0.0976, -0.0847, -0.0861, -0.2876, -0.1889]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 34
	action: tensor([[-0.9790, -0.0806,  0.2344,  0.0587,  0.0517,  0.0921,  0.3225]],
       dtype=torch.float64)
	q_value: tensor([[-0.3068]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.998807403468247, distance: 1.6178645832755285 entropy tensor([[-0.2399, -0.0377,  0.1273,  0.1441,  0.1824, -0.0086,  0.0646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 35
	action: tensor([[-0.5353, -0.2614,  0.0710,  0.0948, -0.0708, -0.0007, -0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-0.3280]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5461426086919376, distance: 1.4229232236712044 entropy tensor([[-0.2057,  0.0452,  0.1316,  0.1348,  0.1655,  0.0328,  0.1116]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 36
	action: tensor([[-0.5053,  0.6284,  0.0544, -0.1723, -0.4666,  0.1380, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-0.2754]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4229232236712044 entropy tensor([[-0.3796, -0.1397, -0.0681, -0.0549, -0.0762, -0.2934, -0.1716]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 37
	action: tensor([[-1.4872, -0.0113,  0.3967, -0.4197, -0.3230,  0.4592, -0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-0.4046]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.6988000911101198, distance: 1.8799316142547517 entropy tensor([[0.2633, 0.5114, 0.5255, 0.3965, 0.5936, 0.4926, 0.6708]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 38
	action: tensor([[-0.7941,  0.0906,  0.6008,  0.2497, -0.1556,  0.7360, -0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-0.4427]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.11220272993924074, distance: 1.2068371529596342 entropy tensor([[0.0748, 0.3070, 0.3969, 0.4331, 0.4817, 0.3645, 0.4381]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 39
	action: tensor([[-0.8796,  0.2096,  0.0027,  0.3117, -0.1898,  0.4432,  0.6447]],
       dtype=torch.float64)
	q_value: tensor([[-0.3717]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.40049482023678085, distance: 1.3542456421161957 entropy tensor([[-0.0878,  0.1049,  0.3110,  0.3025,  0.3216,  0.1723,  0.2361]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 40
	action: tensor([[-0.6505, -0.0728, -0.0676, -0.0608, -0.1482,  0.5829, -0.1097]],
       dtype=torch.float64)
	q_value: tensor([[-0.3052]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5085046829977358, distance: 1.4054973290797164 entropy tensor([[-0.1209,  0.1014,  0.2272,  0.2068,  0.2419,  0.1109,  0.1879]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 41
	action: tensor([[-0.6653,  0.0736, -0.1939,  0.0824,  0.3231,  0.4028,  0.9594]],
       dtype=torch.float64)
	q_value: tensor([[-0.3186]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.35802840141191994, distance: 1.333555575985554 entropy tensor([[-0.2858, -0.0624,  0.0999,  0.0879,  0.1138, -0.0489,  0.0353]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 42
	action: tensor([[-0.5037, -0.0832,  0.1942,  0.2457, -0.0546,  0.3999, -0.0664]],
       dtype=torch.float64)
	q_value: tensor([[-0.2962]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.12379799947144754, distance: 1.213111781681712 entropy tensor([[-0.1613,  0.0657,  0.0979,  0.1561,  0.1914,  0.1042,  0.1577]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 43
	action: tensor([[-0.6721,  0.0314, -0.2048,  0.2583, -0.0650, -0.0769,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[-0.2853]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.48895790674377326, distance: 1.3963616200276625 entropy tensor([[-0.3562, -0.1277,  0.0494,  0.0338,  0.0410, -0.1495, -0.0860]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 44
	action: tensor([[-0.5954,  0.1032, -0.1119,  0.0587, -0.5033,  0.2262, -0.5803]],
       dtype=torch.float64)
	q_value: tensor([[-0.2548]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.385309721895398, distance: 1.3468838153076266 entropy tensor([[-0.3584, -0.1283, -0.0486, -0.0529, -0.0491, -0.2370, -0.1158]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 45
	action: tensor([[-0.4855, -0.0870,  0.0709,  0.4641, -0.2671,  0.1720, -0.2013]],
       dtype=torch.float64)
	q_value: tensor([[-0.3156]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.09327875182047896, distance: 1.196526019174999 entropy tensor([[-0.2915, -0.0555,  0.0418,  0.0944,  0.0754, -0.1281, -0.0073]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 46
	action: tensor([[-0.4406,  0.2709, -0.1463,  0.4037,  0.0612,  0.2626, -0.3249]],
       dtype=torch.float64)
	q_value: tensor([[-0.2780]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.196526019174999 entropy tensor([[-0.3613, -0.1287,  0.0331,  0.0270,  0.0085, -0.2189, -0.1120]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 47
	action: tensor([[-1.4199,  0.8615, -0.4822,  0.3270,  0.3567,  0.3636, -0.1386]],
       dtype=torch.float64)
	q_value: tensor([[-0.4046]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.196526019174999 entropy tensor([[0.2633, 0.5114, 0.5255, 0.3965, 0.5936, 0.4926, 0.6708]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 48
	action: tensor([[-1.1793,  0.4804,  0.3072,  0.7933, -0.6480,  0.7495, -0.4249]],
       dtype=torch.float64)
	q_value: tensor([[-0.4046]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.06818448738455107, distance: 1.1046422790089574 entropy tensor([[0.2633, 0.5114, 0.5255, 0.3965, 0.5936, 0.4926, 0.6708]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 49
	action: tensor([[-1.5376,  0.1907, -0.1007, -0.4278, -0.2866,  0.4057,  0.3817]],
       dtype=torch.float64)
	q_value: tensor([[-0.4553]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.5908002069417182, distance: 1.8419322622233218 entropy tensor([[0.1330, 0.2989, 0.4848, 0.4977, 0.5277, 0.3778, 0.4485]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 5 actor 74.22881862483638 critic 918.3596122677881
epoch: 6, step: 0
	action: tensor([[-1.2292,  0.2377, -0.2326,  0.5062, -0.1356,  0.6695, -0.0527]],
       dtype=torch.float64)
	q_value: tensor([[-0.7470]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5810209963306137, distance: 1.4388831009477348 entropy tensor([[0.6804, 0.7166, 0.8547, 0.8253, 0.9251, 0.8246, 0.9075]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 1
	action: tensor([[-1.1304,  0.1493,  0.2061, -0.9492, -0.6189,  0.6402,  0.5058]],
       dtype=torch.float64)
	q_value: tensor([[-0.7627]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.3587535805588735, distance: 1.7575107070048333 entropy tensor([[0.6536, 0.6521, 0.8288, 0.7965, 0.8358, 0.7681, 0.8422]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 2
	action: tensor([[-1.4637,  0.9924,  0.3160, -0.8414,  0.3789,  0.0930,  0.4705]],
       dtype=torch.float64)
	q_value: tensor([[-0.7486]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.0767087774376671, distance: 1.649090510213266 entropy tensor([[0.6642, 0.7017, 0.8568, 0.8291, 0.9389, 0.7860, 0.9162]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 3
	action: tensor([[-1.2853, -0.4400, -0.2574, -0.5262,  0.0587,  1.1319,  0.6522]],
       dtype=torch.float64)
	q_value: tensor([[-0.7196]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.35820798372928, distance: 1.7573074327557294 entropy tensor([[0.7138, 0.7730, 0.8848, 0.8520, 1.0272, 0.8499, 0.9753]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 4
	action: tensor([[-9.6363e-01,  1.2300e+00,  1.3107e-01,  6.7958e-04,  6.5651e-01,
         -2.1114e-01,  5.3608e-01]], dtype=torch.float64)
	q_value: tensor([[-0.8045]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.7573074327557294 entropy tensor([[0.7565, 0.8009, 0.9470, 0.9249, 1.0111, 0.9129, 1.0186]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 5
	action: tensor([[-2.2529,  0.7762, -0.1905,  0.0078,  0.2121,  1.7561, -0.7533]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.7573074327557294 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 6
	action: tensor([[-1.2415, -0.5991, -0.8016,  0.8542, -1.4651,  0.8170, -0.7870]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.5478603879399184, distance: 1.8266044303256708 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 7
	action: tensor([[-1.8235,  0.7242,  0.7779,  0.5780, -0.0722, -0.1209, -1.3511]],
       dtype=torch.float64)
	q_value: tensor([[-0.9067]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.8266044303256708 entropy tensor([[1.0039, 1.0017, 1.1410, 1.1675, 1.1546, 1.0897, 1.1704]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 8
	action: tensor([[-0.7728, -0.0467,  0.4473,  0.7039, -1.2998,  1.0315,  0.6560]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2100281482819255, distance: 1.2587933208260682 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 9
	action: tensor([[-0.9242,  0.4015, -0.5104,  1.7397, -1.3770, -1.0684, -0.8344]],
       dtype=torch.float64)
	q_value: tensor([[-0.8130]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.376785514822642, distance: 0.9033903824949402 entropy tensor([[0.8469, 0.8468, 1.0357, 1.0279, 1.0509, 0.9499, 1.0510]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 10
	action: tensor([[-0.7742, -0.0788, -0.4951,  0.2529, -0.5279,  0.0820, -0.4707]],
       dtype=torch.float64)
	q_value: tensor([[-0.8808]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.7764666915343488, distance: 1.5252296096428974 entropy tensor([[0.9387, 0.9383, 1.0380, 1.0265, 1.0523, 0.9680, 1.1370]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 11
	action: tensor([[-1.4248,  0.6601, -0.1313, -0.2573,  0.6946,  0.2128, -0.4488]],
       dtype=torch.float64)
	q_value: tensor([[-0.6647]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.9382898832121473, distance: 1.593184445831634 entropy tensor([[0.4061, 0.4569, 0.5766, 0.5616, 0.5596, 0.4723, 0.5812]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 12
	action: tensor([[-1.1209,  0.8753, -0.1700, -0.0021, -0.1454, -0.4499,  0.2867]],
       dtype=torch.float64)
	q_value: tensor([[-0.7609]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4338310924783202, distance: 1.37026855037603 entropy tensor([[0.6692, 0.7320, 0.8459, 0.7759, 0.9073, 0.8040, 0.9027]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 13
	action: tensor([[-1.5492,  0.2517,  0.8226,  0.3117, -0.1874,  0.4258, -0.3696]],
       dtype=torch.float64)
	q_value: tensor([[-0.6334]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.9004405228890093, distance: 1.577552547890076 entropy tensor([[0.5109, 0.5549, 0.6649, 0.6220, 0.7277, 0.5648, 0.7423]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 14
	action: tensor([[-1.0495,  0.0313,  0.1237,  0.2482, -0.9003, -0.2048, -1.0690]],
       dtype=torch.float64)
	q_value: tensor([[-0.8282]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.900405466909656, distance: 1.5775379978676167 entropy tensor([[0.8286, 0.8212, 0.9698, 0.9760, 1.0270, 0.9233, 1.0395]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 15
	action: tensor([[-1.2812, -0.5798,  0.8773, -0.6696,  0.4353,  0.6343,  0.1692]],
       dtype=torch.float64)
	q_value: tensor([[-0.7882]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.6483653580460302, distance: 1.8622828443632249 entropy tensor([[0.6686, 0.7226, 0.7626, 0.8267, 0.8065, 0.7028, 0.8745]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 16
	action: tensor([[-1.6008,  0.2745,  0.4838, -1.1087, -0.2421, -0.0366,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[-0.8163]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.6711368015424473, distance: 1.8702719425215195 entropy tensor([[0.7339, 0.7939, 0.9022, 0.9309, 1.0099, 0.8727, 1.0170]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 17
	action: tensor([[-1.6312,  1.3227,  0.5306, -0.1112,  1.1457,  0.8028, -0.8777]],
       dtype=torch.float64)
	q_value: tensor([[-0.7791]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.8702719425215195 entropy tensor([[0.7288, 0.8099, 0.8844, 0.8883, 1.0174, 0.8631, 0.9989]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 18
	action: tensor([[-1.5203, -0.2620,  0.4510,  0.2367, -0.1631,  0.1096, -1.8202]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.610820367793448, distance: 1.8490352444361295 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 19
	action: tensor([[-1.4069,  0.0081, -0.2813, -1.1646, -0.3438, -0.1676, -0.1216]],
       dtype=torch.float64)
	q_value: tensor([[-0.8882]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.1248931991884246, distance: 1.6681121534782648 entropy tensor([[0.9551, 0.9870, 1.0562, 1.1207, 1.1029, 1.0225, 1.1674]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 20
	action: tensor([[-1.4002,  0.8785, -0.0262, -0.4778,  0.8574,  1.3649,  0.4993]],
       dtype=torch.float64)
	q_value: tensor([[-0.7411]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1406942851641826, distance: 1.222197317384726 entropy tensor([[0.6332, 0.7442, 0.7940, 0.7769, 0.8814, 0.7690, 0.8958]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 21
	action: tensor([[-1.0302,  0.0177, -0.7823, -0.8930,  0.1865, -0.5797, -0.4825]],
       dtype=torch.float64)
	q_value: tensor([[-0.8311]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4810316493120279, distance: 1.3926399934509595 entropy tensor([[0.8809, 0.8958, 1.0753, 0.9869, 1.1512, 1.0334, 1.1134]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 22
	action: tensor([[-1.2942,  0.1418,  0.7415, -0.1703,  0.2165, -0.5592,  0.1868]],
       dtype=torch.float64)
	q_value: tensor([[-0.6958]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.506551901876989, distance: 1.8117365427717576 entropy tensor([[0.5111, 0.6431, 0.6963, 0.6205, 0.7018, 0.6082, 0.7478]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 23
	action: tensor([[-0.5857, -0.7536, -0.2834, -0.8451, -0.5408,  0.1905, -0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-0.7217]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.878088276129102, distance: 1.568247825718977 entropy tensor([[0.5911, 0.6595, 0.7219, 0.7063, 0.8252, 0.6655, 0.8110]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 24
	action: tensor([[-0.8204, -0.2753,  0.2885,  0.2360,  0.0200,  0.5036, -0.3120]],
       dtype=torch.float64)
	q_value: tensor([[-0.7027]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5975957754598473, distance: 1.4464057679201785 entropy tensor([[0.4608, 0.5471, 0.5890, 0.6361, 0.6291, 0.5221, 0.6605]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 25
	action: tensor([[-0.5785,  0.3311,  0.3971,  0.0296, -0.0600, -0.5060, -0.0281]],
       dtype=torch.float64)
	q_value: tensor([[-0.6904]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3351915525920659, distance: 1.3222953822208916 entropy tensor([[0.4550, 0.4770, 0.6575, 0.6360, 0.6649, 0.5639, 0.6562]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 26
	action: tensor([[-0.5459, -0.0653, -0.2680,  0.0660, -0.5519, -0.1211, -0.1698]],
       dtype=torch.float64)
	q_value: tensor([[-0.5250]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4346196118244092, distance: 1.370645280507596 entropy tensor([[0.2349, 0.2974, 0.3584, 0.3582, 0.4351, 0.2309, 0.4150]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 27
	action: tensor([[-0.3798, -0.1669,  0.0784, -0.3531, -0.2704,  0.8382, -0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-0.5656]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.29480302745491405, distance: 1.3021425974420744 entropy tensor([[0.2519, 0.2891, 0.4066, 0.3996, 0.3910, 0.2635, 0.3993]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 28
	action: tensor([[-1.7143,  0.2572,  0.3872, -0.1318, -0.4731,  0.5982,  0.2060]],
       dtype=torch.float64)
	q_value: tensor([[-0.6521]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3021425974420744 entropy tensor([[0.3147, 0.3793, 0.5572, 0.5379, 0.6026, 0.4380, 0.5638]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 29
	action: tensor([[-0.5746,  0.5037, -0.9203, -0.2642,  0.5121,  0.0650, -0.1556]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.004507740811527161, distance: 1.1417621371869129 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 30
	action: tensor([[-0.8463,  0.1749, -0.5825, -0.4449,  0.2144,  0.2672, -0.2992]],
       dtype=torch.float64)
	q_value: tensor([[-0.6099]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6366019911673801, distance: 1.4639566975099991 entropy tensor([[0.3627, 0.4335, 0.5484, 0.4520, 0.5322, 0.4696, 0.5498]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 31
	action: tensor([[-0.5899,  0.0893, -0.8728,  0.8630, -0.6732,  0.6169, -0.8122]],
       dtype=torch.float64)
	q_value: tensor([[-0.6377]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639566975099991 entropy tensor([[0.3661, 0.4609, 0.5635, 0.5013, 0.5621, 0.4893, 0.5751]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 32
	action: tensor([[-1.6768,  0.2724,  0.7528,  0.2850, -0.5211,  0.3200, -1.9683]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639566975099991 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 33
	action: tensor([[-0.8057,  1.9563,  0.1495, -0.3228,  0.2714,  0.1061, -1.5581]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639566975099991 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 34
	action: tensor([[-2.0989,  0.5292, -0.7125, -0.1650, -0.8535,  0.1451,  0.2197]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639566975099991 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 35
	action: tensor([[-1.2139, -0.5733,  0.0139,  0.1535, -0.5337, -0.4600, -1.0111]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.4627480198240739, distance: 1.795836038054464 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 36
	action: tensor([[-1.5338,  0.3559, -0.1767, -0.7640,  0.5306, -0.1369, -0.2011]],
       dtype=torch.float64)
	q_value: tensor([[-0.7846]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.502040281060463, distance: 1.8101053079052032 entropy tensor([[0.6931, 0.7647, 0.7948, 0.8266, 0.7995, 0.7413, 0.8844]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 37
	action: tensor([[-0.6855,  0.3962, -0.0750,  1.4034,  0.5830, -0.5476, -0.5718]],
       dtype=torch.float64)
	q_value: tensor([[-0.7433]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.12992893340089284, distance: 1.0674168561286412 entropy tensor([[0.6397, 0.7623, 0.8080, 0.7562, 0.8993, 0.7857, 0.9012]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 38
	action: tensor([[-1.4333,  0.8560,  0.0896,  0.1474,  0.0818, -0.1653,  0.2545]],
       dtype=torch.float64)
	q_value: tensor([[-0.7096]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0674168561286412 entropy tensor([[0.6081, 0.6160, 0.7568, 0.6909, 0.7834, 0.6636, 0.8100]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 39
	action: tensor([[-2.0503, -0.3531,  0.1554, -0.4602,  0.0869,  1.6686, -1.1245]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0674168561286412 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 40
	action: tensor([[-1.6986, -0.7677,  0.0371, -0.5475,  1.1366,  0.7063, -0.3244]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0674168561286412 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 41
	action: tensor([[-0.9563, -0.1018, -0.3819,  0.9520, -0.0899,  0.7810, -0.6369]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3640947970894821, distance: 1.3365307941047384 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 42
	action: tensor([[-0.6491,  0.9889, -0.8697,  0.1889,  0.2164,  0.2598, -1.1093]],
       dtype=torch.float64)
	q_value: tensor([[-0.8074]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3365307941047384 entropy tensor([[0.7215, 0.6958, 0.8948, 0.8611, 0.8745, 0.8152, 0.8953]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 43
	action: tensor([[-1.7149, -0.3020, -1.1061,  0.3382, -0.3302,  0.0191, -1.2392]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3365307941047384 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 44
	action: tensor([[-2.2585,  0.7198, -1.1654,  0.7943, -0.2893, -0.3240, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3365307941047384 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 45
	action: tensor([[-1.2629,  0.8848, -0.7034,  0.1562, -0.4289,  0.4127, -1.7212]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3365307941047384 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 46
	action: tensor([[-1.5742,  1.1637, -0.8354, -0.0794,  0.1780,  0.8017, -1.9462]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3365307941047384 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 47
	action: tensor([[-1.3433,  0.4315, -1.4133,  0.6589, -0.3012, -0.6300,  0.2256]],
       dtype=torch.float64)
	q_value: tensor([[-0.8072]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.0647800415150956, distance: 1.6443474534352387 entropy tensor([[0.8273, 0.9373, 1.0396, 0.9226, 1.0667, 0.9865, 1.0876]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 48
	action: tensor([[-0.9284,  0.1200,  0.6479, -0.2203, -0.3228,  0.0460, -1.1172]],
       dtype=torch.float64)
	q_value: tensor([[-0.7994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.0107273134156074, distance: 1.6226814891559738 entropy tensor([[0.7242, 0.7616, 0.8930, 0.8296, 0.8757, 0.8093, 0.9352]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 49
	action: tensor([[-1.8227, -0.1193, -1.1459, -0.2616,  0.7570,  0.8036, -0.1000]],
       dtype=torch.float64)
	q_value: tensor([[-0.7409]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6226814891559738 entropy tensor([[0.6054, 0.6692, 0.7170, 0.7801, 0.8022, 0.6616, 0.8223]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 6 actor 436.16520119209906 critic 2022.9581521107891
epoch: 7, step: 0
	action: tensor([[-1.0716, -0.2508, -1.3249,  1.4192,  0.3486,  0.3047,  0.9883]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.0615044903246815, distance: 1.6430426456637726 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 1
	action: tensor([[-2.4652,  1.2796, -1.5022,  0.3626,  0.4619, -0.6910, -1.2534]],
       dtype=torch.float64)
	q_value: tensor([[-0.9875]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6430426456637726 entropy tensor([[1.4291, 1.3343, 1.4068, 1.4089, 1.3660, 1.4749, 1.5137]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 2
	action: tensor([[-1.8523,  0.2011, -0.6842,  0.8352,  0.6484, -1.0901,  0.2470]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6430426456637726 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 3
	action: tensor([[-0.9872,  2.0831, -2.2634,  0.5921,  2.2448,  0.3625, -0.8282]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6430426456637726 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 4
	action: tensor([[ 0.2288, -1.1284, -0.6714, -0.3048,  1.1340, -0.4230, -0.7719]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.8031867824093932, distance: 1.5366573971817161 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 5
	action: tensor([[-1.3176,  0.0687,  0.2166, -0.1837, -0.1168,  0.7672,  0.7536]],
       dtype=torch.float64)
	q_value: tensor([[-0.9324]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.1723464285333418, distance: 1.6866354936533354 entropy tensor([[1.0719, 1.0494, 1.0830, 1.1318, 0.9693, 1.1211, 1.1477]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 6
	action: tensor([[-1.0766, -0.5912,  0.3496,  0.6045,  1.3068, -0.3631,  2.0673]],
       dtype=torch.float64)
	q_value: tensor([[-0.9726]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.174147203338007, distance: 1.6873344202810985 entropy tensor([[1.2707, 1.2317, 1.3105, 1.3256, 1.3230, 1.3719, 1.3921]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 7
	action: tensor([[-1.0655, -0.3010, -0.1907,  0.3419,  1.2949,  0.8748,  0.2258]],
       dtype=torch.float64)
	q_value: tensor([[-0.9896]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.702495719953327, distance: 1.4931371726780776 entropy tensor([[1.4518, 1.4137, 1.4369, 1.4865, 1.4846, 1.5455, 1.5720]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 8
	action: tensor([[-1.1417, -1.3788, -0.1923,  0.2768, -1.2755, -0.8496, -1.1979]],
       dtype=torch.float64)
	q_value: tensor([[-0.9864]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4931371726780776 entropy tensor([[1.3067, 1.2613, 1.3125, 1.3497, 1.2978, 1.4022, 1.4172]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 9
	action: tensor([[-2.6186,  1.0724, -1.0160,  2.0056, -0.0282, -0.2586,  0.4993]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4931371726780776 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 10
	action: tensor([[-2.5346, -2.2861, -1.4631, -0.1796, -1.0701,  0.3751, -1.2924]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4931371726780776 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 11
	action: tensor([[-2.2162, -0.8434,  0.3940,  0.0537,  1.6415, -0.9448, -0.2971]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4931371726780776 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 12
	action: tensor([[-1.4936,  0.8798, -1.2429,  1.4237, -0.2134,  1.4555, -0.7214]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.24194729755318378, distance: 1.2752880103326807 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 13
	action: tensor([[-1.8416,  0.7323, -1.5700,  2.2651,  0.2807, -0.2339, -1.0272]],
       dtype=torch.float64)
	q_value: tensor([[-0.9989]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2752880103326807 entropy tensor([[1.6999, 1.6054, 1.6978, 1.7061, 1.6575, 1.7610, 1.7633]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 14
	action: tensor([[-1.4082, -0.1980, -0.9804,  1.1307, -0.9807,  0.7146, -2.7338]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.2215845594296564, distance: 1.705642926093325 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 15
	action: tensor([[-4.7248,  2.9930,  0.2695, -1.9381, -3.4639,  2.6257, -0.1318]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.705642926093325 entropy tensor([[1.7896, 1.7452, 1.8109, 1.8761, 1.7563, 1.8829, 1.9106]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 16
	action: tensor([[-1.4137,  0.6290, -2.6750,  0.4294,  0.7378,  1.6665, -1.6606]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.824198482867875, distance: 1.5455844452978709 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 17
	action: tensor([[-2.5885,  0.1892,  1.0730,  2.6575, -1.1271,  0.6081, -0.2182]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5455844452978709 entropy tensor([[1.8377, 1.7881, 1.8836, 1.8866, 1.8168, 1.9290, 1.9599]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 18
	action: tensor([[-0.9295,  0.7589,  0.5168,  0.2074, -0.7145, -0.5425,  1.1762]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.20247593273868625, distance: 1.25485888400797 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 19
	action: tensor([[-1.1386,  0.6280, -0.2004, -0.0269,  0.9696,  0.3266,  1.5601]],
       dtype=torch.float64)
	q_value: tensor([[-0.9516]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4913237249692144, distance: 1.3974705253034592 entropy tensor([[1.2971, 1.2488, 1.2966, 1.3146, 1.3152, 1.3309, 1.4001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 20
	action: tensor([[-1.7342,  0.7249, -0.3071,  0.6360, -1.5264, -1.2153, -0.7691]],
       dtype=torch.float64)
	q_value: tensor([[-0.9755]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3974705253034592 entropy tensor([[1.3550, 1.3233, 1.3648, 1.3770, 1.3969, 1.4496, 1.4771]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 21
	action: tensor([[-1.0089, -0.3914,  0.6239,  0.9618, -0.6412,  0.5090, -1.5881]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.22120734938495312, distance: 1.2645948181725775 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 22
	action: tensor([[0.6399, 1.6675, 0.1457, 0.3972, 0.2390, 1.8617, 0.3347]],
       dtype=torch.float64)
	q_value: tensor([[-0.9963]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2645948181725775 entropy tensor([[1.4952, 1.4307, 1.4975, 1.5885, 1.4655, 1.5676, 1.5974]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 23
	action: tensor([[-0.0968, -0.3083, -0.0695,  0.1705,  2.0476, -0.8189, -0.0807]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06688985303302286, distance: 1.1819973029971735 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 24
	action: tensor([[-0.9684,  0.4500,  0.4830, -0.3921, -1.1367,  1.5909,  0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-0.9556]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.46601918956510713, distance: 1.3855637763918933 entropy tensor([[1.1626, 1.1468, 1.1289, 1.1716, 1.1171, 1.2065, 1.2517]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 25
	action: tensor([[-1.2274,  1.0044, -0.5004, -1.5747,  0.1560, -1.5135,  1.2458]],
       dtype=torch.float64)
	q_value: tensor([[-0.9934]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.07321343700933958, distance: 1.185495047577882 entropy tensor([[1.4818, 1.4229, 1.5238, 1.5658, 1.5352, 1.5593, 1.6002]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 26
	action: tensor([[-1.3060, -1.1579,  0.1140,  0.5269, -1.9136,  0.4026, -0.1435]],
       dtype=torch.float64)
	q_value: tensor([[-0.9768]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.6419146492723216, distance: 1.8600134504691996 entropy tensor([[1.5202, 1.5323, 1.5810, 1.5493, 1.6103, 1.6100, 1.7062]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 27
	action: tensor([[-2.8542,  1.2728, -0.7155,  1.1262, -1.1951,  0.0151, -0.7003]],
       dtype=torch.float64)
	q_value: tensor([[-0.9973]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.8600134504691996 entropy tensor([[1.6225, 1.5871, 1.6463, 1.7109, 1.6347, 1.7116, 1.7390]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 28
	action: tensor([[-3.5910, -1.2748, -1.1351, -0.9831,  0.7826, -1.2269, -0.9494]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.8600134504691996 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 29
	action: tensor([[-1.8163, -0.2186, -1.7246, -0.8203, -0.8923,  1.2532,  1.7562]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.8600134504691996 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 30
	action: tensor([[-1.9074,  0.9124, -1.0150, -0.3106,  0.2901,  1.8073, -2.5484]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.8600134504691996 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 31
	action: tensor([[ 0.1380, -0.2998,  0.4649,  0.3732,  0.2961, -1.0258, -0.7316]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.1572063546324315, distance: 1.050551432922345 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 32
	action: tensor([[-0.1391, -1.0692, -1.4721,  0.7273, -0.2186,  0.2991, -0.1862]],
       dtype=torch.float64)
	q_value: tensor([[-0.8304]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.9966512440415736, distance: 1.6169917339901823 entropy tensor([[0.8486, 0.8247, 0.8217, 0.8579, 0.7783, 0.8556, 0.9334]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 33
	action: tensor([[-2.0915, -0.4125, -0.1290, -0.6056, -0.7763,  1.0646,  0.5024]],
       dtype=torch.float64)
	q_value: tensor([[-0.9716]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6169917339901823 entropy tensor([[1.1850, 1.1316, 1.1961, 1.2046, 1.0998, 1.2200, 1.2709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 34
	action: tensor([[-2.4529,  1.1669,  0.5393,  2.7908,  0.9070,  0.3119, -0.6723]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6169917339901823 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 35
	action: tensor([[-1.2079, -1.3324,  0.6053, -0.2795, -0.2920,  0.5321, -0.8914]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6169917339901823 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 36
	action: tensor([[-1.5887,  0.8033,  0.0473, -0.8827, -1.6333,  1.6295, -0.9837]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.879493244322354, distance: 1.568834306776784 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 37
	action: tensor([[-3.4902,  0.1387,  1.6456, -0.1662, -0.9348,  2.7922, -0.5535]],
       dtype=torch.float64)
	q_value: tensor([[-0.9992]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.568834306776784 entropy tensor([[1.6836, 1.6625, 1.7304, 1.7892, 1.7269, 1.7844, 1.8155]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 38
	action: tensor([[-0.5404,  1.3973,  0.2675,  0.3484,  0.0582,  0.4037, -0.3312]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.568834306776784 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 39
	action: tensor([[-1.6137,  1.0181, -0.6111, -0.3866, -0.5754,  0.9057, -1.5811]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6310216072103398, distance: 1.4614587119510536 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 40
	action: tensor([[-3.6339, -0.2507,  2.0544,  1.8613,  1.6370,  1.3712,  1.6247]],
       dtype=torch.float64)
	q_value: tensor([[-0.9985]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4614587119510536 entropy tensor([[1.5399, 1.5319, 1.5987, 1.6371, 1.5743, 1.6550, 1.6845]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 41
	action: tensor([[-3.3319, -0.0085, -0.3022, -0.1691, -1.5170, -0.8363, -0.8021]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4614587119510536 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 42
	action: tensor([[-2.1063,  0.9176,  0.2746,  0.3799, -0.7921,  0.7182, -1.6463]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4614587119510536 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 43
	action: tensor([[-1.4654, -0.4149, -0.5182,  1.5489,  1.5135,  0.8187,  0.5554]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5126370341657505, distance: 1.4074211005017618 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 44
	action: tensor([[-0.0103, -1.3207, -0.7730, -0.0901,  0.8227,  1.6871,  0.9430]],
       dtype=torch.float64)
	q_value: tensor([[-0.9972]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4074211005017618 entropy tensor([[1.6127, 1.5249, 1.5852, 1.6167, 1.5733, 1.6909, 1.6973]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 45
	action: tensor([[-0.8253,  0.0276,  1.5053, -1.8673, -0.4580,  1.1363,  1.0510]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.8290751720220895, distance: 1.5476489970019627 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 46
	action: tensor([[-1.3081,  1.0502, -1.2425, -1.0254, -0.0506,  1.2825, -0.3443]],
       dtype=torch.float64)
	q_value: tensor([[-0.9930]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.07909067067351794, distance: 1.1887366760668836 entropy tensor([[1.5544, 1.5647, 1.6304, 1.6572, 1.6832, 1.6421, 1.7485]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 47
	action: tensor([[-1.3740, -0.8052,  0.9331,  0.4992, -0.0726,  1.9443,  0.6125]],
       dtype=torch.float64)
	q_value: tensor([[-0.9947]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2959492335695233, distance: 1.3027188216294021 entropy tensor([[1.4492, 1.4481, 1.5592, 1.5214, 1.5258, 1.5928, 1.6088]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 48
	action: tensor([[-2.8350,  1.3579, -2.0116,  3.9559, -2.5717,  1.2882, -0.8015]],
       dtype=torch.float64)
	q_value: tensor([[-0.9974]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3027188216294021 entropy tensor([[1.6943, 1.6154, 1.7425, 1.7748, 1.7372, 1.7886, 1.8197]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 49
	action: tensor([[-1.1918,  0.2946, -0.0317,  1.6021,  0.0956,  0.6560,  0.5665]],
       dtype=torch.float64)
	q_value: tensor([[-0.9913]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.421993266266004, distance: 0.8700077919224907 entropy tensor([[1.4064, 1.4428, 1.5244, 1.4563, 1.4886, 1.5559, 1.5948]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 7 actor 624.5762951890034 critic 2193.883812952431
epoch: 8, step: 0
	action: tensor([[-3.4851,  0.3639, -0.9185,  2.1079, -0.7281,  1.7417,  1.1818]],
       dtype=torch.float64)
	q_value: tensor([[-0.9991]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8700077919224907 entropy tensor([[1.8121, 1.6401, 1.6970, 1.8433, 1.6551, 1.8516, 1.8473]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 1
	action: tensor([[-3.8250, -0.3158, -2.8193,  1.4473, -1.0359,  0.7652, -1.6152]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8700077919224907 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 2
	action: tensor([[-2.6213,  0.3619,  1.1198, -0.7260,  0.3737,  2.1779, -4.8675]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8700077919224907 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 3
	action: tensor([[-1.6895,  1.3459, -1.9027, -1.7531,  0.3218, -0.9492,  0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8700077919224907 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 4
	action: tensor([[ 1.8165,  0.7769, -0.6603,  3.3748, -1.9689,  0.6055, -0.2452]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8700077919224907 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 5
	action: tensor([[-2.3101,  1.8786, -1.1743,  0.6159, -1.1958,  1.9702, -2.6135]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8700077919224907 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 6
	action: tensor([[-2.5745,  2.4688, -2.5415,  0.2547, -0.7002, -0.2432, -0.7184]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8700077919224907 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 7
	action: tensor([[ 0.1548,  2.5870,  1.8153,  1.5510,  1.5206, -1.5123,  0.0933]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8700077919224907 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 8
	action: tensor([[-1.4540,  1.1523,  1.1756, -0.7742, -3.6234,  1.5097, -1.5358]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6826646855953797, distance: 1.4844155097292298 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 9
	action: tensor([[-2.4962, -1.9652, -1.5478, -3.5989, -5.5140,  3.6280, -0.9263]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4844155097292298 entropy tensor([[2.2880, 2.1540, 2.2127, 2.3740, 2.1616, 2.3656, 2.3928]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 10
	action: tensor([[-2.2744,  1.0956, -0.1542, -0.8318,  1.3602,  0.1450,  1.9472]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4844155097292298 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 11
	action: tensor([[ 0.0050,  2.1312,  1.4376,  1.8255,  1.2048, -0.3718, -0.2845]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4844155097292298 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 12
	action: tensor([[ 0.1303,  0.3972,  0.2405,  1.8253,  0.8948,  1.3833, -1.5209]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7057930397441783, distance: 0.6207020285480433 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 13
	action: tensor([[ 0.2747,  2.4033, -1.1493, -1.5223,  0.3011, -0.3307, -3.0154]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6207020285480433 entropy tensor([[1.9338, 1.7267, 1.7962, 1.9605, 1.7460, 1.9422, 1.9392]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 14
	action: tensor([[-2.3254, -2.5702,  1.3079, -0.4369,  1.6598, -1.4061, -0.4499]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6207020285480433 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 15
	action: tensor([[-2.2493,  0.5071, -0.0558, -0.2040, -0.7041, -0.1638,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6207020285480433 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 16
	action: tensor([[-1.6016,  1.4139,  1.2783, -1.3769,  0.1330, -1.0540, -0.3277]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5025856904711001, distance: 1.40273721011182 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 17
	action: tensor([[-2.5915,  0.1834, -1.5632, -0.4754,  2.8482, -1.4342,  4.0434]],
       dtype=torch.float64)
	q_value: tensor([[-0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.40273721011182 entropy tensor([[2.0142, 1.9210, 1.9305, 2.0904, 1.9388, 2.0835, 2.1449]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 18
	action: tensor([[-0.6117,  2.5727,  2.2193, -1.3084, -0.3471,  0.7483, -1.7112]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.40273721011182 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 19
	action: tensor([[ 0.0489,  3.8231, -0.0477, -0.7134, -1.0382, -1.0955, -0.5217]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.40273721011182 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 20
	action: tensor([[-1.4775,  0.0459,  1.8516,  1.6868, -2.6528, -1.0961,  1.5534]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.10720095967684129, distance: 1.0812684988572105 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 21
	action: tensor([[-2.3547, -2.4655,  1.0082, -0.6205,  3.6321, -3.0702,  0.7737]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0812684988572105 entropy tensor([[2.4047, 2.2425, 2.2784, 2.4427, 2.2599, 2.4401, 2.4508]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 22
	action: tensor([[ 0.1447,  1.3269,  1.1528, -0.1485, -1.8061, -1.8090, -1.7019]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8686036018582026, distance: 0.4148092415527354 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 23
	action: tensor([[-3.6880,  0.1682, -0.5912,  0.1992,  2.9504, -1.3936,  2.6655]],
       dtype=torch.float64)
	q_value: tensor([[-0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.4148092415527354 entropy tensor([[2.0450, 1.8955, 1.9004, 2.0682, 1.9059, 2.0642, 2.1195]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 24
	action: tensor([[-0.4417, -1.3591, -0.3225,  0.7669, -1.0020,  2.1228,  0.3898]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.4148092415527354 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 25
	action: tensor([[-1.7590,  2.6378, -2.3700,  2.9037,  0.0150, -2.4948,  2.1267]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.4148092415527354 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 26
	action: tensor([[-4.5886, -0.4379, -0.3932,  2.3620, -0.8299,  0.0824, -0.2057]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.4148092415527354 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 27
	action: tensor([[-1.2061, -0.6195,  1.3500,  0.5634,  0.3202, -0.2438, -0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.1743237600139085, distance: 1.687402930846539 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 28
	action: tensor([[ 0.7179, -0.7186, -0.3386,  0.1451, -2.7486,  1.9706, -3.5285]],
       dtype=torch.float64)
	q_value: tensor([[-0.9982]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.027587614650715464, distance: 1.1600217277497011 entropy tensor([[1.7355, 1.5913, 1.6351, 1.8164, 1.6050, 1.7992, 1.8061]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 29
	action: tensor([[-0.7486,  1.5848, -3.0247,  2.6706,  4.1762,  2.6571, -1.9418]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1600217277497011 entropy tensor([[2.2880, 2.1568, 2.2348, 2.3630, 2.2140, 2.3579, 2.3677]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 30
	action: tensor([[-1.7094, -0.6920, -0.7107, -0.7015, -0.0285,  1.3052,  3.8650]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1600217277497011 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 31
	action: tensor([[-0.1409,  1.7321, -0.8775, -1.8213,  1.2055, -1.4072, -1.1200]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1600217277497011 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 32
	action: tensor([[-0.7207,  0.2840, -4.8966, -2.3954,  1.7210,  1.4319,  0.2210]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1600217277497011 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 33
	action: tensor([[-0.8187, -0.4468, -0.4054,  0.8559, -0.5276, -1.8361, -1.8774]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3701295334277559, distance: 1.3394839283399147 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 34
	action: tensor([[-0.4448,  2.2346,  1.4551, -1.5339,  2.1831,  2.1994,  1.2485]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3394839283399147 entropy tensor([[1.9477, 1.8006, 1.8390, 1.9760, 1.7480, 2.0160, 2.0358]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 35
	action: tensor([[-1.9609,  0.3370, -0.7390,  1.8771, -0.4427, -3.0173, -2.7554]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3394839283399147 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 36
	action: tensor([[ 0.7104, -0.5020,  0.5318,  3.2796, -0.0899, -0.6456, -0.3798]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3394839283399147 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 37
	action: tensor([[-3.8855,  0.4630, -0.2176, -0.8629, -0.9434, -0.0689,  1.0935]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3394839283399147 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 38
	action: tensor([[-2.6188, -3.8591, -3.7225, -0.3432,  1.9808, -0.8597, -1.0381]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3394839283399147 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 39
	action: tensor([[-2.1768,  0.7211, -5.1303,  0.0712,  0.7526,  0.1445, -0.0454]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3394839283399147 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 40
	action: tensor([[-3.1334,  0.2147,  0.0155, -0.1381, -2.8382,  2.3699,  0.4989]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3394839283399147 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 41
	action: tensor([[-1.1858,  0.4554, -0.1610, -0.8791, -0.1131, -0.3752,  2.1337]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.9866255968864468, distance: 1.6129269805284532 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 42
	action: tensor([[-0.9203,  1.7438,  0.7535,  0.2748,  0.9854, -0.4292, -1.6680]],
       dtype=torch.float64)
	q_value: tensor([[-0.9985]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6129269805284532 entropy tensor([[1.8993, 1.7763, 1.8234, 1.9686, 1.8243, 1.9854, 1.9975]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 43
	action: tensor([[-0.8074,  0.3475,  1.0119, -1.6678,  2.3219,  1.4598,  0.6967]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.37089722863467767, distance: 1.339859137896139 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 44
	action: tensor([[-2.4628,  0.5148, -1.7295, -0.4976,  0.7133, -4.0018,  2.0331]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.339859137896139 entropy tensor([[2.0958, 1.9910, 2.0429, 2.1918, 2.0359, 2.1943, 2.2248]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 45
	action: tensor([[-4.4350,  0.2404,  1.6134, -1.3502, -0.6410,  2.8402,  0.7586]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.339859137896139 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 46
	action: tensor([[-1.5473, -1.7666,  1.1569,  1.1255,  1.5754, -0.3724, -0.4298]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.339859137896139 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 47
	action: tensor([[2.4204, 0.2301, 0.3388, 1.3491, 1.0811, 0.2147, 1.2402]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.339859137896139 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 48
	action: tensor([[-0.2519,  0.2417,  0.2454,  1.9795, -1.9953,  1.0315, -2.1844]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.420507637546811, distance: 0.8711251483739267 entropy tensor([[1.8601, 1.7603, 1.8237, 1.9273, 1.7715, 1.9585, 1.9929]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 49
	action: tensor([[ 0.9584,  2.4709,  2.1547,  0.2117, -1.9037, -3.2914,  0.6223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.1644, 1.9940, 2.0333, 2.2199, 2.0137, 2.2068, 2.1873]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 8 actor 936.5590072881665 critic 2338.6345187688103
epoch: 9, step: 0
	action: tensor([[-5.3870, -4.0040,  2.9222, -0.5133,  0.1171, -1.2646, -2.7738]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 1
	action: tensor([[-1.4367,  0.8828, -1.6560,  0.5617, -3.6647, -4.2923,  0.7317]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 2
	action: tensor([[ 0.8516, -1.8655, -3.7721,  1.6283,  3.4794, -1.5601, -2.7495]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 3
	action: tensor([[-3.8198,  3.8227,  0.9929, -1.2895, -2.0425,  3.3393, -2.9377]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 4
	action: tensor([[-0.9614, -0.3849,  0.2075,  4.7922,  0.6452,  2.2692, -1.4499]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 5
	action: tensor([[-1.9169, -0.9516,  3.3944,  0.0696,  3.0057, -1.3895, -0.5541]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 6
	action: tensor([[-1.0256,  0.3669, -0.7469,  2.9533,  0.7315, -2.3157,  1.3184]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 7
	action: tensor([[-4.3859, -0.6255,  0.2649, -0.2530, -2.6122,  0.3708, -0.0544]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 8
	action: tensor([[ 0.1512,  0.0442, -2.4236,  0.3239,  0.5954, -2.4879,  1.9348]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 9
	action: tensor([[-1.1800,  2.4261,  1.3731,  1.0045, -0.0367,  1.2320,  3.6492]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 10
	action: tensor([[ 0.3775,  3.2167, -0.1479, -0.8213,  1.2640, -0.6019,  2.1765]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8711251483739267 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 11
	action: tensor([[-0.9369,  1.0583, -0.3184,  2.1063, -0.9781,  1.7151,  0.0131]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06781142154809383, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 12
	action: tensor([[-4.3468,  4.4101,  3.1701,  0.3024,  0.2509, -3.6877,  2.0242]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.4096, 2.1632, 2.2256, 2.3961, 2.2472, 2.4489, 2.4216]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 13
	action: tensor([[-1.1623, -2.0274, -1.7125,  4.0875,  0.5294, -1.3997, -0.2489]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 14
	action: tensor([[-3.9180, -5.3249,  0.4260,  1.9128, -0.3174, -3.6869,  2.5997]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 15
	action: tensor([[-5.4210,  0.0838,  4.8375,  0.1535,  1.0493,  1.8213,  3.1113]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 16
	action: tensor([[ 2.0970, -0.5331, -2.3388, -0.3973,  2.6989, -0.0686, -2.2361]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 17
	action: tensor([[-4.3630, -0.6168,  0.3108, -0.4862, -2.6447,  1.4879, -9.0570]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 18
	action: tensor([[-4.5483,  0.5856, -0.0478,  0.0733, -0.4357,  4.5545, -1.3690]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 19
	action: tensor([[-0.6918,  2.3594, -0.5842,  2.5304, -1.3367, -3.4705, -0.6026]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 20
	action: tensor([[-2.5651,  4.8844, -1.9910,  1.1884, -3.9028, -1.7044, -0.4829]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 21
	action: tensor([[-0.8424, -0.8651, -3.7684, -4.1842,  0.0926,  0.1353,  0.6388]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 22
	action: tensor([[ 2.9017,  4.0839,  4.2038,  6.4724, -0.2554,  0.5107,  0.9631]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1825076913789159 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 23
	action: tensor([[-0.6476, -0.0653, -0.5419,  1.4236,  0.3453, -2.0304,  0.2106]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.29354074782767947, distance: 1.3015077252536165 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 24
	action: tensor([[-1.2066,  2.4703, -0.9222, -2.0680, -0.2952,  1.6131, -0.7798]],
       dtype=torch.float64)
	q_value: tensor([[-0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3015077252536165 entropy tensor([[2.1225, 1.9083, 1.9268, 2.0704, 1.9500, 2.1664, 2.1559]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 25
	action: tensor([[-5.4872, -1.1589, -2.2042, -0.2106, -1.9193,  1.9338, -2.5142]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.14529176364669283, distance: 1.224657825674166 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 26
	action: tensor([[-0.5281, -0.2126,  1.4103, -2.6999,  0.6320, -2.4553, -0.4635]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.224657825674166 entropy tensor([[2.5392, 2.3663, 2.4003, 2.5843, 2.3848, 2.6118, 2.6036]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 27
	action: tensor([[-0.9020,  1.2545,  1.8105,  1.6692,  1.0119, -0.4929,  0.6166]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.224657825674166 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 28
	action: tensor([[-2.3301,  2.9420,  0.9477,  0.0632,  0.6568,  0.6443, -0.4695]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.224657825674166 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 29
	action: tensor([[-2.2657, -1.5013,  1.9589,  3.7519, -1.1001,  0.2190,  0.9905]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.224657825674166 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 30
	action: tensor([[ 1.6820, -0.1731, -0.0620, -4.2753, -0.7646, -3.4009,  0.2300]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.224657825674166 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 31
	action: tensor([[-0.6576,  0.2925,  0.1739, -1.9338,  1.3501,  0.8746, -4.0298]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.17862819637189054, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 32
	action: tensor([[ 1.4649,  1.1837,  2.5420,  0.1471, -1.1493, -1.9368, -6.7990]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.4460, 2.2613, 2.3305, 2.4743, 2.3753, 2.5537, 2.5436]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 33
	action: tensor([[-2.3857,  2.4330,  2.1506,  2.1096, -2.3985,  0.6696,  0.4976]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 34
	action: tensor([[-0.7294,  1.3404,  4.5948,  2.6596,  1.3347,  0.9483, -4.6226]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 35
	action: tensor([[-0.3439,  2.1412,  0.5330,  0.9131, -1.8344,  3.8599,  1.6472]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 36
	action: tensor([[-1.3319,  1.6490,  1.5289,  1.9661, -0.8418, -0.9797, -2.1442]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 37
	action: tensor([[ 5.2691,  1.2105,  2.1351, -0.0502,  1.5618, -4.1040, -3.2179]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 38
	action: tensor([[-0.9454, -2.5989,  0.0227,  3.7756,  0.7266, -2.2644, -0.5149]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 39
	action: tensor([[ 2.1668,  1.3550,  3.0101,  0.4061,  4.0153,  0.0603, -5.8830]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 40
	action: tensor([[-1.6893, -0.5920, -0.0703,  1.6904, -1.6052,  1.5036,  2.4191]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 41
	action: tensor([[-3.5409,  0.7946, -1.3293,  3.2605,  0.1785,  1.5804, -0.4367]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 42
	action: tensor([[-4.2258, -0.9975, -1.3551,  2.1861,  0.9608,  0.7723,  1.3454]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 43
	action: tensor([[-0.3072,  0.1572, -3.5854, -1.3221,  1.3326,  3.9993, -2.0022]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 44
	action: tensor([[-1.7670,  1.2455, -1.3472,  0.0031,  1.1695, -1.0777, -2.6946]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 45
	action: tensor([[ 1.1001, -1.4832,  2.3393, -2.3980,  2.6173,  1.0302, -3.3015]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 46
	action: tensor([[ 0.3682,  0.7820,  3.7640,  2.6118, -3.2401,  0.8591,  1.3657]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 47
	action: tensor([[ 0.2767,  1.1812,  0.3154,  2.1318, -0.9250,  4.8765, -2.9267]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 48
	action: tensor([[-1.5787,  3.8323,  1.4025,  3.2169,  0.0717,  4.3768, -0.5618]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 49
	action: tensor([[-1.4280, -0.9781,  4.3467,  3.4502,  1.3719, -1.1252,  0.8908]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.2718, 2.0813, 2.1790, 2.2932, 2.1707, 2.3603, 2.3865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 9 actor 1104.5004809532768 critic 2380.0170147545705
epoch: 10, step: 0
	action: tensor([[ 3.6098,  3.7266,  1.1042,  6.0685, -1.0943, -0.1831,  5.4533]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 1
	action: tensor([[-0.6148, -0.4649,  2.2489,  0.3336,  1.2269, -3.4710,  1.5133]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 2
	action: tensor([[ 0.6327,  1.0986, -0.6984, -4.4080, -0.4158, -0.1151,  1.0894]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 3
	action: tensor([[-3.0444, -1.9310,  0.8111,  5.0848, -0.1074, -0.9969, -1.4536]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 4
	action: tensor([[-3.4819, -2.4778, -3.7439,  1.4631, -4.0267,  0.9535,  2.7011]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 5
	action: tensor([[-3.4378,  1.2044, -0.2633,  5.2916,  0.0506,  1.0956, -3.1450]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 6
	action: tensor([[-0.2152,  2.9894, -1.8116,  3.4202,  3.6311,  4.7991, -5.2338]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 7
	action: tensor([[-4.0697, -2.8220, -1.7393,  2.9467,  0.8879, -2.4651, -0.1725]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 8
	action: tensor([[-3.3717,  5.1726, -2.4925,  1.3837,  2.0095, -8.9773,  3.8075]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 9
	action: tensor([[-0.1039, -2.9048, -0.1708,  4.0970, -1.1408, -2.0165, -3.1368]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 10
	action: tensor([[-3.8390, -0.8375, -0.9121, -3.3226, -4.0294,  1.1683, -3.3971]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2423532674328428 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 11
	action: tensor([[-0.0998,  1.4582,  5.3582,  1.2501, -2.3729,  1.0695,  2.6399]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.0211013900486956, distance: 1.1563548519147193 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 12
	action: tensor([[ 0.0303, -5.1675, -4.9524, -6.9609,  0.6733,  6.9712, -1.5807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1563548519147193 entropy tensor([[2.8233, 2.5562, 2.6935, 2.8190, 2.6824, 2.9036, 2.8716]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 13
	action: tensor([[ 1.3153, -1.1197,  2.0323, -2.7374, -1.3726, -1.5411,  5.1581]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1563548519147193 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 14
	action: tensor([[ 1.6922, -2.1811,  3.1315, -1.8593,  3.4819,  3.9965,  0.1635]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1563548519147193 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 15
	action: tensor([[ 1.5927,  0.4613,  0.0103,  4.6994,  0.3477,  2.1012, -7.2327]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5515873057821105, distance: 0.7662943837675544 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 16
	action: tensor([[-1.3279,  1.4467, -0.0992, -2.8715, -0.2131,  0.3148,  1.2192]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7662943837675544 entropy tensor([[2.4250, 2.1952, 2.2992, 2.4439, 2.2938, 2.5061, 2.4968]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 17
	action: tensor([[-0.7785,  0.7983,  1.8820,  2.2691,  1.5421,  4.7200,  0.5157]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7662943837675544 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 18
	action: tensor([[-2.6454, -0.8157,  4.3397,  3.6580,  3.1379, -0.8384, -4.0430]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7662943837675544 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 19
	action: tensor([[-5.6086,  0.9990, -4.0795,  1.6821,  0.2085, -0.0457,  3.7380]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7662943837675544 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 20
	action: tensor([[-3.4851,  0.1523, -3.5763,  0.6104,  1.9043,  0.7715, -2.5575]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7662943837675544 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 21
	action: tensor([[ 3.3243,  1.4840,  0.5941, -3.5144, -3.0685, -1.4416,  1.5298]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7662943837675544 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 22
	action: tensor([[ 5.6157,  2.4475, -0.8550,  2.3083, -5.7396, -2.4090, -6.8437]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7662943837675544 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 23
	action: tensor([[ 2.9234,  3.4603,  2.6337,  6.0683,  1.0054,  3.6208, -0.4149]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7662943837675544 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 24
	action: tensor([[-2.8605,  1.9685, -1.3331, -3.1476,  4.8842,  6.9901, -1.8124]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7662943837675544 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 25
	action: tensor([[ 1.2927,  0.5937,  4.5960,  0.5668,  4.0159, -6.1850, -1.1196]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9666453851856991, distance: 0.20899440421691617 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 26
	action: tensor([[-1.0713,  1.1044,  1.5412,  0.7220, -2.5745, -2.1462,  1.4725]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.45054011311522957, distance: 0.8482516038598271 entropy tensor([[2.3932, 2.1566, 2.2415, 2.3745, 2.2304, 2.4546, 2.4464]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 27
	action: tensor([[-3.5290, -0.8524, -2.4690,  4.8119, -1.4787, -3.8957, -2.8052]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8482516038598271 entropy tensor([[2.9004, 2.6496, 2.7147, 2.8519, 2.7493, 2.9438, 2.9246]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 28
	action: tensor([[-10.4397,   1.4512,   1.4585,  -0.9098,   3.0989,  -1.0720,  -2.7917]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8482516038598271 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 29
	action: tensor([[-6.0126,  0.8154,  2.4261,  2.6061,  4.1556, -1.4518, -1.7161]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8482516038598271 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 30
	action: tensor([[ -0.5152,  -0.8669,   1.1557,  -2.2721,  -0.4427,  -7.7354, -11.3958]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8482516038598271 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 31
	action: tensor([[-5.5295, -2.5325,  1.1991,  2.0965, -6.1994,  1.6644,  0.1602]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8482516038598271 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 32
	action: tensor([[ 2.5704, -6.1777, -0.5352, -2.0554, -3.5010,  2.7252,  2.5073]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8482516038598271 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 33
	action: tensor([[-4.1841,  2.2714,  4.4045,  6.1865,  0.0122,  4.6210, -3.4699]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8482516038598271 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 34
	action: tensor([[ 2.9664, -0.4707,  0.2706, -4.2766, -1.7346,  1.4303, -2.5585]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8482516038598271 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 35
	action: tensor([[-7.4425, -1.8253,  3.5144,  4.7448,  1.3984, -2.6576, -0.3230]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8482516038598271 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 36
	action: tensor([[ 1.4316, -0.6406,  5.5692, -1.8518, -3.4899, -2.1097, -7.9865]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.027065451611705704, distance: 1.159726960943452 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 37
	action: tensor([[ 2.7262,  2.3025,  1.1284, -0.4695,  2.1231, -1.5832,  2.1509]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.159726960943452 entropy tensor([[2.8836, 2.6647, 2.7306, 2.8726, 2.7364, 2.9665, 2.9778]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 38
	action: tensor([[ 6.2538, -2.1479, -2.2910, -0.6902, -4.0154,  2.7821, -0.9611]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.159726960943452 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 39
	action: tensor([[ 0.9452,  1.7013, -0.4675, -4.7954, -3.3736, -6.3375, -1.3976]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.159726960943452 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 40
	action: tensor([[ 2.2260, -2.2914, -1.1334, -2.6374,  3.1390,  0.7301, -3.3576]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.159726960943452 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 41
	action: tensor([[-5.6078, -1.0563,  0.6474,  3.3102, -2.4586,  2.2448,  3.5703]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.159726960943452 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 42
	action: tensor([[ 0.5252, -0.2569,  0.4378,  4.8115, -9.1867,  0.7621, -8.7630]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2619615455017452, distance: 1.2855227118798338 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 43
	action: tensor([[-0.2244,  1.0874,  0.7980,  6.0836, -5.0805,  5.8128,  5.8410]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.7559, 2.5513, 2.5997, 2.7765, 2.6155, 2.8227, 2.8155]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 44
	action: tensor([[ 1.9636,  0.5005,  3.0941, -1.0781,  0.8213,  5.3389, -0.0356]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 45
	action: tensor([[ 2.2869, -2.9252,  4.3519,  0.7248, -1.3813,  0.3954, -5.4165]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 46
	action: tensor([[-3.7930, -5.2372,  0.5842,  4.3182,  0.3156,  0.2356, -2.3628]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 47
	action: tensor([[-4.2921, -0.5851,  1.0563,  1.2631,  2.8919, -1.6584,  1.5486]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 48
	action: tensor([[-3.8346, -2.9759, -2.6267,  3.5300, -2.3042, -2.6197, -4.3995]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 49
	action: tensor([[-7.5150,  0.6352,  0.4314,  2.7352,  1.7768,  1.0327, -2.9511]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.5936, 2.3545, 2.4967, 2.5890, 2.4876, 2.6870, 2.7048]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 10 actor 1056.5944518608408 critic 2359.069900970158
epoch: 11, step: 0
	action: tensor([[-7.2262,  0.5233,  2.6892, -3.8296, -0.5326,  0.8752, -2.5515]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 1
	action: tensor([[  7.6308,  -2.3276,  -3.4817,   3.0242,   0.9209,   4.5715, -11.7735]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 2
	action: tensor([[-1.6349e+00, -3.6119e-01,  6.9799e+00,  2.7087e+00, -4.2089e-03,
         -1.0127e+00, -2.7664e-03]], dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 3
	action: tensor([[-1.1791,  2.8516,  2.0875, -0.9356, -4.2198, -0.7745,  8.7246]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 4
	action: tensor([[-7.2568, -1.9973,  6.6274,  0.2893, -1.4072, -1.3648,  3.0829]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 5
	action: tensor([[ 5.1811,  3.4093,  5.0150, -1.0263, -3.6428, -1.2937, -5.4042]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 6
	action: tensor([[-7.9352,  1.5378,  8.4768,  4.1991,  4.7333,  6.3304, -5.1960]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 7
	action: tensor([[-9.1827,  3.4974,  2.4229,  2.6997,  3.9630,  4.1353,  2.1944]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 8
	action: tensor([[-4.6049, -4.1820, -2.2460,  4.8001,  3.3660, -2.5979,  0.7483]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 9
	action: tensor([[-1.2917,  0.8013, -3.7031,  3.2970, -2.1425,  3.0823,  5.5431]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 10
	action: tensor([[ 2.2772, -0.3801,  7.5755, -1.6975,  2.7600, -8.0699, -8.4037]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 11
	action: tensor([[-6.8985,  1.5540, -1.4505,  1.4222, -3.6715, -5.0482, -3.6926]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 12
	action: tensor([[-8.0970, -0.9268,  0.4929,  3.7994, -1.0031,  1.3058, -1.8732]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 13
	action: tensor([[-3.4714,  2.9520, -1.5208, -0.4717, -1.0434, -0.9188, -1.6391]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 14
	action: tensor([[-1.2651, -1.2248,  2.3902, -3.7392,  1.1723,  2.0809,  5.0759]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 15
	action: tensor([[ 4.2659, -2.1832, -2.9602, -1.6802,  1.8510, -1.1930,  6.0521]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 16
	action: tensor([[-4.9355,  2.2200,  2.1881, -0.0688,  6.1450,  0.7597, -3.7857]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 17
	action: tensor([[ 0.5265,  4.1931, -2.6717, -7.6974,  0.1216, -4.3598, -8.1703]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2855227118798338 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 18
	action: tensor([[-4.9578, -0.0394, -0.3148, -0.3622, -2.2552, -5.7408, -8.3629]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.33419839479654967, distance: 0.9337467693251633 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 19
	action: tensor([[-0.9804,  0.7145, -5.2037, -5.5113, -4.5724, -4.4945,  4.0284]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9337467693251633 entropy tensor([[2.7617, 2.4740, 2.6402, 2.7154, 2.6685, 2.8226, 2.7887]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 20
	action: tensor([[-4.1504,  1.3930, -2.4269,  7.9218, -0.8155,  4.4673,  1.1930]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9337467693251633 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 21
	action: tensor([[-1.1979,  0.5595,  3.7052,  7.2799,  0.3513,  2.0757, -4.6312]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2813220182416407, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 22
	action: tensor([[-6.0477, -4.6568, -5.7502, 12.6654, -4.4211, -0.3204, 11.8197]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[3.1306, 2.7792, 3.0258, 3.0621, 3.0189, 3.1902, 3.1552]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 23
	action: tensor([[-0.6333, -1.4461, -4.9709,  7.6683,  4.2776,  3.1270, -4.0806]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 24
	action: tensor([[-0.4608,  4.0352,  2.0180, -0.2030,  1.0996, -0.5741,  6.6784]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 25
	action: tensor([[ 3.5118,  2.3274, -0.3043, -2.6310, -1.3097, -1.4998,  2.5635]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 26
	action: tensor([[-0.2251, -1.6124,  2.7657, -1.1985,  5.7047, -0.6148, -6.3096]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 27
	action: tensor([[ 8.5925, -2.2932, -1.3106,  2.1318, -4.5182,  5.5372,  5.9171]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 28
	action: tensor([[ -3.7138,  -3.3881,  -1.1832,  -1.1581,   1.5129,  -2.7838, -15.0266]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 29
	action: tensor([[ 3.5915,  4.4296, -0.5512,  8.7361, 10.1840,  7.2037, -8.2056]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 30
	action: tensor([[-4.2170,  0.7788,  0.4214, -8.2160, -2.3244,  3.9495, -2.5894]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 31
	action: tensor([[ 3.5873,  0.1285, 10.5489,  9.0817,  2.5551,  7.3113, -3.0833]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 32
	action: tensor([[ 3.6329,  4.2097,  5.8868, -2.3619,  6.7890, 10.1801,  0.1112]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2953461475769343 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 33
	action: tensor([[ 1.4990, -0.2130, -2.3592,  0.7921, -5.6036,  6.7370, -1.4752]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3301718250851985, distance: 0.936566024357307 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 34
	action: tensor([[ 1.6124,  0.5060,  2.6919,  1.1952, -4.0180, -1.2461, -1.4779]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.005682496716812091, distance: 1.1475910143408572 entropy tensor([[2.7228, 2.4013, 2.5839, 2.6649, 2.5984, 2.7765, 2.7580]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 35
	action: tensor([[-2.1753,  2.2576, -1.2376,  4.5214,  1.9915, -0.2373, -5.1702]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.9519, 2.6395, 2.7552, 2.8564, 2.8304, 2.9816, 2.9392]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 36
	action: tensor([[ 1.0311,  3.0755, -2.3336,  6.7346,  4.8258, -6.0742, -5.3891]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 37
	action: tensor([[ 3.1629,  1.9628,  3.3961,  1.2536, -2.1495,  7.2591,  4.1690]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 38
	action: tensor([[-7.4421,  1.1601, -0.6578, -0.1692,  4.9704,  2.5094, -1.9435]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 39
	action: tensor([[-3.7080,  1.7562,  2.3572, -0.2691,  6.0388, -0.7950, -2.6359]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 40
	action: tensor([[ 1.1386, -4.8890,  6.5056,  4.0703, -3.4860, -3.5634, -4.2826]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 41
	action: tensor([[ 3.3374, -4.7080,  3.8304, -1.1034,  1.1106,  6.9208, -5.4114]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 42
	action: tensor([[-1.8892,  3.8319, -1.1115, -2.7488, -5.7093, -8.3379,  0.5082]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 43
	action: tensor([[-4.5857, -4.9248,  3.7495, -1.2701, -7.0015, -1.1223,  1.2057]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 44
	action: tensor([[-3.7518, -1.1222, -3.0955, 10.2717,  1.6035,  4.6839,  3.4836]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 45
	action: tensor([[-0.3646,  4.2681,  2.6462,  4.0132, -0.0838, 10.2124, -4.5619]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 46
	action: tensor([[ 2.9637,  5.7509, -4.2930,  2.9795, -8.2581,  4.1356, -1.1029]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 47
	action: tensor([[ 1.8749,  3.7636, -1.7344, -2.1391, -3.1012, 17.7468,  1.3397]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 48
	action: tensor([[ 0.4914,  2.7723,  0.4885,  9.0227, -3.5454,  5.2220, -1.6860]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 49
	action: tensor([[ 0.2077, -1.8310,  5.5872, -3.9447, -1.4749, -0.6089, -0.5799]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[2.8901, 2.5586, 2.8122, 2.8285, 2.8026, 2.9627, 2.9572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 11 actor 1104.5241394096868 critic 2373.1858238151126
epoch: 12, step: 0
	action: tensor([[ 4.2480,  5.0271,  3.1792,  1.5742, -4.7326,  4.6249,  1.9610]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 1
	action: tensor([[-1.5059, -6.0842,  9.2566,  3.0473, -6.5966, 10.5066,  5.9408]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 2
	action: tensor([[  0.4429,  -5.0297,  -4.6264,   5.2343,   0.6430, -10.3676,   1.4905]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 3
	action: tensor([[ -3.0257,  -3.9524,  -4.0331,   2.8128, -10.3189,  -8.6808,   5.0464]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 4
	action: tensor([[-0.1038, -8.8715, -5.0180,  4.9007,  1.6300,  1.0257,  4.9635]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 5
	action: tensor([[-3.0910, -1.7771, -5.5437, -1.0006,  0.3143, -7.9768, -3.2143]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 6
	action: tensor([[-2.4000, -2.4525,  2.3970,  2.9492, -2.2602, -4.0141,  2.9533]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 7
	action: tensor([[-2.1736, -3.4181,  1.9608, -5.6869,  2.4062,  0.8732,  7.7960]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 8
	action: tensor([[-2.4070, -2.7439,  2.9860, -1.9546, -0.3646,  6.9895,  6.6145]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 9
	action: tensor([[ 0.4352,  0.6907,  2.9492, -0.4591, -5.5101,  2.6649, 12.6226]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 10
	action: tensor([[-9.0125,  4.0617,  1.5398, -8.8022, -6.3660,  7.1327, -4.6904]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 11
	action: tensor([[ 1.3678, -0.6644,  3.5690,  8.1836,  1.5414,  2.1908, -4.4949]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 12
	action: tensor([[ 5.4967,  4.7229, -3.3114,  3.3107, -1.8314, -1.6864, -2.6442]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 13
	action: tensor([[-5.2790, -2.2085,  2.6455, -1.5611, 12.4738,  6.0691, -1.2881]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 14
	action: tensor([[-5.9925, -4.8542,  6.4334, -7.7371,  3.0362,  4.3412, -1.0757]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 15
	action: tensor([[ -5.2271,  -2.9848,   0.8133, -10.0275,  -9.3955,   1.8880,  -6.0479]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 16
	action: tensor([[-1.5273, -2.7265, -4.7976,  0.0507, -6.6553,  0.5289,  2.9004]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 17
	action: tensor([[ -1.6991,   1.4179,  -2.0748,   1.4924,  -6.8104,  -8.4314, -13.8037]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 18
	action: tensor([[12.5260, -0.9740,  2.4135,  8.6022, -9.8464, -6.8839,  0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 19
	action: tensor([[ 2.3991,  2.2714,  5.3845,  1.2869,  0.0990, 14.3334,  2.1345]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 20
	action: tensor([[ 4.3237,  0.3822,  2.6110,  1.4551,  4.9876, -4.9238,  4.0308]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 21
	action: tensor([[ 5.0120, -0.9013, -6.7246, -0.5887, -3.9888, -2.0990,  3.4505]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 22
	action: tensor([[ 0.7600,  8.2652,  3.9109,  2.7620,  5.7183, -5.2307,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 23
	action: tensor([[  1.7368,   0.0502,  -3.0044,  -0.1404, -10.3462,  -9.0084,  -1.2564]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 24
	action: tensor([[ 3.8163,  2.7616,  4.5857,  1.7197, -7.1088,  6.5076, -0.2589]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 25
	action: tensor([[ -0.3861,   4.5665,  -3.5245,  -1.7029, -12.7433,   1.1685, -10.2282]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 26
	action: tensor([[-5.9079, -2.3706, -8.2272, -4.5801, -1.9891,  3.3988,  1.5605]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1475910143408572 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 27
	action: tensor([[  4.9889,   5.5051, -11.2155,  11.2744,  -4.2870,   8.2524,   1.9690]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.4602665738135141, distance: 1.3828426488120085 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 28
	action: tensor([[-14.8728,   8.5869,  -0.5276,   4.7904,  -2.0958,  -1.3846,  13.8243]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3828426488120085 entropy tensor([[3.3828, 2.9466, 3.2798, 3.2511, 3.3107, 3.4540, 3.3640]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 29
	action: tensor([[ -1.9542,   0.8739,  13.9341,   3.2768,  -3.5812,   2.3766, -13.0729]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3828426488120085 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 30
	action: tensor([[-2.9418, -3.8884, -5.1974,  5.0464, -3.9191,  4.0214, -3.5459]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3828426488120085 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 31
	action: tensor([[  6.6204,   1.2427,   5.4554,   5.3171, -10.8122,   5.7647,   0.7943]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7063434908638175, distance: 0.6201211006064461 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 32
	action: tensor([[ -0.1295,  -1.0685,   2.6929,   0.7393,  -1.3305,   5.3067, -14.0708]],
       dtype=torch.float64)
	q_value: tensor([[-0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.0215732365745986, distance: 1.6270519996151862 entropy tensor([[2.9749, 2.5303, 2.8792, 2.8316, 2.9078, 3.0422, 2.9739]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 33
	action: tensor([[ 5.0636, -1.7493,  6.9685,  4.4182, -0.6873,  1.8820, -3.5337]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.2000, 2.7912, 3.0419, 3.0720, 3.1122, 3.2484, 3.1521]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 34
	action: tensor([[-2.9891,  1.4033, -6.8846, -4.1447, -4.2805,  3.5501, -7.5696]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 35
	action: tensor([[  4.8678,   0.7832,   3.1422,  -4.8836, -11.3442,  -3.2036,   2.3476]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 36
	action: tensor([[ -7.1198,   2.7634,   6.5529,  -2.9726,   2.5494,   2.2059, -14.6747]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 37
	action: tensor([[-1.6353,  7.1245, 12.5542,  2.7389, -4.7653,  5.0009, -2.7038]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 38
	action: tensor([[ 4.1230, -3.9541,  2.1168, -0.4498, -0.7897,  0.5364, -0.0959]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 39
	action: tensor([[ 3.0627, -3.2313, -5.0370, -9.9604, -6.2915,  6.0168, -7.8949]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 40
	action: tensor([[-0.0861, -2.8674,  2.7730,  0.4053, -2.3746,  6.8509,  7.0084]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 41
	action: tensor([[ -3.8382,  -1.4655,  -3.0330,  -1.5039,   1.1051,   4.8100, -13.9098]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 42
	action: tensor([[-7.7869, -1.0894,  0.1197, -8.5158,  1.4443,  8.6041, -6.3115]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 43
	action: tensor([[-0.5879,  2.2761,  3.8602, -9.1154, -6.1770,  8.1952, -1.7455]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 44
	action: tensor([[-8.8933,  0.5711, -1.3659, -5.1528, -2.1704, 11.5975, 10.0099]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 45
	action: tensor([[-4.5309,  0.1687, -1.9188,  1.8536, -0.8988,  1.5582,  2.1574]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 46
	action: tensor([[ 0.1270,  0.0980, -3.1066,  4.3428, -8.0315,  8.6557,  2.9197]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 47
	action: tensor([[-1.0209,  4.0115,  4.4207,  1.2004,  8.9132,  5.3007, -0.3948]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 48
	action: tensor([[-2.3497, -2.1584,  9.0463, -3.3112, -1.7035, -2.2019, -4.2255]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 49
	action: tensor([[-9.0516,  1.8132, -5.5369, -0.2674,  3.4881, -3.9519, -0.1499]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.1535, 2.7117, 3.0796, 3.0251, 3.0907, 3.2313, 3.1707]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 12 actor 1128.5449919464008 critic 2382.590851289724
epoch: 13, step: 0
	action: tensor([[11.0202, -1.1189,  6.7286,  2.6672, -8.8865, -0.6213,  2.3074]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 1
	action: tensor([[-0.7174, -4.0325, -0.7371,  5.5243, -7.2856, 11.5311, -0.2810]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 2
	action: tensor([[10.3260, -1.0019, 16.4611, -0.7948, -1.8347,  5.9576, -5.2967]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 3
	action: tensor([[ -8.9593,   8.2690, -11.7117,  -9.6489,  -1.4218,   9.7668, -11.5164]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 4
	action: tensor([[ 6.3807, -2.6672,  0.0426, -5.7517, -1.6987,  9.3031, -5.6473]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 5
	action: tensor([[ 0.1597,  5.4133,  7.1368, -3.3494, -2.3942,  9.4452, -2.4426]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 6
	action: tensor([[-10.8039,  -2.4555,   7.6421,  -5.5685, -10.7114,  -3.3074,  -8.7846]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 7
	action: tensor([[-8.2616,  0.1933, -2.6590,  3.9592,  1.2499,  6.5703, -9.6581]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 8
	action: tensor([[  1.5271,   6.7592,  -6.7216,   2.2847, -16.6361,   9.4978,  -8.4310]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 9
	action: tensor([[  3.0871,   5.3701,   0.1356,   8.1261, -12.3437,   5.0848, -22.4776]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 10
	action: tensor([[-4.0255, -2.7379,  5.3147, 12.1363, -2.8058, -7.0939, -1.1107]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 11
	action: tensor([[ 10.1568,   6.4504, -10.2669,   6.3081, -13.6068,   6.1094, -10.1414]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 12
	action: tensor([[ 1.7056, -9.6813, 15.6839, -3.7973, -5.6920, -5.6946, -5.0782]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 13
	action: tensor([[-1.7073, -5.4416,  5.1823,  6.0785, -4.6303, -1.7979,  2.7682]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6270519996151862 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 14
	action: tensor([[-11.9121,   0.5496,   1.8652,   8.0028,  12.2077,  -0.1186,  -0.2847]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5546672676175455, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 15
	action: tensor([[ -1.0397,  -1.3586,   0.5230,   3.5988,  -2.1051, -10.4949,  -0.1605]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[2.9987, 2.5104, 2.7951, 2.7912, 2.9058, 3.0296, 2.8629]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 16
	action: tensor([[ -0.9912,   2.9511,  -0.9530,   4.0742,  -1.1427,   4.4079, -11.2354]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 17
	action: tensor([[  3.4854,  -2.1975,  10.6191,   5.0745,  -8.0548,   3.0450, -13.4683]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 18
	action: tensor([[-1.9587, -1.4947,  3.9452,  1.6425, -1.2440, 16.8302, -0.9304]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 19
	action: tensor([[ 8.0652,  2.0887, -5.8631, -0.9969, -5.2950,  6.5483,  9.1259]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 20
	action: tensor([[ -4.2964,   0.7815,   1.5570, -11.0072,  -4.2120,  -7.5643,   3.2704]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 21
	action: tensor([[  3.2698,  -0.2510,  -1.9709,   2.8300, -13.0007,  15.1098,   3.6000]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 22
	action: tensor([[ 3.3323,  5.7813,  8.8564,  9.6707, -7.6900, -3.8418,  1.4251]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 23
	action: tensor([[14.0317,  3.1244,  0.5799, -3.7689, -4.6567,  4.7114, -4.1844]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 24
	action: tensor([[ 2.3694,  0.6301,  8.5824, -0.6318, -7.0510,  7.0484,  6.8632]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 25
	action: tensor([[ -1.2496,  -4.0715,   1.5377,  -1.5485,  -2.0137,   0.7500, -15.9230]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7636581691793649 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 26
	action: tensor([[-11.7150,  12.0908,  -4.0922,  17.3955,  -7.5195,  14.0014, -10.8248]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9064035448801442, distance: 0.3500953911762095 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 27
	action: tensor([[-3.8250,  4.3053,  4.7949,  3.1935, -0.8065, -4.1987, -4.6516]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3500953911762095 entropy tensor([[3.3666, 2.9058, 3.2575, 3.2217, 3.3092, 3.4508, 3.2887]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 28
	action: tensor([[  3.8031,   2.4473,   2.1102,   0.4233, -10.7643,  13.8906,  -7.5236]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3500953911762095 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 29
	action: tensor([[ 4.1831, -2.2085,  7.9823,  1.8778, -1.9376,  6.4283, -0.3900]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3500953911762095 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 30
	action: tensor([[ -9.1058,  -5.9940,  -2.4415,  -7.0978, -14.4205,  -4.5378,  -1.6371]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3500953911762095 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 31
	action: tensor([[ 9.4628,  5.9067,  2.8980, -7.0977, -0.1571, -1.9557,  3.8150]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3500953911762095 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 32
	action: tensor([[ 0.2211,  3.7211,  6.9555, -6.6131, -3.0640, -8.1088,  5.9352]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3500953911762095 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 33
	action: tensor([[ 0.4828,  7.1376, -2.5182, -5.7808,  3.2757,  5.3874,  2.3800]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.735848713757079, distance: 0.5881431701978814 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 34
	action: tensor([[-13.0706,  -0.6263,  -0.3737,   5.7885,  -7.8032,   5.3735, -14.0036]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2499864107728904, distance: 1.2794088162751505 entropy tensor([[3.7593, 3.2685, 3.6311, 3.5724, 3.6872, 3.8277, 3.6762]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 35
	action: tensor([[ 2.4188, -3.2699, -1.2342, -4.8481, -8.7858,  2.6667,  2.4753]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.1994, 2.7378, 3.0627, 3.0186, 3.1185, 3.2660, 3.1290]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 36
	action: tensor([[ 17.0796,   3.5669,   7.5490,  -6.7789,   1.3404,  -5.3263, -10.5754]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 37
	action: tensor([[-1.3845, -1.4847, -0.8374,  1.2485, -9.9725,  0.9971, -1.7740]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 38
	action: tensor([[ 8.2003, -7.5608, 10.4710,  4.2843,  2.7190,  5.0905, -7.6095]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 39
	action: tensor([[-4.5016, -8.8788,  7.6930,  0.2569,  2.7992,  0.7715,  3.7341]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 40
	action: tensor([[ -6.7044,  -1.1910,   4.2374,  -2.7951, -21.5452,   9.5053,  -6.0627]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 41
	action: tensor([[-14.2424,   2.8413,  14.0088,  -9.4605, -10.8713,  10.7753,  -6.7969]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 42
	action: tensor([[1.4800, 1.1956, 7.4789, 2.7406, 8.1462, 4.1124, 0.1116]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 43
	action: tensor([[ 1.9420,  3.8850, -7.1434, -6.4855, -1.6756,  6.4293, -6.2515]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 44
	action: tensor([[  4.6463,   6.0652,  -6.7154,   4.5750,  -4.7621,   0.8700, -15.2928]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 45
	action: tensor([[-14.2604,  -2.8492,  -0.1357,   0.3416,  -6.7567,   2.1488, -11.4932]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 46
	action: tensor([[14.8903, -0.8808,  4.7374,  3.7252, -2.2261, -2.0528, -5.3074]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 47
	action: tensor([[  8.9747,   2.9362,   2.0351,   6.0715, -14.9051,   1.5454,  19.7858]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2794088162751505 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 48
	action: tensor([[ -6.8758,   5.5329,   1.4653,   0.2412,   1.9154,   5.3158, -10.6537]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.25048427798493744, distance: 1.2796635839594284 entropy tensor([[3.4184, 2.9338, 3.3013, 3.2354, 3.3473, 3.4934, 3.3554]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 49
	action: tensor([[ 9.6316,  2.5730, -0.8860, -0.1596,  6.0763, -4.9566, -2.7210]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.3018, 2.8231, 3.1355, 3.1052, 3.2143, 3.3601, 3.1945]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 13 actor 1080.6060242759072 critic 2365.658702229429
epoch: 14, step: 0
	action: tensor([[ 2.1446, 12.2002,  7.0737,  0.6792,  1.8971, 10.3040, 12.4874]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 1
	action: tensor([[ -8.9453,  -0.4231,   1.0435,   6.7261, -13.7145, -13.3632,  -5.6684]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 2
	action: tensor([[ 2.6267,  3.5928,  0.1841,  2.4849, 10.8481,  4.9375, 18.6783]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 3
	action: tensor([[-5.2859, -7.9145,  6.6977,  7.6732, -2.4981,  7.2916, 15.8356]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 4
	action: tensor([[-11.6397,   8.0438,   7.8600,  -3.8075,  -3.3647,  16.3769,  -3.0830]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 5
	action: tensor([[ 14.0393,  -9.5300,   9.9545,  13.9308, -29.7241,  -5.1458, -17.3510]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 6
	action: tensor([[ -8.8802,   3.9674,  -8.0922,  12.2073, -17.4985,  11.5085,  -3.0050]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 7
	action: tensor([[ -8.7162,  -3.6151,   1.1576,  10.5065, -12.7933,  16.4949,  -2.6885]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 8
	action: tensor([[ 1.3076,  4.3368,  1.8219, 11.6912,  3.5940,  0.3498, -1.5566]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 9
	action: tensor([[-11.9486,  15.1769,   8.0436, -10.0521, -12.5799,   7.1721, -18.3976]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 10
	action: tensor([[ 0.6723,  2.1101, 18.5845, -7.1126,  2.2210, -3.7123, -5.0903]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 11
	action: tensor([[-3.7119,  3.4402,  8.6101, 18.6396, -9.8513,  2.6592,  5.2262]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 12
	action: tensor([[ 7.4791,  2.9597,  1.1162,  5.8909,  9.2059,  4.0019, 12.9542]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 13
	action: tensor([[-0.2171, -9.4250,  5.5952, -1.5535, -7.2328,  5.2675, -7.6665]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 14
	action: tensor([[ -7.9259,  18.3330,   1.8985, -10.8854,  -1.2310,   6.3433, -16.8479]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 15
	action: tensor([[-11.8922,   5.9282,  -8.3048,  -2.6673,   3.0429,  -5.7775,   4.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 16
	action: tensor([[  6.2357,  -2.8442,  -2.2428,   6.5584, -10.1430,   1.4422, -13.1932]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2796635839594284 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 17
	action: tensor([[ 0.8626, -5.2411, -3.1955, 13.5815,  5.9377, 12.9445, -0.7247]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9339689005986473, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 18
	action: tensor([[-13.3829,  -4.6973,   1.9645,   3.0215,  -5.1424,   4.0055,   1.4627]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.4436, 3.0052, 3.2067, 3.2506, 3.3516, 3.4630, 3.3004]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 19
	action: tensor([[-23.6382,  -7.5283,  -6.3504,  -0.2585,   0.1588,  -8.0671,   5.3472]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 20
	action: tensor([[ -5.7904,   3.4779,   0.8639,   8.8065, -10.8824,   5.4580, -10.3737]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 21
	action: tensor([[-1.4711, -2.4364, -6.9964,  6.3469,  8.1364,  7.7224, -9.3083]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 22
	action: tensor([[  7.0603,   8.9459,   1.8416,   1.1788, -13.5356, -14.8930,  -0.2827]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 23
	action: tensor([[-0.8564,  2.5526,  4.4373, -0.7293, -6.3864,  0.4924, -8.6023]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 24
	action: tensor([[ -9.2564, -10.0478,  11.4080,   4.3947,  12.3362,  14.5184,   1.1809]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 25
	action: tensor([[16.8258,  2.9037, -0.5144,  1.3172, -7.3878, -3.7940, -4.1374]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 26
	action: tensor([[ 3.4777, -1.3224, -8.4800, 14.8803, 17.4361, -6.2788,  1.2300]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 27
	action: tensor([[-7.6697, 11.5049,  3.3894,  1.6529,  9.8015,  2.4647,  6.8600]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 28
	action: tensor([[  2.2425,   2.5925,  -6.7394,  11.6885,  12.8550, -20.1626,  -4.9134]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 29
	action: tensor([[-9.5002, 10.1385,  0.2038,  1.5119,  4.6735, 28.7781,  6.0931]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 30
	action: tensor([[ 5.3217,  0.8608, 15.5126,  3.6822, -7.0882, -2.8179,  8.0921]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 31
	action: tensor([[  4.5059,   8.2742,  -3.0212,   2.0805, -22.7776,   2.9972,  -7.3213]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 32
	action: tensor([[ 4.7833, 11.4044, -2.2186, 17.4530,  6.1299, -2.5094, -4.3747]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 33
	action: tensor([[14.3890, 10.1691,  6.5870,  1.8527, -2.7448, -9.8882,  2.2530]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 34
	action: tensor([[ 10.6703,   4.5273,  -4.7486,   0.4385, -26.4571,   9.2462,   0.3942]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 35
	action: tensor([[-8.4466, -0.2450, -0.2995, -5.6003, 19.8257, -7.6976,  2.6687]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 36
	action: tensor([[ 7.2227, 17.4888, -8.9674, -2.5223, -4.5257,  7.1459, 11.7971]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 37
	action: tensor([[  5.1530,  11.2551, -17.8510,  -4.9977,  -7.1814,  -0.4984,   5.6284]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 38
	action: tensor([[ 2.8367,  5.7239, 18.7022, -4.1812,  5.9271, 17.3830, -2.0403]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 39
	action: tensor([[ 14.9540,  -4.0524,   9.2149,  13.2763, -12.6165,  16.6494,  -7.3426]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 40
	action: tensor([[-12.0952,   8.2478,  -0.7755,   6.1724, -14.1624,   5.5049,  -4.6520]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 41
	action: tensor([[-4.8752, -2.0191, 10.7945, -0.6336, -3.9638, 22.8473, -2.0317]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 42
	action: tensor([[ 4.6193,  0.4396,  3.3494, -4.6288, -8.0639, -2.0514,  3.7705]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.2940566175672667 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 43
	action: tensor([[ -6.7006,  -7.2379,   8.6824,   5.9682,  -5.7106, -13.0983,  -3.2837]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.8383426031587724, distance: 1.5515648034474472 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 44
	action: tensor([[ -2.9836,   2.1678,  10.2569,   2.1410,  -5.5810, -14.5441,  -0.7441]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.6648, 3.2380, 3.4641, 3.4833, 3.5741, 3.7054, 3.5382]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 45
	action: tensor([[ -1.3967,   9.2807,  -1.2423,  11.2197,   5.2311,   7.6788, -19.7657]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 46
	action: tensor([[ 8.1980,  6.5335, -4.5802, 11.3277, -5.4599, 13.2605, 21.4727]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 47
	action: tensor([[  7.4711,   9.5648,   3.4719,   4.5580, -13.4913,   2.8262,   1.5177]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 48
	action: tensor([[ -4.7203,  -0.4109,  -6.9705,  16.2195, -11.4655,  12.1067,   1.7481]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 49
	action: tensor([[ -0.9122,  -3.2266,   0.1033,   5.7282, -16.8340,  11.2884,  -7.4357]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.6708, 3.2460, 3.4955, 3.4857, 3.5884, 3.7174, 3.5742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 14 actor 1152.544420909052 critic 2389.729698195335
epoch: 15, step: 0
	action: tensor([[-1.4295,  6.3595, 12.0300, 11.2003, -6.8562,  9.3989, -2.0690]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 1
	action: tensor([[-11.2440,   2.2520,  -5.9301,   8.5173,  11.4375,   1.6547,  -6.9542]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 2
	action: tensor([[ -2.0110,   4.0983,   5.4382, -16.1150, -15.5016,  13.1067,   0.6372]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 3
	action: tensor([[ 3.8106, 10.8524, -5.3252,  0.5623, -7.8566,  1.0606,  4.5551]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 4
	action: tensor([[-2.6255, -5.7142, -3.2367, -2.3119, -0.9641, 15.4687,  4.3963]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 5
	action: tensor([[16.4575, -1.4014,  0.9262,  2.2439, 13.5801,  4.9561, -1.0263]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 6
	action: tensor([[ 16.7351,   9.5210,   1.9015,   2.6193,   1.5148,  -2.7660, -20.6875]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 7
	action: tensor([[ 16.5968,  -3.8369,  -0.9606,  10.6909,  -2.4409, -27.6834,   4.3039]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 8
	action: tensor([[  8.3636,  17.3224, -10.1123,   7.9262, -13.3474,   3.8182, -12.8182]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 9
	action: tensor([[-12.9703,  -5.7943,  13.1511,   7.3594,  -3.5185,  11.3065,   3.3630]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 10
	action: tensor([[-12.7692,  -3.3300,  16.0720,   7.3454,   0.7931,  -8.9317,   1.4924]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5515648034474472 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 11
	action: tensor([[-12.4068,  -4.9918,  -2.3525,  -1.8421,  -4.5018,  19.7322,  22.4440]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.19128005405450998, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 12
	action: tensor([[-14.9202,   7.7465,  -3.8564,  23.3784,  12.8793,   0.6288, -18.7708]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[4.2208, 3.9305, 4.0386, 4.0863, 4.1575, 4.2924, 4.1239]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 13
	action: tensor([[-4.4780, -4.1531,  7.8655, -7.3824, -2.7992, -2.6537,  7.4657]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 14
	action: tensor([[  1.6926,  -3.5219,  -0.3143,   8.3021, -12.7806,   5.3300, -16.6448]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 15
	action: tensor([[-5.3057, -7.8030,  5.0470, 15.0615,  5.7024,  9.3298, -0.9770]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 16
	action: tensor([[ 6.6874,  0.1606, -7.8335, -2.7286, -3.8487,  6.3214, -5.4071]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 17
	action: tensor([[-1.7605, -5.8529, 12.7628,  7.3094, -8.1950, 20.6934,  4.4992]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 18
	action: tensor([[ 21.2980,  -8.6322,  -0.4782, -12.0331,  -7.5268,  -6.2869,  -1.3279]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 19
	action: tensor([[ 12.1716,  -5.0926,  12.1952, -10.5902,  22.5180,  10.0968,   0.5281]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 20
	action: tensor([[-10.3320, -13.2996,  -3.2181,   8.3501,  -9.1105,  10.5182,   4.6377]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 21
	action: tensor([[-15.4095,  -3.2556,  21.3862,   8.8055,  -3.2023,  12.5594, -20.4345]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 22
	action: tensor([[ -2.3529, -13.0670,   2.9384,  -7.7832, -25.3299,  11.2981,   7.2896]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 23
	action: tensor([[ 11.6775,  -9.4239,  11.3113,   2.3019, -10.5959,  -3.8494, -13.1990]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 24
	action: tensor([[20.4848, -0.2704,  6.2501,  5.7495,  5.8260, 24.2271, -6.7928]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 25
	action: tensor([[10.6525, -6.6435,  7.5036, -5.3136, -5.7315, 16.1924,  9.6577]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 26
	action: tensor([[15.4561, 10.4393, -7.1326, -2.3125, 14.5999, -3.9001, -0.4711]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 27
	action: tensor([[ 9.7590,  0.9388,  1.5171, 18.0066,  1.0349, -2.8635, -2.5138]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 28
	action: tensor([[-6.6951, -1.3921,  0.1986, 13.2900, -2.4964, -7.5358, -0.7470]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 29
	action: tensor([[ -0.7558,   9.7876,   4.2553,   7.5437,   9.4773,   6.9886, -10.6630]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 30
	action: tensor([[  7.3438, -13.1348,   7.1369,  -4.5123, -12.4778,  18.5701, -20.4015]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.029095716555796 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 31
	action: tensor([[ 5.5063, -6.2823,  3.9745,  5.8337, -6.1592,  1.0091, -1.0816]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6036178637280871, distance: 1.449129294857803 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 32
	action: tensor([[-10.5061,   2.0242,   7.8822,  -1.7068,  14.6049,  12.5698,  -5.5207]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.449129294857803 entropy tensor([[3.8331, 3.5442, 3.6501, 3.7017, 3.7657, 3.9073, 3.7345]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 33
	action: tensor([[ 0.0420,  0.9166,  3.2848, -1.3067, 11.9362, 22.2234, -2.8614]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.449129294857803 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 34
	action: tensor([[ 14.6890,  -2.0376,   9.4546, -15.3283, -18.4496, -33.9669,   8.1157]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.449129294857803 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 35
	action: tensor([[ -6.2730,   2.7202,  -0.7192,  10.8134,  -5.3738,  -9.5729, -13.4602]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.449129294857803 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 36
	action: tensor([[ -2.8228,  -1.4702,   1.4853,  15.2542, -17.4084,   0.7954,   2.6709]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.449129294857803 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 37
	action: tensor([[  5.1325,  -5.9372,   0.9872,   5.6267,  -1.6038,   2.0546, -16.7192]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.35656287824589605, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 38
	action: tensor([[ 15.7623,   8.9614,   3.4310,  -7.9958,   3.5868,  -3.0633, -12.4226]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9324, 3.6409, 3.7563, 3.8121, 3.8718, 4.0120, 3.8297]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 39
	action: tensor([[12.1480,  1.3491,  6.4432, 28.5051,  4.7302, 29.1622,  4.8632]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 40
	action: tensor([[  6.4747,   0.5074,  10.3122,  15.4052,   0.2697,  -6.3764, -11.4390]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 41
	action: tensor([[  8.8036,   2.2135,   1.4840,   5.0505, -22.8269,   4.7375,  -5.7114]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 42
	action: tensor([[ 2.3649e+01,  1.5962e+01, -9.7292e+00,  1.2672e+00,  1.2654e-02,
         -3.5026e+00,  3.7547e-01]], dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 43
	action: tensor([[  4.9157,  -1.7958,  -3.7017,   2.4156,  25.2343,  16.8097, -12.6484]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 44
	action: tensor([[-32.1617,  -8.8803,  -2.9760,  -7.4018,  -0.1761,  10.6884,  10.0506]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 45
	action: tensor([[-1.4800,  9.2207,  9.9752,  7.6592,  0.0304, 16.9063, -4.8525]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 46
	action: tensor([[  0.2558, -22.6007,   7.3009,  14.3620,   1.9258,   6.6154,  -5.1234]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 47
	action: tensor([[  1.5919,  -3.9717,   9.0270,   3.2230, -16.2990,   2.8528,  -0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 48
	action: tensor([[-13.5015,   5.9172,  -6.5889, -22.0388, -16.8940,  -3.5448, -17.5257]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 49
	action: tensor([[  3.5484,   3.4617,   8.8904,   3.8313,  -5.9931, -10.9152,   9.6654]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.8432, 3.5547, 3.6667, 3.7112, 3.7813, 3.9243, 3.7484]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 15 actor 1128.5065682469406 critic 2385.739231543083
epoch: 16, step: 0
	action: tensor([[14.7555,  5.0844,  0.6545,  6.7904,  5.9736, 43.0437, -6.2498]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 1
	action: tensor([[  0.5193,  -6.6274,   1.8049,  -7.7165,   1.8325, -28.6468,   0.6355]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 2
	action: tensor([[13.7494, 11.1259, -6.2648, 15.2328, -5.9849, -9.7540, -0.5169]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 3
	action: tensor([[ -0.9011, -10.1455,  15.4110, -13.1444,  -6.3753,  31.1974,  -7.3099]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 4
	action: tensor([[-18.0563,  -8.8516,  -8.3823,  -1.5534, -33.4964,  27.1621,  -4.0375]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 5
	action: tensor([[-10.5399,  -8.4502,   5.2228, -20.4297,   1.4581, -11.2148,   8.3295]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 6
	action: tensor([[ 15.9574,   5.4123,  -0.4064,  -2.8475,  -9.5254, -16.5750,  -4.3188]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 7
	action: tensor([[-12.0062,  15.8553,  29.3235,  16.6324, -15.0677,   8.5929, -24.8791]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 8
	action: tensor([[ -6.6877,  25.2383,   7.7650,  -2.5263, -16.0480,   9.0531, -17.0313]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 9
	action: tensor([[19.7847, 17.6436,  3.7754, -4.5121, -0.6713, 24.5909,  1.5409]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 10
	action: tensor([[ 2.3778, -7.1476, -3.5317,  8.6415,  0.3853,  1.2919, -1.1574]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 11
	action: tensor([[-18.6854,  -8.2877,  -5.9858,  -6.4145,  -0.7574,  18.5594, -17.0315]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 12
	action: tensor([[11.5480,  6.8190, -6.5751,  4.4024,  7.2761, 23.3509,  5.9274]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 13
	action: tensor([[-16.5744,  12.7858,  16.0135,   0.1255,  42.6303,   2.7820, -12.2476]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 14
	action: tensor([[-10.4057,   1.3377,   9.8901,  -9.3412,   0.4208,  15.8518,  -3.4330]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 15
	action: tensor([[-20.3702,  10.1993,  -8.6223,   9.0404,  13.9750,   5.0507,  -6.9526]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 16
	action: tensor([[ 5.0125, -5.8548, 28.9692, 10.0700, -9.4084, 12.6902, 11.9681]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 17
	action: tensor([[  6.9403, -18.6409, -12.0053,   0.2585,   9.6406,  -9.1070,   6.0767]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 18
	action: tensor([[ -2.2201,  17.6192,  -4.4890,   7.4474,   2.8721,  -3.3284, -10.8474]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 19
	action: tensor([[ 4.4221, 10.6560, 23.3616, 21.3432,  9.5358,  2.1727, -7.0957]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 20
	action: tensor([[-15.1460,  11.9846, -10.1727,  13.8050,   5.1515,   6.3935,   7.1390]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 21
	action: tensor([[ -8.3872,  13.4889,  -8.4437,  11.0942,   3.2446, -13.5323,  14.5247]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 22
	action: tensor([[-1.3636, -6.1154,  8.1383, -8.1306,  4.8880,  3.1539,  3.1559]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 23
	action: tensor([[ -3.8271,   8.1363,   1.8091,  32.6197,   0.9880, -11.9288, -17.2955]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 24
	action: tensor([[  6.1236,   9.1971,   2.9807,   9.6744,  -2.6272,   8.2541, -13.2753]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 25
	action: tensor([[ 16.3839,  13.6239, -12.5606, -14.5755,  -8.9794,  19.8075,  -1.4079]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 26
	action: tensor([[-11.0637,  -8.6105, -18.4307,  10.1170, -12.7387,  20.9429, -23.8443]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 27
	action: tensor([[  4.0676,  -0.3498,   1.5954,   6.5550, -13.7947,  11.9919,   3.3140]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 28
	action: tensor([[  4.8831,  14.6705, -17.8354, -14.0509,   9.9849,   4.5072,  -0.9066]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 29
	action: tensor([[-5.9836, -1.0401, -0.8120, 15.1510, -8.1140, -3.6265,  3.9157]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 30
	action: tensor([[17.8697, -4.6361, -0.7551, 25.6025,  2.6486,  2.3535,  0.8465]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 31
	action: tensor([[ -1.0348, -14.0253,  -0.1451,  20.7263,  15.9630,  15.5496,   4.3758]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 32
	action: tensor([[-14.7791,  17.8156,   6.7119,  19.4068,  -2.2286, -17.2003, -18.4156]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 33
	action: tensor([[ -2.5064, -21.4388,  13.5991,  -2.6121,   4.7749,  17.4900, -11.3179]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 34
	action: tensor([[ 10.2626,   3.2222,  -4.2300,  -4.0175, -11.1788, -13.2615,  -6.2044]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 35
	action: tensor([[-20.5415,  -7.6660,   0.6516,  17.7412,  -5.0351, -23.3276,   2.5538]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 36
	action: tensor([[  8.8728,   3.0008,  23.2034,  21.2056, -14.8318, -16.1271, -11.4100]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 37
	action: tensor([[ 23.6634,   9.3309,  13.1381, -14.7101,  -2.7561,  -8.2611,   4.9380]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.332835825155646 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 38
	action: tensor([[  1.4460,   5.2665,   4.2383,   6.8067,  -0.8924, -13.4690,   7.7889]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.1037595728007008, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 39
	action: tensor([[ -6.5845,   0.9115,  11.0134,  16.0366, -17.0876,  -1.7318, -12.9571]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9317, 3.7326, 3.7043, 3.7930, 3.8272, 4.0072, 3.8318]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 40
	action: tensor([[22.6247,  5.0378, 14.0132,  8.1869, -2.0012,  3.0104, -6.2282]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 41
	action: tensor([[-21.9641,  -4.9915,  -7.9994, -13.0749,   8.0865,  25.7347,  -7.6232]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 42
	action: tensor([[-17.7298,  -2.3404,  14.7077,   3.9171,  -6.7567, -23.7241,  -9.0970]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 43
	action: tensor([[11.7149, 23.5660, 19.5879, 25.1230,  6.7444,  2.1195,  6.3397]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 44
	action: tensor([[  7.4133,  -1.9879,   1.1190, -10.0574,  -5.2752,  -9.6562, -15.2176]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 45
	action: tensor([[ 14.5132,   6.7213,  -2.7484,  11.4990,   5.3176, -14.3489, -20.0207]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 46
	action: tensor([[-26.5816,   4.1140,   8.2419,   3.5725,  11.3937,  -4.6580, -22.6283]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 47
	action: tensor([[ 18.2836,   8.4484,   9.6899,  23.2005,  -6.0022,  -2.6521, -10.0187]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 48
	action: tensor([[11.6603, -4.9682, 10.8157, -0.2899,  4.8096, 19.2593, -7.1619]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 16, step: 49
	action: tensor([[ 15.9121, -19.1064,   5.4444,   8.2571,  -2.5577,  17.5159, -21.9232]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[3.9901, 3.8038, 3.7786, 3.8637, 3.8969, 4.0863, 3.9001]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 16 actor 1176.4947000789934 critic 2395.628663468058
epoch: 17, step: 0
	action: tensor([[  2.9965, -22.5495,  -0.3994,  12.6491,  -8.0492,   8.4459,  -4.3461]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 1
	action: tensor([[  9.2856,   1.4062, -10.1456,   6.3664,  -1.7544, -19.1940,  10.3476]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 2
	action: tensor([[ 11.4431, -10.9677,  21.9090,  -4.5679,   4.6187,  20.3309,   3.3955]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 3
	action: tensor([[ 13.3596,   4.4047,  -7.4390,  -7.6033,  17.4959,  23.5543, -18.2114]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 4
	action: tensor([[ -3.2912,  19.4902, -11.4487,   8.1818, -11.8759,   5.0235,   6.3286]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 5
	action: tensor([[  3.0731,  -3.3479,  -1.7057,  -1.6563, -21.7152,  -4.1897,  -1.9251]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 6
	action: tensor([[-28.2591,  13.1350,   6.4780,   7.5779, -11.2550, -21.8549,  16.2920]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 7
	action: tensor([[ -7.6398, -10.8860,   8.5486,  26.0516, -22.1297,  -5.3691, -14.8675]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 8
	action: tensor([[-19.9319,  -7.7234,   4.5802,  -0.8246,  -6.4770,  26.5490,  -5.4406]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 9
	action: tensor([[-18.1683,  -9.0696,  -0.1346, -23.6972,  11.0734,   8.3074,  19.8482]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 10
	action: tensor([[ -4.7799,   7.4527,  -5.4439, -24.9848,  -5.8880,  13.5594,   4.2375]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 11
	action: tensor([[30.3181, -9.5811,  8.3978, 22.0049,  3.8248, 12.8987,  4.9066]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.202247644532071 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 12
	action: tensor([[  0.7220, -26.0342,   6.6682,  17.1817, -20.3214,  25.7845,  -1.1011]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5840050506852126, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 13
	action: tensor([[ -6.0775,   7.8366,   5.1793,  11.0484,   2.1219,  -0.5362, -13.6208]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[3.7544, 3.6097, 3.5095, 3.6269, 3.6275, 3.8654, 3.6914]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 14
	action: tensor([[-17.6712,  -1.9427,  16.0186,  23.6208, -12.5981,  12.1348,  19.8900]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 15
	action: tensor([[ -6.2474, -16.6938,  -1.3730,  17.4387,  16.4888,  36.6626,   9.8933]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 16
	action: tensor([[ 14.8047,   6.8731, -11.9946,   4.2274,  18.5472, -12.2912,   2.6808]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 17
	action: tensor([[ 18.5462,  -2.5868,  -6.2420,   4.6179,   6.2480, -16.6669,  -8.2195]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 18
	action: tensor([[ 7.4497, -8.0146, -3.0389, 12.1538, -1.9019,  5.9938, 10.8572]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 19
	action: tensor([[12.6811, -4.2953,  9.6935, 15.5168,  9.6909, -4.0005,  8.7610]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 20
	action: tensor([[ -8.4734, -24.2217,  21.2391,  -1.3168,  -5.1763, -17.5375,   2.0292]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 21
	action: tensor([[ 22.5965,  -3.1354, -12.7258,  -0.4937,  -8.9076, -18.7157,   8.2958]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 22
	action: tensor([[-14.4362,  19.7361,  -9.4042, -11.2970,  -2.8568,  -0.6284, -19.3162]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 23
	action: tensor([[-19.5135,   7.8929,   5.2178,   4.8445,  -5.5891, -16.1243,  -3.6311]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 24
	action: tensor([[-17.1187,   9.7600,   2.8178,  -7.1314, -18.8660,  -2.2466, -24.8695]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 25
	action: tensor([[-11.7263,   3.0332,   5.7324,  40.8096,  13.8095, -19.4404,  10.1446]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4402403508811472 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 26
	action: tensor([[-13.2572,   5.7295,  -7.4830,  14.1756,   0.7264,  -5.6879,  -0.5511]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.5615833600347495, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 27
	action: tensor([[ 7.1991, -1.7846, 14.8328, -3.3734, -2.2235,  3.4035, -2.8869]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[3.7402, 3.5836, 3.4718, 3.5863, 3.5967, 3.8430, 3.6630]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 28
	action: tensor([[  5.5007, -19.8114,   5.5263,   4.4562,   5.3755,  14.7209,  27.1254]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 29
	action: tensor([[ 14.5068,  21.6644,  -9.4169,   2.2237,  -7.9595,  38.6387, -11.6994]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 30
	action: tensor([[  3.3063,   2.4061, -13.2440,  14.3017,  -9.9872,  11.3965,  -3.5296]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 31
	action: tensor([[-10.6051, -20.4474,   3.5694,  19.7333,  -7.4981,   8.9056,  -6.9445]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 32
	action: tensor([[ -2.4601,   3.4440,   9.1231,  20.5490, -14.2452,  -8.8365,   2.9025]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 33
	action: tensor([[-31.7720,   8.7966,  12.5793,  12.2441,  -7.8013,  -9.2141,  12.1247]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 34
	action: tensor([[14.6372, -4.5340,  3.2191,  5.2444,  3.3318, 20.3135, 10.2788]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 35
	action: tensor([[ -3.4126,  -7.7972,   6.6141,  -0.3635, -17.0111,   3.5754,   1.8259]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 36
	action: tensor([[ -6.6393,  -8.9810,  -1.1521,   1.3152, -31.6169,  -1.7051, -11.1456]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 37
	action: tensor([[  8.6691,   7.0028,   6.7054, -16.4600,  -9.3468, -19.0784,  -6.0503]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 38
	action: tensor([[-4.6357, -2.4796, 23.3902, -4.7671, -3.9306, -2.2331, -6.1443]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 39
	action: tensor([[ 31.0922,  -4.8154,  -0.5062,  -1.5266, -14.5815,  -3.9777,  -7.9951]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 40
	action: tensor([[-10.1818,  20.9699,   4.4191, -14.7610,  -3.3039, -20.1206,  -9.4370]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 41
	action: tensor([[ -0.2444, -14.2915,  18.2830,  36.3743,  13.2550,  20.1323, -14.8494]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 42
	action: tensor([[-38.6244,  11.0344,   7.4102,  24.9955,   1.1446,  22.7203,  27.6231]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 43
	action: tensor([[-14.1787, -16.8931,  -2.1202,   8.4303,  22.1521,   8.5201, -11.1816]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 44
	action: tensor([[  8.2873,  12.7605,   4.1384,  -6.0278,   4.5295, -21.3474,  -1.9620]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 45
	action: tensor([[-22.7001,  -5.8567,  19.2113,   5.9497,  -3.4608,  12.3026,   8.4147]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 46
	action: tensor([[-24.8709,  15.2154,  35.2566,   5.1635,  22.2132,   9.8083, -20.4813]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 47
	action: tensor([[-3.4906, 19.1400, 13.3810,  5.0373, -9.6467, 10.0710,  7.2629]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 48
	action: tensor([[-13.2764, -13.8941,   7.1966,   8.6581,   0.0263,   2.3929,   2.0827]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 17, step: 49
	action: tensor([[ 26.1656,  18.1090,  -3.9131,   6.0989,   4.7903,   8.8091, -12.4858]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.0853, 3.9427, 3.8377, 3.9432, 3.9576, 4.2083, 4.0278]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 17 actor 1152.504095389977 critic 2391.9988248895893
epoch: 18, step: 0
	action: tensor([[ -4.8918,  -7.4867,  11.0781,  14.2243,  -3.1547, -36.7074,  -5.2076]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4300106752743487 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 1
	action: tensor([[-19.6859,  -6.1175,   5.8556,  11.0506, -17.0026,  -0.5102,  -8.1314]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.636558719469918, distance: 1.4639373439010006 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 2
	action: tensor([[ 15.1197, -13.3148,  11.3596, -14.6232,  -1.4357,  23.3849, -12.0077]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639373439010006 entropy tensor([[4.1996, 4.0772, 3.9528, 4.0708, 4.0775, 4.3474, 4.1446]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 3
	action: tensor([[ -9.6412,   9.7489,  -7.1757,   2.8703,   4.6367, -22.4819, -12.8654]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639373439010006 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 4
	action: tensor([[44.8083, -3.8336, 16.1269, 24.1265,  7.8027, 15.0540, -0.2433]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639373439010006 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 5
	action: tensor([[ -6.6050,  -3.5255,   3.4355,   6.5218,  -4.4258, -17.5377, -18.1602]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639373439010006 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 6
	action: tensor([[  7.0957,  20.7320,   1.8368,  11.0964,  -2.4385,  -0.7041, -13.8945]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639373439010006 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 7
	action: tensor([[  3.9297,   5.0162,   6.5288,   6.0638, -15.3875,  -6.2090, -14.4591]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639373439010006 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 8
	action: tensor([[-2.6958, -6.1325,  1.6262,  8.9174,  5.7312, -8.7601, -3.5231]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639373439010006 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 9
	action: tensor([[-18.6196,   5.7922,  -9.6099,  12.4299,  -8.4887,  33.7008, -12.0284]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639373439010006 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 10
	action: tensor([[  1.6057,   2.5443,  -6.1561,   8.1584,   4.3534,   1.3679, -12.1885]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.4639373439010006 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 11
	action: tensor([[ -5.8045,  -0.5083, -26.3122, -12.6170,   4.9290,  18.8201,  -2.5966]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.16377433867626334, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 12
	action: tensor([[ -4.0888,  -5.8034,   2.4322, -14.1341, -19.5548,   6.6475,   5.2874]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.0107, 3.8863, 3.7552, 3.8775, 3.8870, 4.1471, 3.9487]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 13
	action: tensor([[ -0.9380, -14.0574,  -4.0125,  22.1938, -20.8833,  12.4237,   0.3596]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 14
	action: tensor([[ -9.1674, -16.2229,  38.8692,  24.0936,   7.0188,   6.5760,  18.2403]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 15
	action: tensor([[-18.6578,  10.7741,   4.9591,  -0.7977,  -7.2038,  -2.4722, -17.5170]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 16
	action: tensor([[  6.1135, -16.5087,   8.4324,   9.7647, -21.7640,  -2.4735,   2.4665]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 17
	action: tensor([[-13.3134,  17.5774, -13.7510,  15.5010, -15.2372, -19.3022,   2.1625]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 18
	action: tensor([[  5.0379, -18.2889,   2.0181,  -0.7380,   2.3366, -14.8592,  19.6535]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 19
	action: tensor([[-16.3639,  -8.0458, -15.4475,  20.8493,  -6.7878, -20.1023, -23.4412]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 20
	action: tensor([[ -8.1126,   5.6894,  11.4817,  -3.8371,  17.0553,  16.1743, -13.6648]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 21
	action: tensor([[ -7.8564,   6.6442, -10.6954, -17.1691, -32.4076,  -3.1671,  22.9482]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 22
	action: tensor([[  9.0404, -19.0598,  25.5117,  37.0230, -32.4616,  11.7180,  22.4543]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 23
	action: tensor([[ -8.6944,  -5.7059,  -9.8040,  -2.5370,   7.8296, -37.7233,  -4.9704]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 24
	action: tensor([[  4.9905,  -6.3440, -16.2342,   2.6875,   4.1055,  18.9992,  18.9378]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 25
	action: tensor([[17.1090, -7.0818, 14.4364,  4.0148,  2.0126, 24.9092,  0.2404]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 26
	action: tensor([[-12.9513,  -4.3768,  -7.2689,  13.6483,  17.0418, -36.2285,  32.8051]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 27
	action: tensor([[-15.2186,  -0.9981,  12.7067,   0.3093, -10.4830,  -1.8629, -12.5206]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 28
	action: tensor([[  0.9719,  -1.0628,  -5.8371,  30.6981, -15.5166,  -2.2492,  18.5593]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 29
	action: tensor([[  6.9029,   6.6605, -14.7455,  -0.4527, -12.7461,  -8.8463, -10.0585]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 30
	action: tensor([[-13.4239,   6.9987,  -8.4709,  28.0628,   5.0145,  40.3491,   0.1407]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 31
	action: tensor([[ -6.3997,  16.7479,  20.2925,  13.3905, -11.4664,   5.5961,  22.9771]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 32
	action: tensor([[-26.9455,   7.6235,  -7.8677,  -1.6568, -30.0338,  -1.8421,  -3.5433]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 33
	action: tensor([[  1.1645, -10.2526,  13.8436,   5.1321, -14.0746,   9.1513,   8.0558]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 34
	action: tensor([[ 27.7975,  19.2724,   3.8224,  -7.0146,  -7.0846, -37.1575,  11.2191]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 35
	action: tensor([[ -3.3360,   7.9782,   8.6764,   7.7543,   9.2307, -30.8847,   8.4178]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 36
	action: tensor([[-18.7628,  29.7398,  12.5541,  10.1522,   0.5462, -20.0090,  -3.3446]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 37
	action: tensor([[ -4.4242,   8.2813,  14.0646,  11.2019,  -4.6826,   5.5943, -14.5035]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 38
	action: tensor([[-15.3575,   3.2207,  13.6435,  -2.6485, -14.0908,   9.4510,  28.8702]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 39
	action: tensor([[ -1.6043,  -1.8177, -13.2286,  18.7843,  -3.8432, -30.9061,  -9.3286]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 40
	action: tensor([[ 26.4174,   5.5488, -21.6452,  -6.9875,   0.8830,  15.9551,  -4.1791]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 41
	action: tensor([[ 12.9082,   1.8328, -10.1324,  -4.0101,  -2.1551, -25.9776,   3.5695]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 42
	action: tensor([[-17.7761,  10.8858,  -2.9342,  13.0767,   1.1100,   1.4086,  17.7547]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 43
	action: tensor([[ -1.9356,   9.8985,  -6.4330,   2.4401,  -2.2930, -38.2670, -17.3751]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 44
	action: tensor([[  8.4894,  -1.8979,  -4.2632,   5.7984,  -5.1363, -15.8824,  -1.6613]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 45
	action: tensor([[26.8056,  2.7248,  0.8400, -0.7963,  7.3536, 23.2186, -6.8004]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 46
	action: tensor([[  7.5478,  13.2806,  27.0735,  -4.5307,   0.2904, -23.0463,  -5.3876]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 47
	action: tensor([[ 12.4893,  16.6635, -20.2448,  29.2629,  -8.9256,  -7.9901, -10.5651]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 48
	action: tensor([[ 14.0123,   1.4598,   2.3652,  -6.4150, -22.1406, -35.5473,  -7.3251]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 18, step: 49
	action: tensor([[  1.2110,  21.9367, -10.6675,  28.6259,   6.3596,   0.4743, -28.0298]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.1845, 4.0649, 3.9397, 4.0568, 4.0658, 4.3402, 4.1327]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 18 actor 1152.5112164190823 critic 2390.754366648993
epoch: 19, step: 0
	action: tensor([[-4.4510, 27.3003,  1.8629, 14.1838, -9.4180, 14.5699,  8.3637]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 1
	action: tensor([[ 0.7350, 10.2916, -0.5324, 19.3591, -8.0061, 15.0119,  5.5626]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 2
	action: tensor([[ -9.7066,  -3.6803,   1.4693,  -8.4035, -50.6154,  -9.3419,  14.2462]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 3
	action: tensor([[ -6.3022,  -6.4121,   5.7700,  -2.4610, -10.2224,  13.3833,  -4.2308]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 4
	action: tensor([[-4.1858, 16.9045, 33.0181, 19.1740, -1.3229, -7.7281, 14.0320]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 5
	action: tensor([[-35.3506,  -2.2010,  10.6964, -11.4570,  -0.7562,  29.4196, -39.8603]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 6
	action: tensor([[ -4.0371,   8.0656,  -7.1992,  -8.3609, -17.6372,  53.2051, -26.3855]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 7
	action: tensor([[-13.6653,   3.7417,   8.4543,  19.4287,   2.1368,  38.7200,  13.9871]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 8
	action: tensor([[-10.6857,  18.1667,  -2.2665,  15.5495,   2.7199, -11.4785,  -2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 9
	action: tensor([[ 5.2987,  2.5414, -3.1911,  3.3038,  8.2807, 60.2102, 31.1746]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 10
	action: tensor([[ -6.0773,   6.4457,  -2.7281,  -6.6584,   5.4022,  -4.5218, -12.3858]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 11
	action: tensor([[  7.9590,  11.3169, -13.3552,  23.6745,   1.8956,  17.6087,   4.7255]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 12
	action: tensor([[  2.0912,  10.4453,  27.7083,  41.3262, -25.1103,  -0.2417,  16.2975]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 13
	action: tensor([[  9.9906,   6.2906,  19.0456,  24.2192,  15.6857, -14.8849,  -0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0464498946332272 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 14
	action: tensor([[-18.8259,  -5.5019,  23.8929, -20.5992,  18.1215,  12.1782, -20.7262]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7942022831677291, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 15
	action: tensor([[ -2.2576, -15.9084,  13.9171,  -0.7567,  27.2016,  -3.1937, -14.0438]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2371, 4.1013, 4.0022, 4.1361, 4.1385, 4.3984, 4.1655]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 16
	action: tensor([[-11.3600, -17.1953,  43.3578,  10.3010,   9.7619,   2.8681,  13.2257]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 17
	action: tensor([[ 13.3805, -18.8297,  -0.9606,   3.5160,  -2.1691,   8.7466,  26.8312]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 18
	action: tensor([[ -8.4621,   7.4884,   5.8473, -12.5720, -21.6334,   1.0998,   5.1014]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 19
	action: tensor([[ 11.2067,  14.5401,  -4.8134,   1.9249,  31.3414,  22.8800, -19.5768]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 20
	action: tensor([[ 20.3464, -35.8031,   3.9893,   3.8394, -11.3773, -31.1478, -24.6569]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 21
	action: tensor([[-19.1679,  -1.1257,   7.2869,   2.6274,  16.1312,   4.3249, -14.7828]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 22
	action: tensor([[13.6226, -9.4728, 14.4070,  9.6979, 12.9516, 26.2647, 18.0436]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 23
	action: tensor([[-9.0341, -8.9966,  0.3581,  5.9876,  3.6379, 26.4071, 15.1135]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 24
	action: tensor([[ -6.6167,  -5.4321,  18.3590,  15.5417, -12.6682,  -0.9909,  15.7522]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 25
	action: tensor([[-2.9650, 22.1860, 21.2713, 18.3371,  0.0513, -8.0791, 21.8791]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 26
	action: tensor([[  2.3538, -13.9322,   7.0459,  53.7380,  11.5594, -25.2306, -12.6913]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 27
	action: tensor([[-36.0574,   8.2953,  -4.2147,  19.2819, -18.1465, -15.3575,   3.9517]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 28
	action: tensor([[-10.5204, -32.3889,   0.3599,  27.4499, -10.4796, -31.0642, -35.0770]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 29
	action: tensor([[-16.5678, -32.0057,   1.7354,  16.8029,   8.3385,  11.2241,  34.8340]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 30
	action: tensor([[22.8201, 20.2945, 22.2677, -4.3097, 22.3618,  6.3696,  9.5724]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 31
	action: tensor([[ -4.2065,   7.0728, -12.1976,   2.3787, -12.3500, -15.2967,  -8.0874]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 32
	action: tensor([[ -0.1724,  34.1813,   5.6266,  31.1328, -26.4853,  56.9932,  26.3683]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 33
	action: tensor([[-22.0393,   9.9461,  15.9310,  10.6454,   7.9027,  28.0567,  -5.1504]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 34
	action: tensor([[ -2.6665,   5.3129,  -9.2249,   5.6885, -22.2988, -11.7609,   8.0236]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 35
	action: tensor([[-11.3431,  -7.7624,  -5.4254,   0.5815,  12.1225,  -3.5511,  29.4322]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 36
	action: tensor([[-11.4098, -19.9258, -11.7817,  13.3001,  -0.3718,  17.9307, -27.4250]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 37
	action: tensor([[ 14.0113,  17.8240,  19.8836,   0.3701,  -2.2179,  27.2222, -11.3818]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 38
	action: tensor([[ 16.7915, -12.6159,   5.0541,  -9.5921, -22.7514,  13.1012,  -1.9060]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 39
	action: tensor([[-11.3617,   8.0010, -10.3589,  14.9739, -10.2737,   2.5867,   8.8078]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 40
	action: tensor([[-43.4888,  -9.5779, -22.5631,   8.5340,  -2.0404,  18.9379,  -8.0448]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 41
	action: tensor([[ -7.1576,  -1.6449,   4.7763, -17.3451, -35.4186,   7.2635, -10.2401]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 42
	action: tensor([[ -9.3524, -12.3326, -25.0213,   3.1925, -35.8012,  17.6858,   3.5932]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 43
	action: tensor([[-7.9393, 19.8176,  3.4636, 24.4285, -7.1540, -7.3278, -7.8406]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5191310069200179 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 44
	action: tensor([[-17.6506, -24.7704,   0.3829,  25.5768,   3.6103,   2.0675,  -0.9777]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8939870039659973, distance: 0.3725943349163034 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 45
	action: tensor([[  2.9695,  -5.2869,  -7.5854,  22.6288, -13.7286, -11.7252,   4.9922]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3725943349163034 entropy tensor([[4.0843, 3.9389, 3.8402, 3.9827, 3.9792, 4.2243, 3.9944]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 46
	action: tensor([[ 17.0193,   5.4208,  35.4288, -18.7634, -16.3169,  22.9725,   7.6716]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3725943349163034 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 47
	action: tensor([[-42.5741,   9.5202,  25.1587,  35.1731,  11.0530, -12.4961,  15.5063]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3725943349163034 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 48
	action: tensor([[-24.1221,  -2.9073,  19.3668,   1.3207,  15.3680, -22.2989,   2.6595]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3725943349163034 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 19, step: 49
	action: tensor([[  1.2948,  16.2273,  -0.4422, -17.2153,  19.7067,   8.4938,  25.6881]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3725943349163034 entropy tensor([[4.2971, 4.1603, 4.0594, 4.1957, 4.1949, 4.4629, 4.2242]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 19 actor 1152.5831716622097 critic 2386.7916922226336
epoch: 20, step: 0
	action: tensor([[ -7.7259,  14.0364,   8.0097,   7.5979, -21.4392,  19.5256, -24.6506]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3725943349163034 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 1
	action: tensor([[ 7.8338,  2.7992,  1.5552,  0.1083, -1.7135, 12.7078,  1.5878]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3725943349163034 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 2
	action: tensor([[ -8.4607,   9.6393,   4.0924,  14.5536, -21.7713,  41.2506,  -6.6186]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3725943349163034 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 3
	action: tensor([[ 8.5544, -7.9577, -1.1918,  8.8288, 16.9256, 41.7273, -7.2676]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3725943349163034 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 4
	action: tensor([[ 18.7888, -11.8348,  10.2234,   8.1201,  -5.3566, -26.7416,  33.9816]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.36300788289603014, distance: 0.9133215861401026 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 5
	action: tensor([[-3.8409,  8.0703,  5.9544, 20.0846,  5.4441, 10.9184,  8.5501]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9133215861401026 entropy tensor([[4.6103, 4.4391, 4.3588, 4.5189, 4.5044, 4.7318, 4.5074]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 6
	action: tensor([[-10.6507,  -4.9162,  15.5587,  -7.2596,  -1.8238,   8.9185, -38.7069]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9133215861401026 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 7
	action: tensor([[-12.3180,  12.3573,  16.4090,   1.5032, -62.6879,  27.4376, -19.9251]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9133215861401026 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 8
	action: tensor([[  8.5748, -15.1231, -28.3081,  26.7689,  23.3447, -22.8485, -21.1467]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9133215861401026 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 9
	action: tensor([[ 21.9212,   8.1741,  -0.1135,  20.9230, -23.3646, -15.6344,  -8.5970]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9133215861401026 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 10
	action: tensor([[ 12.4443,  26.2964, -23.8197,  16.8911,   3.9595,  19.4514,  12.4813]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2897711671533836, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 11
	action: tensor([[-24.4444,  10.6140,   0.1412,  13.9254, -10.0809,  14.1961,   4.4795]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.3335, 4.1696, 4.0949, 4.2564, 4.2395, 4.4603, 4.2363]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 12
	action: tensor([[  0.6168,   5.6731,  14.1941,  18.5258, -30.3666,   9.3835,  18.4792]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 13
	action: tensor([[-23.7396,  -5.1571,  20.0806,   4.0483,  -6.8268, -12.7666,   1.5514]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 14
	action: tensor([[ 3.1005e+00,  4.5206e+01, -9.2501e-03,  1.7364e+01, -1.6548e+01,
          2.5616e+01, -1.3907e+01]], dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 15
	action: tensor([[-13.9175,  37.5026,  22.9887, -15.9074,   3.3977, -34.7447,  -1.0653]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 16
	action: tensor([[-29.5460,  -1.9501,  -4.0226, -21.8342,  25.0840,   6.1623,  -9.2102]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 17
	action: tensor([[-10.8245,   1.3765,  -1.7405,  24.6565,  -8.8156,   5.9699,  11.6307]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 18
	action: tensor([[ 21.3431, -49.4892,   7.3476,  44.1809, -13.9539,   0.6240,  25.6814]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 19
	action: tensor([[ -8.7614,  19.5120, -18.0349,  -7.9000,  -5.2984, -24.0783,  14.3711]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 20
	action: tensor([[-22.0495,  26.9607, -14.6489,   9.7028,  14.0216,  27.7920, -34.6301]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 21
	action: tensor([[-25.0837,   4.7344,  -3.7291, -22.2311,  -0.6232,   7.8358, -14.7135]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 22
	action: tensor([[ -8.2172,  13.0142,  -7.2393,  -9.3119, -24.3093,  17.1415, -16.7353]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 23
	action: tensor([[-12.7874, -24.0125,  24.4054,  43.4806,  -6.6358, -12.3207,  34.1023]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 24
	action: tensor([[ 11.8287,  -7.6286,  -4.3494,  -5.8034, -27.8839,  30.2229,  12.0090]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 25
	action: tensor([[-11.0509,  -4.9784,  26.3291,  16.5223,  -6.4661,  41.7416,  44.6883]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 26
	action: tensor([[-52.2649, -34.4399,  14.5984,  14.3093,  16.3886, -26.6701, -16.4617]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 27
	action: tensor([[ 10.2930,  -7.5855,  -9.3761, -13.7022,  -5.3840,  23.7070, -24.0209]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 28
	action: tensor([[ 7.9533, -0.0381,  2.3884,  9.9006,  4.1345, 28.0293, 21.3183]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 29
	action: tensor([[-15.2322,   6.1597,   4.5929,  26.4429, -24.1243,  -4.5398,  -1.4226]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.299609942787701 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 30
	action: tensor([[12.7508, 19.7482, 19.1920, 23.7472, 13.7772, 32.5473, -5.1101]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.63880278258586, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 31
	action: tensor([[  3.0320,  -5.8219,  19.9737,  24.5186, -19.1208, -27.9963,  -7.4398]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.1246, 3.9586, 3.8864, 4.0418, 4.0262, 4.2554, 4.0252]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 32
	action: tensor([[  0.2344,   3.9784,  22.2328,  16.1355, -15.1711,   7.3379,  31.4009]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 33
	action: tensor([[ 19.4174, -17.1574,  37.8305,  27.2977, -22.0484,  50.5492, -13.0565]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 34
	action: tensor([[-14.2038,   2.4765,  13.0427,  -0.4622,  -9.5478,   3.6972, -15.8154]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 35
	action: tensor([[  2.2699,   9.0849,  15.3157,  19.9088, -12.6441,  12.4554, -10.4115]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 36
	action: tensor([[-10.4143,  -6.5173,  17.2799,  -2.5051,   9.5074,   3.2380,   6.0108]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 37
	action: tensor([[ -1.4390, -15.3261,  -1.4571,  -6.1041, -27.9326,  -5.7714,  -7.1059]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 38
	action: tensor([[ 33.8261,  15.3527,   9.7707,  -1.8282,  22.5546,  21.2752, -26.9838]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 39
	action: tensor([[  9.9118,  -2.0763, -36.2495,  29.1469, -19.2673,  21.4930,  -9.4130]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 40
	action: tensor([[  6.1577,  46.5650, -12.4001, -53.3830,  10.3442,   0.5602,  11.0606]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 41
	action: tensor([[-21.4762,  -7.5692, -41.2495,  10.2314, -40.1021,  -2.4015, -17.1852]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 42
	action: tensor([[-2.6252, 42.3157, -4.0991,  8.3473, 22.8159, 18.3279, -0.3499]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 43
	action: tensor([[-39.6485, -22.1403,  20.6011,  -5.6561,  11.6105,   9.5198,   5.0738]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 44
	action: tensor([[-14.8755, -24.2961,   2.1910,  14.8450, -40.7808,   3.9403,  -6.8835]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 45
	action: tensor([[ -1.4872, -19.9264, -18.2192,  24.8423, -47.8459,  -9.1021,  11.1002]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 46
	action: tensor([[ 14.7750,   1.5958,  27.5984,  -8.2734,   8.0182,  10.3173, -29.5437]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 47
	action: tensor([[-56.7047,  -4.2831,   9.9756,  23.4196, -20.1214,   8.3407,  13.8003]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6877472954961574 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 48
	action: tensor([[-29.9906,  -7.1317,  11.2056,  12.9146, -42.2416, -23.0972,  -4.7502]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4668192652351838, distance: 0.835591315636522 entropy tensor([[4.4365, 4.2727, 4.1942, 4.3539, 4.3401, 4.5711, 4.3437]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 20, step: 49
	action: tensor([[ -5.1411,  17.3442, -11.8352,  21.7516, -16.8561, -28.7823, -15.7186]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.3103, 4.1340, 4.0586, 4.2192, 4.1991, 4.4248, 4.2021]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 20 actor 1104.5790511343898 critic 2376.593001341964
epoch: 21, step: 0
	action: tensor([[  2.7391,   5.2981,  29.4237,  -8.1772,  18.3783, -25.0656,  26.2562]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 1
	action: tensor([[55.9168, 25.5073, 10.5382,  6.2376, -3.0285, 10.1188,  1.2533]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 2
	action: tensor([[ 19.2911, -10.6463,  25.0660,  -7.9307, -17.3125,   9.1894,  19.6400]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 3
	action: tensor([[-20.4507,  -3.8346, -21.2556,  35.1431,   2.1703,  14.9539,   9.3989]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 4
	action: tensor([[-29.6348,  49.8775,   1.6418,  13.9990,  38.6131,  -9.8475,   5.4727]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 5
	action: tensor([[ 20.3807,   8.7383, -20.8226,  48.2712, -17.8144,  32.3273,  -7.8466]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 6
	action: tensor([[36.2663, 21.5307,  5.3431, 21.0718,  2.8733, 14.6608, 28.3328]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 7
	action: tensor([[-18.6601,   2.4843,   4.6762,  35.2260,  33.9152,   6.6722, -33.3804]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 8
	action: tensor([[ -8.4975, -23.0945,  36.4392,  26.1567,   9.7142,  -1.7331, -16.1850]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 9
	action: tensor([[ 5.1434, 14.6019, 11.1153, -1.9658, 28.4369,  1.3256,  8.7791]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 10
	action: tensor([[-38.3341,  21.5376,  43.2423,  31.0978,  22.6736,  15.6339, -12.8664]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 11
	action: tensor([[ 40.5470, -23.6028,  -1.7540,  30.7243, -63.5895,  -9.1549,  16.5525]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 12
	action: tensor([[-35.2836, -19.1229,   6.8971,  38.1518,   3.8529,  -3.3846, -35.3631]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 13
	action: tensor([[ 32.2695,  -5.7727,   8.0853, -10.0731,  11.1811, -18.5914,   5.9447]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 14
	action: tensor([[ 18.6644,  -9.1613,   4.5846,  -8.3138, -25.5304,  20.1276, -14.8350]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.835591315636522 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 15
	action: tensor([[-25.7146,   6.5523,  32.6575,  18.0827, -37.3514,  13.6852, -30.8873]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.29123528760484296, distance: 1.3003473781756993 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 16
	action: tensor([[  6.5166,   2.4808,  -0.9982,  -4.8430,  -3.5728, -25.0970, -11.5245]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3003473781756993 entropy tensor([[4.0128, 3.8346, 3.7740, 3.9091, 3.8999, 4.1199, 3.8926]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 17
	action: tensor([[-17.0151,  -8.5993,  24.3577,  -3.5919,   2.4097,  -7.6818,  -7.3797]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3003473781756993 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 18
	action: tensor([[-12.8246, -19.5989,  15.5684,   4.3606,  -6.4330, -24.2062, -22.9035]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3003473781756993 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 19
	action: tensor([[ -0.1966, -23.4081,   4.4756,  25.4079, -15.1152, -10.9864,  -6.4796]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3003473781756993 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 20
	action: tensor([[-6.7342,  8.7958,  5.4043, 10.8774, 10.0823, 14.3795, -8.3083]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3003473781756993 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 21
	action: tensor([[ 18.7862, -18.4540,  13.3895, -15.1644,  -9.7134, -29.6875,  -6.7624]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3003473781756993 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 22
	action: tensor([[-25.1015,  17.7817, -52.1168, -27.1059,  -1.4845, -14.2375,  18.6303]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6425305184630443, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 23
	action: tensor([[  6.7191,  33.3507,  12.1781,  -5.4466,  16.5473, -24.0527,  25.7651]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.7075, 4.5318, 4.4644, 4.6002, 4.5939, 4.8146, 4.5947]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 24
	action: tensor([[ -8.7571,  11.4326,   1.7386, -31.6305,  29.9662, -23.2262,  27.5513]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 25
	action: tensor([[  5.3695,  -4.8799, -15.0864,  22.3983,  43.1791, -13.5277,  -8.2165]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 26
	action: tensor([[  4.1252,  30.7871,   6.7226,  10.5778,   0.2606, -16.6046, -12.8591]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 27
	action: tensor([[  9.7942,  -2.1100,   3.9367, -14.4482,  -5.2201, -37.9865,  -5.3840]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 28
	action: tensor([[ -1.7270,  14.0667,  -6.1274,  16.0264, -13.2542,  26.5467, -13.6523]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 29
	action: tensor([[  1.8323,  -8.6351,   3.1529,  10.4206,  49.8270,   8.9732, -26.0964]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 30
	action: tensor([[25.7841, 28.2489, 15.2256, 41.1217, 27.3881, 24.9559, 11.4883]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 31
	action: tensor([[ 21.3891,   9.3049,  -6.3170,  46.1537, -37.1709, -38.1458,   1.8722]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 32
	action: tensor([[ 14.5261, -32.6331,  12.0764,   2.5783,   4.2653,  50.0067,  13.5246]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 33
	action: tensor([[-41.6456,  -9.8196,   7.7507,  46.3253,  -5.1655,  21.2691, -21.5765]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 34
	action: tensor([[-39.7419,  27.2798, -11.8274,  28.8317, -17.5140,  53.0677, -22.7378]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 35
	action: tensor([[ 22.2015,   2.0605,   2.7595,  27.5036, -62.8376,  53.1174, -15.1706]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 36
	action: tensor([[31.9969,  9.6000, -7.3577, 29.4756,  1.9367,  9.2133, -7.3967]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 37
	action: tensor([[ -6.3489,   5.1795,  32.9316,  16.1110, -12.9119,  38.8013,  -6.8207]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 38
	action: tensor([[-45.6358, -20.6514,  -2.5171,  63.0531,  12.4787,  10.2875,  -6.6599]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 39
	action: tensor([[-13.3535,  -9.1108,   5.9848,  -6.7888,   4.5036,  40.7268,  42.4049]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 40
	action: tensor([[ 17.7127, -47.1042,  11.4256, -22.1732, -15.2142,   7.7171, -11.0833]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 41
	action: tensor([[  5.0395, -30.2231,  27.5415,  25.4985, -14.1234,  -2.7548, -12.4339]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 42
	action: tensor([[ -4.2633,  37.6892,   0.3255, -16.5106,  27.0536, -30.1732, -31.5673]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 43
	action: tensor([[-29.4173,  -8.1389, -10.3145, -31.2921,  -2.2056,  46.5437, -14.7864]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6841891432320639 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 44
	action: tensor([[ 32.2653,   7.3371,  21.5488,  12.6905,  29.3825, -44.6771,   5.5606]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9493152629043005, distance: 0.25762932303466296 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 45
	action: tensor([[ -9.5494,  -0.9726,  -0.6187,  16.5982, -17.3204,  -0.4222, -27.4340]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.1613, 3.9773, 3.9084, 4.0536, 4.0472, 4.2531, 4.0302]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 46
	action: tensor([[-16.9277,   1.9314,  -9.1848,  28.1586,   3.9294,  10.7914, -54.7957]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 47
	action: tensor([[-40.6655,  36.3621, -14.6127, -25.1211,  -5.9817,   6.8812,  -9.5668]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 48
	action: tensor([[ -2.1123,  -5.0404,   2.8991, -21.0807,  27.4250,   5.4118, -16.6180]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 21, step: 49
	action: tensor([[  8.2400,  46.6593,   3.0439,  40.1926,  22.7325, -42.0811,  -4.8220]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.6137, 4.4378, 4.3689, 4.5046, 4.5005, 4.7261, 4.4995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 21 actor 1128.5975814760266 critic 2381.942577835042
epoch: 22, step: 0
	action: tensor([[-16.2935, -41.2527, -20.5850,   7.2616, -23.2213,   7.9160,  29.5406]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 1
	action: tensor([[ -8.8195, -37.0402,  48.2389,   5.8783, -21.4751, -16.5455, -25.6470]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 2
	action: tensor([[-20.8641,  11.4257,   1.0496,   2.9519, -24.5764, -20.2006,  12.3505]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 3
	action: tensor([[  2.1546, -13.0214,  -2.6161, -30.6442, -28.6418,   9.8105,  11.2426]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 4
	action: tensor([[ 26.6393,   0.3461,   2.5337,  17.7649,  -8.9282,  59.7737, -21.3832]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 5
	action: tensor([[-52.3256,  12.9804,  10.1962,  13.0296,  50.2670, -21.8346, -32.0344]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 6
	action: tensor([[ 31.3454,   8.5946, -17.6305,  22.6190, -39.8017,  28.7778,  37.4526]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 7
	action: tensor([[ 15.5125, -21.2360,  14.4110,  10.7609,  38.8401,  52.7235, -31.3239]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 8
	action: tensor([[-47.5647,  -3.5506,  40.6924,  -6.0699,  27.1083, -73.1297, -32.6639]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 9
	action: tensor([[  3.4129,  -0.9125,  12.9134,  23.5902,  14.9213, -38.6715, -16.9243]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.25762932303466296 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 10
	action: tensor([[ 11.1326,  -7.0717,  12.9276,  31.2982, -41.7941,  12.6450,  47.4200]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -1.7810250439631088, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 11
	action: tensor([[-22.8222, -24.6287,   1.7416,   9.4772,  26.2254,  -7.5588,  -5.8606]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.8105, 4.6109, 4.5559, 4.6607, 4.6715, 4.8918, 4.6789]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 12
	action: tensor([[  2.2001, -21.6987, -14.2765,  -2.8118, -36.8722, -11.8720,  11.3865]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 13
	action: tensor([[ -4.5102, -17.8132,  12.3109,  24.9056,  48.7258,  25.2710,  -5.3292]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 14
	action: tensor([[ -2.7119, -35.4573,   6.5003,  71.6459, -27.5789,   8.8311,   4.9330]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 15
	action: tensor([[  2.1191, -17.2250,  39.7290, -18.2784, -59.1970,   8.9814,  -5.9614]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 16
	action: tensor([[-30.1090,  -8.0228, -26.9759, -13.9537, -25.7090,  16.6947,  34.3128]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 17
	action: tensor([[ -9.1696,  19.8465, -20.8527,  24.4915,  24.1533,   8.6545,  23.2811]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 18
	action: tensor([[ -5.9454, -23.7074,  -0.9734, -16.2003,   2.7494, -26.1947, -24.4765]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 19
	action: tensor([[-22.3601,   8.5433,  -1.4625, -14.0670, -26.1526,  45.2804,  -7.2103]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 20
	action: tensor([[-62.2100,  -3.1283,  -5.0218,  19.7005, -11.2611,   5.9885,  -2.4411]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 21
	action: tensor([[ -1.0394,  34.3467,  31.6822,  -8.5450, -33.0164, -11.1124,  40.4734]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 22
	action: tensor([[-23.4251,  30.5438,  12.2727,  -5.2528, -51.5227,  26.0377, -26.1696]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 23
	action: tensor([[-54.1927, -15.7384,  24.1213, -10.3166,  17.8330, -57.2814,   2.1072]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.9083548948398774 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 24
	action: tensor([[-12.4410,   5.7167,  63.5498,  18.5128,  14.3445,  23.6132,  40.6522]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.15403529788746462, distance: 1.0525259556913635 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 25
	action: tensor([[-35.8861, -14.9648, -22.2407,  17.5342,  -0.8533,   0.4069,   3.3725]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0525259556913635 entropy tensor([[4.5991, 4.3979, 4.3468, 4.4496, 4.4609, 4.6809, 4.4627]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 26
	action: tensor([[ 78.2852, -58.2542,  -3.5645,   0.5578,  16.1936,  16.8851,   7.3314]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0525259556913635 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 27
	action: tensor([[ 24.8802, -40.8838, -14.7163,  13.3437, -32.6919,  -9.8216, -12.7240]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0525259556913635 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 28
	action: tensor([[-18.9572, -28.8759,  22.5374,  30.7847,  10.8977, 100.1466,  12.5969]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0525259556913635 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 29
	action: tensor([[ 17.6077,  -5.4012,  19.6178, -12.0499,  24.5762,  -4.5837, -13.2015]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0525259556913635 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 30
	action: tensor([[ 15.3994,  -7.6197,  17.8304,  10.2156, -65.4072,  31.0430,  -4.6323]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0525259556913635 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 31
	action: tensor([[ -9.9530,  18.8213, -12.8370, -23.3657,   5.2333, -40.7826,  15.5531]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0525259556913635 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 32
	action: tensor([[15.0901, -8.6573, -5.2018, 15.4209,  9.0251,  6.2939, 25.7198]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0525259556913635 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 33
	action: tensor([[ -4.4552,  24.1471, -16.9331,  15.4745,  19.9217,  59.4063,  24.2116]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0525259556913635 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 34
	action: tensor([[  5.5704,  12.8253,  16.3027,  45.8565,  -3.7365, -62.0936,  29.2856]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.7957193667233058, distance: 1.5334722684024864 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 35
	action: tensor([[-71.5301,   4.0222,  69.4618,  12.7843,  30.7347,  23.7306, -43.2606]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5334722684024864 entropy tensor([[4.9948, 4.7932, 4.7365, 4.8394, 4.8573, 5.0739, 4.8626]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 36
	action: tensor([[ 18.6207, -14.0512,  14.2331,   6.1420, -22.6260,  29.8867,  30.5910]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5334722684024864 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 37
	action: tensor([[-16.8459, -59.1192,  28.9240,  14.7957,  -1.9064,  52.1015,  16.3116]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5334722684024864 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 38
	action: tensor([[  9.2831,  13.2222,  16.6985,  29.3142, -14.3656, -28.8781,  11.6715]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5334722684024864 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 39
	action: tensor([[-25.2184,  24.7216, -24.0681,  28.9415,  10.5420,   3.9195, -32.4386]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.5334722684024864 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 40
	action: tensor([[-19.8016,   1.0894,  11.6554,  -6.3440, -36.5939, -19.8816,   5.2138]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.22132287300223852, distance: 1.264654630747841 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 41
	action: tensor([[ -5.2840, -20.5165,  29.7009, -25.2006,  24.7714,  -0.4758,   4.9079]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.6004, 4.4035, 4.3481, 4.4480, 4.4677, 4.6884, 4.4753]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 42
	action: tensor([[ 33.6550,  17.9972,  24.4444, -27.7193, -22.3396,  -1.6023,  -6.4721]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 43
	action: tensor([[ 11.0205,  43.4887,  -8.8535,  20.2929,   8.9464, -22.1150,  41.7594]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 44
	action: tensor([[-22.3612,  32.4835,  29.1169,   2.9419, -11.4787,  11.3056,  -9.8081]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 45
	action: tensor([[10.1492, 46.1347, 25.2517, 32.3786, -6.5157, 31.7302, 16.2633]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 46
	action: tensor([[ -9.5425,  23.1107,  17.7285,  55.2188, -19.8995,   0.3355,  20.0315]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 47
	action: tensor([[ 15.6971,  -2.4481,  -0.4239, -17.9490,  -4.4203,  46.2172,  17.5281]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 48
	action: tensor([[ -0.2451,  -8.8282,  -8.4976,   7.1384, -12.1578,  -5.9826,  20.6476]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 22, step: 49
	action: tensor([[ 87.9054,  19.5116,  -8.0662,   3.6013, -11.2377,  16.8486,  13.7340]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.7894, 4.5931, 4.5397, 4.6392, 4.6583, 4.8812, 4.6646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 22 actor 1104.5175650039764 critic 2383.699226095405
epoch: 23, step: 0
	action: tensor([[30.5548, 27.6371, -1.8265, 18.4735, -2.1813, 16.0899,  0.4717]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 1
	action: tensor([[-44.6380,  20.8482,  23.6851, -12.4422,  23.8938,   1.3426, -25.1636]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 2
	action: tensor([[ 49.9274,  18.9980,   7.4498,  21.9381, -21.1212, -68.8654,  15.7028]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 3
	action: tensor([[ 4.9076, 30.1929, 53.1021, 47.6104, 27.9609, 60.5616, -8.8791]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 4
	action: tensor([[ 28.4815, -40.3040, -36.2093,  -7.2925,   8.6020,  31.5253,  13.3191]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 5
	action: tensor([[ 12.8070,  10.5909,  -4.6323,  47.8821, -31.1514,  -1.5081,  24.1730]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 6
	action: tensor([[ 49.7649, -12.8205,  25.9704,  -1.1755,   1.1296,  -4.8508, -52.7638]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 7
	action: tensor([[-75.1095, -42.7477,  14.7486,  41.6608,  20.9503,  50.4522,  28.0365]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 8
	action: tensor([[-10.7524,   0.5369,   7.8188,  20.2335,  55.6912, -12.3345, -17.2693]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 9
	action: tensor([[ 7.3368, 36.0405,  1.1259, 37.9167, 50.2359, 32.4029, 39.3876]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 10
	action: tensor([[ 21.2107,  17.9596,  10.7237, -22.2923,  22.5429,  -6.2998,   1.4435]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 11
	action: tensor([[  2.7356, -25.0159, -23.2588,  -6.6809,  -6.4490, -26.5802,  15.2923]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 12
	action: tensor([[ 33.2122,  18.6598,   3.2040,  58.1696, -30.2973,   8.5748,  22.5276]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 13
	action: tensor([[ 28.4864, -15.5046,  28.0917,  19.8574,  15.4094,  17.6046,  22.0770]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 14
	action: tensor([[-48.2273, -30.5419,  43.8399,  30.4792,  11.0094, -54.9757, -23.3030]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 15
	action: tensor([[ 15.1628,  14.9502, -33.0953,   6.9257,  -6.1021,   4.5129, -40.0981]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 16
	action: tensor([[-33.4683,   6.8536,  -4.6844,  17.1006,  13.4165, -29.8020,  14.6241]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 17
	action: tensor([[  1.4711,   3.7349, -27.7971,  29.2628, -21.1469,  37.8405,  43.0563]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 18
	action: tensor([[ -2.8356,  -5.4770,  20.2081, -11.2338,  12.2074, -32.1045,   1.7228]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 19
	action: tensor([[-48.7265, -12.5143,  63.4844,  49.4868,  21.2335,  41.2729,  -5.4515]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 20
	action: tensor([[ 21.3612,   2.4342,   0.4763,  35.1078,  49.3863, -26.2161, -44.3315]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 21
	action: tensor([[-29.7106,  -5.2247,  20.6629, -10.7782,  -1.1544,  29.3240, -31.4746]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.264654630747841 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 22
	action: tensor([[ 18.4103, -19.4377,  37.0577,  43.6344,  21.5171,  20.2433,  22.3336]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.046864750768462415, distance: 1.1172077991486014 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 23
	action: tensor([[-31.4395, -44.1018,  11.3600,  15.4579, -31.4474,  51.7649,  31.1726]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1172077991486014 entropy tensor([[4.9703, 4.7538, 4.7140, 4.8198, 4.8456, 5.0502, 4.8654]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 24
	action: tensor([[ 22.0738,  29.2322,  16.9767, -38.1253, -25.7257,   8.6266,   5.9314]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1172077991486014 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 25
	action: tensor([[ -6.9388,  28.1042, -25.8451,  69.3190,   9.1894, -32.8226,  -8.1926]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1172077991486014 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 26
	action: tensor([[-15.1205,   1.3504, -15.9640, -24.4762,   8.6711, -36.1149,  -1.6823]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1172077991486014 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 27
	action: tensor([[-31.3738, -16.1838,  24.6697, -15.0824,  -9.9108,  78.0552, -10.5862]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1172077991486014 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 28
	action: tensor([[ -6.9027,  24.5497,  16.9437,  37.5662, -37.8387,  16.4589,  45.4367]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1172077991486014 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 29
	action: tensor([[ 55.1557,  -1.8093, -13.0779, -11.7190,  28.6937,  -9.7304,  39.9308]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1172077991486014 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 30
	action: tensor([[-47.8933,  15.3744,  40.1141, -35.9756,  53.8719, -42.5568,  21.8204]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1172077991486014 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 31
	action: tensor([[ 31.2508, -19.9026,  14.9432,   1.6497,  12.5911,  38.6457, -30.5510]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.24550591878465333, distance: 0.9939959431156609 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 32
	action: tensor([[  0.5611,   5.3773,  10.8862,  20.9777, -32.1424,  -5.6442,  -6.7204]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06763230370373585, distance: 1.1824085085515825 entropy tensor([[4.2086, 3.9867, 3.9533, 4.0635, 4.0805, 4.2793, 4.0906]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 33
	action: tensor([[-50.4345,  -0.3642,   1.9210,  -4.5789,  10.5150,  -6.3911,  24.1207]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.45383241263410956, distance: 0.8457064732916271 entropy tensor([[4.5107, 4.2922, 4.2524, 4.3554, 4.3829, 4.5837, 4.4016]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 34
	action: tensor([[-22.6890,  -9.2912,  -2.3770,  10.3898,   6.5374,  28.2980,  -6.1099]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.3363, 4.1172, 4.0761, 4.1873, 4.2094, 4.4064, 4.2196]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 35
	action: tensor([[ -2.9005,  -1.1382,  -9.6414, -12.3737,  -1.5412,  59.6701,  48.0218]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 36
	action: tensor([[-16.5119,  15.6422, -18.0978,  45.1862,  13.9399,  -8.6685,  16.1261]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 37
	action: tensor([[-21.5458, -18.7903,  16.2087,  -9.1204, -28.1478,  31.3706,  15.1430]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 38
	action: tensor([[  6.8567, -12.4936,   1.2981, -10.2326, -39.6290,  20.2022, -20.7612]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 39
	action: tensor([[ 51.0515, -17.7054,  42.0164,   3.3224, -41.2682,  62.5589, -49.7527]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 40
	action: tensor([[-14.6749,  -7.0581,  25.5007,  23.1886, -10.0715, -11.3931,  41.3746]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 41
	action: tensor([[ 20.1679,  25.9223,   5.8873,   5.1332, -26.2380,  11.1512,  -1.7859]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 42
	action: tensor([[36.0681, 10.2618,  3.7103, -1.0422,  7.6620, 25.3641, 27.8988]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 43
	action: tensor([[55.5078, 16.8915, -3.2284,  0.2614, -4.4961,  3.0012, 50.7367]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 44
	action: tensor([[ -6.2057,  -0.1523,   2.4691,  54.4230,  11.2993,  20.7957, -27.3299]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 45
	action: tensor([[ 63.7528,  14.5823, -24.4844,  23.7238, -46.3638,  18.4371,  22.5258]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 46
	action: tensor([[ -5.8381, -23.9559,   1.5802,  -0.8801,   1.2618, -53.1033,  -3.7294]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 47
	action: tensor([[-67.4691,  -0.1286,  -8.4322,  32.9898, -12.9656, -49.6010, -38.8028]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 48
	action: tensor([[-36.4540,  25.9342,  15.2151, -34.8308,  15.6159, -12.3437, -31.4497]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 23, step: 49
	action: tensor([[-55.1059, -29.8210,  48.6294,  17.5901,  -0.6343,  -4.5549, -19.8260]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[4.9139, 4.7013, 4.6613, 4.7634, 4.7957, 5.0057, 4.8163]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 23 actor 1104.5409702357351 critic 2362.0773408188797
epoch: 24, step: 0
	action: tensor([[-82.1656, -34.3445,  53.5955,  35.2328, -23.6543,   4.8349,   7.5221]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 1
	action: tensor([[-25.4942, -41.9162,  35.5199, -13.7361,  -5.6174, -12.8100,  11.1492]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 2
	action: tensor([[ -4.6555, -27.7589,  -0.4283,  46.8773, -10.3708,  17.9069,   1.0254]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 3
	action: tensor([[-13.0799,  20.8926,  37.2645, -26.4464, -65.1249,  15.6619, -57.2531]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 4
	action: tensor([[ 59.8578,  36.2897,  33.2215, -26.8076,  -5.4791, -51.0237,  29.5523]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 5
	action: tensor([[ 28.8066, -39.7171, -44.4450,  55.0757,  30.4403,  27.6540,  26.7758]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 6
	action: tensor([[ 46.1814,  99.3744, -19.0512,   0.9447,   8.7328, -24.2986, -40.6917]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 7
	action: tensor([[ 7.9732, 24.2831, 37.1587, 18.6490, 34.7217, 39.1639,  2.0383]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 8
	action: tensor([[  3.6543,   5.1052,   4.3828,  63.6150,  41.6732,  -2.8451, -32.3463]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 9
	action: tensor([[-17.8601, -39.5386, -18.0352,  23.1681,  70.7828, -47.8643,  25.1994]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 10
	action: tensor([[ 6.4634, 63.1113, 51.8073,  8.8171,  5.2040, 13.1015, -9.9987]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 11
	action: tensor([[-14.7314,  -6.1099, -36.1567,  24.1478,  42.0978, -23.7009,  14.1383]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 12
	action: tensor([[  3.3538,  14.9006,  53.4995,  29.0808,  22.9400,  -4.2017, -46.8848]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 13
	action: tensor([[ 7.1307, 41.6343, 11.9159, 24.1884, 12.9433, 23.3925, -5.9317]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 14
	action: tensor([[ 17.9363, -10.2128, -14.5075,  33.7720, -32.6268,  27.4613,  -9.9326]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 15
	action: tensor([[ 23.7274, -14.8596,  15.8252, -17.8386,  -8.8470,  37.5108,   1.1827]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 16
	action: tensor([[ 16.2351,  -5.4278,   1.6807,  27.5038, -26.4717,  42.5948,  20.1950]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 17
	action: tensor([[-17.1104,   0.2145,   5.4248,  44.4100,  -6.7225,  50.6525, -29.6077]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 18
	action: tensor([[-25.7060, -15.3491,  24.4188,   3.7700,  31.9444, -16.9466, -42.6609]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 19
	action: tensor([[ -1.3469, -17.9526,  11.5944, -17.7844,  12.0317, -16.9415,  16.0045]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 20
	action: tensor([[-31.0879, -60.6478, -62.0381, -38.5573,  19.7977, -12.4944, -37.3722]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8457064732916271 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 21
	action: tensor([[-44.9737,  51.3769,   8.3818,  18.1983, -28.1231,  13.8211, -36.2871]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.15379155200442107, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 22
	action: tensor([[ -3.8591,  10.8983,  49.3823,  72.9731,  -1.9111,  51.1028, -11.9461]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0883, 4.8626, 4.8237, 4.9542, 4.9770, 5.1721, 5.0102]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 23
	action: tensor([[-51.9207,  14.2038,  24.6385,  15.7914,   6.0847, -19.1621,  -2.0016]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 24
	action: tensor([[ -2.0704, -46.1857,  18.9933, -14.1079, -40.5382,  -9.4706, -72.6809]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 25
	action: tensor([[ 57.6244,  55.5091, -21.2631,   0.2052,  -8.5531, -80.4694, -41.6580]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 26
	action: tensor([[ 52.4068,  22.4614,  12.0342,  59.9907,  30.6831, -18.8965, -10.7324]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 27
	action: tensor([[ 36.8095,   1.3284, -24.7659,  57.9366,  -6.4808,  35.3960,  11.7924]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 28
	action: tensor([[ 11.2517, -10.5692,  37.2609,  72.9129,  15.5315,  65.2606,  31.9897]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 29
	action: tensor([[ -5.2337,  25.4296,  63.7300, -17.3918,  -6.6539,  61.5207, -70.4592]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 30
	action: tensor([[-30.0324,  14.5608,  57.5338, -21.0663, -52.8641,  40.9484, -65.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 31
	action: tensor([[-43.3856,  24.2035,  18.6027,  -6.5560, -70.7782,   2.5348,  11.2403]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 32
	action: tensor([[ -6.3771, -22.0623,  21.1049, -51.1066,  22.5290, -21.3848,  43.5363]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 33
	action: tensor([[-34.8390,  -6.6787,  -0.4567,  60.3158, -43.4739, -13.1862,  29.3681]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 34
	action: tensor([[ 77.2119,  -7.0080,  42.4849,  10.0391,  -8.3994, -40.3061,  38.2222]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 35
	action: tensor([[-89.8866,  27.0128,  16.0724,  17.9290,  -5.3406, -53.8410,  -5.6616]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 36
	action: tensor([[-57.1407,  72.1299,  23.9801,  -2.4298,   8.3217, -16.3916,  -9.1676]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 37
	action: tensor([[-14.1552, -29.6447,   9.4820,  34.3395, -22.9919,  29.5404, -24.5617]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 38
	action: tensor([[-16.1291,  22.2521,  36.9579, -33.9071,  10.8730,  23.0263, -32.2845]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 39
	action: tensor([[-24.1285,  38.1336,  26.5842,  -9.4042, -12.1203, -22.1701, -41.7934]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 40
	action: tensor([[ 39.2314, -25.3164,  51.0280,   5.2123, -17.0828, -61.1676,  -5.4469]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 41
	action: tensor([[-1.9067e+01, -2.1790e+01,  3.3351e-02,  3.2868e+01,  4.0498e-01,
          8.8907e+01,  1.2736e+01]], dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2291938272272294 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 42
	action: tensor([[ 49.7495,   6.1852,  78.4351,  -6.3727,  42.9977,  80.6563, -33.9716]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.38472333653920976, distance: 1.3465987250752711 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 43
	action: tensor([[ 22.6308,   6.6577,  12.9814,  -5.7192, -16.4535, -86.0033,  18.1695]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[4.7960, 4.5698, 4.5243, 4.6627, 4.6809, 4.8735, 4.7141]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 44
	action: tensor([[  7.0651,  15.2097,  11.9460,  44.0301,  34.3758, -31.4247,  57.5285]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 45
	action: tensor([[ 10.7590,  -3.4586,  32.0107, -62.1715, -47.6881, -30.2562,  15.3823]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 46
	action: tensor([[-64.1077, -20.9034,   5.4322,  30.7810, -13.9157, -22.4748,  10.7613]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 47
	action: tensor([[-52.9262,  -2.8420,  14.2249,  36.5802, -56.2202,  69.3061,  -2.5577]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 48
	action: tensor([[-22.1655, -16.0768,  40.5204, -38.4956,  29.5076, -23.9695, -31.5744]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 24, step: 49
	action: tensor([[-12.2153,  28.8592,   1.2090,  13.9232,  -0.4133, -14.5575,  37.3295]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.0203, 4.7963, 4.7511, 4.8798, 4.9096, 5.1109, 4.9496]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 24 actor 1152.5364583077621 critic 2390.8702892477518
epoch: 25, step: 0
	action: tensor([[  5.1895, -56.7615,  46.2504,  16.1137, -39.0449,  27.2234, -33.6130]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 1
	action: tensor([[-23.3797,  14.3244,   8.9593,   8.9755,  36.5086, -13.3986, -36.0429]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 2
	action: tensor([[-40.5106,  85.7966,  55.0696,  37.1838,  16.0551,  41.7589, -84.4560]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 3
	action: tensor([[-57.2080,   2.1352,  20.5098,  34.0957,  -8.3603,  37.3317,   8.5247]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 4
	action: tensor([[ 33.0123,  11.3236,  50.4903,  -9.8288,  -6.5634,  60.0729, -21.1895]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 5
	action: tensor([[ -17.3403,   -7.6034,  -28.3546,   22.9586, -110.8669,  -11.1406,
           59.4612]], dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 6
	action: tensor([[-30.1748,   1.0042,  23.9473,  78.2206,  11.7770, -42.3677, -22.8285]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 7
	action: tensor([[ 88.0275,   5.0187,  12.2544,  11.7197,   4.8275,  35.8746, -57.6302]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 8
	action: tensor([[-16.5411, -58.7995,  19.3214,   7.5484, -74.5600, 100.1433, -48.4689]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 9
	action: tensor([[ 14.2521, -20.2741,  -5.1828,   1.2149,  -6.5985,  38.5239,  -9.9524]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 10
	action: tensor([[-17.5757,  -2.2794, -68.7099,   4.5459,  12.3389,  36.9280,  82.7611]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 11
	action: tensor([[ 22.1428,   8.1433, -20.4539,  28.8480,   1.0899, -48.8063,  45.5711]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 12
	action: tensor([[  35.7838,   -3.6698,   13.4837,  -34.2907, -110.7413,   33.5712,
           54.0231]], dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 13
	action: tensor([[-21.2097, -24.9718,  44.5967,  47.8410,  10.5515,  23.2717,  38.4105]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 14
	action: tensor([[ -1.2839, -35.8017,  13.0415,  -9.7287,  17.6332,  13.1134, -12.6993]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 15
	action: tensor([[ 18.1023,  -3.7609,  50.3594,  21.7834, -53.4111,  68.4342,  -6.0421]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 16
	action: tensor([[ 68.6216,  26.5755,  -8.8343,  28.2764, -20.5411,  18.5003,   0.9534]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 17
	action: tensor([[ 16.1146,   7.4123,  -0.8486, -11.1976, -28.5332,  51.8274,  19.4360]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 18
	action: tensor([[ 13.2037,   3.8026,  35.9075,  26.8800, -16.5681,  17.4409, -43.1633]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 19
	action: tensor([[ 20.2657,  15.0807,  11.0489,  23.0703, -11.5327, -14.8697,  -3.9700]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 20
	action: tensor([[-34.9713,  15.7921,   2.9638,  22.7007,  -4.4619,  35.3855,  88.3031]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 21
	action: tensor([[14.9681, -8.3626, 43.4166, 60.2762,  5.6365, -8.3737, -8.9650]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 22
	action: tensor([[-29.9190,   1.5941,  53.9531,  39.6511,  -2.2213, 112.0622,   9.8723]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 23
	action: tensor([[ 32.7062,  34.2263,  -2.0038,  26.2667,  18.4971,  55.4235, -43.8189]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 24
	action: tensor([[-88.5011, -15.4587,   5.5021,  -5.4811, -38.1747, -38.0832, -47.4694]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 25
	action: tensor([[-46.8129, -19.7793, -12.1944,  38.7938, -18.9127,  16.4083, -49.4735]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 26
	action: tensor([[65.4959, 10.5199, 41.4942, 68.2410, -2.4652, 47.0774,  7.2717]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 27
	action: tensor([[-70.9510,  17.0621,  46.2954,  12.1430,  43.6717,  -1.3043,  -1.0546]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 28
	action: tensor([[-16.3289, -49.1074,  24.0135, -24.6452, -42.5423,  32.9289, -12.6594]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 29
	action: tensor([[ 13.9081,  22.1106, -60.1726,  94.2607,   7.5704, -82.9927, -32.4789]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 30
	action: tensor([[ 39.1664, -17.2228, -10.3119,  30.8822, -34.0452,  33.5225, -39.7148]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 25, step: 31
	action: tensor([[-36.7276,   7.2020, -44.8143,  -5.3567, -34.7991,  10.8433,   5.7496]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3465987250752711 entropy tensor([[5.1352, 4.8996, 4.8308, 4.9749, 5.0163, 5.2037, 5.0709]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
